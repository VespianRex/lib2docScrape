{"content": {"suites": {"0": {"status": {"total_pass": 11, "total_skip": 1, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 2, "total_error": 0}, "tests": {"0": {"status": "PASS", "message": "", "test_name": "test_crawl_with_urlinfo", "rerun": "0"}, "1": {"status": "SKIP", "message": "('/home/alex/DEV/lib2docScrape/tests/test_crawl4ai_extended.py', 123, 'Skipped: Link following is handled by the main Crawler, not the backend directly.')\n", "test_name": "test_crawl_depth_first", "rerun": "0"}, "2": {"status": "PASS", "message": "", "test_name": "test_rate_limiting_precision", "rerun": "0"}, "3": {"status": "PASS", "message": "", "test_name": "test_concurrent_request_limit", "rerun": "0"}, "4": {"status": "FAIL", "message": "       AssertionError: assert 'https://example.com/' == 'https://example.com'\n         \n         - https://example.com\n         + https://example.com/\n         ?                    +\n", "test_name": "test_url_normalization", "rerun": "0"}, "5": {"status": "PASS", "message": "", "test_name": "test_error_propagation", "rerun": "0"}, "6": {"status": "PASS", "message": "", "test_name": "test_retry_behavior", "rerun": "0"}, "7": {"status": "PASS", "message": "", "test_name": "test_metrics_accuracy", "rerun": "0"}, "8": {"status": "PASS", "message": "", "test_name": "test_resource_cleanup", "rerun": "0"}, "9": {"status": "PASS", "message": "", "test_name": "test_url_info_initialization[https://docs.python.org/3/-expected0]", "rerun": "0"}, "10": {"status": "PASS", "message": "", "test_name": "test_url_info_initialization[https://docs.python.org-expected1]", "rerun": "0"}, "11": {"status": "FAIL", "message": "   AssertionError: assert '' == 'invalid-url'\n     \n     - invalid-url\n", "test_name": "test_url_info_initialization[invalid-url-expected2]", "rerun": "0"}, "12": {"status": "PASS", "message": "", "test_name": "test_url_info_hashable", "rerun": "0"}, "13": {"status": "PASS", "message": "", "test_name": "test_url_info_immutable", "rerun": "0"}}, "suite_name": "tests/test_crawl4ai_extended.py"}, "1": {"suite_name": "tests/test_crawler_advanced.py", "tests": {"0": {"status": "PASS", "message": "", "test_name": "test_crawler_basic_crawl", "rerun": "0"}, "1": {"status": "PASS", "message": "", "test_name": "test_crawler_failed_crawl", "rerun": "0"}, "2": {"status": "PASS", "message": "", "test_name": "test_crawler_retry_mechanism", "rerun": "0"}, "3": {"status": "SKIP", "message": "('/home/alex/DEV/lib2docScrape/tests/test_crawler_advanced.py', 157, 'Skipped: Rate limiter timing is difficult to test reliably with mocks')\n", "test_name": "test_crawler_rate_limiting", "rerun": "0"}, "4": {"status": "PASS", "message": "", "test_name": "test_crawler_content_processing", "rerun": "0"}, "5": {"status": "PASS", "message": "", "test_name": "test_crawler_quality_checking", "rerun": "0"}, "6": {"status": "PASS", "message": "", "test_name": "test_crawler_resource_cleanup", "rerun": "0"}, "7": {"status": "PASS", "message": "", "test_name": "test_crawler_concurrent_requests", "rerun": "0"}, "8": {"status": "FAIL", "message": "           AssertionError: Expected documents for http://example.com/./path/../page, but got none.\n           assert []\n            +  where [] = CrawlResult(target=CrawlTarget(url='http://example.com/./path/../page', depth=1, follow_external=False, content_types=..., documents=[], issues=[], metrics={}, structure=None, processed_url=None, failed_urls=[], errors={}, crawled_pages={}).documents\n", "test_name": "test_crawler_url_normalization", "rerun": "0"}, "9": {"status": "PASS", "message": "", "test_name": "test_crawler_error_handling", "rerun": "0"}}, "status": {"total_pass": 8, "total_skip": 1, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 1, "total_error": 0}}}}, "date": "April 11, 2025", "start_time": 1744363315.1780596, "total_suite": 2, "status": "FAIL", "status_list": {"pass": "19", "fail": "3", "skip": "2", "error": "0", "xpass": "0", "xfail": "0", "rerun": "0"}, "total_tests": "24"}