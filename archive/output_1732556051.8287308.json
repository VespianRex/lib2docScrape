{"content": {"suites": {"0": {"status": {"total_pass": 8, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 1, "total_error": 0}, "tests": {"0": {"status": "PASS", "message": "", "test_name": "test_crawler_backend_lifecycle", "rerun": "0"}, "1": {"status": "PASS", "message": "", "test_name": "test_crawler_backend_error_handling", "rerun": "0"}, "2": {"status": "PASS", "message": "", "test_name": "test_backend_selector_registration", "rerun": "0"}, "3": {"status": "PASS", "message": "", "test_name": "test_backend_selector_selection", "rerun": "0"}, "4": {"status": "PASS", "message": "", "test_name": "test_url_normalization", "rerun": "0"}, "5": {"status": "PASS", "message": "", "test_name": "test_processed_content_validation", "rerun": "0"}, "6": {"status": "PASS", "message": "", "test_name": "test_concurrent_backend_operations", "rerun": "0"}, "7": {"status": "PASS", "message": "", "test_name": "test_backend_metrics_update", "rerun": "0"}, "8": {"status": "FAIL", "message": "   Failed: DID NOT RAISE <class 'ValueError'>\n", "test_name": "test_backend_selector_error_handling", "rerun": "0"}}, "suite_name": "tests/test_base.py"}, "1": {"suite_name": "tests/test_crawler.py", "tests": {"0": {"status": "ERROR", "message": "backend_selector = <src.backends.selector.BackendSelector object at 0x13259f190>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x13259f0d0>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x13259eed0>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x132586750>\n\n    @pytest.fixture\n    def crawler(\n        backend_selector: BackendSelector,\n        content_processor: ContentProcessor,\n        quality_checker: QualityChecker,\n        document_organizer: DocumentOrganizer\n    ) -> DocumentationCrawler:\n        \"\"\"Configured documentation crawler.\"\"\"\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=backend_selector,\n            content_processor=content_processor,\n            quality_checker=quality_checker,\n            document_organizer=document_organizer\n        )\n\ntests/conftest.py:233: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x132587890>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x13259f190>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x13259f0d0>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x13259eed0>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x132586750>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_crawler_initialization", "rerun": "0"}, "1": {"status": "ERROR", "message": "backend_selector = <src.backends.selector.BackendSelector object at 0x13259e4d0>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x13259f490>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x13259f4d0>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x13259f1d0>\n\n    @pytest.fixture\n    def crawler(\n        backend_selector: BackendSelector,\n        content_processor: ContentProcessor,\n        quality_checker: QualityChecker,\n        document_organizer: DocumentOrganizer\n    ) -> DocumentationCrawler:\n        \"\"\"Configured documentation crawler.\"\"\"\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=backend_selector,\n            content_processor=content_processor,\n            quality_checker=quality_checker,\n            document_organizer=document_organizer\n        )\n\ntests/conftest.py:233: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x13054e150>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x13259e4d0>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x13259f490>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x13259f4d0>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x13259f1d0>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_url_filtering", "rerun": "0"}, "2": {"status": "ERROR", "message": "backend_selector = <src.backends.selector.BackendSelector object at 0x132664250>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x132664990>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x132664710>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x132665b50>\n\n    @pytest.fixture\n    def crawler(\n        backend_selector: BackendSelector,\n        content_processor: ContentProcessor,\n        quality_checker: QualityChecker,\n        document_organizer: DocumentOrganizer\n    ) -> DocumentationCrawler:\n        \"\"\"Configured documentation crawler.\"\"\"\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=backend_selector,\n            content_processor=content_processor,\n            quality_checker=quality_checker,\n            document_organizer=document_organizer\n        )\n\ntests/conftest.py:233: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x1326653d0>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x132664250>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x132664990>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x132664710>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x132665b50>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_single_url_processing", "rerun": "0"}, "3": {"status": "ERROR", "message": "backend_selector = <src.backends.selector.BackendSelector object at 0x1325dcdd0>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x1325def90>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x1325def10>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x1325df350>\n\n    @pytest.fixture\n    def crawler(\n        backend_selector: BackendSelector,\n        content_processor: ContentProcessor,\n        quality_checker: QualityChecker,\n        document_organizer: DocumentOrganizer\n    ) -> DocumentationCrawler:\n        \"\"\"Configured documentation crawler.\"\"\"\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=backend_selector,\n            content_processor=content_processor,\n            quality_checker=quality_checker,\n            document_organizer=document_organizer\n        )\n\ntests/conftest.py:233: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x1325df150>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x1325dcdd0>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x1325def90>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x1325def10>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x1325df350>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_depth_limited_crawling", "rerun": "0"}, "4": {"status": "ERROR", "message": "backend_selector = <src.backends.selector.BackendSelector object at 0x13259d2d0>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x13259d810>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x132585090>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x132576fd0>\n\n    @pytest.fixture\n    def crawler(\n        backend_selector: BackendSelector,\n        content_processor: ContentProcessor,\n        quality_checker: QualityChecker,\n        document_organizer: DocumentOrganizer\n    ) -> DocumentationCrawler:\n        \"\"\"Configured documentation crawler.\"\"\"\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=backend_selector,\n            content_processor=content_processor,\n            quality_checker=quality_checker,\n            document_organizer=document_organizer\n        )\n\ntests/conftest.py:233: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x132576f90>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x13259d2d0>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x13259d810>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x132585090>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x132576fd0>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_concurrent_processing", "rerun": "0"}, "5": {"status": "ERROR", "message": "backend_selector = <src.backends.selector.BackendSelector object at 0x1325abb90>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x1325aa2d0>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x1325aac50>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x1325a9ed0>\n\n    @pytest.fixture\n    def crawler(\n        backend_selector: BackendSelector,\n        content_processor: ContentProcessor,\n        quality_checker: QualityChecker,\n        document_organizer: DocumentOrganizer\n    ) -> DocumentationCrawler:\n        \"\"\"Configured documentation crawler.\"\"\"\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=backend_selector,\n            content_processor=content_processor,\n            quality_checker=quality_checker,\n            document_organizer=document_organizer\n        )\n\ntests/conftest.py:233: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x1325abdd0>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x1325abb90>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x1325aa2d0>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x1325aac50>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x1325a9ed0>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_rate_limiting", "rerun": "0"}, "6": {"status": "ERROR", "message": "backend_selector = <src.backends.selector.BackendSelector object at 0x13267e6d0>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x13267e090>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x13267ef50>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x13267e350>\n\n    @pytest.fixture\n    def crawler(\n        backend_selector: BackendSelector,\n        content_processor: ContentProcessor,\n        quality_checker: QualityChecker,\n        document_organizer: DocumentOrganizer\n    ) -> DocumentationCrawler:\n        \"\"\"Configured documentation crawler.\"\"\"\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=backend_selector,\n            content_processor=content_processor,\n            quality_checker=quality_checker,\n            document_organizer=document_organizer\n        )\n\ntests/conftest.py:233: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x13267e2d0>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x13267e6d0>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x13267e090>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x13267ef50>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x13267e350>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_error_handling", "rerun": "0"}, "7": {"status": "ERROR", "message": "backend_selector = <src.backends.selector.BackendSelector object at 0x132611350>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x132613410>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x132611f90>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x132613090>\n\n    @pytest.fixture\n    def crawler(\n        backend_selector: BackendSelector,\n        content_processor: ContentProcessor,\n        quality_checker: QualityChecker,\n        document_organizer: DocumentOrganizer\n    ) -> DocumentationCrawler:\n        \"\"\"Configured documentation crawler.\"\"\"\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=backend_selector,\n            content_processor=content_processor,\n            quality_checker=quality_checker,\n            document_organizer=document_organizer\n        )\n\ntests/conftest.py:233: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x132610c50>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x132611350>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x132613410>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x132611f90>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x132613090>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_content_processing_pipeline", "rerun": "0"}, "8": {"status": "ERROR", "message": "backend_selector = <src.backends.selector.BackendSelector object at 0x13268e490>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x13268c090>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x13268d350>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x13268d450>\n\n    @pytest.fixture\n    def crawler(\n        backend_selector: BackendSelector,\n        content_processor: ContentProcessor,\n        quality_checker: QualityChecker,\n        document_organizer: DocumentOrganizer\n    ) -> DocumentationCrawler:\n        \"\"\"Configured documentation crawler.\"\"\"\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=backend_selector,\n            content_processor=content_processor,\n            quality_checker=quality_checker,\n            document_organizer=document_organizer\n        )\n\ntests/conftest.py:233: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x13268c690>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x13268e490>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x13268c090>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x13268d350>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x13268d450>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_cleanup", "rerun": "0"}, "9": {"status": "ERROR", "message": "backend_selector = <src.backends.selector.BackendSelector object at 0x132613410>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x132613690>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x1326139d0>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x132613bd0>\n\n    @pytest.fixture\n    def crawler(\n        backend_selector: BackendSelector,\n        content_processor: ContentProcessor,\n        quality_checker: QualityChecker,\n        document_organizer: DocumentOrganizer\n    ) -> DocumentationCrawler:\n        \"\"\"Configured documentation crawler.\"\"\"\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=backend_selector,\n            content_processor=content_processor,\n            quality_checker=quality_checker,\n            document_organizer=document_organizer\n        )\n\ntests/conftest.py:233: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x1326110d0>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x132613410>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x132613690>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x1326139d0>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x132613bd0>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_max_pages_limit", "rerun": "0"}, "10": {"status": "ERROR", "message": "backend_selector = <src.backends.selector.BackendSelector object at 0x132624850>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x132624210>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x132625090>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x132625010>\n\n    @pytest.fixture\n    def crawler(\n        backend_selector: BackendSelector,\n        content_processor: ContentProcessor,\n        quality_checker: QualityChecker,\n        document_organizer: DocumentOrganizer\n    ) -> DocumentationCrawler:\n        \"\"\"Configured documentation crawler.\"\"\"\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=backend_selector,\n            content_processor=content_processor,\n            quality_checker=quality_checker,\n            document_organizer=document_organizer\n        )\n\ntests/conftest.py:233: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x132625c10>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x132624850>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x132624210>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x132625090>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x132625010>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_statistics_tracking", "rerun": "0"}}, "status": {"total_pass": 0, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 0, "total_error": 11}}, "2": {"suite_name": "tests/test_integration.py", "tests": {"0": {"status": "ERROR", "message": "test_html_dir = '/private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-12/test_full_site_crawl0/test_docs'\n\n    @pytest.fixture\n    def integrated_crawler(test_html_dir: str) -> DocumentationCrawler:\n        \"\"\"Create fully integrated crawler instance.\"\"\"\n        # Configure backend\n        backend = Crawl4AIBackend(\n            config=Crawl4AIConfig(\n                max_retries=2,\n                timeout=5.0,\n                headers={\"User-Agent\": \"TestCrawler/1.0\"}\n            )\n        )\n    \n        # Configure backend selector\n        selector = BackendSelector()\n        selector.register_backend(\n            backend,\n            BackendCriteria(\n                priority=100,\n                content_types=[\"text/html\"],\n                url_patterns=[\"*\"],\n                max_load=0.8,\n                min_success_rate=0.7\n            )\n        )\n    \n        # Configure content processor\n        processor = ContentProcessor(\n            config=ProcessingConfig(\n                allowed_tags=[\n                    \"h1\", \"h2\", \"h3\", \"p\", \"pre\", \"code\",\n                    \"a\", \"ul\", \"li\", \"nav\"\n                ],\n                preserve_whitespace_elements=[\"pre\", \"code\"],\n                code_languages=[\"python\"],\n                max_heading_level=3,\n                max_content_length=10000,\n                min_content_length=10\n            )\n        )\n    \n        # Configure quality checker\n        checker = QualityChecker(\n            config=QualityConfig(\n                min_content_length=50,\n                max_content_length=10000,\n                min_headings=1,\n                max_heading_depth=3,\n                min_internal_links=1,\n                max_broken_links_ratio=0.1,\n                required_metadata_fields={\"title\", \"description\"}\n            )\n        )\n    \n        # Configure document organizer\n        organizer = DocumentOrganizer(\n            config=OrganizationConfig(\n                min_similarity_score=0.3,\n                max_versions_to_keep=3,\n                category_rules={\n                    \"api\": [\"api\", \"reference\", \"endpoint\"],\n                    \"guide\": [\"guide\", \"tutorial\", \"getting started\"],\n                    \"example\": [\"example\", \"sample\", \"demo\"]\n                }\n            )\n        )\n    \n        # Create and return crawler\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=selector,\n            content_processor=processor,\n            quality_checker=checker,\n            document_organizer=organizer\n        )\n\ntests/test_integration.py:175: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x1326a2c90>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x13268fe50>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x13268d250>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x13268e950>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x13268ef10>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_full_site_crawl", "rerun": "0"}, "1": {"status": "ERROR", "message": "test_html_dir = '/private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-12/test_content_processing_pipeli0/test_docs'\n\n    @pytest.fixture\n    def integrated_crawler(test_html_dir: str) -> DocumentationCrawler:\n        \"\"\"Create fully integrated crawler instance.\"\"\"\n        # Configure backend\n        backend = Crawl4AIBackend(\n            config=Crawl4AIConfig(\n                max_retries=2,\n                timeout=5.0,\n                headers={\"User-Agent\": \"TestCrawler/1.0\"}\n            )\n        )\n    \n        # Configure backend selector\n        selector = BackendSelector()\n        selector.register_backend(\n            backend,\n            BackendCriteria(\n                priority=100,\n                content_types=[\"text/html\"],\n                url_patterns=[\"*\"],\n                max_load=0.8,\n                min_success_rate=0.7\n            )\n        )\n    \n        # Configure content processor\n        processor = ContentProcessor(\n            config=ProcessingConfig(\n                allowed_tags=[\n                    \"h1\", \"h2\", \"h3\", \"p\", \"pre\", \"code\",\n                    \"a\", \"ul\", \"li\", \"nav\"\n                ],\n                preserve_whitespace_elements=[\"pre\", \"code\"],\n                code_languages=[\"python\"],\n                max_heading_level=3,\n                max_content_length=10000,\n                min_content_length=10\n            )\n        )\n    \n        # Configure quality checker\n        checker = QualityChecker(\n            config=QualityConfig(\n                min_content_length=50,\n                max_content_length=10000,\n                min_headings=1,\n                max_heading_depth=3,\n                min_internal_links=1,\n                max_broken_links_ratio=0.1,\n                required_metadata_fields={\"title\", \"description\"}\n            )\n        )\n    \n        # Configure document organizer\n        organizer = DocumentOrganizer(\n            config=OrganizationConfig(\n                min_similarity_score=0.3,\n                max_versions_to_keep=3,\n                category_rules={\n                    \"api\": [\"api\", \"reference\", \"endpoint\"],\n                    \"guide\": [\"guide\", \"tutorial\", \"getting started\"],\n                    \"example\": [\"example\", \"sample\", \"demo\"]\n                }\n            )\n        )\n    \n        # Create and return crawler\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=selector,\n            content_processor=processor,\n            quality_checker=checker,\n            document_organizer=organizer\n        )\n\ntests/test_integration.py:175: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x1326806d0>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x132680250>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x1326815d0>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x132683b90>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x132681f90>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_content_processing_pipeline", "rerun": "0"}, "2": {"status": "ERROR", "message": "test_html_dir = '/private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-12/test_quality_checks0/test_docs'\n\n    @pytest.fixture\n    def integrated_crawler(test_html_dir: str) -> DocumentationCrawler:\n        \"\"\"Create fully integrated crawler instance.\"\"\"\n        # Configure backend\n        backend = Crawl4AIBackend(\n            config=Crawl4AIConfig(\n                max_retries=2,\n                timeout=5.0,\n                headers={\"User-Agent\": \"TestCrawler/1.0\"}\n            )\n        )\n    \n        # Configure backend selector\n        selector = BackendSelector()\n        selector.register_backend(\n            backend,\n            BackendCriteria(\n                priority=100,\n                content_types=[\"text/html\"],\n                url_patterns=[\"*\"],\n                max_load=0.8,\n                min_success_rate=0.7\n            )\n        )\n    \n        # Configure content processor\n        processor = ContentProcessor(\n            config=ProcessingConfig(\n                allowed_tags=[\n                    \"h1\", \"h2\", \"h3\", \"p\", \"pre\", \"code\",\n                    \"a\", \"ul\", \"li\", \"nav\"\n                ],\n                preserve_whitespace_elements=[\"pre\", \"code\"],\n                code_languages=[\"python\"],\n                max_heading_level=3,\n                max_content_length=10000,\n                min_content_length=10\n            )\n        )\n    \n        # Configure quality checker\n        checker = QualityChecker(\n            config=QualityConfig(\n                min_content_length=50,\n                max_content_length=10000,\n                min_headings=1,\n                max_heading_depth=3,\n                min_internal_links=1,\n                max_broken_links_ratio=0.1,\n                required_metadata_fields={\"title\", \"description\"}\n            )\n        )\n    \n        # Configure document organizer\n        organizer = DocumentOrganizer(\n            config=OrganizationConfig(\n                min_similarity_score=0.3,\n                max_versions_to_keep=3,\n                category_rules={\n                    \"api\": [\"api\", \"reference\", \"endpoint\"],\n                    \"guide\": [\"guide\", \"tutorial\", \"getting started\"],\n                    \"example\": [\"example\", \"sample\", \"demo\"]\n                }\n            )\n        )\n    \n        # Create and return crawler\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=selector,\n            content_processor=processor,\n            quality_checker=checker,\n            document_organizer=organizer\n        )\n\ntests/test_integration.py:175: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x132472750>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x1324717d0>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x132471510>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x1324738d0>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x1324703d0>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_quality_checks", "rerun": "0"}, "3": {"status": "ERROR", "message": "test_html_dir = '/private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-12/test_document_organization0/test_docs'\n\n    @pytest.fixture\n    def integrated_crawler(test_html_dir: str) -> DocumentationCrawler:\n        \"\"\"Create fully integrated crawler instance.\"\"\"\n        # Configure backend\n        backend = Crawl4AIBackend(\n            config=Crawl4AIConfig(\n                max_retries=2,\n                timeout=5.0,\n                headers={\"User-Agent\": \"TestCrawler/1.0\"}\n            )\n        )\n    \n        # Configure backend selector\n        selector = BackendSelector()\n        selector.register_backend(\n            backend,\n            BackendCriteria(\n                priority=100,\n                content_types=[\"text/html\"],\n                url_patterns=[\"*\"],\n                max_load=0.8,\n                min_success_rate=0.7\n            )\n        )\n    \n        # Configure content processor\n        processor = ContentProcessor(\n            config=ProcessingConfig(\n                allowed_tags=[\n                    \"h1\", \"h2\", \"h3\", \"p\", \"pre\", \"code\",\n                    \"a\", \"ul\", \"li\", \"nav\"\n                ],\n                preserve_whitespace_elements=[\"pre\", \"code\"],\n                code_languages=[\"python\"],\n                max_heading_level=3,\n                max_content_length=10000,\n                min_content_length=10\n            )\n        )\n    \n        # Configure quality checker\n        checker = QualityChecker(\n            config=QualityConfig(\n                min_content_length=50,\n                max_content_length=10000,\n                min_headings=1,\n                max_heading_depth=3,\n                min_internal_links=1,\n                max_broken_links_ratio=0.1,\n                required_metadata_fields={\"title\", \"description\"}\n            )\n        )\n    \n        # Configure document organizer\n        organizer = DocumentOrganizer(\n            config=OrganizationConfig(\n                min_similarity_score=0.3,\n                max_versions_to_keep=3,\n                category_rules={\n                    \"api\": [\"api\", \"reference\", \"endpoint\"],\n                    \"guide\": [\"guide\", \"tutorial\", \"getting started\"],\n                    \"example\": [\"example\", \"sample\", \"demo\"]\n                }\n            )\n        )\n    \n        # Create and return crawler\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=selector,\n            content_processor=processor,\n            quality_checker=checker,\n            document_organizer=organizer\n        )\n\ntests/test_integration.py:175: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x132681710>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x132682b50>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x132682450>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x132682550>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x132682ad0>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_document_organization", "rerun": "0"}, "4": {"status": "ERROR", "message": "test_html_dir = '/private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-12/test_search_functionality0/test_docs'\n\n    @pytest.fixture\n    def integrated_crawler(test_html_dir: str) -> DocumentationCrawler:\n        \"\"\"Create fully integrated crawler instance.\"\"\"\n        # Configure backend\n        backend = Crawl4AIBackend(\n            config=Crawl4AIConfig(\n                max_retries=2,\n                timeout=5.0,\n                headers={\"User-Agent\": \"TestCrawler/1.0\"}\n            )\n        )\n    \n        # Configure backend selector\n        selector = BackendSelector()\n        selector.register_backend(\n            backend,\n            BackendCriteria(\n                priority=100,\n                content_types=[\"text/html\"],\n                url_patterns=[\"*\"],\n                max_load=0.8,\n                min_success_rate=0.7\n            )\n        )\n    \n        # Configure content processor\n        processor = ContentProcessor(\n            config=ProcessingConfig(\n                allowed_tags=[\n                    \"h1\", \"h2\", \"h3\", \"p\", \"pre\", \"code\",\n                    \"a\", \"ul\", \"li\", \"nav\"\n                ],\n                preserve_whitespace_elements=[\"pre\", \"code\"],\n                code_languages=[\"python\"],\n                max_heading_level=3,\n                max_content_length=10000,\n                min_content_length=10\n            )\n        )\n    \n        # Configure quality checker\n        checker = QualityChecker(\n            config=QualityConfig(\n                min_content_length=50,\n                max_content_length=10000,\n                min_headings=1,\n                max_heading_depth=3,\n                min_internal_links=1,\n                max_broken_links_ratio=0.1,\n                required_metadata_fields={\"title\", \"description\"}\n            )\n        )\n    \n        # Configure document organizer\n        organizer = DocumentOrganizer(\n            config=OrganizationConfig(\n                min_similarity_score=0.3,\n                max_versions_to_keep=3,\n                category_rules={\n                    \"api\": [\"api\", \"reference\", \"endpoint\"],\n                    \"guide\": [\"guide\", \"tutorial\", \"getting started\"],\n                    \"example\": [\"example\", \"sample\", \"demo\"]\n                }\n            )\n        )\n    \n        # Create and return crawler\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=selector,\n            content_processor=processor,\n            quality_checker=checker,\n            document_organizer=organizer\n        )\n\ntests/test_integration.py:175: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x1326ade10>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x1326ac1d0>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x1326ada50>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x1326ade50>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x1326addd0>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_search_functionality", "rerun": "0"}, "5": {"status": "ERROR", "message": "test_html_dir = '/private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-12/test_error_handling_and_recove0/test_docs'\n\n    @pytest.fixture\n    def integrated_crawler(test_html_dir: str) -> DocumentationCrawler:\n        \"\"\"Create fully integrated crawler instance.\"\"\"\n        # Configure backend\n        backend = Crawl4AIBackend(\n            config=Crawl4AIConfig(\n                max_retries=2,\n                timeout=5.0,\n                headers={\"User-Agent\": \"TestCrawler/1.0\"}\n            )\n        )\n    \n        # Configure backend selector\n        selector = BackendSelector()\n        selector.register_backend(\n            backend,\n            BackendCriteria(\n                priority=100,\n                content_types=[\"text/html\"],\n                url_patterns=[\"*\"],\n                max_load=0.8,\n                min_success_rate=0.7\n            )\n        )\n    \n        # Configure content processor\n        processor = ContentProcessor(\n            config=ProcessingConfig(\n                allowed_tags=[\n                    \"h1\", \"h2\", \"h3\", \"p\", \"pre\", \"code\",\n                    \"a\", \"ul\", \"li\", \"nav\"\n                ],\n                preserve_whitespace_elements=[\"pre\", \"code\"],\n                code_languages=[\"python\"],\n                max_heading_level=3,\n                max_content_length=10000,\n                min_content_length=10\n            )\n        )\n    \n        # Configure quality checker\n        checker = QualityChecker(\n            config=QualityConfig(\n                min_content_length=50,\n                max_content_length=10000,\n                min_headings=1,\n                max_heading_depth=3,\n                min_internal_links=1,\n                max_broken_links_ratio=0.1,\n                required_metadata_fields={\"title\", \"description\"}\n            )\n        )\n    \n        # Configure document organizer\n        organizer = DocumentOrganizer(\n            config=OrganizationConfig(\n                min_similarity_score=0.3,\n                max_versions_to_keep=3,\n                category_rules={\n                    \"api\": [\"api\", \"reference\", \"endpoint\"],\n                    \"guide\": [\"guide\", \"tutorial\", \"getting started\"],\n                    \"example\": [\"example\", \"sample\", \"demo\"]\n                }\n            )\n        )\n    \n        # Create and return crawler\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=selector,\n            content_processor=processor,\n            quality_checker=checker,\n            document_organizer=organizer\n        )\n\ntests/test_integration.py:175: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x1325abdd0>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x1325aa9d0>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x1325abf90>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x1325aa950>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x1325aa350>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_error_handling_and_recovery", "rerun": "0"}}, "status": {"total_pass": 0, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 0, "total_error": 6}}, "3": {"suite_name": "tests/test_organizer.py", "tests": {"0": {"status": "PASS", "message": "", "test_name": "test_document_organizer_initialization", "rerun": "0"}, "1": {"status": "FAIL", "message": "       AttributeError: 'DocumentOrganizer' object has no attribute '_determine_category'\n", "test_name": "test_document_version_management", "rerun": "0"}, "2": {"status": "FAIL", "message": "       AttributeError: 'DocumentOrganizer' object has no attribute '_determine_category'\n", "test_name": "test_document_categorization", "rerun": "0"}, "3": {"status": "FAIL", "message": "       AttributeError: 'DocumentOrganizer' object has no attribute '_determine_category'\n", "test_name": "test_reference_extraction", "rerun": "0"}, "4": {"status": "FAIL", "message": "       AttributeError: 'DocumentOrganizer' object has no attribute '_determine_category'\n", "test_name": "test_search_functionality", "rerun": "0"}, "5": {"status": "FAIL", "message": "       AttributeError: 'DocumentOrganizer' object has no attribute '_determine_category'\n", "test_name": "test_collection_management", "rerun": "0"}, "6": {"status": "FAIL", "message": "       AttributeError: 'DocumentOrganizer' object has no attribute '_determine_category'\n", "test_name": "test_document_similarity", "rerun": "0"}, "7": {"status": "FAIL", "message": "       AttributeError: 'DocumentOrganizer' object has no attribute '_determine_category'\n", "test_name": "test_version_tracking", "rerun": "0"}, "8": {"status": "FAIL", "message": "       AttributeError: 'DocumentOrganizer' object has no attribute '_determine_category'\n", "test_name": "test_search_index_generation", "rerun": "0"}}, "status": {"total_pass": 1, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 8, "total_error": 0}}, "4": {"suite_name": "tests/test_processor.py", "tests": {"0": {"status": "PASS", "message": "", "test_name": "test_content_processor_initialization", "rerun": "0"}, "1": {"status": "PASS", "message": "", "test_name": "test_processing_rule_validation", "rerun": "0"}, "2": {"status": "PASS", "message": "", "test_name": "test_clean_text_processing", "rerun": "0"}, "3": {"status": "PASS", "message": "", "test_name": "test_code_language_detection", "rerun": "0"}, "4": {"status": "PASS", "message": "", "test_name": "test_code_block_processing", "rerun": "0"}, "5": {"status": "PASS", "message": "", "test_name": "test_link_processing", "rerun": "0"}, "6": {"status": "PASS", "message": "", "test_name": "test_metadata_extraction", "rerun": "0"}, "7": {"status": "PASS", "message": "", "test_name": "test_asset_collection", "rerun": "0"}, "8": {"status": "FAIL", "message": "   assert 0 > 0\n    +  where 0 = len([])\n    +    where [] = <built-in method get of dict object at 0x132753b40>('links', [])\n    +      where <built-in method get of dict object at 0x132753b40> = {'code_blocks': [{'content': 'def hello(): print(\"Hello, World!\")', 'language': 'python'}], 'headings': [{'level': 1, ...t Main Title First paragraph with link. Section 1 Second paragraph. def hello(): print(\"Hello, World!\") Item 1 Item 2'}.get\n    +        where {'code_blocks': [{'content': 'def hello(): print(\"Hello, World!\")', 'language': 'python'}], 'headings': [{'level': 1, ...t Main Title First paragraph with link. Section 1 Second paragraph. def hello(): print(\"Hello, World!\") Item 1 Item 2'} = ProcessedContent(url='https://example.com/test', title='Sample Document', content={'text': 'Sample Document Main Title...locks': [{'language': 'python', 'content': 'def hello(): print(\"Hello, World!\")'}]}, metadata={}, assets={}, errors=[]).content\n", "test_name": "test_full_content_processing", "rerun": "0"}, "9": {"status": "FAIL", "message": "   AssertionError: assert not {'code_blocks': [], 'headings': [], 'text': 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx...xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'}\n    +  where {'code_blocks': [], 'headings': [], 'text': 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx...xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'} = ProcessedContent(url='https://example.com', title='Untitled Document', content={'text': 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxx...xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx', 'headings': [], 'code_blocks': []}, metadata={}, assets={}, errors=[]).content\n", "test_name": "test_content_size_limits", "rerun": "0"}, "10": {"status": "PASS", "message": "", "test_name": "test_malformed_html_handling", "rerun": "0"}, "11": {"status": "PASS", "message": "", "test_name": "test_special_content_handling", "rerun": "0"}, "12": {"status": "PASS", "message": "", "test_name": "test_content_structure_preservation", "rerun": "0"}}, "status": {"total_pass": 11, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 2, "total_error": 0}}, "5": {"suite_name": "tests/test_quality.py", "tests": {"0": {"status": "PASS", "message": "", "test_name": "test_quality_checker_initialization", "rerun": "0"}, "1": {"status": "PASS", "message": "", "test_name": "test_content_length_check", "rerun": "0"}, "2": {"status": "PASS", "message": "", "test_name": "test_heading_structure_check", "rerun": "0"}, "3": {"status": "PASS", "message": "", "test_name": "test_link_validation", "rerun": "0"}, "4": {"status": "PASS", "message": "", "test_name": "test_metadata_check", "rerun": "0"}, "5": {"status": "PASS", "message": "", "test_name": "test_code_block_validation", "rerun": "0"}, "6": {"status": "PASS", "message": "", "test_name": "test_quality_metrics_calculation", "rerun": "0"}, "7": {"status": "PASS", "message": "", "test_name": "test_full_quality_check", "rerun": "0"}, "8": {"status": "PASS", "message": "", "test_name": "test_quality_checker_cleanup", "rerun": "0"}}, "status": {"total_pass": 9, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 0, "total_error": 0}}}}, "date": "November 25, 2024", "start_time": 1732555975.961151, "total_suite": 6, "status": "FAIL", "status_list": {"pass": "29", "fail": "11", "skip": "0", "error": "17", "xpass": "0", "xfail": "0", "rerun": "0"}, "total_tests": "57"}