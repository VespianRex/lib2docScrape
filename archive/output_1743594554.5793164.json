{"content": {"suites": {"0": {"status": {"total_pass": 6, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 0, "total_error": 0}, "tests": {"0": {"status": "PASS", "message": "", "test_name": "test_crawler_backend_lifecycle", "rerun": "0"}, "1": {"status": "PASS", "message": "", "test_name": "test_crawler_backend_error_handling", "rerun": "0"}, "2": {"status": "PASS", "message": "", "test_name": "test_backend_selector_registration", "rerun": "0"}, "3": {"status": "PASS", "message": "", "test_name": "test_backend_selector_selection", "rerun": "0"}, "4": {"status": "PASS", "message": "", "test_name": "test_backend_selector_advanced_selection", "rerun": "0"}, "5": {"status": "PASS", "message": "", "test_name": "test_mock_crawler_backend_edge_cases", "rerun": "0"}}, "suite_name": "tests/test_base.py"}, "1": {"suite_name": "tests/test_content_processor.py", "tests": {"0": {"status": "PASS", "message": "", "test_name": "test_extract_text_basic", "rerun": "0"}, "1": {"status": "PASS", "message": "", "test_name": "test_extract_text_with_complex_content", "rerun": "0"}, "2": {"status": "PASS", "message": "", "test_name": "test_extract_text_with_mixed_content_types", "rerun": "0"}, "3": {"status": "PASS", "message": "", "test_name": "test_extract_text_with_nested_content", "rerun": "0"}, "4": {"status": "PASS", "message": "", "test_name": "test_extract_text_with_special_characters", "rerun": "0"}, "5": {"status": "PASS", "message": "", "test_name": "test_format_as_markdown", "rerun": "0"}, "6": {"status": "PASS", "message": "", "test_name": "test_process_basic", "rerun": "0"}, "7": {"status": "PASS", "message": "", "test_name": "test_extract_text_with_scripts", "rerun": "0"}, "8": {"status": "PASS", "message": "", "test_name": "test_script_tags_non_json_ld", "rerun": "0"}, "9": {"status": "PASS", "message": "", "test_name": "test_script_tags_with_invalid_json", "rerun": "0"}, "10": {"status": "PASS", "message": "", "test_name": "test_script_tags_with_different_types", "rerun": "0"}, "11": {"status": "PASS", "message": "", "test_name": "test_scripts_with_data_attributes", "rerun": "0"}, "12": {"status": "PASS", "message": "", "test_name": "test_extract_code_blocks[javascript-console.log(\"Hello, World!\");-```javascript\\nconsole.log(\"Hello, World!\");\\n```]", "rerun": "0"}, "13": {"status": "PASS", "message": "", "test_name": "test_extract_code_blocks[python-print(\"Hello, World!\")-```python\\nprint(\"Hello, World!\")\\n```]", "rerun": "0"}, "14": {"status": "PASS", "message": "", "test_name": "test_extract_code_blocks[ruby-puts \"Hello, Ruby!\"-```ruby\\nputs \"Hello, Ruby!\"\\n```]", "rerun": "0"}, "15": {"status": "PASS", "message": "", "test_name": "test_extract_code_blocks[None-Inline code-`Inline code`]", "rerun": "0"}, "16": {"status": "PASS", "message": "", "test_name": "test_extract_code_blocks_with_attributes", "rerun": "0"}, "17": {"status": "FAIL", "message": "   assert '```python\\ndef foo():\\n    return \"HTML Content\"\\n```' in '```python\\ndef foo():\\n        return \"HTML Content\"\\n```'\n", "test_name": "test_extract_code_blocks_with_nested_content", "rerun": "0"}, "18": {"status": "FAIL", "message": "   assert '```go\\npackage main\\n```' not in '```go\\npack...thon!\")\\n```'\n     \n     '```go\\npackage main\\n```' is contained here:\n       ```go\n       package main\n       ```\n       \n       ```python\n       print(\"Hello, Python!\")\n       ```\n", "test_name": "test_code_blocks_with_disallowed_languages", "rerun": "0"}, "19": {"status": "PASS", "message": "", "test_name": "test_extract_content_structure", "rerun": "0"}, "20": {"status": "PASS", "message": "", "test_name": "test_extract_headings_hierarchy", "rerun": "0"}, "21": {"status": "PASS", "message": "", "test_name": "test_extract_metadata", "rerun": "0"}, "22": {"status": "PASS", "message": "", "test_name": "test_extract_metadata_with_complex_metadata", "rerun": "0"}, "23": {"status": "PASS", "message": "", "test_name": "test_extract_metadata_with_duplicates", "rerun": "0"}, "24": {"status": "PASS", "message": "", "test_name": "test_extract_metadata_with_invalid_html", "rerun": "0"}, "25": {"status": "PASS", "message": "", "test_name": "test_extract_metadata_with_missing_values", "rerun": "0"}, "26": {"status": "PASS", "message": "", "test_name": "test_extract_metadata_with_schema_org_json_ld", "rerun": "0"}, "27": {"status": "PASS", "message": "", "test_name": "test_extract_metadata_with_all_types", "rerun": "0"}, "28": {"status": "PASS", "message": "", "test_name": "test_collect_assets", "rerun": "0"}, "29": {"status": "PASS", "message": "", "test_name": "test_images_with_data_urls", "rerun": "0"}, "30": {"status": "PASS", "message": "", "test_name": "test_extract_assets_with_invalid_urls", "rerun": "0"}, "31": {"status": "PASS", "message": "", "test_name": "test_extract_links_with_invalid_urls", "rerun": "0"}, "32": {"status": "PASS", "message": "", "test_name": "test_extract_links_with_relative_paths", "rerun": "0"}, "33": {"status": "PASS", "message": "", "test_name": "test_links_with_query_parameters", "rerun": "0"}, "34": {"status": "PASS", "message": "", "test_name": "test_links_with_anchors", "rerun": "0"}, "35": {"status": "PASS", "message": "", "test_name": "test_anchor_tags_without_href", "rerun": "0"}, "36": {"status": "PASS", "message": "", "test_name": "test_links_with_various_protocols", "rerun": "0"}, "37": {"status": "PASS", "message": "", "test_name": "test_multiple_links_in_paragraph", "rerun": "0"}, "38": {"status": "PASS", "message": "", "test_name": "test_links_with_no_text", "rerun": "0"}}, "status": {"total_pass": 37, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 2, "total_error": 0}}, "2": {"suite_name": "tests/test_content_processor_advanced.py", "tests": {"0": {"status": "PASS", "message": "", "test_name": "test_url_processor_port_handling", "rerun": "0"}}, "status": {"total_pass": 1, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 0, "total_error": 0}}, "3": {"suite_name": "tests/test_content_processor_edge.py", "tests": {"0": {"status": "PASS", "message": "", "test_name": "test_empty_content", "rerun": "0"}, "1": {"status": "PASS", "message": "", "test_name": "test_malformed_html", "rerun": "0"}, "2": {"status": "PASS", "message": "", "test_name": "test_special_characters", "rerun": "0"}, "3": {"status": "PASS", "message": "", "test_name": "test_large_content", "rerun": "0"}, "4": {"status": "PASS", "message": "", "test_name": "test_nested_structures", "rerun": "0"}, "5": {"status": "PASS", "message": "", "test_name": "test_javascript_handling", "rerun": "0"}, "6": {"status": "PASS", "message": "", "test_name": "test_style_handling", "rerun": "0"}, "7": {"status": "PASS", "message": "", "test_name": "test_iframe_handling", "rerun": "0"}, "8": {"status": "PASS", "message": "", "test_name": "test_form_handling", "rerun": "0"}}, "status": {"total_pass": 9, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 0, "total_error": 0}}, "4": {"suite_name": "tests/test_crawl4ai.py", "tests": {"0": {"status": "PASS", "message": "", "test_name": "test_crawl_basic", "rerun": "0"}, "1": {"status": "PASS", "message": "", "test_name": "test_crawl_with_rate_limit", "rerun": "0"}, "2": {"status": "PASS", "message": "", "test_name": "test_validate_content", "rerun": "0"}, "3": {"status": "FAIL", "message": "   AssertionError: assert ('content' in {'links': [], 'text': 'Example Domain\\nExample Domain\\nThis domain is for use in illustrative examples in documents. Y...in in literature without prior coordination or asking for permission.\\nMore information...', 'title': 'Example Domain'} or 'error' in {'links': [], 'text': 'Example Domain\\nExample Domain\\nThis domain is for use in illustrative examples in documents. Y...in in literature without prior coordination or asking for permission.\\nMore information...', 'title': 'Example Domain'})\n", "test_name": "test_process_content", "rerun": "0"}, "4": {"status": "PASS", "message": "", "test_name": "test_backend_selection", "rerun": "0"}, "5": {"status": "PASS", "message": "", "test_name": "test_metrics", "rerun": "0"}, "6": {"status": "FAIL", "message": "   AssertionError: assert 0 == 500\n    +  where 0 = CrawlResult(url='https://invalid-url-that-does-not-exist.com', content={}, metadata={}, status=0, error='Failed after ...-does-not-exist.com:443 ssl:default [getaddrinfo failed]', timestamp=datetime.datetime(2025, 4, 2, 11, 40, 44, 997603)).status\n", "test_name": "test_error_handling", "rerun": "0"}, "7": {"status": "PASS", "message": "", "test_name": "test_concurrent_requests", "rerun": "0"}, "8": {"status": "PASS", "message": "", "test_name": "test_cleanup", "rerun": "0"}, "9": {"status": "FAIL", "message": "   Failed: DID NOT RAISE <class 'ValueError'>\n", "test_name": "test_crawl4ai_config_validation", "rerun": "0"}, "10": {"status": "PASS", "message": "", "test_name": "test_ssl_context_configuration", "rerun": "0"}, "11": {"status": "PASS", "message": "", "test_name": "test_custom_headers_handling", "rerun": "0"}, "12": {"status": "FAIL", "message": "   assert 200 == 403\n    +  where 200 = CrawlResult(url='https://other-domain.com/page', content={'html': '\\ufeff<!DOCTYPE html>\\n<html lang=\"ja\">\\n<head>\\n\\t...pe': 'text/html; charset=UTF-8'}}, status=200, error=None, timestamp=datetime.datetime(2025, 4, 2, 11, 40, 52, 541798)).status\n", "test_name": "test_domain_filtering", "rerun": "0"}, "13": {"status": "FAIL", "message": "   AssertionError: assert 5 == 3\n    +  where 5 = len([CrawlResult(url='https://example.com/page0', content={'html': '<!doctype html>\\n<html>\\n<head>\\n    <title>Example Do...T', 'Connection': 'keep-alive'}}, status=404, error=None, timestamp=datetime.datetime(2025, 4, 2, 11, 40, 57, 886044))])\n", "test_name": "test_url_queue_management", "rerun": "0"}}, "status": {"total_pass": 9, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 5, "total_error": 0}}, "5": {"suite_name": "tests/test_crawl4ai_extended.py", "tests": {"0": {"status": "FAIL", "message": "   TypeError: 'coroutine' object does not support the asynchronous context manager protocol\n", "test_name": "test_crawl_with_urlinfo", "rerun": "0"}, "1": {"status": "FAIL", "message": "   TypeError: 'coroutine' object does not support the asynchronous context manager protocol\n", "test_name": "test_crawl_depth_first", "rerun": "0"}, "2": {"status": "FAIL", "message": "   TypeError: 'coroutine' object does not support the asynchronous context manager protocol\n", "test_name": "test_rate_limiting_precision", "rerun": "0"}, "3": {"status": "FAIL", "message": "   TypeError: 'coroutine' object does not support the asynchronous context manager protocol\n", "test_name": "test_concurrent_request_limit", "rerun": "0"}, "4": {"status": "FAIL", "message": "   TypeError: 'coroutine' object does not support the asynchronous context manager protocol\n", "test_name": "test_url_normalization", "rerun": "0"}, "5": {"status": "FAIL", "message": "   TypeError: 'coroutine' object does not support the asynchronous context manager protocol\n", "test_name": "test_error_propagation", "rerun": "0"}, "6": {"status": "FAIL", "message": "   AttributeError: 'coroutine' object has no attribute '_session'\n", "test_name": "test_retry_behavior", "rerun": "0"}, "7": {"status": "FAIL", "message": "   TypeError: 'coroutine' object does not support the asynchronous context manager protocol\n", "test_name": "test_metrics_accuracy", "rerun": "0"}, "8": {"status": "FAIL", "message": "   TypeError: 'coroutine' object does not support the asynchronous context manager protocol\n", "test_name": "test_resource_cleanup", "rerun": "0"}, "9": {"status": "FAIL", "message": "   AttributeError: 'URLInfo' object has no attribute 'original_url'\n", "test_name": "test_url_info_initialization[https://docs.python.org/3/-expected0]", "rerun": "0"}, "10": {"status": "FAIL", "message": "   AttributeError: 'URLInfo' object has no attribute 'original_url'\n", "test_name": "test_url_info_initialization[https://docs.python.org-expected1]", "rerun": "0"}, "11": {"status": "FAIL", "message": "   AttributeError: 'URLInfo' object has no attribute 'original_url'\n", "test_name": "test_url_info_initialization[invalid-url-expected2]", "rerun": "0"}, "12": {"status": "FAIL", "message": "   assert 3 == 2\n    +  where 3 = len({<src.utils.url_info.URLInfo object at 0x000002272DE75D00>, <src.utils.url_info.URLInfo object at 0x000002272EACB410>, <src.utils.url_info.URLInfo object at 0x000002272EAC9490>})\n", "test_name": "test_url_info_hashable", "rerun": "0"}, "13": {"status": "FAIL", "message": "   Failed: DID NOT RAISE <class 'Exception'>\n", "test_name": "test_url_info_immutable", "rerun": "0"}}, "status": {"total_pass": 0, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 14, "total_error": 0}}, "6": {"suite_name": "tests/test_crawler.py", "tests": {"0": {"status": "PASS", "message": "", "test_name": "test_crawler_initialization", "rerun": "0"}, "1": {"status": "PASS", "message": "", "test_name": "test_url_filtering", "rerun": "0"}, "2": {"status": "FAIL", "message": "   assert 0 > 0\n    +  where 0 = len([])\n", "test_name": "test_single_url_processing", "rerun": "0"}, "3": {"status": "FAIL", "message": "   assert 0 > 0\n    +  where 0 = CrawlStats(start_time=datetime.datetime(2025, 4, 2, 11, 41, 1, 802018), end_time=datetime.datetime(2025, 4, 2, 11, 41,..., successful_crawls=0, failed_crawls=2, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0).pages_crawled\n    +    where CrawlStats(start_time=datetime.datetime(2025, 4, 2, 11, 41, 1, 802018), end_time=datetime.datetime(2025, 4, 2, 11, 41,..., successful_crawls=0, failed_crawls=2, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0) = CrawlResult(target=CrawlTarget(url='https://example.com/doc2', depth=2, follow_external=False, content_types=['text/ht...crawl error: name 'stats' is not defined\", location=None, details={})], metrics={}, structure=None, processed_url=None).stats\n", "test_name": "test_depth_limited_crawling", "rerun": "0"}, "4": {"status": "FAIL", "message": "   AssertionError: assert 0 == 3\n    +  where 0 = CrawlStats(start_time=datetime.datetime(2025, 4, 2, 11, 41, 4, 880951), end_time=None, pages_crawled=0, successful_crawls=0, failed_crawls=6, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0).pages_crawled\n    +  and   3 = len({'https://example.com/doc1', 'https://example.com/doc2', 'https://example.com/doc3'})\n", "test_name": "test_concurrent_processing", "rerun": "0"}, "5": {"status": "PASS", "message": "", "test_name": "test_rate_limiting", "rerun": "0"}, "6": {"status": "PASS", "message": "", "test_name": "test_error_handling", "rerun": "0"}, "7": {"status": "FAIL", "message": "   assert 0 == 1\n    +  where 0 = CrawlStats(start_time=datetime.datetime(2025, 4, 2, 11, 41, 14, 25555), end_time=datetime.datetime(2025, 4, 2, 11, 41,..., successful_crawls=0, failed_crawls=2, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0).successful_crawls\n    +    where CrawlStats(start_time=datetime.datetime(2025, 4, 2, 11, 41, 14, 25555), end_time=datetime.datetime(2025, 4, 2, 11, 41,..., successful_crawls=0, failed_crawls=2, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0) = CrawlResult(target=CrawlTarget(url='https://example.com/doc2', depth=1, follow_external=False, content_types=['text/ht...crawl error: name 'stats' is not defined\", location=None, details={})], metrics={}, structure=None, processed_url=None).stats\n", "test_name": "test_content_processing_pipeline", "rerun": "0"}, "8": {"status": "FAIL", "message": "   assert None is not None\n    +  where None = <src.crawler.DocumentationCrawler object at 0x000002272EBCB050>.client_session\n", "test_name": "test_cleanup", "rerun": "0"}, "9": {"status": "PASS", "message": "", "test_name": "test_max_pages_limit", "rerun": "0"}, "10": {"status": "FAIL", "message": "   assert 0.0 > 0\n    +  where 0.0 = CrawlStats(start_time=datetime.datetime(2025, 4, 2, 11, 41, 23, 177696), end_time=datetime.datetime(2025, 4, 2, 11, 41..., successful_crawls=0, failed_crawls=2, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0).total_time\n    +    where CrawlStats(start_time=datetime.datetime(2025, 4, 2, 11, 41, 23, 177696), end_time=datetime.datetime(2025, 4, 2, 11, 41..., successful_crawls=0, failed_crawls=2, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0) = CrawlResult(target=CrawlTarget(url='https://example.com/doc2', depth=1, follow_external=False, content_types=['text/ht...crawl error: name 'stats' is not defined\", location=None, details={})], metrics={}, structure=None, processed_url=None).stats\n", "test_name": "test_statistics_tracking", "rerun": "0"}, "11": {"status": "PASS", "message": "", "test_name": "test_project_type_enum", "rerun": "0"}, "12": {"status": "PASS", "message": "", "test_name": "test_project_identity", "rerun": "0"}, "13": {"status": "PASS", "message": "", "test_name": "test_project_identifier", "rerun": "0"}, "14": {"status": "FAIL", "message": "               src.utils.search.SearchError: Failed to get vqd token: 202\n       src.utils.search.SearchError: Error getting vqd token: Failed to get vqd token: 202\n", "test_name": "test_duckduckgo_search", "rerun": "0"}, "15": {"status": "FAIL", "message": "   AttributeError: 'DocumentationCrawler' object has no attribute '_discover_urls'\n", "test_name": "test_url_discovery", "rerun": "0"}, "16": {"status": "FAIL", "message": "   AttributeError: 'ProjectIdentifier' object has no attribute '_identify_project_type'\n", "test_name": "test_project_type_detection", "rerun": "0"}}, "status": {"total_pass": 8, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 9, "total_error": 0}}, "7": {"suite_name": "tests/test_crawler_advanced.py", "tests": {"0": {"status": "FAIL", "message": "   assert 0 == 1\n    +  where 0 = CrawlStats(start_time=datetime.datetime(2025, 4, 2, 11, 41, 26, 852655), end_time=datetime.datetime(2025, 4, 2, 11, 41..., successful_crawls=0, failed_crawls=2, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0).pages_crawled\n    +    where CrawlStats(start_time=datetime.datetime(2025, 4, 2, 11, 41, 26, 852655), end_time=datetime.datetime(2025, 4, 2, 11, 41..., successful_crawls=0, failed_crawls=2, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0) = CrawlResult(target=CrawlTarget(url='http://example.com', depth=1, follow_external=False, content_types=['text/html'], ...crawl error: name 'stats' is not defined\", location=None, details={})], metrics={}, structure=None, processed_url=None).stats\n", "test_name": "test_crawler_basic_crawl", "rerun": "0"}, "1": {"status": "FAIL", "message": "   assert 2 == 1\n    +  where 2 = CrawlStats(start_time=datetime.datetime(2025, 4, 2, 11, 41, 30, 20181), end_time=datetime.datetime(2025, 4, 2, 11, 41,..., successful_crawls=0, failed_crawls=2, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0).failed_crawls\n    +    where CrawlStats(start_time=datetime.datetime(2025, 4, 2, 11, 41, 30, 20181), end_time=datetime.datetime(2025, 4, 2, 11, 41,..., successful_crawls=0, failed_crawls=2, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0) = CrawlResult(target=CrawlTarget(url='http://nonexistent.com', depth=1, follow_external=False, content_types=['text/html...crawl error: name 'stats' is not defined\", location=None, details={})], metrics={}, structure=None, processed_url=None).stats\n", "test_name": "test_crawler_failed_crawl", "rerun": "0"}, "2": {"status": "FAIL", "message": "   assert 0 == 1\n    +  where 0 = CrawlStats(start_time=datetime.datetime(2025, 4, 2, 11, 41, 33, 86661), end_time=datetime.datetime(2025, 4, 2, 11, 41,..., successful_crawls=0, failed_crawls=2, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0).successful_crawls\n    +    where CrawlStats(start_time=datetime.datetime(2025, 4, 2, 11, 41, 33, 86661), end_time=datetime.datetime(2025, 4, 2, 11, 41,..., successful_crawls=0, failed_crawls=2, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0) = CrawlResult(target=CrawlTarget(url='http://example.com', depth=1, follow_external=False, content_types=['text/html'], ...crawl error: name 'stats' is not defined\", location=None, details={})], metrics={}, structure=None, processed_url=None).stats\n", "test_name": "test_crawler_retry_mechanism", "rerun": "0"}, "3": {"status": "PASS", "message": "", "test_name": "test_crawler_rate_limiting", "rerun": "0"}, "4": {"status": "FAIL", "message": "       AssertionError: Expected 'process' to have been called once. Called 0 times.\n", "test_name": "test_crawler_content_processing", "rerun": "0"}, "5": {"status": "FAIL", "message": "   assert 2 == 0\n    +  where 2 = len([QualityIssue(type=<IssueType.GENERAL: 'general'>, level=<IssueLevel.ERROR: 'error'>, message=\"Unhandled exception: ty...=<IssueLevel.ERROR: 'error'>, message=\"Unhandled crawl error: name 'stats' is not defined\", location=None, details={})])\n    +    where [QualityIssue(type=<IssueType.GENERAL: 'general'>, level=<IssueLevel.ERROR: 'error'>, message=\"Unhandled exception: ty...=<IssueLevel.ERROR: 'error'>, message=\"Unhandled crawl error: name 'stats' is not defined\", location=None, details={})] = CrawlResult(target=CrawlTarget(url='http://example.com', depth=1, follow_external=False, content_types=['text/html'], ...crawl error: name 'stats' is not defined\", location=None, details={})], metrics={}, structure=None, processed_url=None).issues\n", "test_name": "test_crawler_quality_checking", "rerun": "0"}, "6": {"status": "PASS", "message": "", "test_name": "test_crawler_resource_cleanup", "rerun": "0"}, "7": {"status": "FAIL", "message": "   assert 0 == 5\n    +  where 0 = sum(<generator object test_crawler_concurrent_requests.<locals>.<genexpr> at 0x000002272EA484C0>)\n", "test_name": "test_crawler_concurrent_requests", "rerun": "0"}, "8": {"status": "FAIL", "message": "           assert not [QualityIssue(type=<IssueType.GENERAL: 'general'>, level=<IssueLevel.ERROR: 'error'>, message=\"Unhandled exception: ty...=<IssueLevel.ERROR: 'error'>, message=\"Unhandled crawl error: name 'stats' is not defined\", location=None, details={})]\n            +  where [QualityIssue(type=<IssueType.GENERAL: 'general'>, level=<IssueLevel.ERROR: 'error'>, message=\"Unhandled exception: ty...=<IssueLevel.ERROR: 'error'>, message=\"Unhandled crawl error: name 'stats' is not defined\", location=None, details={})] = CrawlResult(target=CrawlTarget(url='http://EXAMPLE.com', depth=1, follow_external=False, content_types=['text/html'], ...crawl error: name 'stats' is not defined\", location=None, details={})], metrics={}, structure=None, processed_url=None).issues\n", "test_name": "test_crawler_url_normalization", "rerun": "0"}, "9": {"status": "FAIL", "message": "       assert 2 == 1\n        +  where 2 = len([QualityIssue(type=<IssueType.GENERAL: 'general'>, level=<IssueLevel.ERROR: 'error'>, message=\"Unhandled exception: ty...=<IssueLevel.ERROR: 'error'>, message=\"Unhandled crawl error: name 'stats' is not defined\", location=None, details={})])\n        +    where [QualityIssue(type=<IssueType.GENERAL: 'general'>, level=<IssueLevel.ERROR: 'error'>, message=\"Unhandled exception: ty...=<IssueLevel.ERROR: 'error'>, message=\"Unhandled crawl error: name 'stats' is not defined\", location=None, details={})] = CrawlResult(target=CrawlTarget(url='http://example.com', depth=1, follow_external=False, content_types=['text/html'], ...crawl error: name 'stats' is not defined\", location=None, details={})], metrics={}, structure=None, processed_url=None).issues\n", "test_name": "test_crawler_error_handling", "rerun": "0"}}, "status": {"total_pass": 2, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 8, "total_error": 0}}, "8": {"suite_name": "tests/test_gui.py", "tests": {"0": {"status": "FAIL", "message": "       jinja2.exceptions.TemplateNotFound: 'index.html' not found in search path: 'src/gui/templates'\n", "test_name": "test_home_page", "rerun": "0"}, "1": {"status": "FAIL", "message": "   AssertionError: assert 'error' == 'success'\n     \n     - success\n     + error\n", "test_name": "test_crawl_request", "rerun": "0"}, "2": {"status": "PASS", "message": "", "test_name": "test_connection_manager", "rerun": "0"}, "3": {"status": "FAIL", "message": "   TypeError: ProcessedContent.__init__() got an unexpected keyword argument 'url'\n", "test_name": "test_crawler_thread", "rerun": "0"}, "4": {"status": "FAIL", "message": "   NameError: name 'ResultsViewer' is not defined\n", "test_name": "test_results_viewer", "rerun": "0"}, "5": {"status": "FAIL", "message": "   TypeError: ProcessedContent.__init__() got an unexpected keyword argument 'url'\n", "test_name": "test_save_results", "rerun": "0"}, "6": {"status": "FAIL", "message": "E   ModuleNotFoundError: No module named 'PyQt6'\n", "test_name": "test_error_handling", "rerun": "0"}}, "status": {"total_pass": 1, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 6, "total_error": 0}}, "9": {"suite_name": "tests/test_helpers.py", "tests": {"0": {"status": "FAIL", "message": "   assert (1743594126.3121495 - 1743594125.7926924) >= 1.0\n", "test_name": "test_rate_limiter", "rerun": "0"}, "1": {"status": "FAIL", "message": "   AssertionError: assert ValueError('Attempt 1 failed') is None\n", "test_name": "test_retry_strategy", "rerun": "0"}, "2": {"status": "FAIL", "message": "   AttributeError: 'Timer' object has no attribute 'end_time'\n", "test_name": "test_timer", "rerun": "0"}, "3": {"status": "PASS", "message": "", "test_name": "test_similarity_calculation", "rerun": "0"}, "4": {"status": "PASS", "message": "", "test_name": "test_checksum_generation", "rerun": "0"}, "5": {"status": "FAIL", "message": "       AssertionError: assert False\n        +  where False = <MagicMock name='getLogger().setLevel' id='2367324149408'>.called\n        +    where <MagicMock name='getLogger().setLevel' id='2367324149408'> = <MagicMock name='getLogger()' id='2367324150752'>.setLevel\n", "test_name": "test_logging_setup", "rerun": "0"}}, "status": {"total_pass": 2, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 4, "total_error": 0}}, "10": {"suite_name": "tests/test_integration.py", "tests": {"0": {"status": "FAIL", "message": "   AssertionError: assert 0 > 0\n    +  where 0 = CrawlStats(start_time=datetime.datetime(2025, 4, 2, 11, 42, 6, 573391), end_time=datetime.datetime(2025, 4, 2, 11, 42,..., successful_crawls=0, failed_crawls=0, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0).pages_crawled\n    +    where CrawlStats(start_time=datetime.datetime(2025, 4, 2, 11, 42, 6, 573391), end_time=datetime.datetime(2025, 4, 2, 11, 42,..., successful_crawls=0, failed_crawls=0, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0) = CrawlResult(target=CrawlTarget(url='file://C:\\\\Users\\\\Andu\\\\AppData\\\\Local\\\\Temp\\\\pytest-of-Andu\\\\pytest-22\\\\test_full...t_full_site_crawl0\\\\test_docs/index.html', location=None, details={})], metrics={}, structure=None, processed_url=None).stats\n", "test_name": "test_full_site_crawl", "rerun": "0"}, "1": {"status": "FAIL", "message": "   AssertionError: assert 0 == 1\n    +  where 0 = len([])\n    +    where [] = CrawlResult(target=CrawlTarget(url='file://C:\\\\Users\\\\Andu\\\\AppData\\\\Local\\\\Temp\\\\pytest-of-Andu\\\\pytest-22\\\\test_cont...processing_pipeli0\\\\test_docs/guide.html', location=None, details={})], metrics={}, structure=None, processed_url=None).documents\n", "test_name": "test_content_processing_pipeline", "rerun": "0"}, "2": {"status": "FAIL", "message": "   AssertionError: assert 0 > 0\n    +  where 0 = len({})\n    +    where {} = CrawlResult(target=CrawlTarget(url='file://C:\\\\Users\\\\Andu\\\\AppData\\\\Local\\\\Temp\\\\pytest-of-Andu\\\\pytest-22\\\\test_qual...test_quality_checks0\\\\test_docs/api.html', location=None, details={})], metrics={}, structure=None, processed_url=None).metrics\n", "test_name": "test_quality_checks", "rerun": "0"}, "3": {"status": "PASS", "message": "", "test_name": "test_document_organization", "rerun": "0"}, "4": {"status": "FAIL", "message": "   assert 0 > 0\n    +  where 0 = len([])\n", "test_name": "test_search_functionality", "rerun": "0"}, "5": {"status": "FAIL", "message": "   AssertionError: assert 0 > 0\n    +  where 0 = CrawlStats(start_time=datetime.datetime(2025, 4, 2, 11, 42, 44, 28553), end_time=datetime.datetime(2025, 4, 2, 11, 42,..., successful_crawls=0, failed_crawls=0, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0).successful_crawls\n    +    where CrawlStats(start_time=datetime.datetime(2025, 4, 2, 11, 42, 44, 28553), end_time=datetime.datetime(2025, 4, 2, 11, 42,..., successful_crawls=0, failed_crawls=0, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0) = CrawlResult(target=CrawlTarget(url='file://C:\\\\Users\\\\Andu\\\\AppData\\\\Local\\\\Temp\\\\pytest-of-Andu\\\\pytest-22\\\\test_erro...ndling_and_recove0\\\\test_docs/index.html', location=None, details={})], metrics={}, structure=None, processed_url=None).stats\n", "test_name": "test_error_handling_and_recovery", "rerun": "0"}}, "status": {"total_pass": 1, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 5, "total_error": 0}}, "11": {"suite_name": "tests/test_integration_advanced.py", "tests": {"0": {"status": "FAIL", "message": "   TypeError: Crawl4AIBackend.__init__() got an unexpected keyword argument 'rate_limiter'\n", "test_name": "test_full_crawl_pipeline", "rerun": "0"}, "1": {"status": "FAIL", "message": "   AttributeError: 'str' object has no attribute 'url'\n", "test_name": "test_error_handling_integration", "rerun": "0"}, "2": {"status": "FAIL", "message": "   TypeError: Crawl4AIBackend.__init__() got an unexpected keyword argument 'rate_limiter'\n", "test_name": "test_rate_limiting_integration", "rerun": "0"}, "3": {"status": "FAIL", "message": "   AttributeError: 'DocumentationCrawler' object has no attribute 'backend'\n", "test_name": "test_content_processing_integration", "rerun": "0"}}, "status": {"total_pass": 0, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 4, "total_error": 0}}, "12": {"suite_name": "tests/test_link_processor.py", "tests": {"0": {"status": "FAIL", "message": "   AssertionError: assert 'https://example.com' in ['https://example.com/', 'https://base.com/relative/path', 'https://base.com/parent/path', 'https://base.com/current/path']\n", "test_name": "test_basic_link_extraction", "rerun": "0"}, "1": {"status": "PASS", "message": "", "test_name": "test_empty_document", "rerun": "0"}, "2": {"status": "PASS", "message": "", "test_name": "test_malformed_links", "rerun": "0"}, "3": {"status": "FAIL", "message": "   assert 0 == 1\n    +  where 0 = len([])\n", "test_name": "test_duplicate_links", "rerun": "0"}, "4": {"status": "FAIL", "message": "   AssertionError: assert 'https://example.com' in ['https://base.com/', 'https://base.com/page.html', 'https://example.com/', 'https://base.com/path']\n", "test_name": "test_fragment_handling", "rerun": "0"}, "5": {"status": "FAIL", "message": "   AssertionError: assert 'https://example.com?param=value' in ['https://base.com/?param=value', 'https://base.com/path?param=value', 'https://example.com/?param=value', 'https://base.com/page.html?param1=value1\u00b6m2=value2']\n", "test_name": "test_query_params", "rerun": "0"}, "6": {"status": "FAIL", "message": "   assert False\n    +  where False = any(<generator object test_special_characters.<locals>.<genexpr> at 0x000002272EADF1D0>)\n", "test_name": "test_special_characters", "rerun": "0"}, "7": {"status": "PASS", "message": "", "test_name": "test_nested_links", "rerun": "0"}, "8": {"status": "PASS", "message": "", "test_name": "test_invalid_base_url", "rerun": "0"}, "9": {"status": "PASS", "message": "", "test_name": "test_data_urls", "rerun": "0"}, "10": {"status": "PASS", "message": "", "test_name": "test_unicode_urls", "rerun": "0"}, "11": {"status": "FAIL", "message": "   AssertionError: assert 'https://example.com' in ['https://example.com/', 'https://cdn.example.com/asset.js', 'https://api.example.com/v1']\n", "test_name": "test_protocol_relative_urls", "rerun": "0"}, "12": {"status": "FAIL", "message": "   AssertionError: assert 'https://different-base.com/subdir/relative' in ['https://base.com/relative', 'https://base.com/absolute', 'https://external.com/']\n", "test_name": "test_base_tag_handling", "rerun": "0"}, "13": {"status": "PASS", "message": "", "test_name": "test_malformed_base_tags", "rerun": "0"}, "14": {"status": "PASS", "message": "", "test_name": "test_link_attributes", "rerun": "0"}, "15": {"status": "FAIL", "message": "   AssertionError: assert 'http://example.com' in ['http://example.com/', 'https://example.com/']\n", "test_name": "test_link_schemes", "rerun": "0"}, "16": {"status": "PASS", "message": "", "test_name": "test_url_normalization", "rerun": "0"}, "17": {"status": "FAIL", "message": "   AssertionError: assert ('HTTPS://EXAMPLE.COM' in ['https://base.com/PATH', 'https://base.com/path', 'https://EXAMPLE.COM/', 'https://example.com/', 'https://base.com/Mixed/Case/PATH'] or 'https://example.com' in ['https://base.com/PATH', 'https://base.com/path', 'https://EXAMPLE.COM/', 'https://example.com/', 'https://base.com/Mixed/Case/PATH'])\n", "test_name": "test_url_case_sensitivity", "rerun": "0"}, "18": {"status": "PASS", "message": "", "test_name": "test_url_encoding_handling", "rerun": "0"}}, "status": {"total_pass": 10, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 9, "total_error": 0}}, "13": {"suite_name": "tests/test_organizer.py", "tests": {"0": {"status": "PASS", "message": "", "test_name": "test_document_organizer_initialization", "rerun": "0"}, "1": {"status": "FAIL", "message": "   TypeError: ProcessedContent.__init__() got an unexpected keyword argument 'url'\n", "test_name": "test_document_version_management", "rerun": "0"}, "2": {"status": "FAIL", "message": "   TypeError: ProcessedContent.__init__() got an unexpected keyword argument 'url'\n", "test_name": "test_document_categorization", "rerun": "0"}, "3": {"status": "FAIL", "message": "   TypeError: ProcessedContent.__init__() got an unexpected keyword argument 'url'\n", "test_name": "test_reference_extraction", "rerun": "0"}, "4": {"status": "FAIL", "message": "   TypeError: ProcessedContent.__init__() got an unexpected keyword argument 'url'\n", "test_name": "test_search_functionality", "rerun": "0"}, "5": {"status": "FAIL", "message": "   TypeError: ProcessedContent.__init__() got an unexpected keyword argument 'url'\n", "test_name": "test_collection_management", "rerun": "0"}, "6": {"status": "FAIL", "message": "   TypeError: ProcessedContent.__init__() got an unexpected keyword argument 'url'\n", "test_name": "test_document_similarity", "rerun": "0"}, "7": {"status": "FAIL", "message": "   TypeError: ProcessedContent.__init__() got an unexpected keyword argument 'url'\n", "test_name": "test_version_tracking", "rerun": "0"}, "8": {"status": "FAIL", "message": "   TypeError: ProcessedContent.__init__() got an unexpected keyword argument 'url'\n", "test_name": "test_search_index_generation", "rerun": "0"}}, "status": {"total_pass": 1, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 8, "total_error": 0}}, "14": {"suite_name": "tests/test_processor.py", "tests": {"0": {"status": "PASS", "message": "", "test_name": "test_content_processor_initialization", "rerun": "0"}, "1": {"status": "FAIL", "message": "   AssertionError: assert 'Test Document' == 'Sample Document'\n     \n     - Sample Document\n     + Test Document\n", "test_name": "test_full_content_processing", "rerun": "0"}, "2": {"status": "FAIL", "message": "   AssertionError: assert not {'formatted_content': '', 'headings': [], 'structure': []}\n    +  where {'formatted_content': '', 'headings': [], 'structure': []} = ProcessedContent(content={'formatted_content': '', 'structure': [], 'headings': []}, metadata={}, assets={'images': []...ors=['Error processing content: Cleaned content too long: 10008 characters (limit: 10000)'], title='Untitled Document').content\n", "test_name": "test_content_size_limits", "rerun": "0"}, "3": {"status": "PASS", "message": "", "test_name": "test_malformed_html_handling", "rerun": "0"}, "4": {"status": "PASS", "message": "", "test_name": "test_special_content_handling", "rerun": "0"}, "5": {"status": "PASS", "message": "", "test_name": "test_content_structure_preservation", "rerun": "0"}}, "status": {"total_pass": 4, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 2, "total_error": 0}}, "15": {"suite_name": "tests/test_quality.py", "tests": {"0": {"status": "FAIL", "message": "   TypeError: ProcessedContent.__init__() got an unexpected keyword argument 'url'\n", "test_name": "test_quality_checker_basic", "rerun": "0"}, "1": {"status": "FAIL", "message": "   TypeError: ProcessedContent.__init__() got an unexpected keyword argument 'url'\n", "test_name": "test_quality_checker_content_length", "rerun": "0"}, "2": {"status": "FAIL", "message": "   TypeError: ProcessedContent.__init__() got an unexpected keyword argument 'url'\n", "test_name": "test_quality_checker_headings", "rerun": "0"}, "3": {"status": "FAIL", "message": "   TypeError: ProcessedContent.__init__() got an unexpected keyword argument 'url'\n", "test_name": "test_quality_checker_links", "rerun": "0"}, "4": {"status": "FAIL", "message": "   TypeError: ProcessedContent.__init__() got an unexpected keyword argument 'url'\n", "test_name": "test_quality_checker_code_blocks", "rerun": "0"}, "5": {"status": "FAIL", "message": "   TypeError: ProcessedContent.__init__() got an unexpected keyword argument 'url'\n", "test_name": "test_quality_checker_metadata", "rerun": "0"}, "6": {"status": "FAIL", "message": "   TypeError: ProcessedContent.__init__() got an unexpected keyword argument 'url'\n", "test_name": "test_quality_checker_custom_config", "rerun": "0"}}, "status": {"total_pass": 0, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 7, "total_error": 0}}, "16": {"suite_name": "tests/test_url_handling.py", "tests": {"0": {"status": "PASS", "message": "", "test_name": "test_urlunparse_behavior", "rerun": "0"}, "1": {"status": "FAIL", "message": "       AttributeError: 'URLInfo' object has no attribute 'normalized'. Did you mean: 'normalized_url'?\n", "test_name": "test_url_info_creation_basic", "rerun": "0"}, "2": {"status": "FAIL", "message": "       AttributeError: 'URLInfo' object has no attribute 'normalized'. Did you mean: 'normalized_url'?\n", "test_name": "test_url_info_path_normalization", "rerun": "0"}, "3": {"status": "FAIL", "message": "       AttributeError: 'URLInfo' object has no attribute 'normalized'. Did you mean: 'normalized_url'?\n", "test_name": "test_url_info_scheme_handling", "rerun": "0"}, "4": {"status": "FAIL", "message": "       AttributeError: 'URLInfo' object has no attribute 'normalized'. Did you mean: 'normalized_url'?\n", "test_name": "test_url_info_query_handling", "rerun": "0"}, "5": {"status": "FAIL", "message": "       AttributeError: 'URLInfo' object has no attribute 'normalized'. Did you mean: 'normalized_url'?\n", "test_name": "test_url_info_fragment_handling", "rerun": "0"}, "6": {"status": "FAIL", "message": "       AttributeError: 'URLInfo' object has no attribute 'normalized'. Did you mean: 'normalized_url'?\n", "test_name": "test_url_info_invalid_urls", "rerun": "0"}, "7": {"status": "FAIL", "message": "   Failed: DID NOT RAISE <class 'Exception'>\n", "test_name": "test_url_info_immutability", "rerun": "0"}, "8": {"status": "FAIL", "message": "   assert <src.utils.url_info.URLInfo object at 0x000002272F8EA720> == <src.utils.url_info.URLInfo object at 0x000002272F8EA690>\n", "test_name": "test_url_info_equality", "rerun": "0"}, "9": {"status": "FAIL", "message": "   AttributeError: 'URLInfo' object has no attribute 'normalized'. Did you mean: 'normalized_url'?\n", "test_name": "test_url_info_type_safety", "rerun": "0"}, "10": {"status": "FAIL", "message": "       AttributeError: 'URLInfo' object has no attribute 'normalized'. Did you mean: 'normalized_url'?\n", "test_name": "test_url_info_edge_cases", "rerun": "0"}, "11": {"status": "FAIL", "message": "       assert True == False\n        +  where True = <src.utils.url_info.URLInfo object at 0x000002272F8EABA0>.is_valid\n", "test_name": "test_url_info_security", "rerun": "0"}, "12": {"status": "FAIL", "message": "       AttributeError: 'URLInfo' object has no attribute 'normalized'. Did you mean: 'normalized_url'?\n", "test_name": "test_url_info_relative_paths", "rerun": "0"}, "13": {"status": "FAIL", "message": "       AttributeError: 'URLInfo' object has no attribute 'normalized'. Did you mean: 'normalized_url'?\n", "test_name": "test_url_info_query_parameters", "rerun": "0"}, "14": {"status": "PASS", "message": "", "test_name": "test_url_info_performance", "rerun": "0"}}, "status": {"total_pass": 2, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 13, "total_error": 0}}}}, "date": "April 02, 2025", "start_time": 1743594173.8801324, "total_suite": 17, "status": "FAIL", "status_list": {"pass": "93", "fail": "96", "skip": "0", "error": "0", "xpass": "0", "xfail": "0", "rerun": "0"}, "total_tests": "189"}