{"content": {"suites": {"0": {"status": {"total_pass": 8, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 1, "total_error": 0}, "tests": {"0": {"status": "PASS", "message": "", "test_name": "test_crawler_backend_lifecycle", "rerun": "0"}, "1": {"status": "PASS", "message": "", "test_name": "test_crawler_backend_error_handling", "rerun": "0"}, "2": {"status": "PASS", "message": "", "test_name": "test_backend_selector_registration", "rerun": "0"}, "3": {"status": "PASS", "message": "", "test_name": "test_backend_selector_selection", "rerun": "0"}, "4": {"status": "PASS", "message": "", "test_name": "test_url_normalization", "rerun": "0"}, "5": {"status": "PASS", "message": "", "test_name": "test_processed_content_validation", "rerun": "0"}, "6": {"status": "PASS", "message": "", "test_name": "test_concurrent_backend_operations", "rerun": "0"}, "7": {"status": "PASS", "message": "", "test_name": "test_backend_metrics_update", "rerun": "0"}, "8": {"status": "FAIL", "message": "   Failed: DID NOT RAISE <class 'ValueError'>\n", "test_name": "test_backend_selector_error_handling", "rerun": "0"}}, "suite_name": "tests/test_base.py"}, "1": {"suite_name": "tests/test_crawler.py", "tests": {"0": {"status": "ERROR", "message": "backend_selector = <src.backends.selector.BackendSelector object at 0x112dd3390>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x112dd3590>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x11054bf10>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x112dd3010>\n\n    @pytest.fixture\n    def crawler(\n        backend_selector: BackendSelector,\n        content_processor: ContentProcessor,\n        quality_checker: QualityChecker,\n        document_organizer: DocumentOrganizer\n    ) -> DocumentationCrawler:\n        \"\"\"Configured documentation crawler.\"\"\"\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=backend_selector,\n            content_processor=content_processor,\n            quality_checker=quality_checker,\n            document_organizer=document_organizer\n        )\n\ntests/conftest.py:233: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x112d0e550>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x112dd3390>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x112dd3590>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x11054bf10>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x112dd3010>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_crawler_initialization", "rerun": "0"}, "1": {"status": "ERROR", "message": "backend_selector = <src.backends.selector.BackendSelector object at 0x112db7550>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x112dcbf50>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x112dc8f50>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x112daa1d0>\n\n    @pytest.fixture\n    def crawler(\n        backend_selector: BackendSelector,\n        content_processor: ContentProcessor,\n        quality_checker: QualityChecker,\n        document_organizer: DocumentOrganizer\n    ) -> DocumentationCrawler:\n        \"\"\"Configured documentation crawler.\"\"\"\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=backend_selector,\n            content_processor=content_processor,\n            quality_checker=quality_checker,\n            document_organizer=document_organizer\n        )\n\ntests/conftest.py:233: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x112daad90>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x112db7550>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x112dcbf50>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x112dc8f50>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x112daa1d0>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_url_filtering", "rerun": "0"}, "2": {"status": "ERROR", "message": "backend_selector = <src.backends.selector.BackendSelector object at 0x112e99110>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x112e98110>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x112e99b50>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x112e99090>\n\n    @pytest.fixture\n    def crawler(\n        backend_selector: BackendSelector,\n        content_processor: ContentProcessor,\n        quality_checker: QualityChecker,\n        document_organizer: DocumentOrganizer\n    ) -> DocumentationCrawler:\n        \"\"\"Configured documentation crawler.\"\"\"\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=backend_selector,\n            content_processor=content_processor,\n            quality_checker=quality_checker,\n            document_organizer=document_organizer\n        )\n\ntests/conftest.py:233: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x112e98490>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x112e99110>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x112e98110>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x112e99b50>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x112e99090>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_single_url_processing", "rerun": "0"}, "3": {"status": "ERROR", "message": "backend_selector = <src.backends.selector.BackendSelector object at 0x112e0c5d0>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x112e0ce50>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x112e0c8d0>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x112e0c510>\n\n    @pytest.fixture\n    def crawler(\n        backend_selector: BackendSelector,\n        content_processor: ContentProcessor,\n        quality_checker: QualityChecker,\n        document_organizer: DocumentOrganizer\n    ) -> DocumentationCrawler:\n        \"\"\"Configured documentation crawler.\"\"\"\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=backend_selector,\n            content_processor=content_processor,\n            quality_checker=quality_checker,\n            document_organizer=document_organizer\n        )\n\ntests/conftest.py:233: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x112e0ff10>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x112e0c5d0>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x112e0ce50>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x112e0c8d0>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x112e0c510>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_depth_limited_crawling", "rerun": "0"}, "4": {"status": "ERROR", "message": "backend_selector = <src.backends.selector.BackendSelector object at 0x112dd2510>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x112dd26d0>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x112dd1850>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x112dd30d0>\n\n    @pytest.fixture\n    def crawler(\n        backend_selector: BackendSelector,\n        content_processor: ContentProcessor,\n        quality_checker: QualityChecker,\n        document_organizer: DocumentOrganizer\n    ) -> DocumentationCrawler:\n        \"\"\"Configured documentation crawler.\"\"\"\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=backend_selector,\n            content_processor=content_processor,\n            quality_checker=quality_checker,\n            document_organizer=document_organizer\n        )\n\ntests/conftest.py:233: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x112e33e10>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x112dd2510>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x112dd26d0>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x112dd1850>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x112dd30d0>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_concurrent_processing", "rerun": "0"}, "5": {"status": "ERROR", "message": "backend_selector = <src.backends.selector.BackendSelector object at 0x112dd8110>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x112dda550>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x112ddad50>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x112ddaa10>\n\n    @pytest.fixture\n    def crawler(\n        backend_selector: BackendSelector,\n        content_processor: ContentProcessor,\n        quality_checker: QualityChecker,\n        document_organizer: DocumentOrganizer\n    ) -> DocumentationCrawler:\n        \"\"\"Configured documentation crawler.\"\"\"\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=backend_selector,\n            content_processor=content_processor,\n            quality_checker=quality_checker,\n            document_organizer=document_organizer\n        )\n\ntests/conftest.py:233: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x112dda050>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x112dd8110>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x112dda550>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x112ddad50>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x112ddaa10>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_rate_limiting", "rerun": "0"}, "6": {"status": "ERROR", "message": "backend_selector = <src.backends.selector.BackendSelector object at 0x112e9ac10>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x112e9aa50>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x112e9b2d0>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x112e9a950>\n\n    @pytest.fixture\n    def crawler(\n        backend_selector: BackendSelector,\n        content_processor: ContentProcessor,\n        quality_checker: QualityChecker,\n        document_organizer: DocumentOrganizer\n    ) -> DocumentationCrawler:\n        \"\"\"Configured documentation crawler.\"\"\"\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=backend_selector,\n            content_processor=content_processor,\n            quality_checker=quality_checker,\n            document_organizer=document_organizer\n        )\n\ntests/conftest.py:233: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x112e9a7d0>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x112e9ac10>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x112e9aa50>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x112e9b2d0>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x112e9a950>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_error_handling", "rerun": "0"}, "7": {"status": "ERROR", "message": "backend_selector = <src.backends.selector.BackendSelector object at 0x112e43690>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x112e43e10>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x112e40a10>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x112e43590>\n\n    @pytest.fixture\n    def crawler(\n        backend_selector: BackendSelector,\n        content_processor: ContentProcessor,\n        quality_checker: QualityChecker,\n        document_organizer: DocumentOrganizer\n    ) -> DocumentationCrawler:\n        \"\"\"Configured documentation crawler.\"\"\"\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=backend_selector,\n            content_processor=content_processor,\n            quality_checker=quality_checker,\n            document_organizer=document_organizer\n        )\n\ntests/conftest.py:233: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x112e40350>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x112e43690>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x112e43e10>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x112e40a10>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x112e43590>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_content_processing_pipeline", "rerun": "0"}, "8": {"status": "ERROR", "message": "backend_selector = <src.backends.selector.BackendSelector object at 0x112ec0f10>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x112ec1050>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x112ec0d50>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x112ec13d0>\n\n    @pytest.fixture\n    def crawler(\n        backend_selector: BackendSelector,\n        content_processor: ContentProcessor,\n        quality_checker: QualityChecker,\n        document_organizer: DocumentOrganizer\n    ) -> DocumentationCrawler:\n        \"\"\"Configured documentation crawler.\"\"\"\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=backend_selector,\n            content_processor=content_processor,\n            quality_checker=quality_checker,\n            document_organizer=document_organizer\n        )\n\ntests/conftest.py:233: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x112ec17d0>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x112ec0f10>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x112ec1050>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x112ec0d50>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x112ec13d0>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_cleanup", "rerun": "0"}, "9": {"status": "ERROR", "message": "backend_selector = <src.backends.selector.BackendSelector object at 0x112e428d0>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x112e40350>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x112e41250>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x112e43c50>\n\n    @pytest.fixture\n    def crawler(\n        backend_selector: BackendSelector,\n        content_processor: ContentProcessor,\n        quality_checker: QualityChecker,\n        document_organizer: DocumentOrganizer\n    ) -> DocumentationCrawler:\n        \"\"\"Configured documentation crawler.\"\"\"\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=backend_selector,\n            content_processor=content_processor,\n            quality_checker=quality_checker,\n            document_organizer=document_organizer\n        )\n\ntests/conftest.py:233: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x112e41d90>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x112e428d0>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x112e40350>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x112e41250>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x112e43c50>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_max_pages_limit", "rerun": "0"}, "10": {"status": "ERROR", "message": "backend_selector = <src.backends.selector.BackendSelector object at 0x112e4cb50>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x112e4c510>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x112e4ff10>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x112e4fd50>\n\n    @pytest.fixture\n    def crawler(\n        backend_selector: BackendSelector,\n        content_processor: ContentProcessor,\n        quality_checker: QualityChecker,\n        document_organizer: DocumentOrganizer\n    ) -> DocumentationCrawler:\n        \"\"\"Configured documentation crawler.\"\"\"\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=backend_selector,\n            content_processor=content_processor,\n            quality_checker=quality_checker,\n            document_organizer=document_organizer\n        )\n\ntests/conftest.py:233: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x112e4ead0>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x112e4cb50>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x112e4c510>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x112e4ff10>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x112e4fd50>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_statistics_tracking", "rerun": "0"}}, "status": {"total_pass": 0, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 0, "total_error": 11}}, "2": {"suite_name": "tests/test_integration.py", "tests": {"0": {"status": "ERROR", "message": "test_html_dir = '/private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-13/test_full_site_crawl0/test_docs'\n\n    @pytest.fixture\n    def integrated_crawler(test_html_dir: str) -> DocumentationCrawler:\n        \"\"\"Create fully integrated crawler instance.\"\"\"\n        # Configure backend\n        backend = Crawl4AIBackend(\n            config=Crawl4AIConfig(\n                max_retries=2,\n                timeout=5.0,\n                headers={\"User-Agent\": \"TestCrawler/1.0\"}\n            )\n        )\n    \n        # Configure backend selector\n        selector = BackendSelector()\n        selector.register_backend(\n            backend,\n            BackendCriteria(\n                priority=100,\n                content_types=[\"text/html\"],\n                url_patterns=[\"*\"],\n                max_load=0.8,\n                min_success_rate=0.7\n            )\n        )\n    \n        # Configure content processor\n        processor = ContentProcessor(\n            config=ProcessingConfig(\n                allowed_tags=[\n                    \"h1\", \"h2\", \"h3\", \"p\", \"pre\", \"code\",\n                    \"a\", \"ul\", \"li\", \"nav\"\n                ],\n                preserve_whitespace_elements=[\"pre\", \"code\"],\n                code_languages=[\"python\"],\n                max_heading_level=3,\n                max_content_length=10000,\n                min_content_length=10\n            )\n        )\n    \n        # Configure quality checker\n        checker = QualityChecker(\n            config=QualityConfig(\n                min_content_length=50,\n                max_content_length=10000,\n                min_headings=1,\n                max_heading_depth=3,\n                min_internal_links=1,\n                max_broken_links_ratio=0.1,\n                required_metadata_fields={\"title\", \"description\"}\n            )\n        )\n    \n        # Configure document organizer\n        organizer = DocumentOrganizer(\n            config=OrganizationConfig(\n                min_similarity_score=0.3,\n                max_versions_to_keep=3,\n                category_rules={\n                    \"api\": [\"api\", \"reference\", \"endpoint\"],\n                    \"guide\": [\"guide\", \"tutorial\", \"getting started\"],\n                    \"example\": [\"example\", \"sample\", \"demo\"]\n                }\n            )\n        )\n    \n        # Create and return crawler\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=selector,\n            content_processor=processor,\n            quality_checker=checker,\n            document_organizer=organizer\n        )\n\ntests/test_integration.py:175: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x112ed2e10>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x112ed2bd0>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x112ed2dd0>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x112ed0910>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x112ed0c10>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_full_site_crawl", "rerun": "0"}, "1": {"status": "ERROR", "message": "test_html_dir = '/private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-13/test_content_processing_pipeli0/test_docs'\n\n    @pytest.fixture\n    def integrated_crawler(test_html_dir: str) -> DocumentationCrawler:\n        \"\"\"Create fully integrated crawler instance.\"\"\"\n        # Configure backend\n        backend = Crawl4AIBackend(\n            config=Crawl4AIConfig(\n                max_retries=2,\n                timeout=5.0,\n                headers={\"User-Agent\": \"TestCrawler/1.0\"}\n            )\n        )\n    \n        # Configure backend selector\n        selector = BackendSelector()\n        selector.register_backend(\n            backend,\n            BackendCriteria(\n                priority=100,\n                content_types=[\"text/html\"],\n                url_patterns=[\"*\"],\n                max_load=0.8,\n                min_success_rate=0.7\n            )\n        )\n    \n        # Configure content processor\n        processor = ContentProcessor(\n            config=ProcessingConfig(\n                allowed_tags=[\n                    \"h1\", \"h2\", \"h3\", \"p\", \"pre\", \"code\",\n                    \"a\", \"ul\", \"li\", \"nav\"\n                ],\n                preserve_whitespace_elements=[\"pre\", \"code\"],\n                code_languages=[\"python\"],\n                max_heading_level=3,\n                max_content_length=10000,\n                min_content_length=10\n            )\n        )\n    \n        # Configure quality checker\n        checker = QualityChecker(\n            config=QualityConfig(\n                min_content_length=50,\n                max_content_length=10000,\n                min_headings=1,\n                max_heading_depth=3,\n                min_internal_links=1,\n                max_broken_links_ratio=0.1,\n                required_metadata_fields={\"title\", \"description\"}\n            )\n        )\n    \n        # Configure document organizer\n        organizer = DocumentOrganizer(\n            config=OrganizationConfig(\n                min_similarity_score=0.3,\n                max_versions_to_keep=3,\n                category_rules={\n                    \"api\": [\"api\", \"reference\", \"endpoint\"],\n                    \"guide\": [\"guide\", \"tutorial\", \"getting started\"],\n                    \"example\": [\"example\", \"sample\", \"demo\"]\n                }\n            )\n        )\n    \n        # Create and return crawler\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=selector,\n            content_processor=processor,\n            quality_checker=checker,\n            document_organizer=organizer\n        )\n\ntests/test_integration.py:175: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x112eb6490>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x112eb7350>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x112eb7790>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x112eb5f50>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x112eb7d10>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_content_processing_pipeline", "rerun": "0"}, "2": {"status": "ERROR", "message": "test_html_dir = '/private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-13/test_quality_checks0/test_docs'\n\n    @pytest.fixture\n    def integrated_crawler(test_html_dir: str) -> DocumentationCrawler:\n        \"\"\"Create fully integrated crawler instance.\"\"\"\n        # Configure backend\n        backend = Crawl4AIBackend(\n            config=Crawl4AIConfig(\n                max_retries=2,\n                timeout=5.0,\n                headers={\"User-Agent\": \"TestCrawler/1.0\"}\n            )\n        )\n    \n        # Configure backend selector\n        selector = BackendSelector()\n        selector.register_backend(\n            backend,\n            BackendCriteria(\n                priority=100,\n                content_types=[\"text/html\"],\n                url_patterns=[\"*\"],\n                max_load=0.8,\n                min_success_rate=0.7\n            )\n        )\n    \n        # Configure content processor\n        processor = ContentProcessor(\n            config=ProcessingConfig(\n                allowed_tags=[\n                    \"h1\", \"h2\", \"h3\", \"p\", \"pre\", \"code\",\n                    \"a\", \"ul\", \"li\", \"nav\"\n                ],\n                preserve_whitespace_elements=[\"pre\", \"code\"],\n                code_languages=[\"python\"],\n                max_heading_level=3,\n                max_content_length=10000,\n                min_content_length=10\n            )\n        )\n    \n        # Configure quality checker\n        checker = QualityChecker(\n            config=QualityConfig(\n                min_content_length=50,\n                max_content_length=10000,\n                min_headings=1,\n                max_heading_depth=3,\n                min_internal_links=1,\n                max_broken_links_ratio=0.1,\n                required_metadata_fields={\"title\", \"description\"}\n            )\n        )\n    \n        # Configure document organizer\n        organizer = DocumentOrganizer(\n            config=OrganizationConfig(\n                min_similarity_score=0.3,\n                max_versions_to_keep=3,\n                category_rules={\n                    \"api\": [\"api\", \"reference\", \"endpoint\"],\n                    \"guide\": [\"guide\", \"tutorial\", \"getting started\"],\n                    \"example\": [\"example\", \"sample\", \"demo\"]\n                }\n            )\n        )\n    \n        # Create and return crawler\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=selector,\n            content_processor=processor,\n            quality_checker=checker,\n            document_organizer=organizer\n        )\n\ntests/test_integration.py:175: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x112c7ef90>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x112c7e410>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x112c7e710>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x112c7c7d0>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x112c7c910>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_quality_checks", "rerun": "0"}, "3": {"status": "ERROR", "message": "test_html_dir = '/private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-13/test_document_organization0/test_docs'\n\n    @pytest.fixture\n    def integrated_crawler(test_html_dir: str) -> DocumentationCrawler:\n        \"\"\"Create fully integrated crawler instance.\"\"\"\n        # Configure backend\n        backend = Crawl4AIBackend(\n            config=Crawl4AIConfig(\n                max_retries=2,\n                timeout=5.0,\n                headers={\"User-Agent\": \"TestCrawler/1.0\"}\n            )\n        )\n    \n        # Configure backend selector\n        selector = BackendSelector()\n        selector.register_backend(\n            backend,\n            BackendCriteria(\n                priority=100,\n                content_types=[\"text/html\"],\n                url_patterns=[\"*\"],\n                max_load=0.8,\n                min_success_rate=0.7\n            )\n        )\n    \n        # Configure content processor\n        processor = ContentProcessor(\n            config=ProcessingConfig(\n                allowed_tags=[\n                    \"h1\", \"h2\", \"h3\", \"p\", \"pre\", \"code\",\n                    \"a\", \"ul\", \"li\", \"nav\"\n                ],\n                preserve_whitespace_elements=[\"pre\", \"code\"],\n                code_languages=[\"python\"],\n                max_heading_level=3,\n                max_content_length=10000,\n                min_content_length=10\n            )\n        )\n    \n        # Configure quality checker\n        checker = QualityChecker(\n            config=QualityConfig(\n                min_content_length=50,\n                max_content_length=10000,\n                min_headings=1,\n                max_heading_depth=3,\n                min_internal_links=1,\n                max_broken_links_ratio=0.1,\n                required_metadata_fields={\"title\", \"description\"}\n            )\n        )\n    \n        # Configure document organizer\n        organizer = DocumentOrganizer(\n            config=OrganizationConfig(\n                min_similarity_score=0.3,\n                max_versions_to_keep=3,\n                category_rules={\n                    \"api\": [\"api\", \"reference\", \"endpoint\"],\n                    \"guide\": [\"guide\", \"tutorial\", \"getting started\"],\n                    \"example\": [\"example\", \"sample\", \"demo\"]\n                }\n            )\n        )\n    \n        # Create and return crawler\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=selector,\n            content_processor=processor,\n            quality_checker=checker,\n            document_organizer=organizer\n        )\n\ntests/test_integration.py:175: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x112dd8bd0>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x112dd9490>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x112ddab10>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x112ddbd50>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x112ddbbd0>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_document_organization", "rerun": "0"}, "4": {"status": "ERROR", "message": "test_html_dir = '/private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-13/test_search_functionality0/test_docs'\n\n    @pytest.fixture\n    def integrated_crawler(test_html_dir: str) -> DocumentationCrawler:\n        \"\"\"Create fully integrated crawler instance.\"\"\"\n        # Configure backend\n        backend = Crawl4AIBackend(\n            config=Crawl4AIConfig(\n                max_retries=2,\n                timeout=5.0,\n                headers={\"User-Agent\": \"TestCrawler/1.0\"}\n            )\n        )\n    \n        # Configure backend selector\n        selector = BackendSelector()\n        selector.register_backend(\n            backend,\n            BackendCriteria(\n                priority=100,\n                content_types=[\"text/html\"],\n                url_patterns=[\"*\"],\n                max_load=0.8,\n                min_success_rate=0.7\n            )\n        )\n    \n        # Configure content processor\n        processor = ContentProcessor(\n            config=ProcessingConfig(\n                allowed_tags=[\n                    \"h1\", \"h2\", \"h3\", \"p\", \"pre\", \"code\",\n                    \"a\", \"ul\", \"li\", \"nav\"\n                ],\n                preserve_whitespace_elements=[\"pre\", \"code\"],\n                code_languages=[\"python\"],\n                max_heading_level=3,\n                max_content_length=10000,\n                min_content_length=10\n            )\n        )\n    \n        # Configure quality checker\n        checker = QualityChecker(\n            config=QualityConfig(\n                min_content_length=50,\n                max_content_length=10000,\n                min_headings=1,\n                max_heading_depth=3,\n                min_internal_links=1,\n                max_broken_links_ratio=0.1,\n                required_metadata_fields={\"title\", \"description\"}\n            )\n        )\n    \n        # Configure document organizer\n        organizer = DocumentOrganizer(\n            config=OrganizationConfig(\n                min_similarity_score=0.3,\n                max_versions_to_keep=3,\n                category_rules={\n                    \"api\": [\"api\", \"reference\", \"endpoint\"],\n                    \"guide\": [\"guide\", \"tutorial\", \"getting started\"],\n                    \"example\": [\"example\", \"sample\", \"demo\"]\n                }\n            )\n        )\n    \n        # Create and return crawler\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=selector,\n            content_processor=processor,\n            quality_checker=checker,\n            document_organizer=organizer\n        )\n\ntests/test_integration.py:175: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x112e31850>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x112e31a10>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x112e33410>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x112e31e50>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x112e32310>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_search_functionality", "rerun": "0"}, "5": {"status": "ERROR", "message": "test_html_dir = '/private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-13/test_error_handling_and_recove0/test_docs'\n\n    @pytest.fixture\n    def integrated_crawler(test_html_dir: str) -> DocumentationCrawler:\n        \"\"\"Create fully integrated crawler instance.\"\"\"\n        # Configure backend\n        backend = Crawl4AIBackend(\n            config=Crawl4AIConfig(\n                max_retries=2,\n                timeout=5.0,\n                headers={\"User-Agent\": \"TestCrawler/1.0\"}\n            )\n        )\n    \n        # Configure backend selector\n        selector = BackendSelector()\n        selector.register_backend(\n            backend,\n            BackendCriteria(\n                priority=100,\n                content_types=[\"text/html\"],\n                url_patterns=[\"*\"],\n                max_load=0.8,\n                min_success_rate=0.7\n            )\n        )\n    \n        # Configure content processor\n        processor = ContentProcessor(\n            config=ProcessingConfig(\n                allowed_tags=[\n                    \"h1\", \"h2\", \"h3\", \"p\", \"pre\", \"code\",\n                    \"a\", \"ul\", \"li\", \"nav\"\n                ],\n                preserve_whitespace_elements=[\"pre\", \"code\"],\n                code_languages=[\"python\"],\n                max_heading_level=3,\n                max_content_length=10000,\n                min_content_length=10\n            )\n        )\n    \n        # Configure quality checker\n        checker = QualityChecker(\n            config=QualityConfig(\n                min_content_length=50,\n                max_content_length=10000,\n                min_headings=1,\n                max_heading_depth=3,\n                min_internal_links=1,\n                max_broken_links_ratio=0.1,\n                required_metadata_fields={\"title\", \"description\"}\n            )\n        )\n    \n        # Configure document organizer\n        organizer = DocumentOrganizer(\n            config=OrganizationConfig(\n                min_similarity_score=0.3,\n                max_versions_to_keep=3,\n                category_rules={\n                    \"api\": [\"api\", \"reference\", \"endpoint\"],\n                    \"guide\": [\"guide\", \"tutorial\", \"getting started\"],\n                    \"example\": [\"example\", \"sample\", \"demo\"]\n                }\n            )\n        )\n    \n        # Create and return crawler\n>       return DocumentationCrawler(\n            config=CrawlerConfig(\n                concurrent_requests=2,\n                requests_per_second=10.0,\n                max_retries=2,\n                request_timeout=5.0\n            ),\n            backend_selector=selector,\n            content_processor=processor,\n            quality_checker=checker,\n            document_organizer=organizer\n        )\n\ntests/test_integration.py:175: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <src.crawler.DocumentationCrawler object at 0x112ebf450>\nconfig = CrawlerConfig(concurrent_requests=2, requests_per_second=10.0, max_retries=2, request_timeout=5.0, respect_robots_txt=True, follow_redirects=True, verify_ssl=True, user_agent='Lib2DocScraper/1.0', headers={})\nbackend_selector = <src.backends.selector.BackendSelector object at 0x112ebee50>\ncontent_processor = <src.processors.content_processor.ContentProcessor object at 0x112ebffd0>\nquality_checker = <src.processors.quality_checker.QualityChecker object at 0x112ebfb10>\ndocument_organizer = <src.organizers.doc_organizer.DocumentOrganizer object at 0x112ebebd0>\n\n    def __init__(\n        self,\n        config: Optional[CrawlerConfig] = None,\n        backend_selector: Optional[BackendSelector] = None,\n        content_processor: Optional[ContentProcessor] = None,\n        quality_checker: Optional[QualityChecker] = None,\n        document_organizer: Optional[DocumentOrganizer] = None\n    ) -> None:\n        \"\"\"\n        Initialize the documentation crawler.\n    \n        Args:\n            config: Optional crawler configuration\n            backend_selector: Optional backend selector\n            content_processor: Optional content processor\n            quality_checker: Optional quality checker\n            document_organizer: Optional document organizer\n        \"\"\"\n        self.config = config or CrawlerConfig()\n        self.backend_selector = backend_selector or BackendSelector()\n        self.content_processor = content_processor or ContentProcessor()\n        self.quality_checker = quality_checker or QualityChecker()\n        self.document_organizer = document_organizer or DocumentOrganizer()\n    \n        self.rate_limiter = RateLimiter(self.config.requests_per_second)\n        self.retry_strategy = RetryStrategy(\n            max_retries=self.config.max_retries\n        )\n    \n        self._crawled_urls: Set[str] = set()\n        self._processing_semaphore = asyncio.Semaphore(\n            self.config.concurrent_requests\n        )\n        self.client_session = None\n>       self._setup_backends()\nE       AttributeError: 'DocumentationCrawler' object has no attribute '_setup_backends'\n\nsrc/crawler.py:101: AttributeError\n", "test_name": "test_error_handling_and_recovery", "rerun": "0"}}, "status": {"total_pass": 0, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 0, "total_error": 6}}, "3": {"suite_name": "tests/test_organizer.py", "tests": {"0": {"status": "PASS", "message": "", "test_name": "test_document_organizer_initialization", "rerun": "0"}, "1": {"status": "FAIL", "message": "       AttributeError: 'DocumentOrganizer' object has no attribute '_extract_tags'\n", "test_name": "test_document_version_management", "rerun": "0"}, "2": {"status": "FAIL", "message": "       AttributeError: 'DocumentOrganizer' object has no attribute '_extract_tags'\n", "test_name": "test_document_categorization", "rerun": "0"}, "3": {"status": "FAIL", "message": "       AttributeError: 'DocumentOrganizer' object has no attribute '_extract_tags'\n", "test_name": "test_reference_extraction", "rerun": "0"}, "4": {"status": "FAIL", "message": "       AttributeError: 'DocumentOrganizer' object has no attribute '_extract_tags'\n", "test_name": "test_search_functionality", "rerun": "0"}, "5": {"status": "FAIL", "message": "       AttributeError: 'DocumentOrganizer' object has no attribute '_extract_tags'\n", "test_name": "test_collection_management", "rerun": "0"}, "6": {"status": "FAIL", "message": "       AttributeError: 'DocumentOrganizer' object has no attribute '_extract_tags'\n", "test_name": "test_document_similarity", "rerun": "0"}, "7": {"status": "FAIL", "message": "       AttributeError: 'DocumentOrganizer' object has no attribute '_extract_tags'\n", "test_name": "test_version_tracking", "rerun": "0"}, "8": {"status": "FAIL", "message": "       AttributeError: 'DocumentOrganizer' object has no attribute '_extract_tags'\n", "test_name": "test_search_index_generation", "rerun": "0"}}, "status": {"total_pass": 1, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 8, "total_error": 0}}, "4": {"suite_name": "tests/test_processor.py", "tests": {"0": {"status": "PASS", "message": "", "test_name": "test_content_processor_initialization", "rerun": "0"}, "1": {"status": "PASS", "message": "", "test_name": "test_processing_rule_validation", "rerun": "0"}, "2": {"status": "PASS", "message": "", "test_name": "test_clean_text_processing", "rerun": "0"}, "3": {"status": "PASS", "message": "", "test_name": "test_code_language_detection", "rerun": "0"}, "4": {"status": "PASS", "message": "", "test_name": "test_code_block_processing", "rerun": "0"}, "5": {"status": "PASS", "message": "", "test_name": "test_link_processing", "rerun": "0"}, "6": {"status": "PASS", "message": "", "test_name": "test_metadata_extraction", "rerun": "0"}, "7": {"status": "PASS", "message": "", "test_name": "test_asset_collection", "rerun": "0"}, "8": {"status": "FAIL", "message": "   assert 0 > 0\n    +  where 0 = len([])\n    +    where [] = <built-in method get of dict object at 0x112f86600>('links', [])\n    +      where <built-in method get of dict object at 0x112f86600> = {'code_blocks': [{'content': 'def hello(): print(\"Hello, World!\")', 'language': 'python'}], 'headings': [{'level': 1, ...t Main Title First paragraph with link. Section 1 Second paragraph. def hello(): print(\"Hello, World!\") Item 1 Item 2'}.get\n    +        where {'code_blocks': [{'content': 'def hello(): print(\"Hello, World!\")', 'language': 'python'}], 'headings': [{'level': 1, ...t Main Title First paragraph with link. Section 1 Second paragraph. def hello(): print(\"Hello, World!\") Item 1 Item 2'} = ProcessedContent(url='https://example.com/test', title='Sample Document', content={'text': 'Sample Document Main Title...locks': [{'language': 'python', 'content': 'def hello(): print(\"Hello, World!\")'}]}, metadata={}, assets={}, errors=[]).content\n", "test_name": "test_full_content_processing", "rerun": "0"}, "9": {"status": "FAIL", "message": "   AssertionError: assert not {'code_blocks': [], 'headings': [], 'text': 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx...xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'}\n    +  where {'code_blocks': [], 'headings': [], 'text': 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx...xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'} = ProcessedContent(url='https://example.com', title='Untitled Document', content={'text': 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxx...xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx', 'headings': [], 'code_blocks': []}, metadata={}, assets={}, errors=[]).content\n", "test_name": "test_content_size_limits", "rerun": "0"}, "10": {"status": "PASS", "message": "", "test_name": "test_malformed_html_handling", "rerun": "0"}, "11": {"status": "PASS", "message": "", "test_name": "test_special_content_handling", "rerun": "0"}, "12": {"status": "PASS", "message": "", "test_name": "test_content_structure_preservation", "rerun": "0"}}, "status": {"total_pass": 11, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 2, "total_error": 0}}, "5": {"suite_name": "tests/test_quality.py", "tests": {"0": {"status": "PASS", "message": "", "test_name": "test_quality_checker_initialization", "rerun": "0"}, "1": {"status": "PASS", "message": "", "test_name": "test_content_length_check", "rerun": "0"}, "2": {"status": "PASS", "message": "", "test_name": "test_heading_structure_check", "rerun": "0"}, "3": {"status": "PASS", "message": "", "test_name": "test_link_validation", "rerun": "0"}, "4": {"status": "PASS", "message": "", "test_name": "test_metadata_check", "rerun": "0"}, "5": {"status": "PASS", "message": "", "test_name": "test_code_block_validation", "rerun": "0"}, "6": {"status": "PASS", "message": "", "test_name": "test_quality_metrics_calculation", "rerun": "0"}, "7": {"status": "PASS", "message": "", "test_name": "test_full_quality_check", "rerun": "0"}, "8": {"status": "PASS", "message": "", "test_name": "test_quality_checker_cleanup", "rerun": "0"}}, "status": {"total_pass": 9, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 0, "total_error": 0}}}}, "date": "November 25, 2024", "start_time": 1732556051.8287308, "total_suite": 6, "status": "FAIL", "status_list": {"pass": "29", "fail": "11", "skip": "0", "error": "17", "xpass": "0", "xfail": "0", "rerun": "0"}, "total_tests": "57"}