{"content": {"suites": {"0": {"status": {"total_pass": 7, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 2, "total_error": 0}, "tests": {"0": {"status": "PASS", "message": "", "test_name": "test_crawler_backend_lifecycle", "rerun": "0"}, "1": {"status": "PASS", "message": "", "test_name": "test_crawler_backend_error_handling", "rerun": "0"}, "2": {"status": "PASS", "message": "", "test_name": "test_backend_selector_registration", "rerun": "0"}, "3": {"status": "PASS", "message": "", "test_name": "test_backend_selector_selection", "rerun": "0"}, "4": {"status": "FAIL", "message": "   AssertionError: assert ':80' not in 'http://example.com:80/path'\n     \n     ':80' is contained here:\n       http://example.com:80/path\n     ?                   +++\n", "test_name": "test_url_normalization", "rerun": "0"}, "5": {"status": "PASS", "message": "", "test_name": "test_processed_content_validation", "rerun": "0"}, "6": {"status": "PASS", "message": "", "test_name": "test_concurrent_backend_operations", "rerun": "0"}, "7": {"status": "PASS", "message": "", "test_name": "test_backend_metrics_update", "rerun": "0"}, "8": {"status": "FAIL", "message": "   Failed: DID NOT RAISE <class 'ValueError'>\n", "test_name": "test_backend_selector_error_handling", "rerun": "0"}}, "suite_name": "tests/test_base.py"}, "1": {"suite_name": "tests/test_crawler.py", "tests": {"0": {"status": "PASS", "message": "", "test_name": "test_crawler_initialization", "rerun": "0"}, "1": {"status": "PASS", "message": "", "test_name": "test_url_filtering", "rerun": "0"}, "2": {"status": "FAIL", "message": "   assert None is not None\n", "test_name": "test_single_url_processing", "rerun": "0"}, "3": {"status": "FAIL", "message": "   AssertionError: assert 0 > 0\n    +  where 0 = CrawlStats(start_time=datetime.datetime(2024, 11, 25, 22, 47, 59, 271173), end_time=datetime.datetime(2024, 11, 25, 22...ccessful_crawls=0, failed_crawls=0, total_time=8.1e-05, average_time_per_page=0.0, quality_issues=0, bytes_processed=0).pages_crawled\n    +    where CrawlStats(start_time=datetime.datetime(2024, 11, 25, 22, 47, 59, 271173), end_time=datetime.datetime(2024, 11, 25, 22...ccessful_crawls=0, failed_crawls=0, total_time=8.1e-05, average_time_per_page=0.0, quality_issues=0, bytes_processed=0) = CrawlResult(target=CrawlTarget(url='https://example.com/doc2', depth=2, follow_external=False, content_types=['text/ht...tal_time=8.1e-05, average_time_per_page=0.0, quality_issues=0, bytes_processed=0), documents=[], issues=[], metrics={}).stats\n", "test_name": "test_depth_limited_crawling", "rerun": "0"}, "4": {"status": "FAIL", "message": "   AssertionError: assert 0 == 3\n    +  where 0 = CrawlStats(start_time=datetime.datetime(2024, 11, 25, 20, 47, 59, 274619), end_time=None, pages_crawled=0, successful_crawls=0, failed_crawls=0, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0).pages_crawled\n    +  and   3 = len({'https://example.com/doc1', 'https://example.com/doc2', 'https://example.com/doc3'})\n", "test_name": "test_concurrent_processing", "rerun": "0"}, "5": {"status": "PASS", "message": "", "test_name": "test_rate_limiting", "rerun": "0"}, "6": {"status": "FAIL", "message": "   AssertionError: assert 0 > 0\n    +  where 0 = CrawlStats(start_time=datetime.datetime(2024, 11, 25, 22, 47, 59, 279464), end_time=datetime.datetime(2024, 11, 25, 22...ccessful_crawls=0, failed_crawls=0, total_time=4.7e-05, average_time_per_page=0.0, quality_issues=0, bytes_processed=0).failed_crawls\n    +    where CrawlStats(start_time=datetime.datetime(2024, 11, 25, 22, 47, 59, 279464), end_time=datetime.datetime(2024, 11, 25, 22...ccessful_crawls=0, failed_crawls=0, total_time=4.7e-05, average_time_per_page=0.0, quality_issues=0, bytes_processed=0) = CrawlResult(target=CrawlTarget(url='https://invalid.example.com', depth=1, follow_external=False, content_types=['text...tal_time=4.7e-05, average_time_per_page=0.0, quality_issues=0, bytes_processed=0), documents=[], issues=[], metrics={}).stats\n", "test_name": "test_error_handling", "rerun": "0"}, "7": {"status": "FAIL", "message": "   AssertionError: assert 0 == 1\n    +  where 0 = CrawlStats(start_time=datetime.datetime(2024, 11, 25, 22, 47, 59, 282945), end_time=datetime.datetime(2024, 11, 25, 22...ccessful_crawls=0, failed_crawls=0, total_time=8.3e-05, average_time_per_page=0.0, quality_issues=0, bytes_processed=0).successful_crawls\n    +    where CrawlStats(start_time=datetime.datetime(2024, 11, 25, 22, 47, 59, 282945), end_time=datetime.datetime(2024, 11, 25, 22...ccessful_crawls=0, failed_crawls=0, total_time=8.3e-05, average_time_per_page=0.0, quality_issues=0, bytes_processed=0) = CrawlResult(target=CrawlTarget(url='https://example.com/doc2', depth=1, follow_external=False, content_types=['text/ht...tal_time=8.3e-05, average_time_per_page=0.0, quality_issues=0, bytes_processed=0), documents=[], issues=[], metrics={}).stats\n", "test_name": "test_content_processing_pipeline", "rerun": "0"}, "8": {"status": "PASS", "message": "", "test_name": "test_cleanup", "rerun": "0"}, "9": {"status": "PASS", "message": "", "test_name": "test_max_pages_limit", "rerun": "0"}, "10": {"status": "FAIL", "message": "   AssertionError: assert 0.0 > 0\n    +  where 0.0 = CrawlStats(start_time=datetime.datetime(2024, 11, 25, 22, 47, 59, 288855), end_time=datetime.datetime(2024, 11, 25, 22...ccessful_crawls=0, failed_crawls=0, total_time=3.4e-05, average_time_per_page=0.0, quality_issues=0, bytes_processed=0).average_time_per_page\n    +    where CrawlStats(start_time=datetime.datetime(2024, 11, 25, 22, 47, 59, 288855), end_time=datetime.datetime(2024, 11, 25, 22...ccessful_crawls=0, failed_crawls=0, total_time=3.4e-05, average_time_per_page=0.0, quality_issues=0, bytes_processed=0) = CrawlResult(target=CrawlTarget(url='https://example.com/doc2', depth=1, follow_external=False, content_types=['text/ht...tal_time=3.4e-05, average_time_per_page=0.0, quality_issues=0, bytes_processed=0), documents=[], issues=[], metrics={}).stats\n", "test_name": "test_statistics_tracking", "rerun": "0"}}, "status": {"total_pass": 5, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 6, "total_error": 0}}, "2": {"suite_name": "tests/test_integration.py", "tests": {"0": {"status": "FAIL", "message": "   AssertionError: assert 0 > 0\n    +  where 0 = CrawlStats(start_time=datetime.datetime(2024, 11, 25, 22, 47, 59, 297356), end_time=datetime.datetime(2024, 11, 25, 22...ccessful_crawls=0, failed_crawls=0, total_time=0.00013, average_time_per_page=0.0, quality_issues=0, bytes_processed=0).pages_crawled\n    +    where CrawlStats(start_time=datetime.datetime(2024, 11, 25, 22, 47, 59, 297356), end_time=datetime.datetime(2024, 11, 25, 22...ccessful_crawls=0, failed_crawls=0, total_time=0.00013, average_time_per_page=0.0, quality_issues=0, bytes_processed=0) = CrawlResult(target=CrawlTarget(url='file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pyt...tal_time=0.00013, average_time_per_page=0.0, quality_issues=0, bytes_processed=0), documents=[], issues=[], metrics={}).stats\n", "test_name": "test_full_site_crawl", "rerun": "0"}, "1": {"status": "FAIL", "message": "   AssertionError: assert 0 == 1\n    +  where 0 = len([])\n    +    where [] = CrawlResult(target=CrawlTarget(url='file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pyt...al_time=0.000112, average_time_per_page=0.0, quality_issues=0, bytes_processed=0), documents=[], issues=[], metrics={}).documents\n", "test_name": "test_content_processing_pipeline", "rerun": "0"}, "2": {"status": "FAIL", "message": "   AssertionError: assert 0 > 0\n    +  where 0 = len({})\n    +    where {} = CrawlResult(target=CrawlTarget(url='file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pyt...al_time=0.000118, average_time_per_page=0.0, quality_issues=0, bytes_processed=0), documents=[], issues=[], metrics={}).metrics\n", "test_name": "test_quality_checks", "rerun": "0"}, "3": {"status": "PASS", "message": "", "test_name": "test_document_organization", "rerun": "0"}, "4": {"status": "FAIL", "message": "   assert 0 > 0\n    +  where 0 = len([])\n", "test_name": "test_search_functionality", "rerun": "0"}, "5": {"status": "FAIL", "message": "   AssertionError: assert 0 > 0\n    +  where 0 = CrawlStats(start_time=datetime.datetime(2024, 11, 25, 22, 47, 59, 339276), end_time=datetime.datetime(2024, 11, 25, 22...ccessful_crawls=0, failed_crawls=0, total_time=7.8e-05, average_time_per_page=0.0, quality_issues=0, bytes_processed=0).successful_crawls\n    +    where CrawlStats(start_time=datetime.datetime(2024, 11, 25, 22, 47, 59, 339276), end_time=datetime.datetime(2024, 11, 25, 22...ccessful_crawls=0, failed_crawls=0, total_time=7.8e-05, average_time_per_page=0.0, quality_issues=0, bytes_processed=0) = CrawlResult(target=CrawlTarget(url='file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pyt...tal_time=7.8e-05, average_time_per_page=0.0, quality_issues=0, bytes_processed=0), documents=[], issues=[], metrics={}).stats\n", "test_name": "test_error_handling_and_recovery", "rerun": "0"}}, "status": {"total_pass": 1, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 5, "total_error": 0}}, "3": {"suite_name": "tests/test_organizer.py", "tests": {"0": {"status": "PASS", "message": "", "test_name": "test_document_organizer_initialization", "rerun": "0"}, "1": {"status": "PASS", "message": "", "test_name": "test_document_version_management", "rerun": "0"}, "2": {"status": "FAIL", "message": "   AssertionError: assert 'example' == 'uncategorized'\n     \n     - uncategorized\n     + example\n", "test_name": "test_document_categorization", "rerun": "0"}, "3": {"status": "PASS", "message": "", "test_name": "test_reference_extraction", "rerun": "0"}, "4": {"status": "PASS", "message": "", "test_name": "test_search_functionality", "rerun": "0"}, "5": {"status": "PASS", "message": "", "test_name": "test_collection_management", "rerun": "0"}, "6": {"status": "PASS", "message": "", "test_name": "test_document_similarity", "rerun": "0"}, "7": {"status": "PASS", "message": "", "test_name": "test_version_tracking", "rerun": "0"}, "8": {"status": "FAIL", "message": "   assert 0 > 0\n    +  where 0 = len({})\n    +    where {} = <src.organizers.doc_organizer.DocumentOrganizer object at 0x105833d10>.search_indices\n", "test_name": "test_search_index_generation", "rerun": "0"}}, "status": {"total_pass": 7, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 2, "total_error": 0}}, "4": {"suite_name": "tests/test_processor.py", "tests": {"0": {"status": "PASS", "message": "", "test_name": "test_content_processor_initialization", "rerun": "0"}, "1": {"status": "FAIL", "message": "   AttributeError: 'ContentProcessor' object has no attribute '_clean_text'\n", "test_name": "test_clean_text_processing", "rerun": "0"}, "2": {"status": "FAIL", "message": "   AttributeError: 'ContentProcessor' object has no attribute '_extract_code_language'\n", "test_name": "test_code_language_detection", "rerun": "0"}, "3": {"status": "FAIL", "message": "   AttributeError: 'ContentProcessor' object has no attribute '_process_code_blocks'\n", "test_name": "test_code_block_processing", "rerun": "0"}, "4": {"status": "FAIL", "message": "   AttributeError: 'ContentProcessor' object has no attribute '_process_links'\n", "test_name": "test_link_processing", "rerun": "0"}, "5": {"status": "FAIL", "message": "   AttributeError: 'ContentProcessor' object has no attribute '_extract_metadata'\n", "test_name": "test_metadata_extraction", "rerun": "0"}, "6": {"status": "FAIL", "message": "   AttributeError: 'ContentProcessor' object has no attribute '_collect_assets'\n", "test_name": "test_asset_collection", "rerun": "0"}, "7": {"status": "PASS", "message": "", "test_name": "test_full_content_processing", "rerun": "0"}, "8": {"status": "FAIL", "message": "   AssertionError: assert not {'code_blocks': [], 'headings': [], 'links': [], 'text': 'xxxxxxxxx'}\n    +  where {'code_blocks': [], 'headings': [], 'links': [], 'text': 'xxxxxxxxx'} = ProcessedContent(url='https://example.com', title='Untitled Document', content={'text': 'xxxxxxxxx', 'headings': [], '...open_graph': {}, 'twitter_cards': {}}, assets={'images': [], 'stylesheets': [], 'scripts': [], 'media': []}, errors=[]).content\n", "test_name": "test_content_size_limits", "rerun": "0"}, "9": {"status": "PASS", "message": "", "test_name": "test_malformed_html_handling", "rerun": "0"}, "10": {"status": "PASS", "message": "", "test_name": "test_special_content_handling", "rerun": "0"}, "11": {"status": "PASS", "message": "", "test_name": "test_content_structure_preservation", "rerun": "0"}}, "status": {"total_pass": 5, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 7, "total_error": 0}}, "5": {"suite_name": "tests/test_quality.py", "tests": {"0": {"status": "PASS", "message": "", "test_name": "test_quality_checker_initialization", "rerun": "0"}, "1": {"status": "PASS", "message": "", "test_name": "test_content_length_check", "rerun": "0"}, "2": {"status": "PASS", "message": "", "test_name": "test_heading_structure_check", "rerun": "0"}, "3": {"status": "PASS", "message": "", "test_name": "test_link_validation", "rerun": "0"}, "4": {"status": "PASS", "message": "", "test_name": "test_metadata_check", "rerun": "0"}, "5": {"status": "PASS", "message": "", "test_name": "test_code_block_validation", "rerun": "0"}, "6": {"status": "PASS", "message": "", "test_name": "test_quality_metrics_calculation", "rerun": "0"}, "7": {"status": "PASS", "message": "", "test_name": "test_full_quality_check", "rerun": "0"}, "8": {"status": "PASS", "message": "", "test_name": "test_quality_checker_cleanup", "rerun": "0"}}, "status": {"total_pass": 9, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 0, "total_error": 0}}}}, "date": "November 25, 2024", "start_time": 1732567680.378284, "total_suite": 6, "status": "FAIL", "status_list": {"pass": "34", "fail": "22", "skip": "0", "error": "0", "xpass": "0", "xfail": "0", "rerun": "0"}, "total_tests": "56"}