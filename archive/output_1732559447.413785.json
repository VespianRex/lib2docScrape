{"content": {"suites": {"0": {"status": {"total_pass": 7, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 2, "total_error": 0}, "tests": {"0": {"status": "PASS", "message": "", "test_name": "test_crawler_backend_lifecycle", "rerun": "0"}, "1": {"status": "PASS", "message": "", "test_name": "test_crawler_backend_error_handling", "rerun": "0"}, "2": {"status": "PASS", "message": "", "test_name": "test_backend_selector_registration", "rerun": "0"}, "3": {"status": "PASS", "message": "", "test_name": "test_backend_selector_selection", "rerun": "0"}, "4": {"status": "FAIL", "message": "   AssertionError: assert False\n    +  where False = URLInfo(original='https://example.com/path?q=test', normalized='https://example.com/path?q=test', scheme='https', netloc='example.com', path='/path', is_valid=False).is_valid\n", "test_name": "test_url_normalization", "rerun": "0"}, "5": {"status": "PASS", "message": "", "test_name": "test_processed_content_validation", "rerun": "0"}, "6": {"status": "PASS", "message": "", "test_name": "test_concurrent_backend_operations", "rerun": "0"}, "7": {"status": "PASS", "message": "", "test_name": "test_backend_metrics_update", "rerun": "0"}, "8": {"status": "FAIL", "message": "   Failed: DID NOT RAISE <class 'ValueError'>\n", "test_name": "test_backend_selector_error_handling", "rerun": "0"}}, "suite_name": "tests/test_base.py"}, "1": {"suite_name": "tests/test_crawler.py", "tests": {"0": {"status": "PASS", "message": "", "test_name": "test_crawler_initialization", "rerun": "0"}, "1": {"status": "FAIL", "message": "   AssertionError: assert False\n    +  where False = _should_crawl_url('https://example.com/doc1', CrawlTarget(url='https://example.com/doc1', depth=2, follow_external=False, content_types=['text/html'], exclude_patterns=['/excluded/'], required_patterns=['/doc'], max_pages=10))\n    +    where _should_crawl_url = <src.crawler.DocumentationCrawler object at 0x105fc6510>._should_crawl_url\n", "test_name": "test_url_filtering", "rerun": "0"}, "2": {"status": "FAIL", "message": "   assert None is not None\n", "test_name": "test_single_url_processing", "rerun": "0"}, "3": {"status": "FAIL", "message": "   AssertionError: assert 0 > 0\n    +  where 0 = CrawlStats(start_time=datetime.datetime(2024, 11, 25, 20, 23, 43, 784367), end_time=datetime.datetime(2024, 11, 25, 20, 23, 43, 784538), pages_crawled=0, successful_crawls=0, failed_crawls=0, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0).pages_crawled\n    +    where CrawlStats(start_time=datetime.datetime(2024, 11, 25, 20, 23, 43, 784367), end_time=datetime.datetime(2024, 11, 25, 20, 23, 43, 784538), pages_crawled=0, successful_crawls=0, failed_crawls=0, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0) = CrawlResult(target=CrawlTarget(url='https://example.com/doc1', depth=2, follow_external=False, content_types=['text/html'], exclude_patterns=[], required_patterns=['/3/'], max_pages=10), stats=CrawlStats(start_time=datetime.datetime(2024, 11, 25, 20, 23, 43, 784367), end_time=datetime.datetime(2024, 11, 25, 20, 23, 43, 784538), pages_crawled=0, successful_crawls=0, failed_crawls=0, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0), documents=[], issues=[], metrics={}).stats\n", "test_name": "test_depth_limited_crawling", "rerun": "0"}, "4": {"status": "FAIL", "message": "   AssertionError: assert 0 == 3\n    +  where 0 = CrawlStats(start_time=datetime.datetime(2024, 11, 25, 18, 23, 43, 787754), end_time=None, pages_crawled=0, successful_crawls=0, failed_crawls=0, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0).pages_crawled\n    +  and   3 = len({'https://example.com/doc1', 'https://example.com/doc2', 'https://example.com/doc3'})\n", "test_name": "test_concurrent_processing", "rerun": "0"}, "5": {"status": "PASS", "message": "", "test_name": "test_rate_limiting", "rerun": "0"}, "6": {"status": "FAIL", "message": "   AssertionError: assert 0 > 0\n    +  where 0 = CrawlStats(start_time=datetime.datetime(2024, 11, 25, 20, 23, 43, 996150), end_time=datetime.datetime(2024, 11, 25, 20, 23, 43, 996291), pages_crawled=0, successful_crawls=0, failed_crawls=0, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0).failed_crawls\n    +    where CrawlStats(start_time=datetime.datetime(2024, 11, 25, 20, 23, 43, 996150), end_time=datetime.datetime(2024, 11, 25, 20, 23, 43, 996291), pages_crawled=0, successful_crawls=0, failed_crawls=0, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0) = CrawlResult(target=CrawlTarget(url='https://invalid.example.com', depth=1, follow_external=False, content_types=['text/html'], exclude_patterns=[], required_patterns=['/3/'], max_pages=1), stats=CrawlStats(start_time=datetime.datetime(2024, 11, 25, 20, 23, 43, 996150), end_time=datetime.datetime(2024, 11, 25, 20, 23, 43, 996291), pages_crawled=0, successful_crawls=0, failed_crawls=0, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0), documents=[], issues=[], metrics={}).stats\n", "test_name": "test_error_handling", "rerun": "0"}, "7": {"status": "FAIL", "message": "   AssertionError: assert 0 == 1\n    +  where 0 = CrawlStats(start_time=datetime.datetime(2024, 11, 25, 20, 23, 43, 999755), end_time=datetime.datetime(2024, 11, 25, 20, 23, 43, 999906), pages_crawled=0, successful_crawls=0, failed_crawls=0, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0).successful_crawls\n    +    where CrawlStats(start_time=datetime.datetime(2024, 11, 25, 20, 23, 43, 999755), end_time=datetime.datetime(2024, 11, 25, 20, 23, 43, 999906), pages_crawled=0, successful_crawls=0, failed_crawls=0, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0) = CrawlResult(target=CrawlTarget(url='https://example.com/doc1', depth=1, follow_external=False, content_types=['text/html'], exclude_patterns=[], required_patterns=['/3/'], max_pages=1), stats=CrawlStats(start_time=datetime.datetime(2024, 11, 25, 20, 23, 43, 999755), end_time=datetime.datetime(2024, 11, 25, 20, 23, 43, 999906), pages_crawled=0, successful_crawls=0, failed_crawls=0, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0), documents=[], issues=[], metrics={}).stats\n", "test_name": "test_content_processing_pipeline", "rerun": "0"}, "8": {"status": "FAIL", "message": "   assert <aiohttp.client.ClientSession object at 0x105fa2dd0> is None\n    +  where <aiohttp.client.ClientSession object at 0x105fa2dd0> = <src.crawler.DocumentationCrawler object at 0x105fc4dd0>.client_session\n", "test_name": "test_cleanup", "rerun": "0"}, "9": {"status": "PASS", "message": "", "test_name": "test_max_pages_limit", "rerun": "0"}, "10": {"status": "FAIL", "message": "   AssertionError: assert 0.0 > 0\n    +  where 0.0 = CrawlStats(start_time=datetime.datetime(2024, 11, 25, 20, 23, 44, 8462), end_time=datetime.datetime(2024, 11, 25, 20, 23, 44, 8571), pages_crawled=0, successful_crawls=0, failed_crawls=0, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0).total_time\n    +    where CrawlStats(start_time=datetime.datetime(2024, 11, 25, 20, 23, 44, 8462), end_time=datetime.datetime(2024, 11, 25, 20, 23, 44, 8571), pages_crawled=0, successful_crawls=0, failed_crawls=0, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0) = CrawlResult(target=CrawlTarget(url='https://example.com/doc1', depth=1, follow_external=False, content_types=['text/html'], exclude_patterns=[], required_patterns=['/3/'], max_pages=3), stats=CrawlStats(start_time=datetime.datetime(2024, 11, 25, 20, 23, 44, 8462), end_time=datetime.datetime(2024, 11, 25, 20, 23, 44, 8571), pages_crawled=0, successful_crawls=0, failed_crawls=0, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0), documents=[], issues=[], metrics={}).stats\n", "test_name": "test_statistics_tracking", "rerun": "0"}}, "status": {"total_pass": 3, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 8, "total_error": 0}}, "2": {"suite_name": "tests/test_integration.py", "tests": {"0": {"status": "FAIL", "message": "   AssertionError: assert 0 > 0\n    +  where 0 = CrawlStats(start_time=datetime.datetime(2024, 11, 25, 20, 23, 44, 15522), end_time=datetime.datetime(2024, 11, 25, 20, 23, 44, 15662), pages_crawled=0, successful_crawls=0, failed_crawls=0, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0).pages_crawled\n    +    where CrawlStats(start_time=datetime.datetime(2024, 11, 25, 20, 23, 44, 15522), end_time=datetime.datetime(2024, 11, 25, 20, 23, 44, 15662), pages_crawled=0, successful_crawls=0, failed_crawls=0, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0) = CrawlResult(target=CrawlTarget(url='file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-19/test_full_site_crawl0/test_docs/index.html', depth=2, follow_external=False, content_types=['text/html'], exclude_patterns=[], required_patterns=['.html'], max_pages=10), stats=CrawlStats(start_time=datetime.datetime(2024, 11, 25, 20, 23, 44, 15522), end_time=datetime.datetime(2024, 11, 25, 20, 23, 44, 15662), pages_crawled=0, successful_crawls=0, failed_crawls=0, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0), documents=[], issues=[], metrics={}).stats\n", "test_name": "test_full_site_crawl", "rerun": "0"}, "1": {"status": "FAIL", "message": "   AssertionError: assert 0 == 1\n    +  where 0 = len([])\n    +    where [] = CrawlResult(target=CrawlTarget(url='file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-19/test_content_processing_pipeli0/test_docs/guide.html', depth=1, follow_external=False, content_types=['text/html'], exclude_patterns=[], required_patterns=['/3/'], max_pages=1), stats=CrawlStats(start_time=datetime.datetime(2024, 11, 25, 20, 23, 44, 20198), end_time=datetime.datetime(2024, 11, 25, 20, 23, 44, 20326), pages_crawled=0, successful_crawls=0, failed_crawls=0, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0), documents=[], issues=[], metrics={}).documents\n", "test_name": "test_content_processing_pipeline", "rerun": "0"}, "2": {"status": "FAIL", "message": "   AssertionError: assert 0 > 0\n    +  where 0 = len({})\n    +    where {} = CrawlResult(target=CrawlTarget(url='file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-19/test_quality_checks0/test_docs/api.html', depth=1, follow_external=False, content_types=['text/html'], exclude_patterns=[], required_patterns=['/3/'], max_pages=1), stats=CrawlStats(start_time=datetime.datetime(2024, 11, 25, 20, 23, 44, 24131), end_time=datetime.datetime(2024, 11, 25, 20, 23, 44, 24257), pages_crawled=0, successful_crawls=0, failed_crawls=0, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0), documents=[], issues=[], metrics={}).metrics\n", "test_name": "test_quality_checks", "rerun": "0"}, "3": {"status": "PASS", "message": "", "test_name": "test_document_organization", "rerun": "0"}, "4": {"status": "FAIL", "message": "   assert 0 > 0\n    +  where 0 = len([])\n", "test_name": "test_search_functionality", "rerun": "0"}, "5": {"status": "FAIL", "message": "   AssertionError: assert 0 > 0\n    +  where 0 = CrawlStats(start_time=datetime.datetime(2024, 11, 25, 20, 23, 44, 33675), end_time=datetime.datetime(2024, 11, 25, 20, 23, 44, 33797), pages_crawled=0, successful_crawls=0, failed_crawls=0, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0).successful_crawls\n    +    where CrawlStats(start_time=datetime.datetime(2024, 11, 25, 20, 23, 44, 33675), end_time=datetime.datetime(2024, 11, 25, 20, 23, 44, 33797), pages_crawled=0, successful_crawls=0, failed_crawls=0, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0) = CrawlResult(target=CrawlTarget(url='file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-19/test_error_handling_and_recove0/test_docs/index.html', depth=2, follow_external=False, content_types=['text/html'], exclude_patterns=[], required_patterns=['.html', '.invalid'], max_pages=10), stats=CrawlStats(start_time=datetime.datetime(2024, 11, 25, 20, 23, 44, 33675), end_time=datetime.datetime(2024, 11, 25, 20, 23, 44, 33797), pages_crawled=0, successful_crawls=0, failed_crawls=0, total_time=0.0, average_time_per_page=0.0, quality_issues=0, bytes_processed=0), documents=[], issues=[], metrics={}).stats\n", "test_name": "test_error_handling_and_recovery", "rerun": "0"}}, "status": {"total_pass": 1, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 5, "total_error": 0}}, "3": {"suite_name": "tests/test_organizer.py", "tests": {"0": {"status": "PASS", "message": "", "test_name": "test_document_organizer_initialization", "rerun": "0"}, "1": {"status": "FAIL", "message": "       pydantic_core._pydantic_core.ValidationError: 1 validation error for DocumentMetadata\n       last_updated\n         Field required [type=missing, input_value={'title': 'Test Document'...': ['test', 'document']}, input_type=dict]\n           For further information visit https://errors.pydantic.dev/2.10/v/missing\n", "test_name": "test_document_version_management", "rerun": "0"}, "2": {"status": "FAIL", "message": "       pydantic_core._pydantic_core.ValidationError: 1 validation error for DocumentMetadata\n       last_updated\n         Field required [type=missing, input_value={'title': 'API Reference'...x_terms': ['reference']}, input_type=dict]\n           For further information visit https://errors.pydantic.dev/2.10/v/missing\n", "test_name": "test_document_categorization", "rerun": "0"}, "3": {"status": "FAIL", "message": "       pydantic_core._pydantic_core.ValidationError: 1 validation error for DocumentMetadata\n       last_updated\n         Field required [type=missing, input_value={'title': 'Test Document'..., 'document', 'python']}, input_type=dict]\n           For further information visit https://errors.pydantic.dev/2.10/v/missing\n", "test_name": "test_reference_extraction", "rerun": "0"}, "4": {"status": "FAIL", "message": "       pydantic_core._pydantic_core.ValidationError: 1 validation error for DocumentMetadata\n       last_updated\n         Field required [type=missing, input_value={'title': 'Python Program...programming', 'python']}, input_type=dict]\n           For further information visit https://errors.pydantic.dev/2.10/v/missing\n", "test_name": "test_search_functionality", "rerun": "0"}, "5": {"status": "FAIL", "message": "       pydantic_core._pydantic_core.ValidationError: 1 validation error for DocumentMetadata\n       last_updated\n         Field required [type=missing, input_value={'title': 'Document 0', '...ex_terms': ['document']}, input_type=dict]\n           For further information visit https://errors.pydantic.dev/2.10/v/missing\n", "test_name": "test_collection_management", "rerun": "0"}, "6": {"status": "FAIL", "message": "       pydantic_core._pydantic_core.ValidationError: 1 validation error for DocumentMetadata\n       last_updated\n         Field required [type=missing, input_value={'title': 'Python Guide',...s': ['guide', 'python']}, input_type=dict]\n           For further information visit https://errors.pydantic.dev/2.10/v/missing\n", "test_name": "test_document_similarity", "rerun": "0"}, "7": {"status": "FAIL", "message": "       pydantic_core._pydantic_core.ValidationError: 1 validation error for DocumentMetadata\n       last_updated\n         Field required [type=missing, input_value={'title': 'Test Document'...': ['test', 'document']}, input_type=dict]\n           For further information visit https://errors.pydantic.dev/2.10/v/missing\n", "test_name": "test_version_tracking", "rerun": "0"}, "8": {"status": "FAIL", "message": "       pydantic_core._pydantic_core.ValidationError: 1 validation error for DocumentMetadata\n       last_updated\n         Field required [type=missing, input_value={'title': 'Python Program...'programming', 'guide']}, input_type=dict]\n           For further information visit https://errors.pydantic.dev/2.10/v/missing\n", "test_name": "test_search_index_generation", "rerun": "0"}}, "status": {"total_pass": 1, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 8, "total_error": 0}}, "4": {"suite_name": "tests/test_processor.py", "tests": {"0": {"status": "PASS", "message": "", "test_name": "test_content_processor_initialization", "rerun": "0"}, "1": {"status": "PASS", "message": "", "test_name": "test_processing_rule_validation", "rerun": "0"}, "2": {"status": "PASS", "message": "", "test_name": "test_clean_text_processing", "rerun": "0"}, "3": {"status": "PASS", "message": "", "test_name": "test_code_language_detection", "rerun": "0"}, "4": {"status": "PASS", "message": "", "test_name": "test_code_block_processing", "rerun": "0"}, "5": {"status": "PASS", "message": "", "test_name": "test_link_processing", "rerun": "0"}, "6": {"status": "PASS", "message": "", "test_name": "test_metadata_extraction", "rerun": "0"}, "7": {"status": "PASS", "message": "", "test_name": "test_asset_collection", "rerun": "0"}, "8": {"status": "FAIL", "message": "       AttributeError: 'ContentProcessor' object has no attribute '_extract_headings'\n       NameError: name 'logging' is not defined\n", "test_name": "test_full_content_processing", "rerun": "0"}, "9": {"status": "FAIL", "message": "       NameError: name 'logging' is not defined\n", "test_name": "test_content_size_limits", "rerun": "0"}, "10": {"status": "FAIL", "message": "       AttributeError: 'ContentProcessor' object has no attribute '_extract_headings'\n       NameError: name 'logging' is not defined\n", "test_name": "test_malformed_html_handling", "rerun": "0"}, "11": {"status": "FAIL", "message": "       AttributeError: 'ContentProcessor' object has no attribute '_extract_headings'\n       NameError: name 'logging' is not defined\n", "test_name": "test_special_content_handling", "rerun": "0"}, "12": {"status": "FAIL", "message": "       AttributeError: 'ContentProcessor' object has no attribute '_extract_headings'\n       NameError: name 'logging' is not defined\n", "test_name": "test_content_structure_preservation", "rerun": "0"}}, "status": {"total_pass": 8, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 5, "total_error": 0}}, "5": {"suite_name": "tests/test_quality.py", "tests": {"0": {"status": "PASS", "message": "", "test_name": "test_quality_checker_initialization", "rerun": "0"}, "1": {"status": "PASS", "message": "", "test_name": "test_content_length_check", "rerun": "0"}, "2": {"status": "PASS", "message": "", "test_name": "test_heading_structure_check", "rerun": "0"}, "3": {"status": "PASS", "message": "", "test_name": "test_link_validation", "rerun": "0"}, "4": {"status": "PASS", "message": "", "test_name": "test_metadata_check", "rerun": "0"}, "5": {"status": "PASS", "message": "", "test_name": "test_code_block_validation", "rerun": "0"}, "6": {"status": "PASS", "message": "", "test_name": "test_quality_metrics_calculation", "rerun": "0"}, "7": {"status": "PASS", "message": "", "test_name": "test_full_quality_check", "rerun": "0"}, "8": {"status": "PASS", "message": "", "test_name": "test_quality_checker_cleanup", "rerun": "0"}}, "status": {"total_pass": 9, "total_skip": 0, "total_xpass": 0, "total_xfail": 0, "total_rerun": 0, "total_fail": 0, "total_error": 0}}}}, "date": "November 25, 2024", "start_time": 1732559025.1121001, "total_suite": 6, "status": "FAIL", "status_list": {"pass": "29", "fail": "28", "skip": "0", "error": "0", "xpass": "0", "xfail": "0", "rerun": "0"}, "total_tests": "57"}