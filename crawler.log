2024-11-28 01:07:44,447 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 01:10:00,452 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 01:12:40,552 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 01:12:41,148 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:41,149 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:41,150 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:41,153 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:41,153 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:41,154 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:41,155 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 01:12:41,175 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:41,176 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:41,176 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 01:12:41,180 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:41,180 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:41,185 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:41,185 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:41,190 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:41,190 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:41,191 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:41,192 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:41,196 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:41,196 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:41,197 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 01:12:41,201 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:41,204 [DEBUG] Found 0 links in 
2024-11-28 01:12:41,214 [DEBUG] Found 0 links in 
2024-11-28 01:12:41,224 [DEBUG] Found 0 links in 
2024-11-28 01:12:41,234 [DEBUG] Found 0 links in 
2024-11-28 01:12:41,261 [DEBUG] Found 2 links in 
2024-11-28 01:12:41,261 [DEBUG] Link: https://example.com (external) from content
2024-11-28 01:12:41,261 [DEBUG] Link: relative/path (internal) from content
2024-11-28 01:12:41,270 [DEBUG] Found 0 links in 
2024-11-28 01:12:41,279 [DEBUG] Found 0 links in 
2024-11-28 01:12:41,289 [DEBUG] Found 0 links in 
2024-11-28 01:12:41,299 [DEBUG] Found 0 links in 
2024-11-28 01:12:41,428 [DEBUG] Found 0 links in 
2024-11-28 01:12:41,466 [INFO] Operation took 0.16 seconds
2024-11-28 01:12:41,476 [DEBUG] Found 0 links in 
2024-11-28 01:12:41,486 [DEBUG] Found 0 links in 
2024-11-28 01:12:41,790 [INFO] test_operation took 0.10 seconds
2024-11-28 01:12:41,991 [INFO] inner took 0.10 seconds
2024-11-28 01:12:41,991 [INFO] outer took 0.20 seconds
2024-11-28 01:12:41,991 [INFO] error_operation took 0.00 seconds
2024-11-28 01:12:42,057 [INFO] test_operation took 0.00 seconds
2024-11-28 01:12:42,065 [DEBUG] Found 0 links in https://example.com
2024-11-28 01:12:42,069 [DEBUG] Found 0 links in https://example.com
2024-11-28 01:12:42,087 [DEBUG] Found 0 links in https://example.com
1970-01-01 02:00:00,000 [DEBUG] Found 0 links in https://example.com
2024-11-28 01:12:42,133 [DEBUG] Found 0 links in https://example.com
2024-11-28 01:12:42,154 [DEBUG] Found 0 links in https://example.com
2024-11-28 01:12:42,171 [DEBUG] Found 0 links in https://example.com
2024-11-28 01:12:42,208 [DEBUG] Found 0 links in https://example.com
2024-11-28 01:12:42,226 [DEBUG] Found 0 links in https://example.com
2024-11-28 01:12:42,242 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 01:12:42,242 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:42,243 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 01:12:42,243 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 01:12:42,243 [INFO] Starting crawl of URL: https://example.com
2024-11-28 01:12:42,754 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 01:12:42,754 [DEBUG] Base URL: https://example.com
2024-11-28 01:12:42,754 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 01:12:42,754 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 01:12:42,754 [DEBUG] Initial domain set to: example.com
2024-11-28 01:12:42,754 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 01:12:42,755 [INFO] Successfully crawled https://example.com
2024-11-28 01:12:42,755 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:42,756 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1677517d0>
2024-11-28 01:12:42,756 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x16756c6e0>, 203663.199651541)]']
connector: <aiohttp.connector.TCPConnector object at 0x16603e790>
2024-11-28 01:12:42,756 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 01:12:42,756 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:42,757 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 01:12:42,757 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 01:12:42,757 [INFO] Starting crawl of URL: https://example.com
2024-11-28 01:12:42,773 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-11-28 01:12:42,773 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-11-28 01:12:42,773 [INFO] Starting crawl of URL: https://example.org
2024-11-28 01:12:43,246 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 01:12:43,246 [DEBUG] Base URL: https://example.com
2024-11-28 01:12:43,246 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 01:12:43,246 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 01:12:43,246 [DEBUG] Initial domain set to: example.com
2024-11-28 01:12:43,246 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 01:12:43,246 [INFO] Successfully crawled https://example.com
2024-11-28 01:12:43,754 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 01:12:43,754 [DEBUG] Base URL: https://example.org
2024-11-28 01:12:43,754 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 01:12:43,754 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 01:12:43,754 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 01:12:43,754 [INFO] Successfully crawled https://example.org
2024-11-28 01:12:43,755 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:43,755 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x16766b1d0>
2024-11-28 01:12:43,755 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x16756cf30>, 203663.691981166)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x16756db70>, 203664.199551041)]']
connector: <aiohttp.connector.TCPConnector object at 0x167669a50>
2024-11-28 01:12:43,756 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 01:12:43,756 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:43,756 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 01:12:43,756 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 01:12:43,756 [INFO] Starting crawl of URL: https://example.com
2024-11-28 01:12:44,240 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 01:12:44,240 [DEBUG] Base URL: https://example.com
2024-11-28 01:12:44,240 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 01:12:44,240 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 01:12:44,240 [DEBUG] Initial domain set to: example.com
2024-11-28 01:12:44,240 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 01:12:44,240 [INFO] Successfully crawled https://example.com
2024-11-28 01:12:44,241 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:44,242 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x16779c210>
2024-11-28 01:12:44,242 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x16756dda0>, 203664.685802958)]']
connector: <aiohttp.connector.TCPConnector object at 0x1677536d0>
2024-11-28 01:12:44,242 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 01:12:44,242 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:44,243 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 01:12:44,243 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 01:12:44,243 [INFO] Starting crawl of URL: https://example.com
2024-11-28 01:12:44,732 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 01:12:44,732 [DEBUG] Base URL: https://example.com
2024-11-28 01:12:44,732 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 01:12:44,733 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 01:12:44,733 [DEBUG] Initial domain set to: example.com
2024-11-28 01:12:44,733 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 01:12:44,733 [INFO] Successfully crawled https://example.com
2024-11-28 01:12:44,733 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 01:12:44,733 [DEBUG] Base URL: https://example.com
2024-11-28 01:12:44,733 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 01:12:44,733 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 01:12:44,733 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 01:12:44,738 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:44,739 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 01:12:44,740 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:44,744 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:44,745 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 01:12:44,745 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:44,745 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 01:12:44,745 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 01:12:44,745 [INFO] Starting crawl of URL: https://example.com
2024-11-28 01:12:45,236 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 01:12:45,236 [DEBUG] Base URL: https://example.com
2024-11-28 01:12:45,236 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 01:12:45,236 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 01:12:45,236 [DEBUG] Initial domain set to: example.com
2024-11-28 01:12:45,236 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 01:12:45,236 [INFO] Successfully crawled https://example.com
2024-11-28 01:12:45,237 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:45,237 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x167574590>
2024-11-28 01:12:45,238 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x16756df60>, 203665.681788041)]']
connector: <aiohttp.connector.TCPConnector object at 0x1664553d0>
2024-11-28 01:12:45,238 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 01:12:45,238 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:45,239 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2024-11-28 01:12:45,239 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2024-11-28 01:12:45,239 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2024-11-28 01:12:47,326 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-28 01:12:47,328 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-28 01:12:47,328 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-28 01:12:47,333 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:47,334 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 01:12:47,334 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:47,334 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 01:12:47,335 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 01:12:47,335 [INFO] Starting crawl of URL: https://example.com
2024-11-28 01:12:47,349 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-11-28 01:12:47,349 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-11-28 01:12:47,349 [INFO] Starting crawl of URL: https://example.org
2024-11-28 01:12:47,350 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2024-11-28 01:12:47,350 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2024-11-28 01:12:47,350 [INFO] Starting crawl of URL: https://example.net
2024-11-28 01:12:47,834 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 01:12:47,834 [DEBUG] Base URL: https://example.com
2024-11-28 01:12:47,834 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 01:12:47,834 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 01:12:47,834 [DEBUG] Initial domain set to: example.com
2024-11-28 01:12:47,834 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 01:12:47,834 [INFO] Successfully crawled https://example.com
2024-11-28 01:12:48,336 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 01:12:48,336 [DEBUG] Base URL: https://example.org
2024-11-28 01:12:48,336 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 01:12:48,336 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 01:12:48,336 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 01:12:48,336 [INFO] Successfully crawled https://example.org
2024-11-28 01:12:48,967 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 01:12:48,967 [DEBUG] Base URL: https://example.net
2024-11-28 01:12:48,967 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 01:12:48,967 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 01:12:48,967 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 01:12:48,967 [INFO] Successfully crawled https://example.net
2024-11-28 01:12:48,968 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:48,969 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1378b8550>
2024-11-28 01:12:48,969 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x16756dd30>, 203668.279320458)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x16756f9a0>, 203668.781650583)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x1675d6040>, 203669.41245875)]']
connector: <aiohttp.connector.TCPConnector object at 0x16779ab50>
2024-11-28 01:12:48,969 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 01:12:48,970 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:48,970 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 01:12:48,970 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 01:12:48,970 [INFO] Starting crawl of URL: https://example.com
2024-11-28 01:12:49,467 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 01:12:49,467 [DEBUG] Base URL: https://example.com
2024-11-28 01:12:49,467 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 01:12:49,467 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 01:12:49,467 [DEBUG] Initial domain set to: example.com
2024-11-28 01:12:49,467 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 01:12:49,468 [INFO] Successfully crawled https://example.com
2024-11-28 01:12:49,468 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:49,474 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 01:12:49,474 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:49,475 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 01:12:49,493 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 01:12:49,508 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:49,509 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 01:12:49,509 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:49,509 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 01:12:49,525 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:49,526 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 01:12:49,526 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:49,526 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 01:12:49,526 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2024-11-28 01:12:49,527 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2024-11-28 01:12:49,527 [INFO] Starting crawl of URL: https://example.com/page
2024-11-28 01:12:50,019 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 01:12:50,019 [DEBUG] Base URL: https://example.com/page
2024-11-28 01:12:50,019 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 01:12:50,019 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 01:12:50,019 [DEBUG] Initial domain set to: example.com
2024-11-28 01:12:50,019 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 01:12:50,019 [INFO] Successfully crawled https://example.com/page
2024-11-28 01:12:50,019 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2024-11-28 01:12:50,019 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2024-11-28 01:12:50,019 [INFO] Starting crawl of URL: https://other-domain.com/page
2024-11-28 01:12:56,478 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x16779a3d0>
2024-11-28 01:12:56,495 [DEBUG] 
Evaluating link: ./
2024-11-28 01:12:56,495 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,495 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-28 01:12:56,495 [DEBUG] Absolute link: https://other-domain.com/
2024-11-28 01:12:56,495 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,495 [DEBUG] 
Evaluating link: ./
2024-11-28 01:12:56,495 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,495 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-28 01:12:56,495 [DEBUG] Absolute link: https://other-domain.com/
2024-11-28 01:12:56,495 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,495 [DEBUG] 
Evaluating link: ./hiyou.php
2024-11-28 01:12:56,495 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,495 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-11-28 01:12:56,495 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-11-28 01:12:56,495 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,495 [DEBUG] 
Evaluating link: ./date.php
2024-11-28 01:12:56,495 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,495 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-11-28 01:12:56,496 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-11-28 01:12:56,496 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,496 [DEBUG] 
Evaluating link: ./contact.php
2024-11-28 01:12:56,496 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,496 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2024-11-28 01:12:56,496 [DEBUG] Absolute link: https://other-domain.com/contact.php
2024-11-28 01:12:56,496 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,496 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 01:12:56,496 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,496 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 01:12:56,496 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 01:12:56,496 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-28 01:12:56,496 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 01:12:56,496 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,496 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 01:12:56,496 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 01:12:56,496 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 01:12:56,496 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 01:12:56,496 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,496 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 01:12:56,496 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 01:12:56,497 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 01:12:56,497 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 01:12:56,497 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,497 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 01:12:56,497 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 01:12:56,497 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 01:12:56,497 [DEBUG] 
Evaluating link: ./contact
2024-11-28 01:12:56,497 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,497 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-11-28 01:12:56,497 [DEBUG] Absolute link: https://other-domain.com/contact
2024-11-28 01:12:56,497 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,497 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 01:12:56,497 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,497 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 01:12:56,497 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 01:12:56,497 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 01:12:56,497 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 01:12:56,497 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,497 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 01:12:56,497 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 01:12:56,498 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 01:12:56,498 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 01:12:56,498 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,498 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 01:12:56,498 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 01:12:56,498 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 01:12:56,498 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-11-28 01:12:56,498 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,498 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 01:12:56,498 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 01:12:56,498 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-11-28 01:12:56,498 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 01:12:56,498 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,498 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 01:12:56,498 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 01:12:56,498 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-28 01:12:56,498 [DEBUG] 
Evaluating link: #indexes
2024-11-28 01:12:56,498 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,498 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2024-11-28 01:12:56,498 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,498 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,498 [DEBUG] 
Evaluating link: #a
2024-11-28 01:12:56,499 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,499 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2024-11-28 01:12:56,499 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,499 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,499 [DEBUG] 
Evaluating link: #b
2024-11-28 01:12:56,499 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,499 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-11-28 01:12:56,499 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,499 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,499 [DEBUG] 
Evaluating link: #c
2024-11-28 01:12:56,499 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,499 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-11-28 01:12:56,499 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,499 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,499 [DEBUG] 
Evaluating link: #d
2024-11-28 01:12:56,499 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,499 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-11-28 01:12:56,499 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,499 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,499 [DEBUG] 
Evaluating link: #e
2024-11-28 01:12:56,499 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,499 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-11-28 01:12:56,499 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,499 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,499 [DEBUG] 
Evaluating link: #f
2024-11-28 01:12:56,499 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,500 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-11-28 01:12:56,500 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,500 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,500 [DEBUG] 
Evaluating link: #g
2024-11-28 01:12:56,500 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,500 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-11-28 01:12:56,500 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,500 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,500 [DEBUG] 
Evaluating link: #h
2024-11-28 01:12:56,500 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,500 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-11-28 01:12:56,500 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,500 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,500 [DEBUG] 
Evaluating link: #i
2024-11-28 01:12:56,500 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,500 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-11-28 01:12:56,500 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,500 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,500 [DEBUG] 
Evaluating link: #j
2024-11-28 01:12:56,501 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,501 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-11-28 01:12:56,501 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,501 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,501 [DEBUG] 
Evaluating link: #k
2024-11-28 01:12:56,501 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,501 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-11-28 01:12:56,501 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,501 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,501 [DEBUG] 
Evaluating link: #l
2024-11-28 01:12:56,501 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,501 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-11-28 01:12:56,501 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,501 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,501 [DEBUG] 
Evaluating link: #m
2024-11-28 01:12:56,501 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,501 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-11-28 01:12:56,501 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,501 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,502 [DEBUG] 
Evaluating link: #n
2024-11-28 01:12:56,502 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,502 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-11-28 01:12:56,502 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,502 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,502 [DEBUG] 
Evaluating link: #o
2024-11-28 01:12:56,502 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,502 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-11-28 01:12:56,502 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,502 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,502 [DEBUG] 
Evaluating link: #p
2024-11-28 01:12:56,502 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,502 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-11-28 01:12:56,502 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,502 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,502 [DEBUG] 
Evaluating link: #q
2024-11-28 01:12:56,502 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,502 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-11-28 01:12:56,503 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,503 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,503 [DEBUG] 
Evaluating link: #r
2024-11-28 01:12:56,503 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,503 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-11-28 01:12:56,503 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,503 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,503 [DEBUG] 
Evaluating link: #s
2024-11-28 01:12:56,503 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,503 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-11-28 01:12:56,503 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,503 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,503 [DEBUG] 
Evaluating link: #t
2024-11-28 01:12:56,503 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,503 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-11-28 01:12:56,503 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,503 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,503 [DEBUG] 
Evaluating link: #u
2024-11-28 01:12:56,503 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,504 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-11-28 01:12:56,504 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,504 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,504 [DEBUG] 
Evaluating link: #v
2024-11-28 01:12:56,504 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,504 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-11-28 01:12:56,504 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,504 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,504 [DEBUG] 
Evaluating link: #w
2024-11-28 01:12:56,504 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,504 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-11-28 01:12:56,504 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,504 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,504 [DEBUG] 
Evaluating link: #x
2024-11-28 01:12:56,504 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,504 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-11-28 01:12:56,504 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,504 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,504 [DEBUG] 
Evaluating link: #y
2024-11-28 01:12:56,504 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,505 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-11-28 01:12:56,505 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,505 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,505 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 01:12:56,505 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,505 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 01:12:56,505 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 01:12:56,505 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 01:12:56,505 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 01:12:56,505 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,505 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 01:12:56,505 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 01:12:56,505 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 01:12:56,505 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 01:12:56,505 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,505 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 01:12:56,505 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 01:12:56,505 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 01:12:56,505 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-11-28 01:12:56,506 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,506 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 01:12:56,506 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 01:12:56,506 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-11-28 01:12:56,506 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 01:12:56,506 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,506 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 01:12:56,506 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 01:12:56,506 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-28 01:12:56,506 [DEBUG] 
Evaluating link: https://spriq.jp/
2024-11-28 01:12:56,506 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,506 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2024-11-28 01:12:56,506 [DEBUG] Absolute link: https://spriq.jp/
2024-11-28 01:12:56,506 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2024-11-28 01:12:56,506 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 01:12:56,506 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,506 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 01:12:56,506 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 01:12:56,507 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 01:12:56,507 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 01:12:56,507 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,507 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 01:12:56,507 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 01:12:56,507 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 01:12:56,507 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 01:12:56,507 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,507 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 01:12:56,507 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 01:12:56,507 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 01:12:56,507 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 01:12:56,507 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,507 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 01:12:56,507 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 01:12:56,507 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 01:12:56,507 [DEBUG] 
Evaluating link: ./contact
2024-11-28 01:12:56,508 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,508 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-11-28 01:12:56,508 [DEBUG] Absolute link: https://other-domain.com/contact
2024-11-28 01:12:56,508 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,508 [DEBUG] 
Evaluating link: ./
2024-11-28 01:12:56,508 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,508 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-28 01:12:56,508 [DEBUG] Absolute link: https://other-domain.com/
2024-11-28 01:12:56,508 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,508 [DEBUG] 
Evaluating link: ./hiyou.php
2024-11-28 01:12:56,508 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,508 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-11-28 01:12:56,508 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-11-28 01:12:56,508 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,508 [DEBUG] 
Evaluating link: ./date.php
2024-11-28 01:12:56,508 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,508 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-11-28 01:12:56,508 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-11-28 01:12:56,508 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,508 [DEBUG] 
Evaluating link: #u
2024-11-28 01:12:56,508 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,509 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-11-28 01:12:56,509 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,509 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,509 [DEBUG] 
Evaluating link: #v
2024-11-28 01:12:56,509 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,509 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-11-28 01:12:56,509 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,509 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,509 [DEBUG] 
Evaluating link: #w
2024-11-28 01:12:56,509 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,509 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-11-28 01:12:56,509 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,509 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,509 [DEBUG] 
Evaluating link: #x
2024-11-28 01:12:56,509 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,509 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-11-28 01:12:56,509 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,509 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,509 [DEBUG] 
Evaluating link: #y
2024-11-28 01:12:56,510 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,510 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-11-28 01:12:56,510 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,510 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,510 [DEBUG] 
Evaluating link: #p
2024-11-28 01:12:56,510 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,510 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-11-28 01:12:56,510 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,510 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,510 [DEBUG] 
Evaluating link: #q
2024-11-28 01:12:56,510 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,510 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-11-28 01:12:56,510 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,510 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,510 [DEBUG] 
Evaluating link: #r
2024-11-28 01:12:56,510 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,510 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-11-28 01:12:56,510 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,510 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,511 [DEBUG] 
Evaluating link: #s
2024-11-28 01:12:56,511 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,511 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-11-28 01:12:56,511 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,511 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,511 [DEBUG] 
Evaluating link: #t
2024-11-28 01:12:56,511 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,511 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-11-28 01:12:56,511 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,511 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,511 [DEBUG] 
Evaluating link: #k
2024-11-28 01:12:56,511 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,511 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-11-28 01:12:56,511 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,511 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,511 [DEBUG] 
Evaluating link: #l
2024-11-28 01:12:56,511 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,511 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-11-28 01:12:56,512 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,512 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,512 [DEBUG] 
Evaluating link: #m
2024-11-28 01:12:56,512 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,512 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-11-28 01:12:56,512 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,512 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,512 [DEBUG] 
Evaluating link: #n
2024-11-28 01:12:56,512 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,512 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-11-28 01:12:56,512 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,512 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,512 [DEBUG] 
Evaluating link: #o
2024-11-28 01:12:56,512 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,512 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-11-28 01:12:56,512 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,512 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,512 [DEBUG] 
Evaluating link: #f
2024-11-28 01:12:56,512 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,512 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-11-28 01:12:56,513 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,513 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,513 [DEBUG] 
Evaluating link: #g
2024-11-28 01:12:56,513 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,513 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-11-28 01:12:56,513 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,513 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,513 [DEBUG] 
Evaluating link: #h
2024-11-28 01:12:56,513 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,513 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-11-28 01:12:56,513 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,513 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,513 [DEBUG] 
Evaluating link: #i
2024-11-28 01:12:56,513 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,513 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-11-28 01:12:56,513 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,513 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,513 [DEBUG] 
Evaluating link: #j
2024-11-28 01:12:56,513 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,513 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-11-28 01:12:56,514 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,514 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,514 [DEBUG] 
Evaluating link: #b
2024-11-28 01:12:56,514 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,514 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-11-28 01:12:56,514 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,514 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,514 [DEBUG] 
Evaluating link: #c
2024-11-28 01:12:56,514 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,514 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-11-28 01:12:56,514 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,514 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,514 [DEBUG] 
Evaluating link: #d
2024-11-28 01:12:56,514 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,514 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-11-28 01:12:56,514 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,514 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,514 [DEBUG] 
Evaluating link: #e
2024-11-28 01:12:56,514 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,514 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-11-28 01:12:56,515 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,515 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,515 [DEBUG] 
Evaluating link: #site
2024-11-28 01:12:56,515 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 01:12:56,515 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2024-11-28 01:12:56,515 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 01:12:56,515 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 01:12:56,515 [INFO] Successfully crawled https://other-domain.com/page
2024-11-28 01:12:56,520 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:56,522 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 01:12:56,522 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:12:56,523 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-11-28 01:12:56,523 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-11-28 01:12:56,523 [INFO] Starting crawl of URL: https://example.com/page0
2024-11-28 01:12:57,010 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 01:12:57,010 [DEBUG] Base URL: https://example.com/page0
2024-11-28 01:12:57,010 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 01:12:57,010 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 01:12:57,011 [DEBUG] Initial domain set to: example.com
2024-11-28 01:12:57,011 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 01:12:57,011 [INFO] Successfully crawled https://example.com/page0
2024-11-28 01:12:57,011 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 01:12:57,011 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 01:12:57,011 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 01:12:57,160 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 01:12:57,160 [DEBUG] Base URL: https://example.com/page1
2024-11-28 01:12:57,160 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 01:12:57,160 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 01:12:57,160 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 01:12:57,160 [INFO] Successfully crawled https://example.com/page1
2024-11-28 01:12:57,160 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 01:12:57,160 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 01:12:57,160 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 01:12:57,662 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 01:12:57,662 [DEBUG] Base URL: https://example.com/page2
2024-11-28 01:12:57,663 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 01:12:57,663 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 01:12:57,663 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 01:12:57,663 [INFO] Successfully crawled https://example.com/page2
2024-11-28 01:12:57,663 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-28 01:12:57,663 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-28 01:12:57,663 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-28 01:12:58,170 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 01:12:58,170 [DEBUG] Base URL: https://example.com/page3
2024-11-28 01:12:58,170 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 01:12:58,170 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 01:12:58,171 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 01:12:58,171 [INFO] Successfully crawled https://example.com/page3
2024-11-28 01:12:58,171 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-11-28 01:12:58,171 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-11-28 01:12:58,171 [INFO] Starting crawl of URL: https://example.com/page4
2024-11-28 01:12:58,665 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 01:12:58,666 [DEBUG] Base URL: https://example.com/page4
2024-11-28 01:12:58,666 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 01:12:58,666 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 01:12:58,666 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 01:12:58,666 [INFO] Successfully crawled https://example.com/page4
2024-11-28 01:12:58,666 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 01:12:58,666 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-11-28 01:12:58,666 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-11-28 01:12:58,666 [INFO] Starting crawl of URL: https://example.com/page0
2024-11-28 01:12:59,173 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 01:12:59,173 [DEBUG] Base URL: https://example.com/page0
2024-11-28 01:12:59,173 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 01:12:59,174 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 01:12:59,174 [DEBUG] Initial domain set to: example.com
2024-11-28 01:12:59,174 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 01:12:59,174 [INFO] Successfully crawled https://example.com/page0
2024-11-28 01:12:59,174 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 01:12:59,174 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 01:12:59,174 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 01:12:59,302 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 01:12:59,302 [DEBUG] Base URL: https://example.com/page1
2024-11-28 01:12:59,302 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 01:12:59,303 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 01:12:59,303 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 01:12:59,303 [INFO] Successfully crawled https://example.com/page1
2024-11-28 01:12:59,303 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 01:12:59,303 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 01:12:59,303 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 01:12:59,803 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 01:12:59,803 [DEBUG] Base URL: https://example.com/page2
2024-11-28 01:12:59,803 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 01:12:59,803 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 01:12:59,803 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 01:12:59,803 [INFO] Successfully crawled https://example.com/page2
2024-11-28 01:12:59,803 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-28 01:12:59,803 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-28 01:12:59,803 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-28 01:13:00,312 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 01:13:00,312 [DEBUG] Base URL: https://example.com/page3
2024-11-28 01:13:00,313 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 01:13:00,313 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 01:13:00,313 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 01:13:00,313 [INFO] Successfully crawled https://example.com/page3
2024-11-28 01:13:00,313 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-11-28 01:13:00,313 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-11-28 01:13:00,313 [INFO] Starting crawl of URL: https://example.com/page4
2024-11-28 01:13:00,810 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 01:13:00,810 [DEBUG] Base URL: https://example.com/page4
2024-11-28 01:13:00,810 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 01:13:00,810 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 01:13:00,811 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 01:13:00,811 [INFO] Successfully crawled https://example.com/page4
2024-11-28 01:13:00,816 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:13:00,818 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:13:00,819 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 01:13:00,824 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:13:00,825 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:13:00,826 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 01:13:00,827 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 01:13:00,827 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 01:13:00,827 [INFO] Starting crawl of URL: https://example.com
2024-11-28 01:13:02,828 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 01:13:10,828 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 01:13:10,828 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 01:13:10,833 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:13:10,836 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:13:10,837 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 01:13:10,838 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 01:13:10,838 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 01:13:10,838 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 01:13:10,838 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 01:13:10,838 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 01:13:10,838 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 01:13:12,839 [WARNING] Retry 1/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 01:13:22,840 [WARNING] Retry 1/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 01:13:30,840 [WARNING] Retry 2/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 01:13:30,841 [ERROR] Error crawling https://example.com/page1: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 01:13:40,841 [WARNING] Retry 2/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 01:13:40,841 [ERROR] Error crawling https://example.com/page2: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 01:13:40,847 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:13:40,847 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:13:40,848 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 01:13:40,849 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 01:13:40,849 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 01:13:40,849 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 01:13:40,849 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 01:13:40,849 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 01:13:40,849 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 01:13:40,849 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-28 01:13:40,849 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-28 01:13:40,849 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-28 01:13:42,850 [WARNING] Retry 1/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 01:13:52,851 [WARNING] Retry 1/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 01:14:02,852 [WARNING] Retry 1/2 for https://example.com/page3: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 01:14:10,852 [WARNING] Retry 2/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 01:14:10,853 [ERROR] Error crawling https://example.com/page1: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 01:14:20,854 [WARNING] Retry 2/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 01:14:20,854 [ERROR] Error crawling https://example.com/page2: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 01:14:30,855 [WARNING] Retry 2/2 for https://example.com/page3: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 01:14:30,855 [ERROR] Error crawling https://example.com/page3: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 01:14:30,856 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:14:30,857 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:14:30,858 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 01:14:30,863 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:14:30,863 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:14:30,864 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 01:14:30,864 [DEBUG] Using raw URL string: https://nonexistent.example.com (type: <class 'str'>)
2024-11-28 01:14:30,865 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='nonexistent.example.com', path='', params='', query='', fragment='')
2024-11-28 01:14:30,865 [INFO] Starting crawl of URL: https://nonexistent.example.com
2024-11-28 01:14:32,865 [WARNING] Retry 1/2 for https://nonexistent.example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 01:14:40,866 [WARNING] Retry 2/2 for https://nonexistent.example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 01:14:40,866 [ERROR] Error crawling https://nonexistent.example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 01:14:40,871 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:14:40,872 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:14:40,873 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 01:14:40,877 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:14:40,877 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:14:40,878 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 01:14:40,878 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 01:14:40,879 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 01:14:40,879 [INFO] Starting crawl of URL: https://example.com
2024-11-28 01:14:42,880 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 01:14:50,879 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 01:14:50,879 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 01:14:50,884 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:14:50,885 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:14:50,886 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 01:14:50,886 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 01:14:50,887 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 01:14:50,887 [INFO] Starting crawl of URL: https://example.com
2024-11-28 01:14:52,887 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 01:15:00,888 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 01:15:00,888 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 01:15:00,893 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:00,920 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 01:15:00,921 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:00,922 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:00,923 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 01:15:00,923 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:00,924 [DEBUG] 
Evaluating link: https://example.com/doc1
2024-11-28 01:15:00,924 [DEBUG] Target domain: example.com
2024-11-28 01:15:00,924 [DEBUG] URL domain: example.com
2024-11-28 01:15:00,924 [DEBUG] URL approved for crawling: https://example.com/doc1
2024-11-28 01:15:00,924 [DEBUG] 
Evaluating link: https://example.com/excluded/page
2024-11-28 01:15:00,924 [DEBUG] Target domain: example.com
2024-11-28 01:15:00,924 [DEBUG] URL domain: example.com
2024-11-28 01:15:00,924 [DEBUG] URL approved for crawling: https://example.com/excluded/page
2024-11-28 01:15:00,931 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:00,932 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 01:15:00,932 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:00,938 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:00,939 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 01:15:00,940 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:00,940 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-28 01:15:00,955 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:00,956 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 01:15:00,956 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:00,962 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:00,963 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 01:15:00,963 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:00,964 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-28 01:15:00,988 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1664540d0>
2024-11-28 01:15:00,988 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x16756c6e0>, 203665.176349416)]']
connector: <aiohttp.connector.TCPConnector object at 0x166454f90>
2024-11-28 01:15:00,988 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x16768e550>
2024-11-28 01:15:00,988 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1675d69e0>, 203670.464413208)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x1675d70e0>, 203672.986970958)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x1675d52b0>, 203674.694936958)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x1675d6a50>, 203676.922137875)]']
connector: <aiohttp.connector.TCPConnector object at 0x16768e2d0>
2024-11-28 01:15:00,988 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x167722ad0>
2024-11-28 01:15:00,988 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1675d6580>, 203679.111217666)]']
connector: <aiohttp.connector.TCPConnector object at 0x1677340d0>
2024-11-28 01:15:00,988 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x167722c50>
2024-11-28 01:15:00,988 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1675d65f0>, 203681.255954333)]']
connector: <aiohttp.connector.TCPConnector object at 0x1677214d0>
2024-11-28 01:15:01,005 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:01,007 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 01:15:01,007 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:01,008 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-28 01:15:01,022 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:01,023 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 01:15:01,024 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:01,024 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-28 01:15:01,038 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:01,040 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 01:15:01,040 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:01,040 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-28 01:15:01,055 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:01,056 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 01:15:01,056 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:01,057 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-28 01:15:01,071 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:01,073 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 01:15:01,073 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:01,073 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-28 01:15:01,088 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:01,105 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:01,111 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:01,112 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:01,112 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 01:15:01,117 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:01,118 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:01,119 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 01:15:01,124 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:01,128 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 01:15:01,139 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 01:15:01,149 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 01:15:01,179 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 01:15:01,190 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 01:15:01,200 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 01:15:01,211 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 01:15:01,222 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 01:15:01,232 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 01:15:01,242 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 01:15:01,272 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:01,635 [DEBUG] Using selector: KqueueSelector
2024-11-28 01:15:01,637 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 01:15:01,637 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 01:15:01,637 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 01:15:01,637 [INFO] Starting crawl of URL: https://example.com
2024-11-28 01:15:02,129 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 01:15:02,130 [DEBUG] Base URL: https://example.com
2024-11-28 01:15:02,130 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 01:15:02,130 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 01:15:02,130 [DEBUG] Initial domain set to: example.com
2024-11-28 01:15:02,130 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 01:15:02,130 [INFO] Successfully crawled https://example.com
2024-11-28 01:15:02,130 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x16a24ef10>
2024-11-28 01:15:02,130 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x169ed6900>, 203802.573881333)]']
connector: <aiohttp.connector.TCPConnector object at 0x16a2f26d0>
2024-11-28 01:15:02,132 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2024-11-28 01:15:03,477 [INFO] Operation took 0.10 seconds
2024-11-28 02:55:48,295 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 02:55:48,930 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:48,931 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:48,932 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:48,934 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:48,935 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:48,935 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:48,936 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:55:48,956 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:48,957 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:48,957 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:55:48,961 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:48,962 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:48,966 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:48,966 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:48,971 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:48,971 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:48,972 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:48,973 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:48,976 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:48,977 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:48,977 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:55:48,981 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:48,984 [DEBUG] Found 0 links in 
2024-11-28 02:55:48,995 [DEBUG] Found 0 links in 
2024-11-28 02:55:49,005 [DEBUG] Found 0 links in 
2024-11-28 02:55:49,015 [DEBUG] Found 0 links in 
2024-11-28 02:55:49,042 [DEBUG] Found 2 links in 
2024-11-28 02:55:49,042 [DEBUG] Link: https://example.com (external) from content
2024-11-28 02:55:49,042 [DEBUG] Link: relative/path (internal) from content
2024-11-28 02:55:49,051 [DEBUG] Found 0 links in 
2024-11-28 02:55:49,060 [DEBUG] Found 0 links in 
2024-11-28 02:55:49,070 [DEBUG] Found 0 links in 
2024-11-28 02:55:49,079 [DEBUG] Found 0 links in 
2024-11-28 02:55:49,203 [DEBUG] Found 0 links in 
2024-11-28 02:55:49,239 [INFO] Operation took 0.15 seconds
2024-11-28 02:55:49,248 [DEBUG] Found 0 links in 
2024-11-28 02:55:49,258 [DEBUG] Found 0 links in 
2024-11-28 02:55:49,565 [INFO] test_operation took 0.11 seconds
2024-11-28 02:55:49,773 [INFO] inner took 0.10 seconds
2024-11-28 02:55:49,773 [INFO] outer took 0.21 seconds
2024-11-28 02:55:49,773 [INFO] error_operation took 0.00 seconds
2024-11-28 02:55:49,837 [INFO] test_operation took 0.00 seconds
2024-11-28 02:55:49,846 [DEBUG] Found 0 links in https://example.com
2024-11-28 02:55:49,850 [DEBUG] Found 0 links in https://example.com
2024-11-28 02:55:49,868 [DEBUG] Found 0 links in https://example.com
1970-01-01 02:00:00,000 [DEBUG] Found 0 links in https://example.com
2024-11-28 02:55:49,914 [DEBUG] Found 0 links in https://example.com
2024-11-28 02:55:49,935 [DEBUG] Found 0 links in https://example.com
2024-11-28 02:55:49,952 [DEBUG] Found 0 links in https://example.com
2024-11-28 02:55:49,969 [DEBUG] Found 0 links in https://example.com
2024-11-28 02:55:50,005 [DEBUG] Found 0 links in https://example.com
2024-11-28 02:55:50,021 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:55:50,021 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:50,022 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 02:55:50,022 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 02:55:50,022 [INFO] Starting crawl of URL: https://example.com
2024-11-28 02:55:50,521 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:55:50,521 [DEBUG] Base URL: https://example.com
2024-11-28 02:55:50,521 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:55:50,521 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:55:50,521 [DEBUG] Initial domain set to: example.com
2024-11-28 02:55:50,521 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:55:50,521 [INFO] Successfully crawled https://example.com
2024-11-28 02:55:50,522 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:50,523 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x138882950>
2024-11-28 02:55:50,523 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1389f8d00>, 209850.962126333)]']
connector: <aiohttp.connector.TCPConnector object at 0x13848eed0>
2024-11-28 02:55:50,523 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:55:50,524 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:50,524 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 02:55:50,524 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 02:55:50,524 [INFO] Starting crawl of URL: https://example.com
2024-11-28 02:55:50,540 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-11-28 02:55:50,540 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-11-28 02:55:50,540 [INFO] Starting crawl of URL: https://example.org
2024-11-28 02:55:51,009 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:55:51,009 [DEBUG] Base URL: https://example.com
2024-11-28 02:55:51,009 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:55:51,009 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:55:51,009 [DEBUG] Initial domain set to: example.com
2024-11-28 02:55:51,009 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:55:51,009 [INFO] Successfully crawled https://example.com
2024-11-28 02:55:51,523 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:55:51,523 [DEBUG] Base URL: https://example.org
2024-11-28 02:55:51,523 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:55:51,523 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:55:51,523 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:55:51,524 [INFO] Successfully crawled https://example.org
2024-11-28 02:55:51,525 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:51,526 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1389afdd0>
2024-11-28 02:55:51,526 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1389f9160>, 209851.450961375)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x1389f9b70>, 209851.962157416)]']
connector: <aiohttp.connector.TCPConnector object at 0x1389acfd0>
2024-11-28 02:55:51,526 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:55:51,526 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:51,527 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 02:55:51,527 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 02:55:51,527 [INFO] Starting crawl of URL: https://example.com
2024-11-28 02:55:52,016 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:55:52,016 [DEBUG] Base URL: https://example.com
2024-11-28 02:55:52,016 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:55:52,016 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:55:52,016 [DEBUG] Initial domain set to: example.com
2024-11-28 02:55:52,016 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:55:52,016 [INFO] Successfully crawled https://example.com
2024-11-28 02:55:52,017 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:52,017 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x138b5e6d0>
2024-11-28 02:55:52,018 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1389f9da0>, 209852.457675708)]']
connector: <aiohttp.connector.TCPConnector object at 0x138aadd90>
2024-11-28 02:55:52,018 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:55:52,018 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:52,019 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 02:55:52,019 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 02:55:52,019 [INFO] Starting crawl of URL: https://example.com
2024-11-28 02:55:52,503 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:55:52,504 [DEBUG] Base URL: https://example.com
2024-11-28 02:55:52,504 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:55:52,504 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:55:52,504 [DEBUG] Initial domain set to: example.com
2024-11-28 02:55:52,504 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:55:52,504 [INFO] Successfully crawled https://example.com
2024-11-28 02:55:52,505 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:55:52,505 [DEBUG] Base URL: https://example.com
2024-11-28 02:55:52,505 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:55:52,505 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:55:52,505 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:55:52,509 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:52,510 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:55:52,511 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:52,514 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x138a85490>
2024-11-28 02:55:52,515 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1389f8d00>, 209852.943655)]']
connector: <aiohttp.connector.TCPConnector object at 0x138a85d50>
2024-11-28 02:55:52,516 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:52,517 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:55:52,517 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:52,517 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 02:55:52,517 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 02:55:52,518 [INFO] Starting crawl of URL: https://example.com
2024-11-28 02:55:53,006 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:55:53,007 [DEBUG] Base URL: https://example.com
2024-11-28 02:55:53,007 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:55:53,007 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:55:53,007 [DEBUG] Initial domain set to: example.com
2024-11-28 02:55:53,007 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:55:53,007 [INFO] Successfully crawled https://example.com
2024-11-28 02:55:53,007 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:53,008 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x138b3fdd0>
2024-11-28 02:55:53,008 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1389f9b70>, 209853.448276541)]']
connector: <aiohttp.connector.TCPConnector object at 0x138b3ecd0>
2024-11-28 02:55:53,008 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:55:53,009 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:53,009 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2024-11-28 02:55:53,009 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2024-11-28 02:55:53,009 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2024-11-28 02:55:55,078 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-28 02:55:55,081 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-28 02:55:55,081 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-28 02:55:55,088 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:55,089 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:55:55,089 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:55,090 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 02:55:55,090 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 02:55:55,090 [INFO] Starting crawl of URL: https://example.com
2024-11-28 02:55:55,108 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-11-28 02:55:55,108 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-11-28 02:55:55,108 [INFO] Starting crawl of URL: https://example.org
2024-11-28 02:55:55,108 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2024-11-28 02:55:55,108 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2024-11-28 02:55:55,108 [INFO] Starting crawl of URL: https://example.net
2024-11-28 02:55:55,577 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:55:55,577 [DEBUG] Base URL: https://example.com
2024-11-28 02:55:55,577 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:55:55,577 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:55:55,577 [DEBUG] Initial domain set to: example.com
2024-11-28 02:55:55,577 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:55:55,577 [INFO] Successfully crawled https://example.com
2024-11-28 02:55:56,085 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:55:56,085 [DEBUG] Base URL: https://example.org
2024-11-28 02:55:56,085 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:55:56,085 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:55:56,085 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:55:56,086 [INFO] Successfully crawled https://example.org
2024-11-28 02:55:56,710 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:55:56,710 [DEBUG] Base URL: https://example.net
2024-11-28 02:55:56,710 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:55:56,710 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:55:56,710 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:55:56,710 [INFO] Successfully crawled https://example.net
2024-11-28 02:55:56,711 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:56,711 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x138b3f750>
2024-11-28 02:55:56,711 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1389fbe00>, 209856.018355666)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x1389fb4d0>, 209856.526788833)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x1389fb690>, 209857.15151525)]']
connector: <aiohttp.connector.TCPConnector object at 0x138b87490>
2024-11-28 02:55:56,712 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:55:56,712 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:56,712 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 02:55:56,712 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 02:55:56,712 [INFO] Starting crawl of URL: https://example.com
2024-11-28 02:55:57,198 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:55:57,198 [DEBUG] Base URL: https://example.com
2024-11-28 02:55:57,198 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:55:57,198 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:55:57,198 [DEBUG] Initial domain set to: example.com
2024-11-28 02:55:57,198 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:55:57,198 [INFO] Successfully crawled https://example.com
2024-11-28 02:55:57,199 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:57,205 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:55:57,205 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:57,206 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:55:57,224 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:55:57,239 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:57,240 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:55:57,240 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:57,241 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:55:57,257 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:57,257 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:55:57,257 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:55:57,258 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:55:57,258 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2024-11-28 02:55:57,258 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2024-11-28 02:55:57,258 [INFO] Starting crawl of URL: https://example.com/page
2024-11-28 02:55:57,749 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:55:57,750 [DEBUG] Base URL: https://example.com/page
2024-11-28 02:55:57,750 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:55:57,750 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:55:57,750 [DEBUG] Initial domain set to: example.com
2024-11-28 02:55:57,750 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:55:57,750 [INFO] Successfully crawled https://example.com/page
2024-11-28 02:55:57,750 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2024-11-28 02:55:57,750 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2024-11-28 02:55:57,750 [INFO] Starting crawl of URL: https://other-domain.com/page
2024-11-28 02:56:04,291 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x138b87d90>
2024-11-28 02:56:04,302 [DEBUG] 
Evaluating link: ./
2024-11-28 02:56:04,302 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,302 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-28 02:56:04,302 [DEBUG] Absolute link: https://other-domain.com/
2024-11-28 02:56:04,302 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,302 [DEBUG] 
Evaluating link: ./
2024-11-28 02:56:04,302 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,302 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-28 02:56:04,302 [DEBUG] Absolute link: https://other-domain.com/
2024-11-28 02:56:04,302 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,302 [DEBUG] 
Evaluating link: ./hiyou.php
2024-11-28 02:56:04,302 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,302 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-11-28 02:56:04,302 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-11-28 02:56:04,302 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,303 [DEBUG] 
Evaluating link: ./date.php
2024-11-28 02:56:04,303 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,303 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-11-28 02:56:04,303 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-11-28 02:56:04,303 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,303 [DEBUG] 
Evaluating link: ./contact.php
2024-11-28 02:56:04,303 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,303 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2024-11-28 02:56:04,303 [DEBUG] Absolute link: https://other-domain.com/contact.php
2024-11-28 02:56:04,303 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,303 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 02:56:04,303 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,303 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 02:56:04,303 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 02:56:04,303 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-28 02:56:04,303 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:56:04,303 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,303 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:56:04,303 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:56:04,303 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 02:56:04,303 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 02:56:04,303 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,303 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 02:56:04,304 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 02:56:04,304 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 02:56:04,304 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 02:56:04,304 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,304 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 02:56:04,304 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 02:56:04,304 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 02:56:04,304 [DEBUG] 
Evaluating link: ./contact
2024-11-28 02:56:04,304 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,304 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-11-28 02:56:04,304 [DEBUG] Absolute link: https://other-domain.com/contact
2024-11-28 02:56:04,304 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,304 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:56:04,304 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,304 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:56:04,304 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:56:04,304 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 02:56:04,304 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 02:56:04,304 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,304 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 02:56:04,305 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 02:56:04,305 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 02:56:04,305 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 02:56:04,305 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,305 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 02:56:04,305 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 02:56:04,305 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 02:56:04,305 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-11-28 02:56:04,305 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,305 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:56:04,305 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:56:04,305 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-11-28 02:56:04,305 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 02:56:04,305 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,305 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 02:56:04,305 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 02:56:04,305 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-28 02:56:04,305 [DEBUG] 
Evaluating link: #indexes
2024-11-28 02:56:04,305 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,306 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2024-11-28 02:56:04,306 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,306 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,306 [DEBUG] 
Evaluating link: #a
2024-11-28 02:56:04,306 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,306 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2024-11-28 02:56:04,306 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,306 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,306 [DEBUG] 
Evaluating link: #b
2024-11-28 02:56:04,306 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,306 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-11-28 02:56:04,306 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,306 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,306 [DEBUG] 
Evaluating link: #c
2024-11-28 02:56:04,306 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,306 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-11-28 02:56:04,306 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,306 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,306 [DEBUG] 
Evaluating link: #d
2024-11-28 02:56:04,306 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,306 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-11-28 02:56:04,306 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,306 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,306 [DEBUG] 
Evaluating link: #e
2024-11-28 02:56:04,307 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,307 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-11-28 02:56:04,307 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,307 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,307 [DEBUG] 
Evaluating link: #f
2024-11-28 02:56:04,307 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,307 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-11-28 02:56:04,307 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,307 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,307 [DEBUG] 
Evaluating link: #g
2024-11-28 02:56:04,307 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,307 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-11-28 02:56:04,307 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,307 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,307 [DEBUG] 
Evaluating link: #h
2024-11-28 02:56:04,307 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,307 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-11-28 02:56:04,307 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,307 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,307 [DEBUG] 
Evaluating link: #i
2024-11-28 02:56:04,307 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,307 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-11-28 02:56:04,308 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,308 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,308 [DEBUG] 
Evaluating link: #j
2024-11-28 02:56:04,308 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,308 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-11-28 02:56:04,308 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,308 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,308 [DEBUG] 
Evaluating link: #k
2024-11-28 02:56:04,308 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,308 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-11-28 02:56:04,308 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,308 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,308 [DEBUG] 
Evaluating link: #l
2024-11-28 02:56:04,308 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,308 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-11-28 02:56:04,308 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,308 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,308 [DEBUG] 
Evaluating link: #m
2024-11-28 02:56:04,308 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,308 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-11-28 02:56:04,308 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,308 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,308 [DEBUG] 
Evaluating link: #n
2024-11-28 02:56:04,308 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,308 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-11-28 02:56:04,309 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,309 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,309 [DEBUG] 
Evaluating link: #o
2024-11-28 02:56:04,309 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,309 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-11-28 02:56:04,309 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,309 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,309 [DEBUG] 
Evaluating link: #p
2024-11-28 02:56:04,309 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,309 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-11-28 02:56:04,309 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,309 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,309 [DEBUG] 
Evaluating link: #q
2024-11-28 02:56:04,309 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,309 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-11-28 02:56:04,309 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,309 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,309 [DEBUG] 
Evaluating link: #r
2024-11-28 02:56:04,309 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,309 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-11-28 02:56:04,309 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,309 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,309 [DEBUG] 
Evaluating link: #s
2024-11-28 02:56:04,309 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,310 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-11-28 02:56:04,310 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,310 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,310 [DEBUG] 
Evaluating link: #t
2024-11-28 02:56:04,310 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,310 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-11-28 02:56:04,310 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,310 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,310 [DEBUG] 
Evaluating link: #u
2024-11-28 02:56:04,310 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,310 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-11-28 02:56:04,310 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,310 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,310 [DEBUG] 
Evaluating link: #v
2024-11-28 02:56:04,310 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,310 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-11-28 02:56:04,310 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,310 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,310 [DEBUG] 
Evaluating link: #w
2024-11-28 02:56:04,310 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,310 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-11-28 02:56:04,310 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,310 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,310 [DEBUG] 
Evaluating link: #x
2024-11-28 02:56:04,311 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,311 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-11-28 02:56:04,311 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,311 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,311 [DEBUG] 
Evaluating link: #y
2024-11-28 02:56:04,311 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,311 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-11-28 02:56:04,311 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,311 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,311 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:56:04,311 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,311 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:56:04,311 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:56:04,311 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 02:56:04,311 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 02:56:04,311 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,311 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 02:56:04,311 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 02:56:04,311 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 02:56:04,311 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 02:56:04,311 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,311 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 02:56:04,311 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 02:56:04,311 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 02:56:04,312 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-11-28 02:56:04,312 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,312 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:56:04,312 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:56:04,312 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-11-28 02:56:04,312 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 02:56:04,312 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,312 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 02:56:04,312 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 02:56:04,312 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-28 02:56:04,312 [DEBUG] 
Evaluating link: https://spriq.jp/
2024-11-28 02:56:04,312 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,312 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2024-11-28 02:56:04,312 [DEBUG] Absolute link: https://spriq.jp/
2024-11-28 02:56:04,312 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2024-11-28 02:56:04,312 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:56:04,312 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,312 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:56:04,312 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:56:04,312 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 02:56:04,312 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:56:04,312 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,312 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:56:04,312 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:56:04,312 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 02:56:04,312 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 02:56:04,313 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,313 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 02:56:04,313 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 02:56:04,313 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 02:56:04,313 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 02:56:04,313 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,313 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 02:56:04,313 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 02:56:04,313 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 02:56:04,313 [DEBUG] 
Evaluating link: ./contact
2024-11-28 02:56:04,313 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,313 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-11-28 02:56:04,313 [DEBUG] Absolute link: https://other-domain.com/contact
2024-11-28 02:56:04,313 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,313 [DEBUG] 
Evaluating link: ./
2024-11-28 02:56:04,313 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,313 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-28 02:56:04,313 [DEBUG] Absolute link: https://other-domain.com/
2024-11-28 02:56:04,313 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,313 [DEBUG] 
Evaluating link: ./hiyou.php
2024-11-28 02:56:04,313 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,313 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-11-28 02:56:04,313 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-11-28 02:56:04,313 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,313 [DEBUG] 
Evaluating link: ./date.php
2024-11-28 02:56:04,313 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,314 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-11-28 02:56:04,314 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-11-28 02:56:04,314 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,314 [DEBUG] 
Evaluating link: #u
2024-11-28 02:56:04,314 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,314 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-11-28 02:56:04,314 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,314 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,314 [DEBUG] 
Evaluating link: #v
2024-11-28 02:56:04,314 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,314 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-11-28 02:56:04,314 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,314 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,314 [DEBUG] 
Evaluating link: #w
2024-11-28 02:56:04,314 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,314 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-11-28 02:56:04,314 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,314 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,314 [DEBUG] 
Evaluating link: #x
2024-11-28 02:56:04,314 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,314 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-11-28 02:56:04,314 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,314 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,314 [DEBUG] 
Evaluating link: #y
2024-11-28 02:56:04,314 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,314 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-11-28 02:56:04,315 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,315 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,315 [DEBUG] 
Evaluating link: #p
2024-11-28 02:56:04,315 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,315 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-11-28 02:56:04,315 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,315 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,315 [DEBUG] 
Evaluating link: #q
2024-11-28 02:56:04,315 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,315 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-11-28 02:56:04,315 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,315 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,315 [DEBUG] 
Evaluating link: #r
2024-11-28 02:56:04,315 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,315 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-11-28 02:56:04,315 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,315 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,315 [DEBUG] 
Evaluating link: #s
2024-11-28 02:56:04,315 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,315 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-11-28 02:56:04,315 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,315 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,315 [DEBUG] 
Evaluating link: #t
2024-11-28 02:56:04,315 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,316 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-11-28 02:56:04,316 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,316 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,316 [DEBUG] 
Evaluating link: #k
2024-11-28 02:56:04,316 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,316 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-11-28 02:56:04,316 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,316 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,316 [DEBUG] 
Evaluating link: #l
2024-11-28 02:56:04,316 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,316 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-11-28 02:56:04,316 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,316 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,316 [DEBUG] 
Evaluating link: #m
2024-11-28 02:56:04,316 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,316 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-11-28 02:56:04,316 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,316 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,316 [DEBUG] 
Evaluating link: #n
2024-11-28 02:56:04,316 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,316 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-11-28 02:56:04,316 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,316 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,316 [DEBUG] 
Evaluating link: #o
2024-11-28 02:56:04,316 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,317 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-11-28 02:56:04,317 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,317 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,317 [DEBUG] 
Evaluating link: #f
2024-11-28 02:56:04,317 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,317 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-11-28 02:56:04,317 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,317 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,317 [DEBUG] 
Evaluating link: #g
2024-11-28 02:56:04,317 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,317 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-11-28 02:56:04,317 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,317 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,317 [DEBUG] 
Evaluating link: #h
2024-11-28 02:56:04,317 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,317 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-11-28 02:56:04,317 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,317 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,317 [DEBUG] 
Evaluating link: #i
2024-11-28 02:56:04,317 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,317 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-11-28 02:56:04,317 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,317 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,317 [DEBUG] 
Evaluating link: #j
2024-11-28 02:56:04,317 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,317 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-11-28 02:56:04,317 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,318 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,318 [DEBUG] 
Evaluating link: #b
2024-11-28 02:56:04,318 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,318 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-11-28 02:56:04,318 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,318 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,318 [DEBUG] 
Evaluating link: #c
2024-11-28 02:56:04,318 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,318 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-11-28 02:56:04,318 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,318 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,318 [DEBUG] 
Evaluating link: #d
2024-11-28 02:56:04,318 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,318 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-11-28 02:56:04,318 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,318 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,318 [DEBUG] 
Evaluating link: #e
2024-11-28 02:56:04,318 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,318 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-11-28 02:56:04,318 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,318 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,318 [DEBUG] 
Evaluating link: #site
2024-11-28 02:56:04,318 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:56:04,318 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2024-11-28 02:56:04,318 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:56:04,318 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:56:04,319 [INFO] Successfully crawled https://other-domain.com/page
2024-11-28 02:56:04,323 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:56:04,324 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:56:04,325 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:56:04,325 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-11-28 02:56:04,325 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-11-28 02:56:04,325 [INFO] Starting crawl of URL: https://example.com/page0
2024-11-28 02:56:04,827 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:56:04,827 [DEBUG] Base URL: https://example.com/page0
2024-11-28 02:56:04,827 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:56:04,827 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:56:04,827 [DEBUG] Initial domain set to: example.com
2024-11-28 02:56:04,827 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:56:04,828 [INFO] Successfully crawled https://example.com/page0
2024-11-28 02:56:04,828 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 02:56:04,828 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 02:56:04,828 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 02:56:04,963 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:56:04,963 [DEBUG] Base URL: https://example.com/page1
2024-11-28 02:56:04,963 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:56:04,964 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:56:04,964 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:56:04,964 [INFO] Successfully crawled https://example.com/page1
2024-11-28 02:56:04,964 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 02:56:04,964 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 02:56:04,964 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 02:56:05,479 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:56:05,479 [DEBUG] Base URL: https://example.com/page2
2024-11-28 02:56:05,480 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:56:05,480 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:56:05,480 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:56:05,480 [INFO] Successfully crawled https://example.com/page2
2024-11-28 02:56:05,480 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-28 02:56:05,480 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-28 02:56:05,480 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-28 02:56:05,982 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:56:05,982 [DEBUG] Base URL: https://example.com/page3
2024-11-28 02:56:05,982 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:56:05,982 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:56:05,982 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:56:05,982 [INFO] Successfully crawled https://example.com/page3
2024-11-28 02:56:05,982 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-11-28 02:56:05,982 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-11-28 02:56:05,982 [INFO] Starting crawl of URL: https://example.com/page4
2024-11-28 02:56:06,472 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:56:06,472 [DEBUG] Base URL: https://example.com/page4
2024-11-28 02:56:06,472 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:56:06,472 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:56:06,472 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:56:06,473 [INFO] Successfully crawled https://example.com/page4
2024-11-28 02:56:06,473 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:56:06,473 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-11-28 02:56:06,473 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-11-28 02:56:06,473 [INFO] Starting crawl of URL: https://example.com/page0
2024-11-28 02:56:06,966 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:56:06,967 [DEBUG] Base URL: https://example.com/page0
2024-11-28 02:56:06,967 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:56:06,967 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:56:06,967 [DEBUG] Initial domain set to: example.com
2024-11-28 02:56:06,967 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:56:06,967 [INFO] Successfully crawled https://example.com/page0
2024-11-28 02:56:06,967 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 02:56:06,967 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 02:56:06,967 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 02:56:07,113 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:56:07,113 [DEBUG] Base URL: https://example.com/page1
2024-11-28 02:56:07,113 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:56:07,113 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:56:07,113 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:56:07,113 [INFO] Successfully crawled https://example.com/page1
2024-11-28 02:56:07,114 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 02:56:07,114 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 02:56:07,114 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 02:56:07,613 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:56:07,613 [DEBUG] Base URL: https://example.com/page2
2024-11-28 02:56:07,614 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:56:07,614 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:56:07,614 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:56:07,614 [INFO] Successfully crawled https://example.com/page2
2024-11-28 02:56:07,614 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-28 02:56:07,614 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-28 02:56:07,614 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-28 02:56:08,118 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:56:08,118 [DEBUG] Base URL: https://example.com/page3
2024-11-28 02:56:08,118 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:56:08,118 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:56:08,118 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:56:08,118 [INFO] Successfully crawled https://example.com/page3
2024-11-28 02:56:08,118 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-11-28 02:56:08,118 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-11-28 02:56:08,118 [INFO] Starting crawl of URL: https://example.com/page4
2024-11-28 02:56:08,619 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:56:08,619 [DEBUG] Base URL: https://example.com/page4
2024-11-28 02:56:08,619 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:56:08,620 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:56:08,620 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:56:08,620 [INFO] Successfully crawled https://example.com/page4
2024-11-28 02:56:08,624 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:56:08,626 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:56:08,627 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 02:56:08,631 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x138a35b50>
2024-11-28 02:56:08,631 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1389fbe00>, 209866.91365575)]']
connector: <aiohttp.connector.TCPConnector object at 0x138a35dd0>
2024-11-28 02:56:08,631 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x138a359d0>
2024-11-28 02:56:08,631 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1389d7b60>, 209869.060950333)]']
connector: <aiohttp.connector.TCPConnector object at 0x138997d50>
2024-11-28 02:56:08,635 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:56:08,636 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:56:08,637 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 02:56:08,637 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 02:56:08,637 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 02:56:08,637 [INFO] Starting crawl of URL: https://example.com
2024-11-28 02:56:10,638 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 02:56:18,639 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 02:56:18,639 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 02:56:18,646 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:56:18,650 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:56:18,651 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 02:56:18,652 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 02:56:18,652 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 02:56:18,652 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 02:56:18,652 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 02:56:18,652 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 02:56:18,652 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 02:56:20,653 [WARNING] Retry 1/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 02:56:30,654 [WARNING] Retry 1/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 02:56:38,654 [WARNING] Retry 2/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 02:56:38,655 [ERROR] Error crawling https://example.com/page1: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 02:56:48,655 [WARNING] Retry 2/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 02:56:48,657 [ERROR] Error crawling https://example.com/page2: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 02:56:48,666 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:56:48,667 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:56:48,668 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 02:56:48,669 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 02:56:48,669 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 02:56:48,669 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 02:56:48,670 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 02:56:48,670 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 02:56:48,670 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 02:56:48,670 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-28 02:56:48,670 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-28 02:56:48,670 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-28 02:56:50,671 [WARNING] Retry 1/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 02:57:00,671 [WARNING] Retry 1/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 02:57:10,672 [WARNING] Retry 1/2 for https://example.com/page3: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 02:57:18,672 [WARNING] Retry 2/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 02:57:18,672 [ERROR] Error crawling https://example.com/page1: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 02:57:28,673 [WARNING] Retry 2/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 02:57:28,674 [ERROR] Error crawling https://example.com/page2: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 02:57:38,674 [WARNING] Retry 2/2 for https://example.com/page3: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 02:57:38,675 [ERROR] Error crawling https://example.com/page3: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 02:57:38,676 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:57:38,677 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:57:38,678 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 02:57:38,683 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:57:38,684 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:57:38,685 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 02:57:38,685 [DEBUG] Using raw URL string: https://nonexistent.example.com (type: <class 'str'>)
2024-11-28 02:57:38,685 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='nonexistent.example.com', path='', params='', query='', fragment='')
2024-11-28 02:57:38,685 [INFO] Starting crawl of URL: https://nonexistent.example.com
2024-11-28 02:57:40,686 [WARNING] Retry 1/2 for https://nonexistent.example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 02:57:48,686 [WARNING] Retry 2/2 for https://nonexistent.example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 02:57:48,686 [ERROR] Error crawling https://nonexistent.example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 02:57:48,691 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:57:48,692 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:57:48,693 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 02:57:48,697 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:57:48,698 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:57:48,699 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 02:57:48,699 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 02:57:48,699 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 02:57:48,699 [INFO] Starting crawl of URL: https://example.com
2024-11-28 02:57:50,700 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 02:57:58,701 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 02:57:58,701 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 02:57:58,709 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:57:58,710 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:57:58,712 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 02:57:58,712 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 02:57:58,712 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 02:57:58,713 [INFO] Starting crawl of URL: https://example.com
2024-11-28 02:58:00,714 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 02:58:08,714 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 02:58:08,715 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 02:58:08,722 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:08,754 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:58:08,754 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:08,756 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:08,757 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:58:08,757 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:08,758 [DEBUG] 
Evaluating link: https://example.com/doc1
2024-11-28 02:58:08,758 [DEBUG] Target domain: example.com
2024-11-28 02:58:08,758 [DEBUG] URL domain: example.com
2024-11-28 02:58:08,758 [DEBUG] URL approved for crawling: https://example.com/doc1
2024-11-28 02:58:08,758 [DEBUG] 
Evaluating link: https://example.com/excluded/page
2024-11-28 02:58:08,758 [DEBUG] Target domain: example.com
2024-11-28 02:58:08,758 [DEBUG] URL domain: example.com
2024-11-28 02:58:08,758 [DEBUG] URL approved for crawling: https://example.com/excluded/page
2024-11-28 02:58:08,766 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:08,767 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:58:08,767 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:08,773 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:08,774 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:58:08,774 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:08,775 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-28 02:58:08,781 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:08,782 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:58:08,782 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:08,787 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:08,788 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:58:08,789 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:08,789 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-28 02:58:08,790 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:08,791 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:58:08,791 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:08,791 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-28 02:58:08,797 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:08,798 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:58:08,798 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:08,799 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-28 02:58:08,805 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:08,805 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:58:08,806 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:08,806 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-28 02:58:08,835 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1389cf950>
2024-11-28 02:58:08,836 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1389d65f0>, 209858.191258625)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x1389d5e80>, 209859.78845775)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x1389d6040>, 209862.449259125)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x1389d6270>, 209864.724747041)]']
connector: <aiohttp.connector.TCPConnector object at 0x1389cd510>
2024-11-28 02:58:08,842 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:08,843 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:58:08,843 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:08,844 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-28 02:58:08,844 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:08,845 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:58:08,845 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:08,846 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-28 02:58:08,851 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:08,868 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:08,874 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:08,875 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:08,875 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:58:08,881 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:08,881 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:08,882 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:58:08,887 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:08,891 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:58:08,901 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:58:08,913 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:58:08,923 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:58:08,933 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:58:08,944 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:58:08,953 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:58:08,963 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:58:08,973 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:58:08,983 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:58:08,996 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:09,339 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:58:09,341 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:58:09,341 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 02:58:09,341 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 02:58:09,341 [INFO] Starting crawl of URL: https://example.com
2024-11-28 02:58:09,830 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:58:09,830 [DEBUG] Base URL: https://example.com
2024-11-28 02:58:09,830 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:58:09,830 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:58:09,830 [DEBUG] Initial domain set to: example.com
2024-11-28 02:58:09,830 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:58:09,830 [INFO] Successfully crawled https://example.com
2024-11-28 02:58:09,830 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x13f4d0350>
2024-11-28 02:58:09,830 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x13f27d550>, 209990.270623375)]']
connector: <aiohttp.connector.TCPConnector object at 0x13f422990>
2024-11-28 02:58:09,833 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2024-11-28 02:58:11,201 [INFO] Operation took 0.10 seconds
2024-11-28 02:59:43,832 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 02:59:44,416 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:44,417 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:44,418 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:44,420 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:44,421 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:44,421 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:44,422 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:59:44,442 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:44,443 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:44,443 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:59:44,447 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:44,448 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:44,452 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:44,452 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:44,457 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:44,457 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:44,458 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:44,459 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:44,463 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:44,464 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:44,464 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:59:44,468 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:44,471 [DEBUG] Found 0 links in 
2024-11-28 02:59:44,482 [DEBUG] Found 0 links in 
2024-11-28 02:59:44,492 [DEBUG] Found 0 links in 
2024-11-28 02:59:44,502 [DEBUG] Found 0 links in 
2024-11-28 02:59:44,530 [DEBUG] Found 2 links in 
2024-11-28 02:59:44,530 [DEBUG] Link: https://example.com (external) from content
2024-11-28 02:59:44,530 [DEBUG] Link: relative/path (internal) from content
2024-11-28 02:59:44,539 [DEBUG] Found 0 links in 
2024-11-28 02:59:44,548 [DEBUG] Found 0 links in 
2024-11-28 02:59:44,557 [DEBUG] Found 0 links in 
2024-11-28 02:59:44,567 [DEBUG] Found 0 links in 
2024-11-28 02:59:44,692 [DEBUG] Found 0 links in 
2024-11-28 02:59:44,730 [INFO] Operation took 0.16 seconds
2024-11-28 02:59:44,740 [DEBUG] Found 0 links in 
2024-11-28 02:59:44,749 [DEBUG] Found 0 links in 
2024-11-28 02:59:45,113 [INFO] test_operation took 0.10 seconds
2024-11-28 02:59:45,323 [INFO] inner took 0.10 seconds
2024-11-28 02:59:45,323 [INFO] outer took 0.21 seconds
2024-11-28 02:59:45,323 [INFO] error_operation took 0.00 seconds
2024-11-28 02:59:45,372 [INFO] test_operation took 0.00 seconds
2024-11-28 02:59:45,380 [DEBUG] Found 0 links in https://example.com
2024-11-28 02:59:45,385 [DEBUG] Found 0 links in https://example.com
2024-11-28 02:59:45,401 [DEBUG] Found 0 links in https://example.com
1970-01-01 02:00:00,000 [DEBUG] Found 0 links in https://example.com
2024-11-28 02:59:45,448 [DEBUG] Found 0 links in https://example.com
2024-11-28 02:59:45,488 [DEBUG] Found 0 links in https://example.com
2024-11-28 02:59:45,506 [DEBUG] Found 0 links in https://example.com
2024-11-28 02:59:45,522 [DEBUG] Found 0 links in https://example.com
2024-11-28 02:59:45,540 [DEBUG] Found 0 links in https://example.com
2024-11-28 02:59:45,557 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:59:45,557 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:45,558 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 02:59:45,558 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 02:59:45,558 [INFO] Starting crawl of URL: https://example.com
2024-11-28 02:59:46,056 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:59:46,056 [DEBUG] Base URL: https://example.com
2024-11-28 02:59:46,056 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:59:46,056 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:59:46,056 [DEBUG] Initial domain set to: example.com
2024-11-28 02:59:46,056 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:59:46,056 [INFO] Successfully crawled https://example.com
2024-11-28 02:59:46,057 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:46,058 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x127b97ed0>
2024-11-28 02:59:46,058 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1279d4b40>, 210086.496386958)]']
connector: <aiohttp.connector.TCPConnector object at 0x127b95050>
2024-11-28 02:59:46,058 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:59:46,058 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:46,059 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 02:59:46,059 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 02:59:46,059 [INFO] Starting crawl of URL: https://example.com
2024-11-28 02:59:46,075 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-11-28 02:59:46,075 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-11-28 02:59:46,075 [INFO] Starting crawl of URL: https://example.org
2024-11-28 02:59:46,554 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:59:46,554 [DEBUG] Base URL: https://example.com
2024-11-28 02:59:46,555 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:59:46,556 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:59:46,556 [DEBUG] Initial domain set to: example.com
2024-11-28 02:59:46,556 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:59:46,556 [INFO] Successfully crawled https://example.com
2024-11-28 02:59:47,046 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:59:47,046 [DEBUG] Base URL: https://example.org
2024-11-28 02:59:47,046 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:59:47,046 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:59:47,046 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:59:47,046 [INFO] Successfully crawled https://example.org
2024-11-28 02:59:47,047 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:47,048 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x127a14890>
2024-11-28 02:59:47,048 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1279d4b40>, 210086.989955)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x1279d6430>, 210087.48647575)]']
connector: <aiohttp.connector.TCPConnector object at 0x127a14290>
2024-11-28 02:59:47,048 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:59:47,048 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:47,049 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 02:59:47,049 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 02:59:47,049 [INFO] Starting crawl of URL: https://example.com
2024-11-28 02:59:47,549 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:59:47,549 [DEBUG] Base URL: https://example.com
2024-11-28 02:59:47,549 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:59:47,549 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:59:47,549 [DEBUG] Initial domain set to: example.com
2024-11-28 02:59:47,549 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:59:47,549 [INFO] Successfully crawled https://example.com
2024-11-28 02:59:47,550 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:47,550 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x127b94510>
2024-11-28 02:59:47,551 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1279d6660>, 210087.989328333)]']
connector: <aiohttp.connector.TCPConnector object at 0x127a14fd0>
2024-11-28 02:59:47,551 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:59:47,551 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:47,552 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 02:59:47,552 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 02:59:47,552 [INFO] Starting crawl of URL: https://example.com
2024-11-28 02:59:48,050 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:59:48,050 [DEBUG] Base URL: https://example.com
2024-11-28 02:59:48,050 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:59:48,050 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:59:48,050 [DEBUG] Initial domain set to: example.com
2024-11-28 02:59:48,050 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:59:48,050 [INFO] Successfully crawled https://example.com
2024-11-28 02:59:48,051 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:59:48,051 [DEBUG] Base URL: https://example.com
2024-11-28 02:59:48,051 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:59:48,051 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:59:48,051 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:59:48,056 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:48,057 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:59:48,057 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:48,061 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1279894d0>
2024-11-28 02:59:48,061 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1279d4f30>, 210088.488630958)]']
connector: <aiohttp.connector.TCPConnector object at 0x127989dd0>
2024-11-28 02:59:48,062 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:48,063 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:59:48,063 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:48,064 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 02:59:48,064 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 02:59:48,064 [INFO] Starting crawl of URL: https://example.com
2024-11-28 02:59:48,561 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:59:48,562 [DEBUG] Base URL: https://example.com
2024-11-28 02:59:48,562 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:59:48,562 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:59:48,562 [DEBUG] Initial domain set to: example.com
2024-11-28 02:59:48,562 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:59:48,562 [INFO] Successfully crawled https://example.com
2024-11-28 02:59:48,562 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:48,563 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x127b98c50>
2024-11-28 02:59:48,563 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1279d6580>, 210089.0018835)]']
connector: <aiohttp.connector.TCPConnector object at 0x127b99e10>
2024-11-28 02:59:48,563 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:59:48,564 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:48,564 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2024-11-28 02:59:48,564 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2024-11-28 02:59:48,564 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2024-11-28 02:59:50,582 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-28 02:59:50,583 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-28 02:59:50,584 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-28 02:59:50,589 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:50,590 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:59:50,590 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:50,591 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 02:59:50,591 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 02:59:50,591 [INFO] Starting crawl of URL: https://example.com
2024-11-28 02:59:50,607 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-11-28 02:59:50,607 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-11-28 02:59:50,607 [INFO] Starting crawl of URL: https://example.org
2024-11-28 02:59:50,607 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2024-11-28 02:59:50,607 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2024-11-28 02:59:50,607 [INFO] Starting crawl of URL: https://example.net
2024-11-28 02:59:51,086 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:59:51,087 [DEBUG] Base URL: https://example.com
2024-11-28 02:59:51,087 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:59:51,087 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:59:51,087 [DEBUG] Initial domain set to: example.com
2024-11-28 02:59:51,087 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:59:51,087 [INFO] Successfully crawled https://example.com
2024-11-28 02:59:51,588 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:59:51,588 [DEBUG] Base URL: https://example.org
2024-11-28 02:59:51,588 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:59:51,589 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:59:51,589 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:59:51,589 [INFO] Successfully crawled https://example.org
2024-11-28 02:59:52,085 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:59:52,085 [DEBUG] Base URL: https://example.net
2024-11-28 02:59:52,085 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:59:52,085 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:59:52,085 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:59:52,085 [INFO] Successfully crawled https://example.net
2024-11-28 02:59:52,086 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:52,087 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x127b99990>
2024-11-28 02:59:52,087 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1279dc520>, 210091.526409458)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x1279dcb40>, 210092.028532)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x1279dd4e0>, 210092.525240208)]']
connector: <aiohttp.connector.TCPConnector object at 0x127b99590>
2024-11-28 02:59:52,087 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:59:52,087 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:52,088 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 02:59:52,088 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 02:59:52,088 [INFO] Starting crawl of URL: https://example.com
2024-11-28 02:59:52,576 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:59:52,576 [DEBUG] Base URL: https://example.com
2024-11-28 02:59:52,576 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:59:52,577 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:59:52,577 [DEBUG] Initial domain set to: example.com
2024-11-28 02:59:52,577 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:59:52,577 [INFO] Successfully crawled https://example.com
2024-11-28 02:59:52,578 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:52,584 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:59:52,584 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:52,585 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 02:59:52,604 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:59:52,620 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:52,621 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:59:52,621 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:52,621 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:59:52,636 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:52,637 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:59:52,637 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:52,638 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:59:52,638 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2024-11-28 02:59:52,638 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2024-11-28 02:59:52,638 [INFO] Starting crawl of URL: https://example.com/page
2024-11-28 02:59:53,141 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:59:53,142 [DEBUG] Base URL: https://example.com/page
2024-11-28 02:59:53,142 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:59:53,142 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:59:53,142 [DEBUG] Initial domain set to: example.com
2024-11-28 02:59:53,142 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:59:53,142 [INFO] Successfully crawled https://example.com/page
2024-11-28 02:59:53,142 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2024-11-28 02:59:53,142 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2024-11-28 02:59:53,142 [INFO] Starting crawl of URL: https://other-domain.com/page
2024-11-28 02:59:56,850 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x127a06310>
2024-11-28 02:59:56,867 [DEBUG] 
Evaluating link: ./
2024-11-28 02:59:56,867 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,867 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-28 02:59:56,867 [DEBUG] Absolute link: https://other-domain.com/
2024-11-28 02:59:56,867 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,867 [DEBUG] 
Evaluating link: ./
2024-11-28 02:59:56,867 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,867 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-28 02:59:56,867 [DEBUG] Absolute link: https://other-domain.com/
2024-11-28 02:59:56,868 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,868 [DEBUG] 
Evaluating link: ./hiyou.php
2024-11-28 02:59:56,868 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,868 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-11-28 02:59:56,868 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-11-28 02:59:56,868 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,868 [DEBUG] 
Evaluating link: ./date.php
2024-11-28 02:59:56,868 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,868 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-11-28 02:59:56,868 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-11-28 02:59:56,868 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,868 [DEBUG] 
Evaluating link: ./contact.php
2024-11-28 02:59:56,868 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,868 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2024-11-28 02:59:56,868 [DEBUG] Absolute link: https://other-domain.com/contact.php
2024-11-28 02:59:56,868 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,869 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 02:59:56,869 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,869 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 02:59:56,869 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 02:59:56,869 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-28 02:59:56,869 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:59:56,869 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,869 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:59:56,869 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:59:56,869 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 02:59:56,869 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 02:59:56,869 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,869 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 02:59:56,869 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 02:59:56,869 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 02:59:56,870 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 02:59:56,870 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,870 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 02:59:56,870 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 02:59:56,870 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 02:59:56,870 [DEBUG] 
Evaluating link: ./contact
2024-11-28 02:59:56,870 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,870 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-11-28 02:59:56,870 [DEBUG] Absolute link: https://other-domain.com/contact
2024-11-28 02:59:56,870 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,870 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:59:56,870 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,870 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:59:56,870 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:59:56,870 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 02:59:56,870 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 02:59:56,870 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,870 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 02:59:56,871 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 02:59:56,871 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 02:59:56,871 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 02:59:56,871 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,871 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 02:59:56,871 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 02:59:56,871 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 02:59:56,871 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-11-28 02:59:56,871 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,871 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:59:56,871 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:59:56,871 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-11-28 02:59:56,871 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 02:59:56,871 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,871 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 02:59:56,871 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 02:59:56,871 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-28 02:59:56,872 [DEBUG] 
Evaluating link: #indexes
2024-11-28 02:59:56,872 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,872 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2024-11-28 02:59:56,872 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,872 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,872 [DEBUG] 
Evaluating link: #a
2024-11-28 02:59:56,872 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,872 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2024-11-28 02:59:56,872 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,872 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,872 [DEBUG] 
Evaluating link: #b
2024-11-28 02:59:56,872 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,873 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-11-28 02:59:56,873 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,873 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,873 [DEBUG] 
Evaluating link: #c
2024-11-28 02:59:56,873 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,873 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-11-28 02:59:56,873 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,873 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,873 [DEBUG] 
Evaluating link: #d
2024-11-28 02:59:56,873 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,873 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-11-28 02:59:56,873 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,873 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,873 [DEBUG] 
Evaluating link: #e
2024-11-28 02:59:56,873 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,873 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-11-28 02:59:56,874 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,874 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,874 [DEBUG] 
Evaluating link: #f
2024-11-28 02:59:56,874 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,874 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-11-28 02:59:56,874 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,874 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,874 [DEBUG] 
Evaluating link: #g
2024-11-28 02:59:56,874 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,874 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-11-28 02:59:56,874 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,874 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,874 [DEBUG] 
Evaluating link: #h
2024-11-28 02:59:56,874 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,874 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-11-28 02:59:56,874 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,875 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,875 [DEBUG] 
Evaluating link: #i
2024-11-28 02:59:56,875 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,875 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-11-28 02:59:56,875 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,875 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,875 [DEBUG] 
Evaluating link: #j
2024-11-28 02:59:56,875 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,875 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-11-28 02:59:56,875 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,875 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,875 [DEBUG] 
Evaluating link: #k
2024-11-28 02:59:56,875 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,875 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-11-28 02:59:56,875 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,876 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,876 [DEBUG] 
Evaluating link: #l
2024-11-28 02:59:56,876 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,876 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-11-28 02:59:56,876 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,876 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,876 [DEBUG] 
Evaluating link: #m
2024-11-28 02:59:56,876 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,876 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-11-28 02:59:56,876 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,876 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,876 [DEBUG] 
Evaluating link: #n
2024-11-28 02:59:56,876 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,876 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-11-28 02:59:56,876 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,876 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,877 [DEBUG] 
Evaluating link: #o
2024-11-28 02:59:56,877 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,877 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-11-28 02:59:56,877 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,877 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,877 [DEBUG] 
Evaluating link: #p
2024-11-28 02:59:56,877 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,877 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-11-28 02:59:56,877 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,877 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,877 [DEBUG] 
Evaluating link: #q
2024-11-28 02:59:56,877 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,877 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-11-28 02:59:56,877 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,877 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,877 [DEBUG] 
Evaluating link: #r
2024-11-28 02:59:56,877 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,878 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-11-28 02:59:56,878 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,878 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,878 [DEBUG] 
Evaluating link: #s
2024-11-28 02:59:56,878 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,878 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-11-28 02:59:56,878 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,878 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,878 [DEBUG] 
Evaluating link: #t
2024-11-28 02:59:56,878 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,878 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-11-28 02:59:56,878 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,878 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,878 [DEBUG] 
Evaluating link: #u
2024-11-28 02:59:56,878 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,878 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-11-28 02:59:56,878 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,878 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,878 [DEBUG] 
Evaluating link: #v
2024-11-28 02:59:56,879 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,879 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-11-28 02:59:56,879 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,879 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,879 [DEBUG] 
Evaluating link: #w
2024-11-28 02:59:56,879 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,879 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-11-28 02:59:56,879 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,879 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,879 [DEBUG] 
Evaluating link: #x
2024-11-28 02:59:56,879 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,879 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-11-28 02:59:56,879 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,879 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,879 [DEBUG] 
Evaluating link: #y
2024-11-28 02:59:56,879 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,880 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-11-28 02:59:56,880 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,880 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,880 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:59:56,880 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,880 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:59:56,880 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:59:56,880 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 02:59:56,880 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 02:59:56,880 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,880 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 02:59:56,880 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 02:59:56,880 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 02:59:56,880 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 02:59:56,880 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,881 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 02:59:56,881 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 02:59:56,881 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 02:59:56,881 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-11-28 02:59:56,881 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,881 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:59:56,881 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:59:56,881 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-11-28 02:59:56,881 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 02:59:56,881 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,881 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 02:59:56,881 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 02:59:56,881 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-28 02:59:56,881 [DEBUG] 
Evaluating link: https://spriq.jp/
2024-11-28 02:59:56,881 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,881 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2024-11-28 02:59:56,881 [DEBUG] Absolute link: https://spriq.jp/
2024-11-28 02:59:56,881 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2024-11-28 02:59:56,881 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:59:56,882 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,882 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:59:56,882 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:59:56,882 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 02:59:56,882 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:59:56,882 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,882 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:59:56,882 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 02:59:56,882 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 02:59:56,882 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 02:59:56,882 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,882 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 02:59:56,882 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 02:59:56,882 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 02:59:56,882 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 02:59:56,882 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,882 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 02:59:56,882 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 02:59:56,883 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 02:59:56,883 [DEBUG] 
Evaluating link: ./contact
2024-11-28 02:59:56,883 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,883 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-11-28 02:59:56,883 [DEBUG] Absolute link: https://other-domain.com/contact
2024-11-28 02:59:56,883 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,883 [DEBUG] 
Evaluating link: ./
2024-11-28 02:59:56,883 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,883 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-28 02:59:56,883 [DEBUG] Absolute link: https://other-domain.com/
2024-11-28 02:59:56,883 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,883 [DEBUG] 
Evaluating link: ./hiyou.php
2024-11-28 02:59:56,883 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,883 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-11-28 02:59:56,883 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-11-28 02:59:56,883 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,883 [DEBUG] 
Evaluating link: ./date.php
2024-11-28 02:59:56,883 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,883 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-11-28 02:59:56,884 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-11-28 02:59:56,884 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,884 [DEBUG] 
Evaluating link: #u
2024-11-28 02:59:56,884 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,884 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-11-28 02:59:56,884 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,884 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,884 [DEBUG] 
Evaluating link: #v
2024-11-28 02:59:56,884 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,884 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-11-28 02:59:56,884 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,884 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,884 [DEBUG] 
Evaluating link: #w
2024-11-28 02:59:56,884 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,884 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-11-28 02:59:56,884 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,884 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,884 [DEBUG] 
Evaluating link: #x
2024-11-28 02:59:56,884 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,884 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-11-28 02:59:56,885 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,885 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,885 [DEBUG] 
Evaluating link: #y
2024-11-28 02:59:56,885 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,885 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-11-28 02:59:56,885 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,885 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,885 [DEBUG] 
Evaluating link: #p
2024-11-28 02:59:56,885 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,885 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-11-28 02:59:56,885 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,885 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,885 [DEBUG] 
Evaluating link: #q
2024-11-28 02:59:56,885 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,885 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-11-28 02:59:56,885 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,885 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,885 [DEBUG] 
Evaluating link: #r
2024-11-28 02:59:56,885 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,886 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-11-28 02:59:56,886 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,886 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,886 [DEBUG] 
Evaluating link: #s
2024-11-28 02:59:56,886 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,886 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-11-28 02:59:56,886 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,886 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,886 [DEBUG] 
Evaluating link: #t
2024-11-28 02:59:56,886 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,886 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-11-28 02:59:56,886 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,886 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,886 [DEBUG] 
Evaluating link: #k
2024-11-28 02:59:56,886 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,886 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-11-28 02:59:56,886 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,886 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,886 [DEBUG] 
Evaluating link: #l
2024-11-28 02:59:56,887 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,887 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-11-28 02:59:56,887 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,887 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,887 [DEBUG] 
Evaluating link: #m
2024-11-28 02:59:56,887 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,887 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-11-28 02:59:56,887 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,887 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,887 [DEBUG] 
Evaluating link: #n
2024-11-28 02:59:56,887 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,887 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-11-28 02:59:56,887 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,887 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,887 [DEBUG] 
Evaluating link: #o
2024-11-28 02:59:56,887 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,887 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-11-28 02:59:56,887 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,887 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,888 [DEBUG] 
Evaluating link: #f
2024-11-28 02:59:56,888 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,888 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-11-28 02:59:56,888 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,888 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,888 [DEBUG] 
Evaluating link: #g
2024-11-28 02:59:56,888 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,888 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-11-28 02:59:56,888 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,888 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,888 [DEBUG] 
Evaluating link: #h
2024-11-28 02:59:56,888 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,888 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-11-28 02:59:56,888 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,888 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,889 [DEBUG] 
Evaluating link: #i
2024-11-28 02:59:56,889 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,889 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-11-28 02:59:56,889 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,889 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,889 [DEBUG] 
Evaluating link: #j
2024-11-28 02:59:56,889 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,889 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-11-28 02:59:56,889 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,889 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,889 [DEBUG] 
Evaluating link: #b
2024-11-28 02:59:56,889 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,889 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-11-28 02:59:56,889 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,889 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,889 [DEBUG] 
Evaluating link: #c
2024-11-28 02:59:56,889 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,890 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-11-28 02:59:56,890 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,890 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,890 [DEBUG] 
Evaluating link: #d
2024-11-28 02:59:56,890 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,890 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-11-28 02:59:56,890 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,890 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,890 [DEBUG] 
Evaluating link: #e
2024-11-28 02:59:56,890 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,890 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-11-28 02:59:56,890 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,890 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,890 [DEBUG] 
Evaluating link: #site
2024-11-28 02:59:56,890 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 02:59:56,890 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2024-11-28 02:59:56,890 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 02:59:56,890 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 02:59:56,891 [INFO] Successfully crawled https://other-domain.com/page
2024-11-28 02:59:56,896 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:56,898 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:59:56,898 [DEBUG] Using selector: KqueueSelector
2024-11-28 02:59:56,899 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-11-28 02:59:56,900 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-11-28 02:59:56,900 [INFO] Starting crawl of URL: https://example.com/page0
2024-11-28 02:59:57,399 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:59:57,400 [DEBUG] Base URL: https://example.com/page0
2024-11-28 02:59:57,400 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:59:57,400 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:59:57,400 [DEBUG] Initial domain set to: example.com
2024-11-28 02:59:57,400 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:59:57,400 [INFO] Successfully crawled https://example.com/page0
2024-11-28 02:59:57,400 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 02:59:57,401 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 02:59:57,401 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 02:59:57,543 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:59:57,543 [DEBUG] Base URL: https://example.com/page1
2024-11-28 02:59:57,543 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:59:57,543 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:59:57,543 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:59:57,544 [INFO] Successfully crawled https://example.com/page1
2024-11-28 02:59:57,544 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 02:59:57,544 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 02:59:57,544 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 02:59:58,085 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:59:58,085 [DEBUG] Base URL: https://example.com/page2
2024-11-28 02:59:58,085 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:59:58,085 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:59:58,085 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:59:58,085 [INFO] Successfully crawled https://example.com/page2
2024-11-28 02:59:58,085 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-28 02:59:58,085 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-28 02:59:58,085 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-28 02:59:58,554 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:59:58,554 [DEBUG] Base URL: https://example.com/page3
2024-11-28 02:59:58,555 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:59:58,555 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:59:58,555 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:59:58,555 [INFO] Successfully crawled https://example.com/page3
2024-11-28 02:59:58,555 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-11-28 02:59:58,556 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-11-28 02:59:58,557 [INFO] Starting crawl of URL: https://example.com/page4
2024-11-28 02:59:59,051 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:59:59,051 [DEBUG] Base URL: https://example.com/page4
2024-11-28 02:59:59,051 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:59:59,051 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:59:59,051 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:59:59,051 [INFO] Successfully crawled https://example.com/page4
2024-11-28 02:59:59,051 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 02:59:59,051 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-11-28 02:59:59,051 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-11-28 02:59:59,051 [INFO] Starting crawl of URL: https://example.com/page0
2024-11-28 02:59:59,537 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:59:59,537 [DEBUG] Base URL: https://example.com/page0
2024-11-28 02:59:59,537 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:59:59,537 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:59:59,537 [DEBUG] Initial domain set to: example.com
2024-11-28 02:59:59,537 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:59:59,538 [INFO] Successfully crawled https://example.com/page0
2024-11-28 02:59:59,538 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 02:59:59,538 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 02:59:59,538 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 02:59:59,687 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 02:59:59,687 [DEBUG] Base URL: https://example.com/page1
2024-11-28 02:59:59,688 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 02:59:59,688 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 02:59:59,688 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 02:59:59,688 [INFO] Successfully crawled https://example.com/page1
2024-11-28 02:59:59,688 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 02:59:59,688 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 02:59:59,688 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 03:00:00,193 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 03:00:00,193 [DEBUG] Base URL: https://example.com/page2
2024-11-28 03:00:00,193 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 03:00:00,193 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 03:00:00,194 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 03:00:00,194 [INFO] Successfully crawled https://example.com/page2
2024-11-28 03:00:00,194 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-28 03:00:00,194 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-28 03:00:00,194 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-28 03:00:00,692 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 03:00:00,692 [DEBUG] Base URL: https://example.com/page3
2024-11-28 03:00:00,692 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 03:00:00,692 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 03:00:00,693 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 03:00:00,693 [INFO] Successfully crawled https://example.com/page3
2024-11-28 03:00:00,693 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-11-28 03:00:00,693 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-11-28 03:00:00,693 [INFO] Starting crawl of URL: https://example.com/page4
2024-11-28 03:00:01,194 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 03:00:01,194 [DEBUG] Base URL: https://example.com/page4
2024-11-28 03:00:01,194 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 03:00:01,195 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 03:00:01,195 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 03:00:01,195 [INFO] Successfully crawled https://example.com/page4
2024-11-28 03:00:01,200 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:00:01,201 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:00:01,201 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 03:00:01,204 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x127bd0610>
2024-11-28 03:00:01,204 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1279dcb40>, 210099.4909825)]']
connector: <aiohttp.connector.TCPConnector object at 0x1279ab110>
2024-11-28 03:00:01,204 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x127bd19d0>
2024-11-28 03:00:01,204 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1279df850>, 210101.634774833)]']
connector: <aiohttp.connector.TCPConnector object at 0x127bd1cd0>
2024-11-28 03:00:01,210 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:00:01,211 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:00:01,212 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 03:00:01,212 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 03:00:01,212 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 03:00:01,212 [INFO] Starting crawl of URL: https://example.com
2024-11-28 03:00:03,213 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 03:00:11,213 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 03:00:11,214 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 03:00:11,220 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:00:11,224 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:00:11,225 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 03:00:11,226 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 03:00:11,226 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 03:00:11,226 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 03:00:11,226 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 03:00:11,226 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 03:00:11,226 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 03:00:13,227 [WARNING] Retry 1/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 03:00:23,228 [WARNING] Retry 1/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 03:00:31,228 [WARNING] Retry 2/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 03:00:31,229 [ERROR] Error crawling https://example.com/page1: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 03:00:41,230 [WARNING] Retry 2/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 03:00:41,231 [ERROR] Error crawling https://example.com/page2: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 03:00:41,238 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:00:41,239 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:00:41,241 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 03:00:41,241 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 03:00:41,241 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 03:00:41,241 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 03:00:41,242 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 03:00:41,242 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 03:00:41,242 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 03:00:41,242 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-28 03:00:41,242 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-28 03:00:41,242 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-28 03:00:43,243 [WARNING] Retry 1/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 03:00:53,243 [WARNING] Retry 1/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 03:01:03,243 [WARNING] Retry 1/2 for https://example.com/page3: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 03:01:11,244 [WARNING] Retry 2/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 03:01:11,244 [ERROR] Error crawling https://example.com/page1: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 03:01:21,245 [WARNING] Retry 2/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 03:01:21,246 [ERROR] Error crawling https://example.com/page2: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 03:01:31,245 [WARNING] Retry 2/2 for https://example.com/page3: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 03:01:31,246 [ERROR] Error crawling https://example.com/page3: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 03:01:31,246 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:01:31,247 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:01:31,248 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 03:01:31,253 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:01:31,254 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:01:31,255 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 03:01:31,255 [DEBUG] Using raw URL string: https://nonexistent.example.com (type: <class 'str'>)
2024-11-28 03:01:31,255 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='nonexistent.example.com', path='', params='', query='', fragment='')
2024-11-28 03:01:31,255 [INFO] Starting crawl of URL: https://nonexistent.example.com
2024-11-28 03:01:33,255 [WARNING] Retry 1/2 for https://nonexistent.example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 03:01:41,254 [WARNING] Retry 2/2 for https://nonexistent.example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 03:01:41,255 [ERROR] Error crawling https://nonexistent.example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 03:01:41,260 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:01:41,261 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:01:41,263 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 03:01:41,268 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:01:41,269 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:01:41,270 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 03:01:41,270 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 03:01:41,270 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 03:01:41,270 [INFO] Starting crawl of URL: https://example.com
2024-11-28 03:01:43,271 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 03:01:51,270 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 03:01:51,270 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 03:01:51,278 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:01:51,279 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:01:51,280 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 03:01:51,280 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 03:01:51,280 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 03:01:51,280 [INFO] Starting crawl of URL: https://example.com
2024-11-28 03:01:53,281 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 03:02:01,281 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 03:02:01,281 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 03:02:01,286 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,323 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x126826bd0>
2024-11-28 03:02:01,323 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1279df460>, 210093.581636708)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x1279dc050>, 210094.803364958)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x1279dc600>, 210095.738183875)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x1279dc6e0>, 210097.27865625)]']
connector: <aiohttp.connector.TCPConnector object at 0x126825210>
2024-11-28 03:02:01,332 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 03:02:01,332 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,333 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,334 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 03:02:01,334 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,335 [DEBUG] 
Evaluating link: https://example.com/doc1
2024-11-28 03:02:01,335 [DEBUG] Target domain: example.com
2024-11-28 03:02:01,335 [DEBUG] URL domain: example.com
2024-11-28 03:02:01,335 [DEBUG] URL approved for crawling: https://example.com/doc1
2024-11-28 03:02:01,335 [DEBUG] 
Evaluating link: https://example.com/excluded/page
2024-11-28 03:02:01,335 [DEBUG] Target domain: example.com
2024-11-28 03:02:01,335 [DEBUG] URL domain: example.com
2024-11-28 03:02:01,336 [DEBUG] URL approved for crawling: https://example.com/excluded/page
2024-11-28 03:02:01,341 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,342 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 03:02:01,343 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,348 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,349 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 03:02:01,350 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,350 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-28 03:02:01,355 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,356 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 03:02:01,356 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,362 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,363 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 03:02:01,363 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,364 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-28 03:02:01,364 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,365 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 03:02:01,365 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,366 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-28 03:02:01,371 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,372 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 03:02:01,372 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,372 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-28 03:02:01,378 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,378 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 03:02:01,379 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,379 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-28 03:02:01,384 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,385 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 03:02:01,385 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,386 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-28 03:02:01,386 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,387 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 03:02:01,387 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,388 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-28 03:02:01,393 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,410 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,415 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,415 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,416 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 03:02:01,422 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,423 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,423 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 03:02:01,429 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,433 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 03:02:01,445 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 03:02:01,455 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 03:02:01,467 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 03:02:01,477 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 03:02:01,487 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 03:02:01,497 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 03:02:01,507 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 03:02:01,517 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 03:02:01,528 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 03:02:01,541 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,866 [DEBUG] Using selector: KqueueSelector
2024-11-28 03:02:01,871 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 03:02:01,871 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 03:02:01,871 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 03:02:01,871 [INFO] Starting crawl of URL: https://example.com
2024-11-28 03:02:02,379 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 03:02:02,379 [DEBUG] Base URL: https://example.com
2024-11-28 03:02:02,379 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 03:02:02,379 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 03:02:02,379 [DEBUG] Initial domain set to: example.com
2024-11-28 03:02:02,379 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 03:02:02,379 [INFO] Successfully crawled https://example.com
2024-11-28 03:02:02,380 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x168911350>
2024-11-28 03:02:02,380 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1686856a0>, 210222.825638208)]']
connector: <aiohttp.connector.TCPConnector object at 0x168911490>
2024-11-28 03:02:02,381 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2024-11-28 03:02:03,710 [INFO] Operation took 0.10 seconds
2024-11-28 09:55:13,888 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 09:55:40,871 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 09:57:25,101 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 09:58:03,794 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 10:02:29,915 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 10:03:31,971 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 10:04:42,073 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 10:05:59,549 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 10:08:14,933 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 10:08:55,735 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 10:11:12,536 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 10:12:10,844 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 10:13:16,371 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 10:15:25,716 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 10:18:21,135 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 10:20:36,134 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 10:22:47,557 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 10:23:47,807 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 10:24:23,016 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 10:26:29,028 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 10:28:24,945 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 10:32:12,569 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 10:32:55,571 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 10:35:35,034 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 10:40:39,186 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 10:48:45,009 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 13:05:31,991 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 16:12:53,721 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 18:05:22,877 [DEBUG] Found 3 links in https://example.com
2024-11-28 18:05:22,877 [DEBUG] Link: https://example.com/home (internal) from navigation
2024-11-28 18:05:22,877 [DEBUG] Link: https://external.com (external) from navigation
2024-11-28 18:05:22,878 [DEBUG] Link: https://example.com/docs (internal) from content
2024-11-28 18:05:22,894 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:05:22,905 [WARNING] Content exceeds size limit (1000001 > 1000000)
2024-11-28 18:05:23,215 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:05:23,216 [DEBUG] Found 3 links in https://example.com/docs/
2024-11-28 18:05:23,216 [DEBUG] Link: https://example.com/docs/page.html (internal) from content
2024-11-28 18:05:23,216 [DEBUG] Link: https://example.com/page.html (internal) from content
2024-11-28 18:05:23,216 [DEBUG] Link: https://example.com/docs/subdirectory/page.html (internal) from content
2024-11-28 18:05:23,260 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:05:23,285 [DEBUG] Found 1 links in https://example.com
2024-11-28 18:05:23,285 [DEBUG] Link: https://example.com/test.html (internal) from content
2024-11-28 18:05:23,296 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:05:23,307 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:05:23,319 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:05:23,331 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:05:23,353 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:05:23,365 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:05:23,376 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:05:23,387 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:05:23,397 [DEBUG] Found 1 links in https://example.com
2024-11-28 18:05:23,397 [DEBUG] Link: https://example.com/page.html (internal) from content
2024-11-28 18:05:23,465 [WARNING] Content exceeds size limit (1400000 > 1000000)
2024-11-28 18:05:23,465 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:05:23,477 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:05:23,488 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:05:23,499 [DEBUG] Found 1 links in https://example.com
2024-11-28 18:05:23,499 [DEBUG] Link: https://example.com/page.html (internal) from content
2024-11-28 18:05:23,566 [DEBUG] Found 6 links in https://example.com
2024-11-28 18:05:23,566 [DEBUG] Link: https://example.com/home (internal) from navigation
2024-11-28 18:05:23,567 [DEBUG] Link: https://example.com/docs (internal) from navigation
2024-11-28 18:05:23,567 [DEBUG] Link: https://example.com/section1 (internal) from navigation
2024-11-28 18:05:23,567 [DEBUG] Link: https://example.com/section2 (internal) from navigation
2024-11-28 18:05:23,567 [DEBUG] Link: https://example.com/features (internal) from content
2024-11-28 18:05:23,567 [DEBUG] Link: https://example.com/relative/path (internal) from content
2024-11-28 18:05:23,568 [DEBUG] Found 3 links in https://example.com
2024-11-28 18:05:23,568 [DEBUG] Link: https://example.com/guide/intro.html (internal) from content
2024-11-28 18:05:23,568 [DEBUG] Link: https://example.com/absolute/path (internal) from content
2024-11-28 18:05:23,568 [DEBUG] Link: https://external.com (external) from content
2024-11-28 18:05:23,611 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:08:09,191 [DEBUG] Found 3 links in https://example.com
2024-11-28 18:08:09,192 [DEBUG] Link: https://example.com/home (internal) from navigation
2024-11-28 18:08:09,192 [DEBUG] Link: https://external.com (external) from navigation
2024-11-28 18:08:09,192 [DEBUG] Link: https://example.com/docs (internal) from content
2024-11-28 18:08:09,226 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:08:09,226 [WARNING] Content exceeds size limit (1000001 > 1000000)
2024-11-28 18:08:09,275 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:08:09,276 [DEBUG] Found 3 links in https://example.com/docs/
2024-11-28 18:08:09,276 [DEBUG] Link: https://example.com/docs/page.html (internal) from content
2024-11-28 18:08:09,276 [DEBUG] Link: https://example.com/page.html (internal) from content
2024-11-28 18:08:09,276 [DEBUG] Link: https://example.com/docs/subdirectory/page.html (internal) from content
2024-11-28 18:08:09,310 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:08:09,333 [DEBUG] Found 1 links in https://example.com
2024-11-28 18:08:09,333 [DEBUG] Link: https://example.com/test.html (internal) from content
2024-11-28 18:08:09,335 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:08:09,337 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:08:09,360 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:08:09,362 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:08:09,373 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:08:09,374 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:08:09,386 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:08:09,388 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:08:09,399 [DEBUG] Found 1 links in https://example.com
2024-11-28 18:08:09,399 [DEBUG] Link: https://example.com/page.html (internal) from content
2024-11-28 18:08:09,468 [WARNING] Content exceeds size limit (1400000 > 1000000)
2024-11-28 18:08:09,469 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:08:09,486 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:08:09,490 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:08:09,523 [DEBUG] Found 1 links in https://example.com
2024-11-28 18:08:09,524 [DEBUG] Link: https://example.com/page.html (internal) from content
2024-11-28 18:08:09,549 [DEBUG] Found 6 links in https://example.com
2024-11-28 18:08:09,549 [DEBUG] Link: https://example.com/home (internal) from navigation
2024-11-28 18:08:09,549 [DEBUG] Link: https://example.com/docs (internal) from navigation
2024-11-28 18:08:09,549 [DEBUG] Link: https://example.com/section1 (internal) from navigation
2024-11-28 18:08:09,549 [DEBUG] Link: https://example.com/section2 (internal) from navigation
2024-11-28 18:08:09,549 [DEBUG] Link: https://example.com/features (internal) from content
2024-11-28 18:08:09,550 [DEBUG] Link: https://example.com/relative/path (internal) from content
2024-11-28 18:08:09,550 [DEBUG] Found 3 links in https://example.com
2024-11-28 18:08:09,550 [DEBUG] Link: https://example.com/guide/intro.html (internal) from content
2024-11-28 18:08:09,550 [DEBUG] Link: https://example.com/absolute/path (internal) from content
2024-11-28 18:08:09,550 [DEBUG] Link: https://external.com (external) from content
2024-11-28 18:08:09,592 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:09:52,232 [DEBUG] Found 3 links in https://example.com
2024-11-28 18:09:52,233 [DEBUG] Link: https://example.com/home (internal) from navigation
2024-11-28 18:09:52,233 [DEBUG] Link: https://external.com (external) from navigation
2024-11-28 18:09:52,233 [DEBUG] Link: https://example.com/docs (internal) from content
2024-11-28 18:09:52,263 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:09:52,264 [WARNING] Content exceeds size limit (1000001 > 1000000)
2024-11-28 18:09:52,323 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:09:52,324 [DEBUG] Found 3 links in https://example.com/docs/
2024-11-28 18:09:52,324 [DEBUG] Link: https://example.com/docs/page.html (internal) from content
2024-11-28 18:09:52,324 [DEBUG] Link: https://example.com/page.html (internal) from content
2024-11-28 18:09:52,324 [DEBUG] Link: https://example.com/docs/subdirectory/page.html (internal) from content
2024-11-28 18:09:52,358 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:09:52,404 [DEBUG] Found 1 links in https://example.com
2024-11-28 18:09:52,404 [DEBUG] Link: https://example.com/test.html (internal) from content
2024-11-28 18:09:52,406 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:09:52,407 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:09:52,419 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:09:52,420 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:09:52,431 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:09:52,432 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:09:52,449 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:09:52,457 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:09:52,487 [DEBUG] Found 1 links in https://example.com
2024-11-28 18:09:52,487 [DEBUG] Link: https://example.com/page.html (internal) from content
2024-11-28 18:09:52,568 [WARNING] Content exceeds size limit (1400000 > 1000000)
2024-11-28 18:09:52,569 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:09:52,579 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:09:52,580 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:09:52,592 [DEBUG] Found 1 links in https://example.com
2024-11-28 18:09:52,592 [DEBUG] Link: https://example.com/page.html (internal) from content
2024-11-28 18:09:52,626 [DEBUG] Found 6 links in https://example.com
2024-11-28 18:09:52,626 [DEBUG] Link: https://example.com/home (internal) from navigation
2024-11-28 18:09:52,626 [DEBUG] Link: https://example.com/docs (internal) from navigation
2024-11-28 18:09:52,626 [DEBUG] Link: https://example.com/section1 (internal) from navigation
2024-11-28 18:09:52,626 [DEBUG] Link: https://example.com/section2 (internal) from navigation
2024-11-28 18:09:52,626 [DEBUG] Link: https://example.com/features (internal) from content
2024-11-28 18:09:52,626 [DEBUG] Link: https://example.com/relative/path (internal) from content
2024-11-28 18:09:52,627 [DEBUG] Found 3 links in https://example.com
2024-11-28 18:09:52,627 [DEBUG] Link: https://example.com/guide/intro.html (internal) from content
2024-11-28 18:09:52,627 [DEBUG] Link: https://example.com/absolute/path (internal) from content
2024-11-28 18:09:52,627 [DEBUG] Link: https://external.com (external) from content
2024-11-28 18:09:52,659 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:24:11,894 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 18:24:12,091 [DEBUG] Found 3 links in https://example.com
2024-11-28 18:24:12,091 [DEBUG] Link: https://example.com/home (internal) from navigation
2024-11-28 18:24:12,091 [DEBUG] Link: https://external.com (external) from navigation
2024-11-28 18:24:12,091 [DEBUG] Link: https://example.com/docs (internal) from content
2024-11-28 18:24:12,121 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:24:12,122 [WARNING] Content exceeds size limit (1000001 > 1000000)
2024-11-28 18:24:12,171 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:24:12,172 [DEBUG] Found 3 links in https://example.com/docs/
2024-11-28 18:24:12,172 [DEBUG] Link: https://example.com/docs/page.html (internal) from content
2024-11-28 18:24:12,172 [DEBUG] Link: https://example.com/page.html (internal) from content
2024-11-28 18:24:12,172 [DEBUG] Link: https://example.com/docs/subdirectory/page.html (internal) from content
2024-11-28 18:24:12,220 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:24:12,242 [DEBUG] Found 1 links in https://example.com
2024-11-28 18:24:12,243 [DEBUG] Link: https://example.com/test.html (internal) from content
2024-11-28 18:24:12,244 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:24:12,246 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:24:12,247 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:24:12,249 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:24:12,260 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:24:12,262 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:24:12,287 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:24:12,288 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:24:12,299 [DEBUG] Found 1 links in https://example.com
2024-11-28 18:24:12,299 [DEBUG] Link: https://example.com/page.html (internal) from content
2024-11-28 18:24:12,355 [WARNING] Content exceeds size limit (1400000 > 1000000)
2024-11-28 18:24:12,355 [WARNING] Content too small (12 < 50)
2024-11-28 18:24:12,356 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:24:12,357 [WARNING] Content too small (11 < 50)
2024-11-28 18:24:12,358 [DEBUG] Found 1 links in https://example.com
2024-11-28 18:24:12,358 [DEBUG] Link: https://example.com/page.html (internal) from content
2024-11-28 18:24:12,396 [DEBUG] Found 6 links in https://example.com
2024-11-28 18:24:12,396 [DEBUG] Link: https://example.com/home (internal) from navigation
2024-11-28 18:24:12,396 [DEBUG] Link: https://example.com/docs (internal) from navigation
2024-11-28 18:24:12,396 [DEBUG] Link: https://example.com/section1 (internal) from navigation
2024-11-28 18:24:12,396 [DEBUG] Link: https://example.com/section2 (internal) from navigation
2024-11-28 18:24:12,396 [DEBUG] Link: https://example.com/features (internal) from content
2024-11-28 18:24:12,396 [DEBUG] Link: https://example.com/relative/path (internal) from content
2024-11-28 18:24:12,397 [DEBUG] Found 3 links in https://example.com
2024-11-28 18:24:12,397 [DEBUG] Link: https://example.com/guide/intro.html (internal) from content
2024-11-28 18:24:12,397 [DEBUG] Link: https://example.com/absolute/path (internal) from content
2024-11-28 18:24:12,397 [DEBUG] Link: https://external.com (external) from content
2024-11-28 18:24:12,429 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:24:12,740 [DEBUG] Found 0 links in 
2024-11-28 18:24:12,742 [DEBUG] Found 0 links in 
2024-11-28 18:24:12,744 [DEBUG] Found 0 links in 
2024-11-28 18:25:28,405 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 18:25:28,468 [DEBUG] Found 3 links in https://example.com
2024-11-28 18:25:28,468 [DEBUG] Link: https://example.com/home (internal) from navigation
2024-11-28 18:25:28,469 [DEBUG] Link: https://external.com (external) from navigation
2024-11-28 18:25:28,469 [DEBUG] Link: https://example.com/docs (internal) from content
2024-11-28 18:25:28,500 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:25:28,501 [WARNING] Content exceeds size limit (1000001 > 1000000)
2024-11-28 18:25:28,597 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:25:28,598 [DEBUG] Found 3 links in https://example.com/docs/
2024-11-28 18:25:28,598 [DEBUG] Link: https://example.com/docs/page.html (internal) from content
2024-11-28 18:25:28,598 [DEBUG] Link: https://example.com/page.html (internal) from content
2024-11-28 18:25:28,598 [DEBUG] Link: https://example.com/docs/subdirectory/page.html (internal) from content
2024-11-28 18:25:28,653 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:25:28,680 [DEBUG] Found 1 links in https://example.com
2024-11-28 18:25:28,680 [DEBUG] Link: https://example.com/test.html (internal) from content
2024-11-28 18:25:28,682 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:25:28,684 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:25:28,686 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:25:28,688 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:25:28,699 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:25:28,701 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:25:28,702 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:25:28,703 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:25:28,715 [DEBUG] Found 1 links in https://example.com
2024-11-28 18:25:28,715 [DEBUG] Link: https://example.com/page.html (internal) from content
2024-11-28 18:25:28,785 [WARNING] Content exceeds size limit (1400000 > 1000000)
2024-11-28 18:25:28,785 [WARNING] Content too small (12 < 50)
2024-11-28 18:25:28,786 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:25:28,787 [WARNING] Content too small (11 < 50)
2024-11-28 18:25:28,788 [DEBUG] Found 1 links in https://example.com
2024-11-28 18:25:28,788 [DEBUG] Link: https://example.com/page.html (internal) from content
2024-11-28 18:25:28,811 [DEBUG] Found 6 links in https://example.com
2024-11-28 18:25:28,811 [DEBUG] Link: https://example.com/home (internal) from navigation
2024-11-28 18:25:28,811 [DEBUG] Link: https://example.com/docs (internal) from navigation
2024-11-28 18:25:28,811 [DEBUG] Link: https://example.com/section1 (internal) from navigation
2024-11-28 18:25:28,811 [DEBUG] Link: https://example.com/section2 (internal) from navigation
2024-11-28 18:25:28,811 [DEBUG] Link: https://example.com/features (internal) from content
2024-11-28 18:25:28,812 [DEBUG] Link: https://example.com/relative/path (internal) from content
2024-11-28 18:25:28,813 [DEBUG] Found 3 links in https://example.com
2024-11-28 18:25:28,813 [DEBUG] Link: https://example.com/guide/intro.html (internal) from content
2024-11-28 18:25:28,813 [DEBUG] Link: https://example.com/absolute/path (internal) from content
2024-11-28 18:25:28,813 [DEBUG] Link: https://external.com (external) from content
2024-11-28 18:25:28,860 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:25:29,088 [DEBUG] Found 0 links in 
2024-11-28 18:25:29,089 [DEBUG] Found 0 links in 
2024-11-28 18:25:29,092 [DEBUG] Found 0 links in 
2024-11-28 18:27:49,343 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 18:27:49,446 [DEBUG] Found 3 links in https://example.com
2024-11-28 18:27:49,446 [DEBUG] Link: https://example.com/home (internal) from navigation
2024-11-28 18:27:49,446 [DEBUG] Link: https://external.com (external) from navigation
2024-11-28 18:27:49,446 [DEBUG] Link: https://example.com/docs (internal) from content
2024-11-28 18:27:49,481 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:27:49,481 [WARNING] Content exceeds size limit (1000001 > 1000000)
2024-11-28 18:27:49,548 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:27:49,548 [DEBUG] Found 3 links in https://example.com/docs/
2024-11-28 18:27:49,548 [DEBUG] Link: https://example.com/docs/page.html (internal) from content
2024-11-28 18:27:49,548 [DEBUG] Link: https://example.com/page.html (internal) from content
2024-11-28 18:27:49,549 [DEBUG] Link: https://example.com/docs/subdirectory/page.html (internal) from content
2024-11-28 18:27:49,593 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:27:49,623 [DEBUG] Found 1 links in https://example.com
2024-11-28 18:27:49,623 [DEBUG] Link: https://example.com/test.html (internal) from content
2024-11-28 18:27:49,624 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:27:49,626 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:27:49,628 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:27:49,630 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:27:49,641 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:27:49,643 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:27:49,644 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:27:49,645 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:27:49,656 [DEBUG] Found 1 links in https://example.com
2024-11-28 18:27:49,656 [DEBUG] Link: https://example.com/page.html (internal) from content
2024-11-28 18:27:49,716 [WARNING] Content exceeds size limit (1400000 > 1000000)
2024-11-28 18:27:49,717 [WARNING] Content too small (12 < 50)
2024-11-28 18:27:49,718 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:27:49,718 [WARNING] Content too small (11 < 50)
2024-11-28 18:27:49,720 [DEBUG] Found 1 links in https://example.com
2024-11-28 18:27:49,720 [DEBUG] Link: https://example.com/page.html (internal) from content
2024-11-28 18:27:49,744 [DEBUG] Found 6 links in https://example.com
2024-11-28 18:27:49,744 [DEBUG] Link: https://example.com/home (internal) from navigation
2024-11-28 18:27:49,744 [DEBUG] Link: https://example.com/docs (internal) from navigation
2024-11-28 18:27:49,744 [DEBUG] Link: https://example.com/section1 (internal) from navigation
2024-11-28 18:27:49,744 [DEBUG] Link: https://example.com/section2 (internal) from navigation
2024-11-28 18:27:49,744 [DEBUG] Link: https://example.com/features (internal) from content
2024-11-28 18:27:49,744 [DEBUG] Link: https://example.com/relative/path (internal) from content
2024-11-28 18:27:49,744 [DEBUG] Found 3 links in https://example.com
2024-11-28 18:27:49,745 [DEBUG] Link: https://example.com/guide/intro.html (internal) from content
2024-11-28 18:27:49,745 [DEBUG] Link: https://example.com/absolute/path (internal) from content
2024-11-28 18:27:49,745 [DEBUG] Link: https://external.com (external) from content
2024-11-28 18:27:49,791 [DEBUG] Found 0 links in https://example.com
2024-11-28 18:27:50,133 [DEBUG] Found 0 links in 
2024-11-28 18:27:50,134 [DEBUG] Found 0 links in 
2024-11-28 18:27:50,137 [DEBUG] Found 0 links in 
2024-11-28 18:30:12,696 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 18:30:12,800 [WARNING] Content exceeds size limit (1000001 > 1000000)
2024-11-28 18:30:12,971 [ERROR] Error processing content from https://example.com: 'NoneType' object has no attribute 'strip'
2024-11-28 18:30:13,098 [WARNING] Content exceeds size limit (1400000 > 1000000)
2024-11-28 18:30:13,099 [WARNING] Content too small (12 < 50)
2024-11-28 18:30:13,100 [WARNING] Content too small (11 < 50)
2024-11-28 18:51:26,471 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 18:51:26,706 [WARNING] Content exceeds size limit (1000001 > 1000000)
2024-11-28 18:51:27,379 [ERROR] Error processing content from : sequence item 5: expected str instance, dict found
2024-11-28 18:51:27,391 [ERROR] Error processing content from : sequence item 0: expected str instance, dict found
2024-11-28 18:51:27,556 [WARNING] Content too small (15 < 50)
2024-11-28 18:56:18,610 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 18:59:05,473 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 18:59:52,521 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 19:01:15,504 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 19:01:15,782 [WARNING] Content exceeds size limit (1048577 > 1048576)
2024-11-28 19:01:16,424 [ERROR] Error processing content from : sequence item 0: expected str instance, dict found
2024-11-28 19:01:16,496 [ERROR] Error processing content from : sequence item 5: expected str instance, dict found
2024-11-28 19:01:16,508 [ERROR] Error processing content from : sequence item 0: expected str instance, dict found
2024-11-28 19:01:16,518 [WARNING] Content too small (80 < 100)
2024-11-28 19:01:16,530 [WARNING] Content too small (97 < 100)
2024-11-28 19:01:16,626 [WARNING] Content too small (104 < 100)
2024-11-28 19:01:16,637 [WARNING] Content too small (15 < 100)
2024-11-28 19:03:32,202 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 19:03:32,355 [WARNING] Content exceeds size limit (1048577 > 1048576)
2024-11-28 19:03:33,151 [WARNING] Content too small (80 < 100)
2024-11-28 19:03:33,162 [WARNING] Content too small (97 < 100)
2024-11-28 19:03:33,268 [WARNING] Content too small (104 < 100)
2024-11-28 19:03:33,280 [WARNING] Content too small (15 < 100)
2024-11-28 19:13:55,684 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 19:13:55,918 [WARNING] Content exceeds size limit (1048577 > 1048576)
2024-11-28 19:13:56,339 [WARNING] Content too small (80 < 100)
2024-11-28 19:13:56,349 [WARNING] Content too small (97 < 100)
2024-11-28 19:13:56,426 [WARNING] Content too small (104 < 100)
2024-11-28 19:13:56,436 [WARNING] Content too small (15 < 100)
2024-11-28 19:36:10,756 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 19:36:10,974 [WARNING] Content exceeds size limit (1048577 > 1048576)
2024-11-28 19:36:11,355 [WARNING] Content too small (80 < 100)
2024-11-28 19:36:11,364 [WARNING] Content too small (97 < 100)
2024-11-28 19:36:11,447 [WARNING] Content too small (104 < 100)
2024-11-28 19:36:11,456 [WARNING] Content too small (15 < 100)
2024-11-28 19:38:41,958 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 21:11:45,722 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 21:57:42,143 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 22:12:15,809 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 22:51:25,007 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 22:51:42,674 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 22:51:43,114 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:43,115 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:43,116 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:43,117 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:43,118 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:43,118 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:43,119 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:51:43,142 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:43,143 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:43,143 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:51:43,146 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:43,146 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:43,149 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:43,149 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:43,153 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:43,153 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:43,154 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:43,155 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:43,157 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:43,157 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:43,158 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:51:43,163 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:43,241 [WARNING] Content exceeds size limit (1048577 > 1048576)
2024-11-28 22:51:43,634 [WARNING] Content too small (80 < 100)
2024-11-28 22:51:43,643 [WARNING] Content too small (97 < 100)
2024-11-28 22:51:43,731 [WARNING] Content too small (104 < 100)
2024-11-28 22:51:43,740 [WARNING] Content too small (15 < 100)
2024-11-28 22:51:44,154 [INFO] test_operation took 0.11 seconds
2024-11-28 22:51:44,359 [INFO] inner took 0.10 seconds
2024-11-28 22:51:44,359 [INFO] outer took 0.21 seconds
2024-11-28 22:51:44,359 [INFO] error_operation took 0.00 seconds
2024-11-28 22:51:44,392 [INFO] test_operation took 0.00 seconds
2024-11-28 22:51:44,397 [WARNING] Content too small (0 < 100)
2024-11-28 22:51:44,400 [WARNING] Content too small (25 < 100)
2024-11-28 22:51:44,428 [WARNING] Content too small (49 < 100)
2024-11-28 22:51:44,506 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:51:44,506 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:44,506 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:51:44,506 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:51:44,506 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:51:45,007 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:51:45,007 [DEBUG] Base URL: https://example.com
2024-11-28 22:51:45,007 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:51:45,007 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:51:45,007 [DEBUG] Initial domain set to: example.com
2024-11-28 22:51:45,008 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:51:45,009 [INFO] Successfully crawled https://example.com
2024-11-28 22:51:45,010 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:45,010 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x13e61bf10>
2024-11-28 22:51:45,010 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x13e6b27b0>, 281605.539344208)]']
connector: <aiohttp.connector.TCPConnector object at 0x13e6195d0>
2024-11-28 22:51:45,011 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:51:45,011 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:45,011 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:51:45,011 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:51:45,011 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:51:45,035 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-11-28 22:51:45,036 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-11-28 22:51:45,036 [INFO] Starting crawl of URL: https://example.org
2024-11-28 22:51:45,507 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:51:45,508 [DEBUG] Base URL: https://example.com
2024-11-28 22:51:45,508 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:51:45,508 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:51:45,508 [DEBUG] Initial domain set to: example.com
2024-11-28 22:51:45,508 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:51:45,508 [INFO] Successfully crawled https://example.com
2024-11-28 22:51:46,017 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:51:46,017 [DEBUG] Base URL: https://example.org
2024-11-28 22:51:46,017 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:51:46,017 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:51:46,017 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:51:46,017 [INFO] Successfully crawled https://example.org
2024-11-28 22:51:46,018 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:46,019 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x13e85f950>
2024-11-28 22:51:46,019 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x13e6b26d0>, 281606.039666666)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x13e6b34d0>, 281606.550060958)]']
connector: <aiohttp.connector.TCPConnector object at 0x13e85e250>
2024-11-28 22:51:46,019 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:51:46,019 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:46,020 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:51:46,020 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:51:46,020 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:51:46,518 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:51:46,518 [DEBUG] Base URL: https://example.com
2024-11-28 22:51:46,518 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:51:46,519 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:51:46,519 [DEBUG] Initial domain set to: example.com
2024-11-28 22:51:46,519 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:51:46,519 [INFO] Successfully crawled https://example.com
2024-11-28 22:51:46,520 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:46,521 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x13e85e910>
2024-11-28 22:51:46,521 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x13e6b0600>, 281607.049844083)]']
connector: <aiohttp.connector.TCPConnector object at 0x13e85e490>
2024-11-28 22:51:46,521 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:51:46,522 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:46,522 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:51:46,522 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:51:46,523 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:51:47,012 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:51:47,012 [DEBUG] Base URL: https://example.com
2024-11-28 22:51:47,012 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:51:47,012 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:51:47,012 [DEBUG] Initial domain set to: example.com
2024-11-28 22:51:47,012 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:51:47,013 [INFO] Successfully crawled https://example.com
2024-11-28 22:51:47,018 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:51:47,018 [DEBUG] Base URL: https://example.com
2024-11-28 22:51:47,018 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:51:47,018 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:51:47,018 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:51:47,051 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:47,052 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:51:47,052 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:47,056 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:47,056 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:51:47,056 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:47,057 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:51:47,057 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:51:47,057 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:51:47,551 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:51:47,551 [DEBUG] Base URL: https://example.com
2024-11-28 22:51:47,551 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:51:47,551 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:51:47,551 [DEBUG] Initial domain set to: example.com
2024-11-28 22:51:47,551 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:51:47,551 [INFO] Successfully crawled https://example.com
2024-11-28 22:51:47,552 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:47,553 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x13e600850>
2024-11-28 22:51:47,554 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x13e6b09f0>, 281608.082516833)]']
connector: <aiohttp.connector.TCPConnector object at 0x13e600350>
2024-11-28 22:51:47,554 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:51:47,554 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:47,555 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2024-11-28 22:51:47,556 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2024-11-28 22:51:47,556 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2024-11-28 22:51:49,634 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-28 22:51:49,636 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-28 22:51:49,636 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-28 22:51:49,641 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:49,642 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:51:49,642 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:49,642 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:51:49,643 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:51:49,643 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:51:49,665 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-11-28 22:51:49,665 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-11-28 22:51:49,665 [INFO] Starting crawl of URL: https://example.org
2024-11-28 22:51:49,665 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2024-11-28 22:51:49,665 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2024-11-28 22:51:49,665 [INFO] Starting crawl of URL: https://example.net
2024-11-28 22:51:50,130 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:51:50,130 [DEBUG] Base URL: https://example.com
2024-11-28 22:51:50,130 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:51:50,130 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:51:50,131 [DEBUG] Initial domain set to: example.com
2024-11-28 22:51:50,131 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:51:50,131 [INFO] Successfully crawled https://example.com
2024-11-28 22:51:50,632 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:51:50,632 [DEBUG] Base URL: https://example.org
2024-11-28 22:51:50,632 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:51:50,632 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:51:50,632 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:51:50,632 [INFO] Successfully crawled https://example.org
2024-11-28 22:51:51,279 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:51:51,279 [DEBUG] Base URL: https://example.net
2024-11-28 22:51:51,280 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:51:51,280 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:51:51,280 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:51:51,280 [INFO] Successfully crawled https://example.net
2024-11-28 22:51:51,280 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:51,281 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x13e735d50>
2024-11-28 22:51:51,281 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x13e6b2c80>, 281610.663440041)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x13e6b3a80>, 281611.165362375)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x13e6b3ee0>, 281611.812674416)]']
connector: <aiohttp.connector.TCPConnector object at 0x13e736f90>
2024-11-28 22:51:51,282 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:51:51,282 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:51,282 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:51:51,282 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:51:51,282 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:51:51,777 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:51:51,777 [DEBUG] Base URL: https://example.com
2024-11-28 22:51:51,777 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:51:51,777 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:51:51,777 [DEBUG] Initial domain set to: example.com
2024-11-28 22:51:51,777 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:51:51,778 [INFO] Successfully crawled https://example.com
2024-11-28 22:51:51,780 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:51,786 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:51:51,786 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:51,787 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:51:51,812 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:51:51,833 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:51,833 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:51:51,833 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:51,834 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:51:51,853 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:51,854 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:51:51,854 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:51,855 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:51:51,855 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2024-11-28 22:51:51,855 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2024-11-28 22:51:51,855 [INFO] Starting crawl of URL: https://example.com/page
2024-11-28 22:51:52,342 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:51:52,342 [DEBUG] Base URL: https://example.com/page
2024-11-28 22:51:52,343 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:51:52,343 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:51:52,343 [DEBUG] Initial domain set to: example.com
2024-11-28 22:51:52,343 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:51:52,343 [INFO] Successfully crawled https://example.com/page
2024-11-28 22:51:52,343 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2024-11-28 22:51:52,343 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2024-11-28 22:51:52,343 [INFO] Starting crawl of URL: https://other-domain.com/page
2024-11-28 22:51:56,816 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x13e737590>
2024-11-28 22:51:56,825 [DEBUG] 
Evaluating link: ./
2024-11-28 22:51:56,825 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,825 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-28 22:51:56,825 [DEBUG] Absolute link: https://other-domain.com/
2024-11-28 22:51:56,825 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,825 [DEBUG] 
Evaluating link: ./
2024-11-28 22:51:56,825 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,825 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-28 22:51:56,825 [DEBUG] Absolute link: https://other-domain.com/
2024-11-28 22:51:56,825 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,825 [DEBUG] 
Evaluating link: ./hiyou.php
2024-11-28 22:51:56,825 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,825 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-11-28 22:51:56,825 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-11-28 22:51:56,825 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,825 [DEBUG] 
Evaluating link: ./date.php
2024-11-28 22:51:56,825 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,825 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-11-28 22:51:56,825 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-11-28 22:51:56,825 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,826 [DEBUG] 
Evaluating link: ./contact.php
2024-11-28 22:51:56,826 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,826 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2024-11-28 22:51:56,826 [DEBUG] Absolute link: https://other-domain.com/contact.php
2024-11-28 22:51:56,826 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,826 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:51:56,826 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,826 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:51:56,826 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:51:56,826 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-28 22:51:56,826 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:51:56,826 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,826 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:51:56,826 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:51:56,826 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 22:51:56,826 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:51:56,826 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,826 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:51:56,826 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:51:56,826 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 22:51:56,826 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:51:56,826 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,826 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:51:56,826 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:51:56,826 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 22:51:56,826 [DEBUG] 
Evaluating link: ./contact
2024-11-28 22:51:56,826 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,826 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-11-28 22:51:56,826 [DEBUG] Absolute link: https://other-domain.com/contact
2024-11-28 22:51:56,826 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,826 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:51:56,826 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,826 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:51:56,827 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:51:56,827 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 22:51:56,827 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:51:56,827 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,827 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:51:56,827 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:51:56,827 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 22:51:56,827 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:51:56,827 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,827 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:51:56,827 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:51:56,827 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 22:51:56,827 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-11-28 22:51:56,827 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,827 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:51:56,827 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:51:56,827 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-11-28 22:51:56,827 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:51:56,827 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,827 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:51:56,827 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:51:56,827 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-28 22:51:56,827 [DEBUG] 
Evaluating link: #indexes
2024-11-28 22:51:56,827 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,827 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2024-11-28 22:51:56,827 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,827 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,827 [DEBUG] 
Evaluating link: #a
2024-11-28 22:51:56,827 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,827 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2024-11-28 22:51:56,827 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,827 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,827 [DEBUG] 
Evaluating link: #b
2024-11-28 22:51:56,827 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,828 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-11-28 22:51:56,828 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,828 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,828 [DEBUG] 
Evaluating link: #c
2024-11-28 22:51:56,828 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,828 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-11-28 22:51:56,828 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,828 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,828 [DEBUG] 
Evaluating link: #d
2024-11-28 22:51:56,828 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,828 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-11-28 22:51:56,828 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,828 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,828 [DEBUG] 
Evaluating link: #e
2024-11-28 22:51:56,828 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,828 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-11-28 22:51:56,828 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,828 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,828 [DEBUG] 
Evaluating link: #f
2024-11-28 22:51:56,828 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,828 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-11-28 22:51:56,828 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,828 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,828 [DEBUG] 
Evaluating link: #g
2024-11-28 22:51:56,828 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,828 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-11-28 22:51:56,828 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,829 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,829 [DEBUG] 
Evaluating link: #h
2024-11-28 22:51:56,829 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,829 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-11-28 22:51:56,829 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,829 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,829 [DEBUG] 
Evaluating link: #i
2024-11-28 22:51:56,829 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,829 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-11-28 22:51:56,829 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,829 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,829 [DEBUG] 
Evaluating link: #j
2024-11-28 22:51:56,829 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,829 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-11-28 22:51:56,829 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,829 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,829 [DEBUG] 
Evaluating link: #k
2024-11-28 22:51:56,829 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,829 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-11-28 22:51:56,829 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,829 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,829 [DEBUG] 
Evaluating link: #l
2024-11-28 22:51:56,829 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,829 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-11-28 22:51:56,830 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,830 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,830 [DEBUG] 
Evaluating link: #m
2024-11-28 22:51:56,830 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,830 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-11-28 22:51:56,830 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,830 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,830 [DEBUG] 
Evaluating link: #n
2024-11-28 22:51:56,830 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,830 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-11-28 22:51:56,830 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,830 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,830 [DEBUG] 
Evaluating link: #o
2024-11-28 22:51:56,830 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,830 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-11-28 22:51:56,830 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,830 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,830 [DEBUG] 
Evaluating link: #p
2024-11-28 22:51:56,831 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,831 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-11-28 22:51:56,831 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,831 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,831 [DEBUG] 
Evaluating link: #q
2024-11-28 22:51:56,831 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,831 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-11-28 22:51:56,831 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,831 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,831 [DEBUG] 
Evaluating link: #r
2024-11-28 22:51:56,831 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,831 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-11-28 22:51:56,831 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,831 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,831 [DEBUG] 
Evaluating link: #s
2024-11-28 22:51:56,831 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,831 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-11-28 22:51:56,831 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,831 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,831 [DEBUG] 
Evaluating link: #t
2024-11-28 22:51:56,831 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,831 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-11-28 22:51:56,831 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,831 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,831 [DEBUG] 
Evaluating link: #u
2024-11-28 22:51:56,831 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,831 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-11-28 22:51:56,831 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,831 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,831 [DEBUG] 
Evaluating link: #v
2024-11-28 22:51:56,831 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,831 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-11-28 22:51:56,832 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,832 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,832 [DEBUG] 
Evaluating link: #w
2024-11-28 22:51:56,832 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,832 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-11-28 22:51:56,832 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,832 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,832 [DEBUG] 
Evaluating link: #x
2024-11-28 22:51:56,832 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,832 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-11-28 22:51:56,832 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,832 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,832 [DEBUG] 
Evaluating link: #y
2024-11-28 22:51:56,832 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,832 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-11-28 22:51:56,832 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,832 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,832 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:51:56,832 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,832 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:51:56,832 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:51:56,832 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 22:51:56,832 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:51:56,832 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,832 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:51:56,832 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:51:56,832 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 22:51:56,832 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:51:56,832 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,832 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:51:56,832 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:51:56,833 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 22:51:56,833 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-11-28 22:51:56,833 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,833 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:51:56,833 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:51:56,833 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-11-28 22:51:56,833 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:51:56,833 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,833 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:51:56,833 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:51:56,833 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-28 22:51:56,833 [DEBUG] 
Evaluating link: https://spriq.jp/
2024-11-28 22:51:56,833 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,833 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2024-11-28 22:51:56,833 [DEBUG] Absolute link: https://spriq.jp/
2024-11-28 22:51:56,833 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2024-11-28 22:51:56,833 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:51:56,833 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,833 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:51:56,833 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:51:56,833 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 22:51:56,833 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:51:56,833 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,833 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:51:56,833 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:51:56,833 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 22:51:56,833 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:51:56,833 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,833 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:51:56,834 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:51:56,834 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 22:51:56,834 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:51:56,834 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,834 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:51:56,834 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:51:56,834 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 22:51:56,834 [DEBUG] 
Evaluating link: ./contact
2024-11-28 22:51:56,834 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,834 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-11-28 22:51:56,834 [DEBUG] Absolute link: https://other-domain.com/contact
2024-11-28 22:51:56,834 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,834 [DEBUG] 
Evaluating link: ./
2024-11-28 22:51:56,834 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,834 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-28 22:51:56,834 [DEBUG] Absolute link: https://other-domain.com/
2024-11-28 22:51:56,834 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,834 [DEBUG] 
Evaluating link: ./hiyou.php
2024-11-28 22:51:56,834 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,834 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-11-28 22:51:56,834 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-11-28 22:51:56,834 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,834 [DEBUG] 
Evaluating link: ./date.php
2024-11-28 22:51:56,834 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,834 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-11-28 22:51:56,834 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-11-28 22:51:56,834 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,834 [DEBUG] 
Evaluating link: #u
2024-11-28 22:51:56,834 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,834 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-11-28 22:51:56,834 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,834 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,834 [DEBUG] 
Evaluating link: #v
2024-11-28 22:51:56,834 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,834 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-11-28 22:51:56,834 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,834 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,834 [DEBUG] 
Evaluating link: #w
2024-11-28 22:51:56,835 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,835 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-11-28 22:51:56,835 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,835 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,835 [DEBUG] 
Evaluating link: #x
2024-11-28 22:51:56,835 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,835 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-11-28 22:51:56,835 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,835 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,835 [DEBUG] 
Evaluating link: #y
2024-11-28 22:51:56,835 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,835 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-11-28 22:51:56,835 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,835 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,835 [DEBUG] 
Evaluating link: #p
2024-11-28 22:51:56,835 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,835 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-11-28 22:51:56,835 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,835 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,835 [DEBUG] 
Evaluating link: #q
2024-11-28 22:51:56,835 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,835 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-11-28 22:51:56,835 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,835 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,835 [DEBUG] 
Evaluating link: #r
2024-11-28 22:51:56,835 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,835 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-11-28 22:51:56,835 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,835 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,835 [DEBUG] 
Evaluating link: #s
2024-11-28 22:51:56,835 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,835 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-11-28 22:51:56,835 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,835 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,835 [DEBUG] 
Evaluating link: #t
2024-11-28 22:51:56,835 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,835 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-11-28 22:51:56,836 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,836 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,836 [DEBUG] 
Evaluating link: #k
2024-11-28 22:51:56,836 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,836 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-11-28 22:51:56,836 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,836 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,836 [DEBUG] 
Evaluating link: #l
2024-11-28 22:51:56,836 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,836 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-11-28 22:51:56,836 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,836 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,836 [DEBUG] 
Evaluating link: #m
2024-11-28 22:51:56,836 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,836 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-11-28 22:51:56,836 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,836 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,836 [DEBUG] 
Evaluating link: #n
2024-11-28 22:51:56,836 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,836 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-11-28 22:51:56,836 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,836 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,836 [DEBUG] 
Evaluating link: #o
2024-11-28 22:51:56,836 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,836 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-11-28 22:51:56,836 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,836 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,836 [DEBUG] 
Evaluating link: #f
2024-11-28 22:51:56,836 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,836 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-11-28 22:51:56,836 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,836 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,836 [DEBUG] 
Evaluating link: #g
2024-11-28 22:51:56,836 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,836 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-11-28 22:51:56,836 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,836 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,836 [DEBUG] 
Evaluating link: #h
2024-11-28 22:51:56,836 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,837 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-11-28 22:51:56,837 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,837 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,837 [DEBUG] 
Evaluating link: #i
2024-11-28 22:51:56,837 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,837 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-11-28 22:51:56,837 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,837 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,837 [DEBUG] 
Evaluating link: #j
2024-11-28 22:51:56,837 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,837 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-11-28 22:51:56,837 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,837 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,837 [DEBUG] 
Evaluating link: #b
2024-11-28 22:51:56,837 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,837 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-11-28 22:51:56,837 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,837 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,837 [DEBUG] 
Evaluating link: #c
2024-11-28 22:51:56,837 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,837 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-11-28 22:51:56,837 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,837 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,837 [DEBUG] 
Evaluating link: #d
2024-11-28 22:51:56,837 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,837 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-11-28 22:51:56,837 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,837 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,837 [DEBUG] 
Evaluating link: #e
2024-11-28 22:51:56,837 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,837 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-11-28 22:51:56,837 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,837 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,838 [DEBUG] 
Evaluating link: #site
2024-11-28 22:51:56,838 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:51:56,838 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2024-11-28 22:51:56,838 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:51:56,838 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:51:56,838 [INFO] Successfully crawled https://other-domain.com/page
2024-11-28 22:51:56,842 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:56,844 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:51:56,844 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:51:56,844 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-11-28 22:51:56,844 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-11-28 22:51:56,845 [INFO] Starting crawl of URL: https://example.com/page0
2024-11-28 22:51:57,343 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:51:57,343 [DEBUG] Base URL: https://example.com/page0
2024-11-28 22:51:57,343 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:51:57,343 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:51:57,343 [DEBUG] Initial domain set to: example.com
2024-11-28 22:51:57,343 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:51:57,343 [INFO] Successfully crawled https://example.com/page0
2024-11-28 22:51:57,343 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 22:51:57,344 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 22:51:57,344 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 22:51:57,488 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:51:57,488 [DEBUG] Base URL: https://example.com/page1
2024-11-28 22:51:57,488 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:51:57,488 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:51:57,488 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:51:57,488 [INFO] Successfully crawled https://example.com/page1
2024-11-28 22:51:57,488 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 22:51:57,488 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 22:51:57,488 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 22:51:57,988 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:51:57,989 [DEBUG] Base URL: https://example.com/page2
2024-11-28 22:51:57,989 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:51:57,989 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:51:57,989 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:51:57,989 [INFO] Successfully crawled https://example.com/page2
2024-11-28 22:51:57,989 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-28 22:51:57,989 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-28 22:51:57,989 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-28 22:51:58,490 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:51:58,490 [DEBUG] Base URL: https://example.com/page3
2024-11-28 22:51:58,490 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:51:58,490 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:51:58,491 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:51:58,491 [INFO] Successfully crawled https://example.com/page3
2024-11-28 22:51:58,491 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-11-28 22:51:58,491 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-11-28 22:51:58,491 [INFO] Starting crawl of URL: https://example.com/page4
2024-11-28 22:51:58,991 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:51:58,991 [DEBUG] Base URL: https://example.com/page4
2024-11-28 22:51:58,991 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:51:58,991 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:51:58,991 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:51:58,991 [INFO] Successfully crawled https://example.com/page4
2024-11-28 22:51:58,991 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:51:58,991 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-11-28 22:51:58,991 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-11-28 22:51:58,991 [INFO] Starting crawl of URL: https://example.com/page0
2024-11-28 22:51:59,482 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:51:59,482 [DEBUG] Base URL: https://example.com/page0
2024-11-28 22:51:59,482 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:51:59,482 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:51:59,482 [DEBUG] Initial domain set to: example.com
2024-11-28 22:51:59,482 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:51:59,482 [INFO] Successfully crawled https://example.com/page0
2024-11-28 22:51:59,482 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 22:51:59,482 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 22:51:59,483 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 22:51:59,637 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:51:59,637 [DEBUG] Base URL: https://example.com/page1
2024-11-28 22:51:59,637 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:51:59,637 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:51:59,637 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:51:59,638 [INFO] Successfully crawled https://example.com/page1
2024-11-28 22:51:59,638 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 22:51:59,638 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 22:51:59,638 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 22:52:00,144 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:52:00,144 [DEBUG] Base URL: https://example.com/page2
2024-11-28 22:52:00,144 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:52:00,145 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:52:00,145 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:52:00,145 [INFO] Successfully crawled https://example.com/page2
2024-11-28 22:52:00,145 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-28 22:52:00,145 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-28 22:52:00,145 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-28 22:52:00,644 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:52:00,644 [DEBUG] Base URL: https://example.com/page3
2024-11-28 22:52:00,644 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:52:00,644 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:52:00,644 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:52:00,644 [INFO] Successfully crawled https://example.com/page3
2024-11-28 22:52:00,644 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-11-28 22:52:00,644 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-11-28 22:52:00,644 [INFO] Starting crawl of URL: https://example.com/page4
2024-11-28 22:52:01,138 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:52:01,138 [DEBUG] Base URL: https://example.com/page4
2024-11-28 22:52:01,138 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:52:01,138 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:52:01,138 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:52:01,139 [INFO] Successfully crawled https://example.com/page4
2024-11-28 22:52:01,144 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:52:01,146 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:52:01,146 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:52:01,150 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x13e7c8b10>
2024-11-28 22:52:01,150 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x13e6b20b0>, 281619.5234135)]']
connector: <aiohttp.connector.TCPConnector object at 0x13e7c8bd0>
2024-11-28 22:52:01,150 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x13e857410>
2024-11-28 22:52:01,150 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x13e6989f0>, 281621.670435041)]']
connector: <aiohttp.connector.TCPConnector object at 0x13e678e10>
2024-11-28 22:52:01,154 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:52:01,155 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:52:01,156 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:52:01,156 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:52:01,156 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:52:01,156 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:52:03,157 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:52:11,157 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:52:11,157 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:52:11,165 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:52:11,168 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:52:11,169 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:52:11,170 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 22:52:11,170 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 22:52:11,170 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 22:52:11,170 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 22:52:11,170 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 22:52:11,170 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 22:52:13,171 [WARNING] Retry 1/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:52:23,172 [WARNING] Retry 1/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:52:31,172 [WARNING] Retry 2/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:52:31,172 [ERROR] Error crawling https://example.com/page1: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:52:41,173 [WARNING] Retry 2/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:52:41,173 [ERROR] Error crawling https://example.com/page2: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:52:41,177 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:52:41,178 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:52:41,178 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:52:41,179 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 22:52:41,179 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 22:52:41,179 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 22:52:41,179 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 22:52:41,179 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 22:52:41,179 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 22:52:41,179 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-28 22:52:41,179 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-28 22:52:41,179 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-28 22:52:43,180 [WARNING] Retry 1/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:52:53,180 [WARNING] Retry 1/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:53:03,181 [WARNING] Retry 1/2 for https://example.com/page3: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:53:03,939 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 22:53:04,212 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:04,213 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:04,214 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:04,214 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:04,215 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:04,215 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:04,215 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:53:04,234 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:04,234 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:04,235 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:53:04,253 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:04,253 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:04,256 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:04,256 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:04,260 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:04,260 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:04,261 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:04,261 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:04,264 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:04,264 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:04,265 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:53:04,267 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:04,306 [WARNING] Content exceeds size limit (1048577 > 1048576)
2024-11-28 22:53:04,678 [WARNING] Content too small (80 < 100)
2024-11-28 22:53:04,717 [WARNING] Content too small (97 < 100)
2024-11-28 22:53:04,798 [WARNING] Content too small (104 < 100)
2024-11-28 22:53:04,807 [WARNING] Content too small (15 < 100)
2024-11-28 22:53:05,172 [INFO] test_operation took 0.10 seconds
2024-11-28 22:53:05,380 [INFO] inner took 0.10 seconds
2024-11-28 22:53:05,380 [INFO] outer took 0.21 seconds
2024-11-28 22:53:05,380 [INFO] error_operation took 0.00 seconds
2024-11-28 22:53:05,438 [INFO] test_operation took 0.00 seconds
2024-11-28 22:53:05,444 [WARNING] Content too small (0 < 100)
2024-11-28 22:53:05,446 [WARNING] Content too small (25 < 100)
2024-11-28 22:53:05,456 [WARNING] Content too small (49 < 100)
2024-11-28 22:53:05,551 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:53:05,551 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:05,552 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:53:05,552 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:53:05,552 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:53:06,046 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:53:06,046 [DEBUG] Base URL: https://example.com
2024-11-28 22:53:06,046 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:53:06,046 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:53:06,046 [DEBUG] Initial domain set to: example.com
2024-11-28 22:53:06,046 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:53:06,046 [INFO] Successfully crawled https://example.com
2024-11-28 22:53:06,047 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:06,048 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x104cb5f10>
2024-11-28 22:53:06,048 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1052d7930>, 281686.579724)])']
connector: <aiohttp.connector.TCPConnector object at 0x1046317d0>
2024-11-28 22:53:06,049 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:53:06,049 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:06,049 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:53:06,049 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:53:06,049 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:53:06,071 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-11-28 22:53:06,071 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-11-28 22:53:06,071 [INFO] Starting crawl of URL: https://example.org
2024-11-28 22:53:06,551 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:53:06,551 [DEBUG] Base URL: https://example.com
2024-11-28 22:53:06,551 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:53:06,551 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:53:06,551 [DEBUG] Initial domain set to: example.com
2024-11-28 22:53:06,551 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:53:06,551 [INFO] Successfully crawled https://example.com
2024-11-28 22:53:07,051 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:53:07,051 [DEBUG] Base URL: https://example.org
2024-11-28 22:53:07,051 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:53:07,051 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:53:07,052 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:53:07,052 [INFO] Successfully crawled https://example.org
2024-11-28 22:53:07,053 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:07,054 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x105262a10>
2024-11-28 22:53:07,054 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1052d4c20>, 281687.08422925)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1052d44b0>, 281687.584202416)])']
connector: <aiohttp.connector.TCPConnector object at 0x105349d50>
2024-11-28 22:53:07,054 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:53:07,054 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:07,055 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:53:07,055 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:53:07,055 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:53:07,540 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:53:07,540 [DEBUG] Base URL: https://example.com
2024-11-28 22:53:07,541 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:53:07,541 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:53:07,541 [DEBUG] Initial domain set to: example.com
2024-11-28 22:53:07,541 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:53:07,541 [INFO] Successfully crawled https://example.com
2024-11-28 22:53:07,542 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:07,543 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1052f0e90>
2024-11-28 22:53:07,544 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1052d4130>, 281688.07474475)])']
connector: <aiohttp.connector.TCPConnector object at 0x105348410>
2024-11-28 22:53:07,544 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:53:07,544 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:07,545 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:53:07,545 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:53:07,545 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:53:08,041 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:53:08,041 [DEBUG] Base URL: https://example.com
2024-11-28 22:53:08,041 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:53:08,041 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:53:08,041 [DEBUG] Initial domain set to: example.com
2024-11-28 22:53:08,042 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:53:08,042 [INFO] Successfully crawled https://example.com
2024-11-28 22:53:08,047 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:53:08,048 [DEBUG] Base URL: https://example.com
2024-11-28 22:53:08,048 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:53:08,048 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:53:08,048 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:53:08,052 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:08,053 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:53:08,054 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:08,056 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:08,057 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:53:08,057 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:08,057 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:53:08,057 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:53:08,057 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:53:08,555 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:53:08,556 [DEBUG] Base URL: https://example.com
2024-11-28 22:53:08,556 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:53:08,556 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:53:08,556 [DEBUG] Initial domain set to: example.com
2024-11-28 22:53:08,556 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:53:08,556 [INFO] Successfully crawled https://example.com
2024-11-28 22:53:08,557 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:08,558 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x105236990>
2024-11-28 22:53:08,558 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1052d43d0>, 281689.087523083)])']
connector: <aiohttp.connector.TCPConnector object at 0x1052376d0>
2024-11-28 22:53:08,560 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:53:08,560 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:08,562 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2024-11-28 22:53:08,562 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2024-11-28 22:53:08,562 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2024-11-28 22:53:10,586 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-28 22:53:10,588 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-28 22:53:10,588 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-28 22:53:10,596 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:10,597 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:53:10,597 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:10,598 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:53:10,598 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:53:10,598 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:53:10,622 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-11-28 22:53:10,622 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-11-28 22:53:10,622 [INFO] Starting crawl of URL: https://example.org
2024-11-28 22:53:10,622 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2024-11-28 22:53:10,622 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2024-11-28 22:53:10,622 [INFO] Starting crawl of URL: https://example.net
2024-11-28 22:53:11,086 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:53:11,086 [DEBUG] Base URL: https://example.com
2024-11-28 22:53:11,086 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:53:11,087 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:53:11,087 [DEBUG] Initial domain set to: example.com
2024-11-28 22:53:11,087 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:53:11,087 [INFO] Successfully crawled https://example.com
2024-11-28 22:53:11,181 [WARNING] Retry 2/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:53:11,181 [ERROR] Error crawling https://example.com/page1: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:53:11,597 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:53:11,598 [DEBUG] Base URL: https://example.org
2024-11-28 22:53:11,598 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:53:11,598 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:53:11,598 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:53:11,598 [INFO] Successfully crawled https://example.org
2024-11-28 22:53:12,102 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:53:12,102 [DEBUG] Base URL: https://example.net
2024-11-28 22:53:12,102 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:53:12,102 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:53:12,102 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:53:12,103 [INFO] Successfully crawled https://example.net
2024-11-28 22:53:12,104 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:12,105 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x105234590>
2024-11-28 22:53:12,106 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1052d6660>, 281691.619910625)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1052d6ac0>, 281692.131145208)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1052d6eb0>, 281692.635571916)])']
connector: <aiohttp.connector.TCPConnector object at 0x105234c50>
2024-11-28 22:53:12,107 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:53:12,107 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:12,108 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:53:12,108 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:53:12,108 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:53:12,607 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:53:12,607 [DEBUG] Base URL: https://example.com
2024-11-28 22:53:12,607 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:53:12,607 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:53:12,607 [DEBUG] Initial domain set to: example.com
2024-11-28 22:53:12,607 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:53:12,607 [INFO] Successfully crawled https://example.com
2024-11-28 22:53:12,609 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:12,615 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:53:12,615 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:12,616 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:53:12,642 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:53:12,663 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:12,664 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:53:12,664 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:12,664 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:53:12,683 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:12,683 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:53:12,684 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:12,684 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:53:12,684 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2024-11-28 22:53:12,684 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2024-11-28 22:53:12,684 [INFO] Starting crawl of URL: https://example.com/page
2024-11-28 22:53:13,167 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:53:13,168 [DEBUG] Base URL: https://example.com/page
2024-11-28 22:53:13,168 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:53:13,168 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:53:13,168 [DEBUG] Initial domain set to: example.com
2024-11-28 22:53:13,168 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:53:13,169 [INFO] Successfully crawled https://example.com/page
2024-11-28 22:53:13,169 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2024-11-28 22:53:13,169 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2024-11-28 22:53:13,169 [INFO] Starting crawl of URL: https://other-domain.com/page
2024-11-28 22:53:15,450 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x10536d0d0>
2024-11-28 22:53:15,450 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1052d4600>, 281688.574275458)])']
connector: <aiohttp.connector.TCPConnector object at 0x10536e510>
2024-11-28 22:53:15,450 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x105464490>
2024-11-28 22:53:16,622 [DEBUG] 
Evaluating link: ./
2024-11-28 22:53:16,622 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,622 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-28 22:53:16,622 [DEBUG] Absolute link: https://other-domain.com/
2024-11-28 22:53:16,622 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,622 [DEBUG] 
Evaluating link: ./
2024-11-28 22:53:16,622 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,622 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-28 22:53:16,622 [DEBUG] Absolute link: https://other-domain.com/
2024-11-28 22:53:16,622 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,622 [DEBUG] 
Evaluating link: ./hiyou.php
2024-11-28 22:53:16,622 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,622 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-11-28 22:53:16,622 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-11-28 22:53:16,622 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,622 [DEBUG] 
Evaluating link: ./date.php
2024-11-28 22:53:16,622 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,622 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-11-28 22:53:16,623 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-11-28 22:53:16,623 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,623 [DEBUG] 
Evaluating link: ./contact.php
2024-11-28 22:53:16,623 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,623 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2024-11-28 22:53:16,623 [DEBUG] Absolute link: https://other-domain.com/contact.php
2024-11-28 22:53:16,623 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,623 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:53:16,623 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,623 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:53:16,623 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:53:16,623 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-28 22:53:16,623 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:53:16,623 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,623 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:53:16,623 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:53:16,623 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 22:53:16,623 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:53:16,623 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,623 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:53:16,623 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:53:16,623 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 22:53:16,623 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:53:16,623 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,623 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:53:16,623 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:53:16,623 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 22:53:16,623 [DEBUG] 
Evaluating link: ./contact
2024-11-28 22:53:16,623 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,623 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-11-28 22:53:16,623 [DEBUG] Absolute link: https://other-domain.com/contact
2024-11-28 22:53:16,624 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,624 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:53:16,624 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,624 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:53:16,624 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:53:16,624 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 22:53:16,624 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:53:16,624 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,624 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:53:16,624 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:53:16,624 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 22:53:16,624 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:53:16,624 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,624 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:53:16,624 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:53:16,624 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 22:53:16,624 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-11-28 22:53:16,624 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,624 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:53:16,624 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:53:16,624 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-11-28 22:53:16,624 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:53:16,624 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,624 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:53:16,624 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:53:16,624 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-28 22:53:16,625 [DEBUG] 
Evaluating link: #indexes
2024-11-28 22:53:16,625 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,625 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2024-11-28 22:53:16,625 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,625 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,625 [DEBUG] 
Evaluating link: #a
2024-11-28 22:53:16,625 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,625 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2024-11-28 22:53:16,625 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,625 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,625 [DEBUG] 
Evaluating link: #b
2024-11-28 22:53:16,625 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,625 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-11-28 22:53:16,625 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,625 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,625 [DEBUG] 
Evaluating link: #c
2024-11-28 22:53:16,625 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,625 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-11-28 22:53:16,625 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,625 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,625 [DEBUG] 
Evaluating link: #d
2024-11-28 22:53:16,625 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,625 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-11-28 22:53:16,625 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,625 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,625 [DEBUG] 
Evaluating link: #e
2024-11-28 22:53:16,625 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,625 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-11-28 22:53:16,625 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,625 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,625 [DEBUG] 
Evaluating link: #f
2024-11-28 22:53:16,625 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,625 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-11-28 22:53:16,626 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,626 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,626 [DEBUG] 
Evaluating link: #g
2024-11-28 22:53:16,626 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,626 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-11-28 22:53:16,626 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,626 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,626 [DEBUG] 
Evaluating link: #h
2024-11-28 22:53:16,626 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,626 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-11-28 22:53:16,626 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,626 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,626 [DEBUG] 
Evaluating link: #i
2024-11-28 22:53:16,626 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,626 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-11-28 22:53:16,626 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,626 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,626 [DEBUG] 
Evaluating link: #j
2024-11-28 22:53:16,626 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,626 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-11-28 22:53:16,626 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,626 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,626 [DEBUG] 
Evaluating link: #k
2024-11-28 22:53:16,626 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,626 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-11-28 22:53:16,626 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,626 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,626 [DEBUG] 
Evaluating link: #l
2024-11-28 22:53:16,626 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,626 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-11-28 22:53:16,626 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,626 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,626 [DEBUG] 
Evaluating link: #m
2024-11-28 22:53:16,626 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,626 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-11-28 22:53:16,627 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,627 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,627 [DEBUG] 
Evaluating link: #n
2024-11-28 22:53:16,627 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,627 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-11-28 22:53:16,627 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,627 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,627 [DEBUG] 
Evaluating link: #o
2024-11-28 22:53:16,627 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,627 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-11-28 22:53:16,627 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,627 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,627 [DEBUG] 
Evaluating link: #p
2024-11-28 22:53:16,627 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,627 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-11-28 22:53:16,627 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,627 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,627 [DEBUG] 
Evaluating link: #q
2024-11-28 22:53:16,627 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,627 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-11-28 22:53:16,627 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,627 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,627 [DEBUG] 
Evaluating link: #r
2024-11-28 22:53:16,627 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,627 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-11-28 22:53:16,628 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,628 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,628 [DEBUG] 
Evaluating link: #s
2024-11-28 22:53:16,628 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,628 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-11-28 22:53:16,628 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,628 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,628 [DEBUG] 
Evaluating link: #t
2024-11-28 22:53:16,628 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,628 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-11-28 22:53:16,628 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,628 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,628 [DEBUG] 
Evaluating link: #u
2024-11-28 22:53:16,628 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,628 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-11-28 22:53:16,628 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,628 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,628 [DEBUG] 
Evaluating link: #v
2024-11-28 22:53:16,628 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,628 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-11-28 22:53:16,628 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,628 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,628 [DEBUG] 
Evaluating link: #w
2024-11-28 22:53:16,628 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,629 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-11-28 22:53:16,629 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,629 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,629 [DEBUG] 
Evaluating link: #x
2024-11-28 22:53:16,629 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,629 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-11-28 22:53:16,629 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,629 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,629 [DEBUG] 
Evaluating link: #y
2024-11-28 22:53:16,629 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,629 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-11-28 22:53:16,629 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,629 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,629 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:53:16,629 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,629 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:53:16,629 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:53:16,629 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 22:53:16,629 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:53:16,629 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,629 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:53:16,629 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:53:16,629 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 22:53:16,630 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:53:16,630 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,630 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:53:16,630 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:53:16,630 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 22:53:16,630 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-11-28 22:53:16,630 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,630 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:53:16,630 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:53:16,630 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-11-28 22:53:16,630 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:53:16,630 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,630 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:53:16,630 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:53:16,630 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-28 22:53:16,630 [DEBUG] 
Evaluating link: https://spriq.jp/
2024-11-28 22:53:16,630 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,630 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2024-11-28 22:53:16,630 [DEBUG] Absolute link: https://spriq.jp/
2024-11-28 22:53:16,630 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2024-11-28 22:53:16,630 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:53:16,630 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,630 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:53:16,630 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:53:16,630 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 22:53:16,630 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:53:16,630 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,630 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:53:16,630 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:53:16,630 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 22:53:16,630 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:53:16,630 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,630 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:53:16,631 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:53:16,631 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 22:53:16,631 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:53:16,631 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,631 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:53:16,631 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:53:16,631 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 22:53:16,631 [DEBUG] 
Evaluating link: ./contact
2024-11-28 22:53:16,631 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,631 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-11-28 22:53:16,631 [DEBUG] Absolute link: https://other-domain.com/contact
2024-11-28 22:53:16,631 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,631 [DEBUG] 
Evaluating link: ./
2024-11-28 22:53:16,631 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,631 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-28 22:53:16,631 [DEBUG] Absolute link: https://other-domain.com/
2024-11-28 22:53:16,631 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,631 [DEBUG] 
Evaluating link: ./hiyou.php
2024-11-28 22:53:16,631 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,631 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-11-28 22:53:16,631 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-11-28 22:53:16,631 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,631 [DEBUG] 
Evaluating link: ./date.php
2024-11-28 22:53:16,631 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,631 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-11-28 22:53:16,631 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-11-28 22:53:16,631 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,631 [DEBUG] 
Evaluating link: #u
2024-11-28 22:53:16,631 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,631 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-11-28 22:53:16,631 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,631 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,631 [DEBUG] 
Evaluating link: #v
2024-11-28 22:53:16,631 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,631 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-11-28 22:53:16,632 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,632 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,632 [DEBUG] 
Evaluating link: #w
2024-11-28 22:53:16,632 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,632 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-11-28 22:53:16,632 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,632 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,632 [DEBUG] 
Evaluating link: #x
2024-11-28 22:53:16,632 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,632 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-11-28 22:53:16,632 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,632 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,632 [DEBUG] 
Evaluating link: #y
2024-11-28 22:53:16,632 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,632 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-11-28 22:53:16,632 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,632 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,632 [DEBUG] 
Evaluating link: #p
2024-11-28 22:53:16,632 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,632 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-11-28 22:53:16,632 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,632 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,632 [DEBUG] 
Evaluating link: #q
2024-11-28 22:53:16,632 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,632 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-11-28 22:53:16,632 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,632 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,632 [DEBUG] 
Evaluating link: #r
2024-11-28 22:53:16,632 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,632 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-11-28 22:53:16,632 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,632 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,633 [DEBUG] 
Evaluating link: #s
2024-11-28 22:53:16,633 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,633 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-11-28 22:53:16,633 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,633 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,633 [DEBUG] 
Evaluating link: #t
2024-11-28 22:53:16,633 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,633 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-11-28 22:53:16,633 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,633 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,633 [DEBUG] 
Evaluating link: #k
2024-11-28 22:53:16,633 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,633 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-11-28 22:53:16,633 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,633 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,633 [DEBUG] 
Evaluating link: #l
2024-11-28 22:53:16,633 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,633 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-11-28 22:53:16,633 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,633 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,633 [DEBUG] 
Evaluating link: #m
2024-11-28 22:53:16,633 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,633 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-11-28 22:53:16,633 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,633 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,633 [DEBUG] 
Evaluating link: #n
2024-11-28 22:53:16,633 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,633 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-11-28 22:53:16,633 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,633 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,633 [DEBUG] 
Evaluating link: #o
2024-11-28 22:53:16,633 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,633 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-11-28 22:53:16,633 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,633 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,634 [DEBUG] 
Evaluating link: #f
2024-11-28 22:53:16,634 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,634 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-11-28 22:53:16,634 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,634 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,634 [DEBUG] 
Evaluating link: #g
2024-11-28 22:53:16,634 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,634 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-11-28 22:53:16,634 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,634 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,634 [DEBUG] 
Evaluating link: #h
2024-11-28 22:53:16,634 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,634 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-11-28 22:53:16,634 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,634 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,634 [DEBUG] 
Evaluating link: #i
2024-11-28 22:53:16,634 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,634 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-11-28 22:53:16,634 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,634 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,634 [DEBUG] 
Evaluating link: #j
2024-11-28 22:53:16,634 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,634 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-11-28 22:53:16,634 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,634 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,634 [DEBUG] 
Evaluating link: #b
2024-11-28 22:53:16,634 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,634 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-11-28 22:53:16,634 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,634 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,634 [DEBUG] 
Evaluating link: #c
2024-11-28 22:53:16,634 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,634 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-11-28 22:53:16,634 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,635 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,635 [DEBUG] 
Evaluating link: #d
2024-11-28 22:53:16,635 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,635 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-11-28 22:53:16,635 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,635 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,635 [DEBUG] 
Evaluating link: #e
2024-11-28 22:53:16,635 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,635 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-11-28 22:53:16,635 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,635 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,635 [DEBUG] 
Evaluating link: #site
2024-11-28 22:53:16,635 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:53:16,635 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2024-11-28 22:53:16,635 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:53:16,635 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:53:16,635 [INFO] Successfully crawled https://other-domain.com/page
2024-11-28 22:53:16,639 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:16,641 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:53:16,641 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:16,642 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-11-28 22:53:16,642 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-11-28 22:53:16,642 [INFO] Starting crawl of URL: https://example.com/page0
2024-11-28 22:53:17,130 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:53:17,130 [DEBUG] Base URL: https://example.com/page0
2024-11-28 22:53:17,130 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:53:17,130 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:53:17,130 [DEBUG] Initial domain set to: example.com
2024-11-28 22:53:17,130 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:53:17,130 [INFO] Successfully crawled https://example.com/page0
2024-11-28 22:53:17,131 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 22:53:17,131 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 22:53:17,131 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 22:53:17,280 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:53:17,281 [DEBUG] Base URL: https://example.com/page1
2024-11-28 22:53:17,281 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:53:17,281 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:53:17,281 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:53:17,281 [INFO] Successfully crawled https://example.com/page1
2024-11-28 22:53:17,281 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 22:53:17,281 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 22:53:17,281 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 22:53:17,783 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:53:17,784 [DEBUG] Base URL: https://example.com/page2
2024-11-28 22:53:17,784 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:53:17,784 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:53:17,784 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:53:17,784 [INFO] Successfully crawled https://example.com/page2
2024-11-28 22:53:17,784 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-28 22:53:17,784 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-28 22:53:17,785 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-28 22:53:18,288 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:53:18,288 [DEBUG] Base URL: https://example.com/page3
2024-11-28 22:53:18,288 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:53:18,288 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:53:18,288 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:53:18,289 [INFO] Successfully crawled https://example.com/page3
2024-11-28 22:53:18,289 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-11-28 22:53:18,289 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-11-28 22:53:18,289 [INFO] Starting crawl of URL: https://example.com/page4
2024-11-28 22:53:18,786 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:53:18,786 [DEBUG] Base URL: https://example.com/page4
2024-11-28 22:53:18,786 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:53:18,786 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:53:18,786 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:53:18,786 [INFO] Successfully crawled https://example.com/page4
2024-11-28 22:53:18,786 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:53:18,787 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-11-28 22:53:18,787 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-11-28 22:53:18,787 [INFO] Starting crawl of URL: https://example.com/page0
2024-11-28 22:53:19,286 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:53:19,286 [DEBUG] Base URL: https://example.com/page0
2024-11-28 22:53:19,286 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:53:19,286 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:53:19,286 [DEBUG] Initial domain set to: example.com
2024-11-28 22:53:19,286 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:53:19,286 [INFO] Successfully crawled https://example.com/page0
2024-11-28 22:53:19,286 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 22:53:19,286 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 22:53:19,286 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 22:53:19,432 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:53:19,433 [DEBUG] Base URL: https://example.com/page1
2024-11-28 22:53:19,433 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:53:19,433 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:53:19,433 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:53:19,433 [INFO] Successfully crawled https://example.com/page1
2024-11-28 22:53:19,433 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 22:53:19,433 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 22:53:19,433 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 22:53:19,933 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:53:19,933 [DEBUG] Base URL: https://example.com/page2
2024-11-28 22:53:19,933 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:53:19,933 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:53:19,933 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:53:19,933 [INFO] Successfully crawled https://example.com/page2
2024-11-28 22:53:19,933 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-28 22:53:19,933 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-28 22:53:19,933 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-28 22:53:20,436 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:53:20,436 [DEBUG] Base URL: https://example.com/page3
2024-11-28 22:53:20,436 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:53:20,436 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:53:20,436 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:53:20,436 [INFO] Successfully crawled https://example.com/page3
2024-11-28 22:53:20,436 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-11-28 22:53:20,436 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-11-28 22:53:20,436 [INFO] Starting crawl of URL: https://example.com/page4
2024-11-28 22:53:20,943 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:53:20,943 [DEBUG] Base URL: https://example.com/page4
2024-11-28 22:53:20,943 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:53:20,943 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:53:20,943 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:53:20,943 [INFO] Successfully crawled https://example.com/page4
2024-11-28 22:53:20,948 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:20,949 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:20,950 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:53:20,955 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:20,955 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:20,956 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:53:20,957 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:53:20,957 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:53:20,957 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:53:21,181 [WARNING] Retry 2/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:53:21,182 [ERROR] Error crawling https://example.com/page2: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:53:22,958 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:53:30,958 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:53:30,958 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:53:30,968 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:30,971 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:30,972 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:53:30,972 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 22:53:30,972 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 22:53:30,972 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 22:53:30,972 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 22:53:30,972 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 22:53:30,972 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 22:53:31,182 [WARNING] Retry 2/2 for https://example.com/page3: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:53:31,182 [ERROR] Error crawling https://example.com/page3: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:53:31,182 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:31,184 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:31,185 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:53:31,189 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:31,190 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:31,191 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:53:31,192 [DEBUG] Using raw URL string: https://nonexistent.example.com (type: <class 'str'>)
2024-11-28 22:53:31,192 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='nonexistent.example.com', path='', params='', query='', fragment='')
2024-11-28 22:53:31,192 [INFO] Starting crawl of URL: https://nonexistent.example.com
2024-11-28 22:53:32,973 [WARNING] Retry 1/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:53:33,193 [WARNING] Retry 1/2 for https://nonexistent.example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:53:41,193 [WARNING] Retry 2/2 for https://nonexistent.example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:53:41,193 [ERROR] Error crawling https://nonexistent.example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:53:41,198 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:41,198 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:41,199 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:53:41,203 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:41,204 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:41,205 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:53:41,205 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:53:41,205 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:53:41,205 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:53:42,974 [WARNING] Retry 1/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:53:43,206 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:53:50,974 [WARNING] Retry 2/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:53:50,975 [ERROR] Error crawling https://example.com/page1: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:53:51,206 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:53:51,207 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:53:51,214 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:51,215 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:53:51,216 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:53:51,217 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:53:51,217 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:53:51,217 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:53:53,218 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:54:00,975 [WARNING] Retry 2/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:54:00,975 [ERROR] Error crawling https://example.com/page2: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:54:00,987 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:54:00,987 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:54:00,988 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:54:00,989 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 22:54:00,989 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 22:54:00,989 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 22:54:00,989 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 22:54:00,989 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 22:54:00,989 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 22:54:00,989 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-28 22:54:00,989 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-28 22:54:00,989 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-28 22:54:01,218 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:54:01,218 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:54:01,223 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:54:01,245 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:54:01,248 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:54:01,251 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:54:01,255 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:54:01,258 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:54:01,272 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:54:01,293 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:54:01,296 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:54:01,299 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:54:01,304 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:54:01,307 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:54:01,320 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:54:01,323 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:54:01,324 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:54:01,324 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:54:01,327 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:54:01,328 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:54:01,328 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:54:01,331 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:54:01,334 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:54:01,342 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:54:01,348 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:54:01,354 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:54:01,361 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:54:01,367 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:54:01,373 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:54:01,379 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:54:01,386 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:54:01,391 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:54:01,424 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x13e7a8910>
2024-11-28 22:54:01,425 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x13e6b0830>, 281607.544472375)]']
connector: <aiohttp.connector.TCPConnector object at 0x13e696e50>
2024-11-28 22:54:01,425 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x12e8578d0>
2024-11-28 22:54:01,425 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x13e6b3230>, 281612.873088583)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x13e6b09f0>, 281614.447157708)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x13e6b1be0>, 281616.114615458)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x13e6b0d70>, 281617.341849208)]']
connector: <aiohttp.connector.TCPConnector object at 0x13e85cdd0>
2024-11-28 22:54:01,499 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:54:01,652 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:54:01,654 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:54:01,654 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:54:01,654 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:54:01,654 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:54:02,140 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:54:02,140 [DEBUG] Base URL: https://example.com
2024-11-28 22:54:02,141 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:54:02,141 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:54:02,141 [DEBUG] Initial domain set to: example.com
2024-11-28 22:54:02,141 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:54:02,141 [INFO] Successfully crawled https://example.com
2024-11-28 22:54:02,141 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x13e98f750>
2024-11-28 22:54:02,141 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x14aa51cc0>, 281742.675213125)]']
connector: <aiohttp.connector.TCPConnector object at 0x13e33dd10>
2024-11-28 22:54:02,142 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2024-11-28 22:54:02,990 [WARNING] Retry 1/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:54:03,494 [INFO] Operation took 0.10 seconds
2024-11-28 22:54:12,990 [WARNING] Retry 1/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:54:22,992 [WARNING] Retry 1/2 for https://example.com/page3: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:54:30,991 [WARNING] Retry 2/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:54:30,992 [ERROR] Error crawling https://example.com/page1: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:54:40,992 [WARNING] Retry 2/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:54:40,993 [ERROR] Error crawling https://example.com/page2: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:54:50,993 [WARNING] Retry 2/2 for https://example.com/page3: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:54:50,994 [ERROR] Error crawling https://example.com/page3: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:54:50,996 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:54:50,997 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:54:50,999 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:54:51,005 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:54:51,005 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:54:51,006 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:54:51,007 [DEBUG] Using raw URL string: https://nonexistent.example.com (type: <class 'str'>)
2024-11-28 22:54:51,007 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='nonexistent.example.com', path='', params='', query='', fragment='')
2024-11-28 22:54:51,007 [INFO] Starting crawl of URL: https://nonexistent.example.com
2024-11-28 22:54:53,008 [WARNING] Retry 1/2 for https://nonexistent.example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:55:01,008 [WARNING] Retry 2/2 for https://nonexistent.example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:55:01,009 [ERROR] Error crawling https://nonexistent.example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:55:01,018 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:01,018 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:01,019 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:55:01,023 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:01,024 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:01,026 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:55:01,026 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:55:01,026 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:55:01,026 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:55:03,027 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:55:11,027 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:55:11,028 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:55:11,037 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:11,038 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:11,039 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:55:11,040 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:55:11,040 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:55:11,040 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:55:13,040 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:55:13,596 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 22:55:14,035 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:14,035 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:14,036 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:14,038 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:14,038 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:14,039 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:14,039 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:14,062 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:14,063 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:14,063 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:14,065 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:14,066 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:14,069 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:14,069 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:14,072 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:14,072 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:14,073 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:14,073 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:14,076 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:14,076 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:14,077 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:14,079 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:14,166 [WARNING] Content exceeds size limit (1048577 > 1048576)
2024-11-28 22:55:14,569 [WARNING] Content too small (80 < 100)
2024-11-28 22:55:14,578 [WARNING] Content too small (97 < 100)
2024-11-28 22:55:14,669 [WARNING] Content too small (104 < 100)
2024-11-28 22:55:14,679 [WARNING] Content too small (15 < 100)
2024-11-28 22:55:15,098 [INFO] test_operation took 0.11 seconds
2024-11-28 22:55:15,303 [INFO] inner took 0.10 seconds
2024-11-28 22:55:15,303 [INFO] outer took 0.20 seconds
2024-11-28 22:55:15,303 [INFO] error_operation took 0.00 seconds
2024-11-28 22:55:15,367 [INFO] test_operation took 0.00 seconds
2024-11-28 22:55:15,373 [WARNING] Content too small (0 < 100)
2024-11-28 22:55:15,376 [WARNING] Content too small (25 < 100)
2024-11-28 22:55:15,385 [WARNING] Content too small (49 < 100)
2024-11-28 22:55:15,488 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:15,488 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:15,489 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:55:15,489 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:55:15,489 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:55:15,975 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:15,975 [DEBUG] Base URL: https://example.com
2024-11-28 22:55:15,976 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:15,976 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:15,976 [DEBUG] Initial domain set to: example.com
2024-11-28 22:55:15,976 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:15,976 [INFO] Successfully crawled https://example.com
2024-11-28 22:55:15,978 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:15,979 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1277ccd50>
2024-11-28 22:55:15,979 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x127a195c0>, 281816.5102385)]']
connector: <aiohttp.connector.TCPConnector object at 0x127707190>
2024-11-28 22:55:15,980 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:15,980 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:15,981 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:55:15,981 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:55:15,981 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:55:16,004 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-11-28 22:55:16,004 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-11-28 22:55:16,004 [INFO] Starting crawl of URL: https://example.org
2024-11-28 22:55:16,474 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:16,474 [DEBUG] Base URL: https://example.com
2024-11-28 22:55:16,474 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:16,474 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:16,475 [DEBUG] Initial domain set to: example.com
2024-11-28 22:55:16,475 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:16,475 [INFO] Successfully crawled https://example.com
2024-11-28 22:55:16,974 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:16,974 [DEBUG] Base URL: https://example.org
2024-11-28 22:55:16,974 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:16,974 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:16,974 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:16,974 [INFO] Successfully crawled https://example.org
2024-11-28 22:55:16,975 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:16,976 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1279a7590>
2024-11-28 22:55:16,977 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x127a195c0>, 281817.00925175)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x127a1bcb0>, 281817.509126708)]']
connector: <aiohttp.connector.TCPConnector object at 0x1279a4f10>
2024-11-28 22:55:16,977 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:16,977 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:16,978 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:55:16,979 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:55:16,979 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:55:17,478 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:17,478 [DEBUG] Base URL: https://example.com
2024-11-28 22:55:17,478 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:17,478 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:17,478 [DEBUG] Initial domain set to: example.com
2024-11-28 22:55:17,478 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:17,479 [INFO] Successfully crawled https://example.com
2024-11-28 22:55:17,480 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:17,481 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x127a2b450>
2024-11-28 22:55:17,481 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x127a182f0>, 281818.012798625)]']
connector: <aiohttp.connector.TCPConnector object at 0x1279a5890>
2024-11-28 22:55:17,481 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:17,481 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:17,482 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:55:17,483 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:55:17,483 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:55:18,161 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:18,161 [DEBUG] Base URL: https://example.com
2024-11-28 22:55:18,161 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:18,161 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:18,161 [DEBUG] Initial domain set to: example.com
2024-11-28 22:55:18,161 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:18,162 [INFO] Successfully crawled https://example.com
2024-11-28 22:55:18,166 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:18,166 [DEBUG] Base URL: https://example.com
2024-11-28 22:55:18,167 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:18,167 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:18,167 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:18,171 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:18,172 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:18,172 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:18,176 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:18,177 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:18,177 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:18,178 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:55:18,178 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:55:18,178 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:55:18,676 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:18,676 [DEBUG] Base URL: https://example.com
2024-11-28 22:55:18,676 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:18,677 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:18,677 [DEBUG] Initial domain set to: example.com
2024-11-28 22:55:18,677 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:18,677 [INFO] Successfully crawled https://example.com
2024-11-28 22:55:18,678 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:18,679 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x127a27090>
2024-11-28 22:55:18,679 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x127a19d30>, 281819.211051916)]']
connector: <aiohttp.connector.TCPConnector object at 0x127e04450>
2024-11-28 22:55:18,680 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:18,680 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:18,682 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2024-11-28 22:55:18,682 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2024-11-28 22:55:18,682 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2024-11-28 22:55:20,708 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-28 22:55:20,710 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-28 22:55:20,710 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-28 22:55:20,715 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:20,716 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:20,716 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:20,717 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:55:20,717 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:55:20,717 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:55:20,739 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-11-28 22:55:20,739 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-11-28 22:55:20,739 [INFO] Starting crawl of URL: https://example.org
2024-11-28 22:55:20,739 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2024-11-28 22:55:20,740 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2024-11-28 22:55:20,740 [INFO] Starting crawl of URL: https://example.net
2024-11-28 22:55:21,041 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:55:21,042 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:55:21,047 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:21,079 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1052d7460>, 281693.700121083)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1052d7b60>, 281694.957138375)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1052d7ee0>, 281695.983730833)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1052d4280>, 281697.141490125)])']
connector: <aiohttp.connector.TCPConnector object at 0x105261490>
2024-11-28 22:55:21,079 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1053bd5d0>
2024-11-28 22:55:21,079 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1052d6970>, 281699.320398958)])']
connector: <aiohttp.connector.TCPConnector object at 0x1053bc5d0>
2024-11-28 22:55:21,079 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1053bf450>
2024-11-28 22:55:21,079 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1054de190>, 281701.476757583)])']
connector: <aiohttp.connector.TCPConnector object at 0x1053bf810>
2024-11-28 22:55:21,092 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:21,096 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:21,099 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:21,103 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:21,106 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:21,109 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:21,112 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:21,115 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:21,118 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:21,120 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:21,123 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:21,135 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:21,139 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:21,139 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:21,139 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:21,143 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:21,143 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:21,143 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:21,147 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:21,149 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:21,157 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:21,162 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:21,168 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:21,174 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:21,181 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:21,187 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:21,193 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:21,198 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:21,202 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:21,202 [DEBUG] Base URL: https://example.com
2024-11-28 22:55:21,202 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:21,202 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:21,202 [DEBUG] Initial domain set to: example.com
2024-11-28 22:55:21,202 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:21,202 [INFO] Successfully crawled https://example.com
2024-11-28 22:55:21,204 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:21,294 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:21,433 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:21,435 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:21,435 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:55:21,435 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:55:21,435 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:55:21,705 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:21,705 [DEBUG] Base URL: https://example.org
2024-11-28 22:55:21,705 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:21,705 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:21,705 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:21,705 [INFO] Successfully crawled https://example.org
2024-11-28 22:55:21,924 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:21,924 [DEBUG] Base URL: https://example.com
2024-11-28 22:55:21,924 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:21,924 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:21,924 [DEBUG] Initial domain set to: example.com
2024-11-28 22:55:21,924 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:21,924 [INFO] Successfully crawled https://example.com
2024-11-28 22:55:21,924 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x10538d090>
2024-11-28 22:55:21,925 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10836d8d0>, 281822.45971975)])']
connector: <aiohttp.connector.TCPConnector object at 0x10856c710>
2024-11-28 22:55:21,926 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2024-11-28 22:55:22,214 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:22,215 [DEBUG] Base URL: https://example.net
2024-11-28 22:55:22,215 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:22,215 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:22,215 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:22,215 [INFO] Successfully crawled https://example.net
2024-11-28 22:55:22,216 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:22,216 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x127e040d0>
2024-11-28 22:55:22,216 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x127a19b70>, 281821.738423125)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x127a1a2e0>, 281822.240838958)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x127a1a4a0>, 281822.750555208)]']
connector: <aiohttp.connector.TCPConnector object at 0x127ad7cd0>
2024-11-28 22:55:22,217 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:22,217 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:22,218 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:55:22,218 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:55:22,218 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:55:22,706 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:22,706 [DEBUG] Base URL: https://example.com
2024-11-28 22:55:22,707 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:22,707 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:22,707 [DEBUG] Initial domain set to: example.com
2024-11-28 22:55:22,707 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:22,707 [INFO] Successfully crawled https://example.com
2024-11-28 22:55:22,709 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:22,712 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x127e04350>
2024-11-28 22:55:22,712 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x127a18440>, 281818.696032375)]']
connector: <aiohttp.connector.TCPConnector object at 0x127e04610>
2024-11-28 22:55:22,712 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x127ae7410>
2024-11-28 22:55:22,718 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:22,718 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:22,718 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:22,743 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:22,763 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:22,764 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:22,764 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:22,765 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:22,783 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:22,784 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:22,784 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:22,785 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:22,785 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2024-11-28 22:55:22,785 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2024-11-28 22:55:22,785 [INFO] Starting crawl of URL: https://example.com/page
2024-11-28 22:55:23,094 [INFO] Operation took 0.11 seconds
2024-11-28 22:55:23,274 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:23,274 [DEBUG] Base URL: https://example.com/page
2024-11-28 22:55:23,274 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:23,274 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:23,274 [DEBUG] Initial domain set to: example.com
2024-11-28 22:55:23,274 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:23,274 [INFO] Successfully crawled https://example.com/page
2024-11-28 22:55:23,274 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2024-11-28 22:55:23,274 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2024-11-28 22:55:23,274 [INFO] Starting crawl of URL: https://other-domain.com/page
2024-11-28 22:55:26,762 [DEBUG] 
Evaluating link: ./
2024-11-28 22:55:26,762 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,762 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-28 22:55:26,762 [DEBUG] Absolute link: https://other-domain.com/
2024-11-28 22:55:26,762 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,762 [DEBUG] 
Evaluating link: ./
2024-11-28 22:55:26,762 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,762 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-28 22:55:26,762 [DEBUG] Absolute link: https://other-domain.com/
2024-11-28 22:55:26,762 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,762 [DEBUG] 
Evaluating link: ./hiyou.php
2024-11-28 22:55:26,762 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,762 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-11-28 22:55:26,763 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-11-28 22:55:26,763 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,763 [DEBUG] 
Evaluating link: ./date.php
2024-11-28 22:55:26,763 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,763 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-11-28 22:55:26,763 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-11-28 22:55:26,763 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,763 [DEBUG] 
Evaluating link: ./contact.php
2024-11-28 22:55:26,763 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,763 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2024-11-28 22:55:26,763 [DEBUG] Absolute link: https://other-domain.com/contact.php
2024-11-28 22:55:26,763 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,763 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:55:26,763 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,763 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:55:26,763 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:55:26,763 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-28 22:55:26,763 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:26,763 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,763 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:26,764 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:26,764 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 22:55:26,764 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:55:26,764 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,764 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:55:26,764 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:55:26,764 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 22:55:26,764 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:55:26,764 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,764 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:55:26,764 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:55:26,764 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 22:55:26,764 [DEBUG] 
Evaluating link: ./contact
2024-11-28 22:55:26,764 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,764 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-11-28 22:55:26,764 [DEBUG] Absolute link: https://other-domain.com/contact
2024-11-28 22:55:26,764 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,764 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:26,764 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,764 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:26,764 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:26,764 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 22:55:26,764 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:55:26,765 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,765 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:55:26,765 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:55:26,765 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 22:55:26,765 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:55:26,765 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,765 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:55:26,765 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:55:26,765 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 22:55:26,765 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-11-28 22:55:26,765 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,765 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:26,765 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:26,765 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-11-28 22:55:26,765 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:55:26,765 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,765 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:55:26,765 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:55:26,765 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-28 22:55:26,765 [DEBUG] 
Evaluating link: #indexes
2024-11-28 22:55:26,765 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,765 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2024-11-28 22:55:26,765 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,765 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,765 [DEBUG] 
Evaluating link: #a
2024-11-28 22:55:26,765 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,765 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2024-11-28 22:55:26,765 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,765 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,765 [DEBUG] 
Evaluating link: #b
2024-11-28 22:55:26,765 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,765 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-11-28 22:55:26,765 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,765 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,765 [DEBUG] 
Evaluating link: #c
2024-11-28 22:55:26,765 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,766 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-11-28 22:55:26,766 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,766 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,766 [DEBUG] 
Evaluating link: #d
2024-11-28 22:55:26,766 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,766 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-11-28 22:55:26,766 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,766 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,766 [DEBUG] 
Evaluating link: #e
2024-11-28 22:55:26,766 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,766 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-11-28 22:55:26,766 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,766 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,766 [DEBUG] 
Evaluating link: #f
2024-11-28 22:55:26,766 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,766 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-11-28 22:55:26,766 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,766 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,766 [DEBUG] 
Evaluating link: #g
2024-11-28 22:55:26,766 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,766 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-11-28 22:55:26,766 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,766 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,766 [DEBUG] 
Evaluating link: #h
2024-11-28 22:55:26,766 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,766 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-11-28 22:55:26,766 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,766 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,766 [DEBUG] 
Evaluating link: #i
2024-11-28 22:55:26,766 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,766 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-11-28 22:55:26,766 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,766 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,766 [DEBUG] 
Evaluating link: #j
2024-11-28 22:55:26,766 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,766 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-11-28 22:55:26,767 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,767 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,767 [DEBUG] 
Evaluating link: #k
2024-11-28 22:55:26,767 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,767 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-11-28 22:55:26,767 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,767 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,767 [DEBUG] 
Evaluating link: #l
2024-11-28 22:55:26,767 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,767 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-11-28 22:55:26,767 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,767 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,767 [DEBUG] 
Evaluating link: #m
2024-11-28 22:55:26,767 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,767 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-11-28 22:55:26,767 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,767 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,767 [DEBUG] 
Evaluating link: #n
2024-11-28 22:55:26,767 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,767 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-11-28 22:55:26,767 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,767 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,767 [DEBUG] 
Evaluating link: #o
2024-11-28 22:55:26,767 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,767 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-11-28 22:55:26,767 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,767 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,767 [DEBUG] 
Evaluating link: #p
2024-11-28 22:55:26,767 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,767 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-11-28 22:55:26,767 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,767 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,768 [DEBUG] 
Evaluating link: #q
2024-11-28 22:55:26,768 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,768 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-11-28 22:55:26,768 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,768 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,768 [DEBUG] 
Evaluating link: #r
2024-11-28 22:55:26,768 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,768 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-11-28 22:55:26,768 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,768 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,768 [DEBUG] 
Evaluating link: #s
2024-11-28 22:55:26,768 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,768 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-11-28 22:55:26,768 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,768 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,768 [DEBUG] 
Evaluating link: #t
2024-11-28 22:55:26,768 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,768 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-11-28 22:55:26,768 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,768 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,768 [DEBUG] 
Evaluating link: #u
2024-11-28 22:55:26,768 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,768 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-11-28 22:55:26,768 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,768 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,768 [DEBUG] 
Evaluating link: #v
2024-11-28 22:55:26,768 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,768 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-11-28 22:55:26,768 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,768 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,768 [DEBUG] 
Evaluating link: #w
2024-11-28 22:55:26,768 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,768 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-11-28 22:55:26,768 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,768 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,769 [DEBUG] 
Evaluating link: #x
2024-11-28 22:55:26,769 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,769 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-11-28 22:55:26,769 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,769 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,769 [DEBUG] 
Evaluating link: #y
2024-11-28 22:55:26,769 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,769 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-11-28 22:55:26,769 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,769 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,769 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:26,769 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,769 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:26,769 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:26,769 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 22:55:26,769 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:55:26,769 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,769 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:55:26,769 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:55:26,769 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 22:55:26,769 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:55:26,769 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,769 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:55:26,769 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:55:26,770 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 22:55:26,770 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-11-28 22:55:26,770 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,770 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:26,770 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:26,770 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-11-28 22:55:26,770 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:55:26,770 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,770 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:55:26,770 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:55:26,770 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-28 22:55:26,770 [DEBUG] 
Evaluating link: https://spriq.jp/
2024-11-28 22:55:26,770 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,770 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2024-11-28 22:55:26,770 [DEBUG] Absolute link: https://spriq.jp/
2024-11-28 22:55:26,770 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2024-11-28 22:55:26,770 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:26,770 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,770 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:26,770 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:26,770 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 22:55:26,770 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:26,770 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,770 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:26,770 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:26,771 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 22:55:26,771 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:55:26,771 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,771 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:55:26,771 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:55:26,771 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 22:55:26,771 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:55:26,771 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,771 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:55:26,771 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:55:26,771 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 22:55:26,771 [DEBUG] 
Evaluating link: ./contact
2024-11-28 22:55:26,771 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,771 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-11-28 22:55:26,771 [DEBUG] Absolute link: https://other-domain.com/contact
2024-11-28 22:55:26,771 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,771 [DEBUG] 
Evaluating link: ./
2024-11-28 22:55:26,771 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,771 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-28 22:55:26,771 [DEBUG] Absolute link: https://other-domain.com/
2024-11-28 22:55:26,771 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,771 [DEBUG] 
Evaluating link: ./hiyou.php
2024-11-28 22:55:26,771 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,771 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-11-28 22:55:26,771 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-11-28 22:55:26,771 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,771 [DEBUG] 
Evaluating link: ./date.php
2024-11-28 22:55:26,772 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,772 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-11-28 22:55:26,772 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-11-28 22:55:26,772 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,772 [DEBUG] 
Evaluating link: #u
2024-11-28 22:55:26,772 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,772 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-11-28 22:55:26,772 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,772 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,772 [DEBUG] 
Evaluating link: #v
2024-11-28 22:55:26,772 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,772 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-11-28 22:55:26,772 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,772 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,772 [DEBUG] 
Evaluating link: #w
2024-11-28 22:55:26,772 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,772 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-11-28 22:55:26,772 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,772 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,772 [DEBUG] 
Evaluating link: #x
2024-11-28 22:55:26,772 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,772 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-11-28 22:55:26,772 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,772 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,772 [DEBUG] 
Evaluating link: #y
2024-11-28 22:55:26,772 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,772 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-11-28 22:55:26,772 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,772 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,773 [DEBUG] 
Evaluating link: #p
2024-11-28 22:55:26,773 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,773 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-11-28 22:55:26,773 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,773 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,773 [DEBUG] 
Evaluating link: #q
2024-11-28 22:55:26,773 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,773 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-11-28 22:55:26,773 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,773 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,773 [DEBUG] 
Evaluating link: #r
2024-11-28 22:55:26,773 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,773 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-11-28 22:55:26,773 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,773 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,773 [DEBUG] 
Evaluating link: #s
2024-11-28 22:55:26,773 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,773 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-11-28 22:55:26,773 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,773 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,773 [DEBUG] 
Evaluating link: #t
2024-11-28 22:55:26,773 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,773 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-11-28 22:55:26,773 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,773 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,773 [DEBUG] 
Evaluating link: #k
2024-11-28 22:55:26,773 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,773 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-11-28 22:55:26,773 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,773 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,773 [DEBUG] 
Evaluating link: #l
2024-11-28 22:55:26,774 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,774 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-11-28 22:55:26,774 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,774 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,774 [DEBUG] 
Evaluating link: #m
2024-11-28 22:55:26,774 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,774 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-11-28 22:55:26,774 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,774 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,774 [DEBUG] 
Evaluating link: #n
2024-11-28 22:55:26,774 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,774 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-11-28 22:55:26,774 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,774 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,774 [DEBUG] 
Evaluating link: #o
2024-11-28 22:55:26,774 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,774 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-11-28 22:55:26,774 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,774 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,774 [DEBUG] 
Evaluating link: #f
2024-11-28 22:55:26,774 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,774 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-11-28 22:55:26,774 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,774 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,774 [DEBUG] 
Evaluating link: #g
2024-11-28 22:55:26,774 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,774 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-11-28 22:55:26,774 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,774 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,774 [DEBUG] 
Evaluating link: #h
2024-11-28 22:55:26,774 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,774 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-11-28 22:55:26,774 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,775 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,775 [DEBUG] 
Evaluating link: #i
2024-11-28 22:55:26,775 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,775 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-11-28 22:55:26,775 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,775 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,775 [DEBUG] 
Evaluating link: #j
2024-11-28 22:55:26,775 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,775 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-11-28 22:55:26,775 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,775 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,775 [DEBUG] 
Evaluating link: #b
2024-11-28 22:55:26,775 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,775 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-11-28 22:55:26,775 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,775 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,775 [DEBUG] 
Evaluating link: #c
2024-11-28 22:55:26,775 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,775 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-11-28 22:55:26,775 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,775 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,775 [DEBUG] 
Evaluating link: #d
2024-11-28 22:55:26,775 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,776 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-11-28 22:55:26,776 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,776 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,776 [DEBUG] 
Evaluating link: #e
2024-11-28 22:55:26,776 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,776 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-11-28 22:55:26,776 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,776 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,776 [DEBUG] 
Evaluating link: #site
2024-11-28 22:55:26,776 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:26,776 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2024-11-28 22:55:26,776 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:26,776 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:26,776 [INFO] Successfully crawled https://other-domain.com/page
2024-11-28 22:55:26,780 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:26,782 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:26,782 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:26,783 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-11-28 22:55:26,783 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-11-28 22:55:26,783 [INFO] Starting crawl of URL: https://example.com/page0
2024-11-28 22:55:27,269 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:27,269 [DEBUG] Base URL: https://example.com/page0
2024-11-28 22:55:27,269 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:27,269 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:27,269 [DEBUG] Initial domain set to: example.com
2024-11-28 22:55:27,269 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:27,269 [INFO] Successfully crawled https://example.com/page0
2024-11-28 22:55:27,269 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 22:55:27,269 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 22:55:27,269 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 22:55:27,423 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:27,423 [DEBUG] Base URL: https://example.com/page1
2024-11-28 22:55:27,423 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:27,423 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:27,423 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:27,423 [INFO] Successfully crawled https://example.com/page1
2024-11-28 22:55:27,423 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 22:55:27,423 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 22:55:27,423 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 22:55:27,923 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:27,923 [DEBUG] Base URL: https://example.com/page2
2024-11-28 22:55:27,923 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:27,923 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:27,923 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:27,923 [INFO] Successfully crawled https://example.com/page2
2024-11-28 22:55:27,923 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-28 22:55:27,924 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-28 22:55:27,924 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-28 22:55:28,426 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:28,426 [DEBUG] Base URL: https://example.com/page3
2024-11-28 22:55:28,426 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:28,427 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:28,427 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:28,427 [INFO] Successfully crawled https://example.com/page3
2024-11-28 22:55:28,427 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-11-28 22:55:28,427 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-11-28 22:55:28,427 [INFO] Starting crawl of URL: https://example.com/page4
2024-11-28 22:55:28,938 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:28,939 [DEBUG] Base URL: https://example.com/page4
2024-11-28 22:55:28,939 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:28,939 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:28,939 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:28,939 [INFO] Successfully crawled https://example.com/page4
2024-11-28 22:55:28,939 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:28,939 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-11-28 22:55:28,939 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-11-28 22:55:28,940 [INFO] Starting crawl of URL: https://example.com/page0
2024-11-28 22:55:29,435 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:29,436 [DEBUG] Base URL: https://example.com/page0
2024-11-28 22:55:29,436 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:29,436 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:29,436 [DEBUG] Initial domain set to: example.com
2024-11-28 22:55:29,436 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:29,436 [INFO] Successfully crawled https://example.com/page0
2024-11-28 22:55:29,436 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 22:55:29,436 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 22:55:29,436 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 22:55:29,584 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:29,585 [DEBUG] Base URL: https://example.com/page1
2024-11-28 22:55:29,585 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:29,585 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:29,585 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:29,585 [INFO] Successfully crawled https://example.com/page1
2024-11-28 22:55:29,586 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 22:55:29,586 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 22:55:29,586 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 22:55:30,086 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:30,086 [DEBUG] Base URL: https://example.com/page2
2024-11-28 22:55:30,086 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:30,086 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:30,087 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:30,087 [INFO] Successfully crawled https://example.com/page2
2024-11-28 22:55:30,087 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-28 22:55:30,087 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-28 22:55:30,087 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-28 22:55:30,589 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:30,590 [DEBUG] Base URL: https://example.com/page3
2024-11-28 22:55:30,590 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:30,590 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:30,590 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:30,590 [INFO] Successfully crawled https://example.com/page3
2024-11-28 22:55:30,590 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-11-28 22:55:30,590 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-11-28 22:55:30,590 [INFO] Starting crawl of URL: https://example.com/page4
2024-11-28 22:55:31,088 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x127ae6850>
2024-11-28 22:55:31,088 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x127a19b00>, 281823.810240708)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x127a19d30>, 281825.187498416)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x127a1aa50>, 281826.08521575)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x127a1aba0>, 281827.283308333)]']
connector: <aiohttp.connector.TCPConnector object at 0x127ae4550>
2024-11-28 22:55:31,092 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:31,092 [DEBUG] Base URL: https://example.com/page4
2024-11-28 22:55:31,092 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:31,092 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:31,092 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:31,092 [INFO] Successfully crawled https://example.com/page4
2024-11-28 22:55:31,097 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:31,099 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:31,100 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:55:31,105 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:31,106 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:31,106 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:55:31,107 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:55:31,107 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:55:31,107 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:55:33,107 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:55:37,356 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 22:55:37,630 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:37,630 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:37,631 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:37,632 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:37,632 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:37,632 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:37,633 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:37,651 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:37,652 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:37,652 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:37,654 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:37,655 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:37,658 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:37,658 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:37,661 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:37,661 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:37,662 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:37,663 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:37,665 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:37,665 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:37,666 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:37,668 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:37,706 [WARNING] Content exceeds size limit (1048577 > 1048576)
2024-11-28 22:55:38,099 [WARNING] Content too small (80 < 100)
2024-11-28 22:55:38,109 [WARNING] Content too small (97 < 100)
2024-11-28 22:55:38,193 [WARNING] Content too small (104 < 100)
2024-11-28 22:55:38,202 [WARNING] Content too small (15 < 100)
2024-11-28 22:55:38,574 [INFO] test_operation took 0.11 seconds
2024-11-28 22:55:38,783 [INFO] inner took 0.10 seconds
2024-11-28 22:55:38,783 [INFO] outer took 0.21 seconds
2024-11-28 22:55:38,783 [INFO] error_operation took 0.00 seconds
2024-11-28 22:55:38,819 [INFO] test_operation took 0.00 seconds
2024-11-28 22:55:38,823 [WARNING] Content too small (0 < 100)
2024-11-28 22:55:38,825 [WARNING] Content too small (25 < 100)
2024-11-28 22:55:38,834 [WARNING] Content too small (49 < 100)
2024-11-28 22:55:38,926 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:38,926 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:38,927 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:55:38,927 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:55:38,927 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:55:39,408 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:39,408 [DEBUG] Base URL: https://example.com
2024-11-28 22:55:39,408 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:39,408 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:39,408 [DEBUG] Initial domain set to: example.com
2024-11-28 22:55:39,408 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:39,408 [INFO] Successfully crawled https://example.com
2024-11-28 22:55:39,409 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:39,409 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x11329fbd0>
2024-11-28 22:55:39,409 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1131a97f0>, 281839.943699041)])']
connector: <aiohttp.connector.TCPConnector object at 0x110cce690>
2024-11-28 22:55:39,409 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:39,410 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:39,410 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:55:39,411 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:55:39,411 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:55:39,433 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-11-28 22:55:39,433 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-11-28 22:55:39,433 [INFO] Starting crawl of URL: https://example.org
2024-11-28 22:55:39,911 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:39,912 [DEBUG] Base URL: https://example.com
2024-11-28 22:55:39,912 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:39,912 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:39,912 [DEBUG] Initial domain set to: example.com
2024-11-28 22:55:39,912 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:39,912 [INFO] Successfully crawled https://example.com
2024-11-28 22:55:40,411 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:40,412 [DEBUG] Base URL: https://example.org
2024-11-28 22:55:40,412 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:40,412 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:40,412 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:40,412 [INFO] Successfully crawled https://example.org
2024-11-28 22:55:40,413 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:40,414 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x113356a50>
2024-11-28 22:55:40,414 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1131a81a0>, 281840.446849583)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1131a8670>, 281840.946415041)])']
connector: <aiohttp.connector.TCPConnector object at 0x113355c90>
2024-11-28 22:55:40,415 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:40,415 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:40,416 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:55:40,416 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:55:40,416 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:55:40,912 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:40,912 [DEBUG] Base URL: https://example.com
2024-11-28 22:55:40,913 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:40,913 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:40,913 [DEBUG] Initial domain set to: example.com
2024-11-28 22:55:40,913 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:40,913 [INFO] Successfully crawled https://example.com
2024-11-28 22:55:40,914 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:40,915 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x11329df50>
2024-11-28 22:55:40,915 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1131a82f0>, 281841.447866291)])']
connector: <aiohttp.connector.TCPConnector object at 0x11329f750>
2024-11-28 22:55:40,915 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:40,915 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:40,916 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:55:40,917 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:55:40,917 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:55:41,107 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:55:41,108 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:55:41,112 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:41,115 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:41,116 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:55:41,116 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 22:55:41,116 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 22:55:41,116 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 22:55:41,117 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 22:55:41,117 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 22:55:41,117 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 22:55:41,416 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:41,417 [DEBUG] Base URL: https://example.com
2024-11-28 22:55:41,417 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:41,417 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:41,417 [DEBUG] Initial domain set to: example.com
2024-11-28 22:55:41,417 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:41,417 [INFO] Successfully crawled https://example.com
2024-11-28 22:55:41,420 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:41,421 [DEBUG] Base URL: https://example.com
2024-11-28 22:55:41,421 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:41,421 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:41,421 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:41,425 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:41,426 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:41,426 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:41,429 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:41,430 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:41,430 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:41,431 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:55:41,431 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:55:41,431 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:55:41,920 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:41,920 [DEBUG] Base URL: https://example.com
2024-11-28 22:55:41,920 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:41,920 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:41,920 [DEBUG] Initial domain set to: example.com
2024-11-28 22:55:41,920 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:41,920 [INFO] Successfully crawled https://example.com
2024-11-28 22:55:41,920 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:41,921 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1131d46d0>
2024-11-28 22:55:41,921 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1131a8590>, 281842.455289708)])']
connector: <aiohttp.connector.TCPConnector object at 0x113060610>
2024-11-28 22:55:41,921 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:41,922 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:41,922 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2024-11-28 22:55:41,922 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2024-11-28 22:55:41,922 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2024-11-28 22:55:43,118 [WARNING] Retry 1/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:55:43,947 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-28 22:55:43,949 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-28 22:55:43,949 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-28 22:55:43,957 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:43,958 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:43,958 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:43,959 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:55:43,959 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:55:43,959 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:55:43,980 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-11-28 22:55:43,981 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-11-28 22:55:43,981 [INFO] Starting crawl of URL: https://example.org
2024-11-28 22:55:43,981 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2024-11-28 22:55:43,981 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2024-11-28 22:55:43,981 [INFO] Starting crawl of URL: https://example.net
2024-11-28 22:55:44,456 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:44,456 [DEBUG] Base URL: https://example.com
2024-11-28 22:55:44,457 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:44,457 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:44,457 [DEBUG] Initial domain set to: example.com
2024-11-28 22:55:44,457 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:44,457 [INFO] Successfully crawled https://example.com
2024-11-28 22:55:44,955 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:44,955 [DEBUG] Base URL: https://example.org
2024-11-28 22:55:44,955 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:44,955 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:44,955 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:44,955 [INFO] Successfully crawled https://example.org
2024-11-28 22:55:45,456 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:45,456 [DEBUG] Base URL: https://example.net
2024-11-28 22:55:45,456 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:45,456 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:45,456 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:45,456 [INFO] Successfully crawled https://example.net
2024-11-28 22:55:45,457 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:45,457 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1131d4690>
2024-11-28 22:55:45,457 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1131aab30>, 281844.9918435)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1131aaf90>, 281845.491386)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1131ab380>, 281845.992164375)])']
connector: <aiohttp.connector.TCPConnector object at 0x112090610>
2024-11-28 22:55:45,457 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:45,457 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:45,458 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:55:45,458 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:55:45,458 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:55:45,947 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:45,947 [DEBUG] Base URL: https://example.com
2024-11-28 22:55:45,947 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:45,947 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:45,947 [DEBUG] Initial domain set to: example.com
2024-11-28 22:55:45,947 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:45,947 [INFO] Successfully crawled https://example.com
2024-11-28 22:55:45,948 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:45,950 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x113514190>
2024-11-28 22:55:45,950 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1131a8980>, 281841.952831291)])']
connector: <aiohttp.connector.TCPConnector object at 0x113517010>
2024-11-28 22:55:45,950 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1120d1050>
2024-11-28 22:55:45,954 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:45,954 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:45,954 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:55:45,973 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:45,988 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:45,989 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:45,989 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:45,989 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:46,005 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:46,006 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:46,006 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:46,006 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:46,006 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2024-11-28 22:55:46,006 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2024-11-28 22:55:46,006 [INFO] Starting crawl of URL: https://example.com/page
2024-11-28 22:55:46,495 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:46,495 [DEBUG] Base URL: https://example.com/page
2024-11-28 22:55:46,495 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:46,496 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:46,496 [DEBUG] Initial domain set to: example.com
2024-11-28 22:55:46,496 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:46,496 [INFO] Successfully crawled https://example.com/page
2024-11-28 22:55:46,496 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2024-11-28 22:55:46,496 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2024-11-28 22:55:46,496 [INFO] Starting crawl of URL: https://other-domain.com/page
2024-11-28 22:55:49,872 [DEBUG] 
Evaluating link: ./
2024-11-28 22:55:49,872 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,872 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-28 22:55:49,872 [DEBUG] Absolute link: https://other-domain.com/
2024-11-28 22:55:49,872 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,872 [DEBUG] 
Evaluating link: ./
2024-11-28 22:55:49,872 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,872 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-28 22:55:49,872 [DEBUG] Absolute link: https://other-domain.com/
2024-11-28 22:55:49,872 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,872 [DEBUG] 
Evaluating link: ./hiyou.php
2024-11-28 22:55:49,873 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,873 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-11-28 22:55:49,873 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-11-28 22:55:49,873 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,873 [DEBUG] 
Evaluating link: ./date.php
2024-11-28 22:55:49,873 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,873 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-11-28 22:55:49,873 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-11-28 22:55:49,873 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,873 [DEBUG] 
Evaluating link: ./contact.php
2024-11-28 22:55:49,873 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,873 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2024-11-28 22:55:49,873 [DEBUG] Absolute link: https://other-domain.com/contact.php
2024-11-28 22:55:49,873 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,873 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:55:49,873 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,873 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:55:49,873 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:55:49,873 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-28 22:55:49,873 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:49,873 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,873 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:49,873 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:49,873 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 22:55:49,873 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:55:49,873 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,873 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:55:49,873 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:55:49,873 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 22:55:49,873 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:55:49,873 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,873 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:55:49,873 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:55:49,873 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 22:55:49,873 [DEBUG] 
Evaluating link: ./contact
2024-11-28 22:55:49,873 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,873 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-11-28 22:55:49,873 [DEBUG] Absolute link: https://other-domain.com/contact
2024-11-28 22:55:49,873 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,873 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:49,873 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,873 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:49,874 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:49,874 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 22:55:49,874 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:55:49,874 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,874 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:55:49,874 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:55:49,874 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 22:55:49,874 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:55:49,874 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,874 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:55:49,874 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:55:49,874 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 22:55:49,874 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-11-28 22:55:49,874 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,874 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:49,874 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:49,874 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-11-28 22:55:49,874 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:55:49,874 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,874 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:55:49,874 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:55:49,874 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-28 22:55:49,874 [DEBUG] 
Evaluating link: #indexes
2024-11-28 22:55:49,874 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,874 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2024-11-28 22:55:49,874 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,874 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,874 [DEBUG] 
Evaluating link: #a
2024-11-28 22:55:49,874 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,874 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2024-11-28 22:55:49,874 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,874 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,874 [DEBUG] 
Evaluating link: #b
2024-11-28 22:55:49,874 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,874 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-11-28 22:55:49,874 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,874 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,874 [DEBUG] 
Evaluating link: #c
2024-11-28 22:55:49,874 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,874 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-11-28 22:55:49,874 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,874 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,874 [DEBUG] 
Evaluating link: #d
2024-11-28 22:55:49,874 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,874 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-11-28 22:55:49,874 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,874 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,874 [DEBUG] 
Evaluating link: #e
2024-11-28 22:55:49,875 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,875 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-11-28 22:55:49,875 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,875 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,875 [DEBUG] 
Evaluating link: #f
2024-11-28 22:55:49,875 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,875 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-11-28 22:55:49,875 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,875 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,875 [DEBUG] 
Evaluating link: #g
2024-11-28 22:55:49,875 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,875 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-11-28 22:55:49,875 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,875 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,875 [DEBUG] 
Evaluating link: #h
2024-11-28 22:55:49,875 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,875 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-11-28 22:55:49,875 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,875 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,875 [DEBUG] 
Evaluating link: #i
2024-11-28 22:55:49,875 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,875 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-11-28 22:55:49,875 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,875 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,875 [DEBUG] 
Evaluating link: #j
2024-11-28 22:55:49,875 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,875 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-11-28 22:55:49,875 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,875 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,875 [DEBUG] 
Evaluating link: #k
2024-11-28 22:55:49,875 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,875 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-11-28 22:55:49,875 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,875 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,875 [DEBUG] 
Evaluating link: #l
2024-11-28 22:55:49,875 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,875 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-11-28 22:55:49,875 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,875 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,876 [DEBUG] 
Evaluating link: #m
2024-11-28 22:55:49,876 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,876 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-11-28 22:55:49,876 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,876 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,876 [DEBUG] 
Evaluating link: #n
2024-11-28 22:55:49,876 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,876 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-11-28 22:55:49,876 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,876 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,876 [DEBUG] 
Evaluating link: #o
2024-11-28 22:55:49,876 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,876 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-11-28 22:55:49,876 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,876 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,876 [DEBUG] 
Evaluating link: #p
2024-11-28 22:55:49,876 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,876 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-11-28 22:55:49,876 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,876 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,876 [DEBUG] 
Evaluating link: #q
2024-11-28 22:55:49,876 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,876 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-11-28 22:55:49,876 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,876 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,876 [DEBUG] 
Evaluating link: #r
2024-11-28 22:55:49,876 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,876 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-11-28 22:55:49,876 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,876 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,876 [DEBUG] 
Evaluating link: #s
2024-11-28 22:55:49,876 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,876 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-11-28 22:55:49,876 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,876 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,876 [DEBUG] 
Evaluating link: #t
2024-11-28 22:55:49,876 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,877 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-11-28 22:55:49,877 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,877 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,877 [DEBUG] 
Evaluating link: #u
2024-11-28 22:55:49,877 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,877 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-11-28 22:55:49,877 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,877 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,877 [DEBUG] 
Evaluating link: #v
2024-11-28 22:55:49,877 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,877 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-11-28 22:55:49,877 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,877 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,877 [DEBUG] 
Evaluating link: #w
2024-11-28 22:55:49,877 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,877 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-11-28 22:55:49,877 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,877 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,877 [DEBUG] 
Evaluating link: #x
2024-11-28 22:55:49,877 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,877 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-11-28 22:55:49,877 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,877 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,877 [DEBUG] 
Evaluating link: #y
2024-11-28 22:55:49,877 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,877 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-11-28 22:55:49,877 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,877 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,877 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:49,877 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,877 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:49,877 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:49,877 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 22:55:49,877 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:55:49,877 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,877 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:55:49,877 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:55:49,877 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 22:55:49,877 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:55:49,877 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,877 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:55:49,877 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:55:49,877 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 22:55:49,877 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-11-28 22:55:49,877 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,877 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:49,878 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:49,878 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-11-28 22:55:49,878 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:55:49,878 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,878 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:55:49,878 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:55:49,878 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-28 22:55:49,878 [DEBUG] 
Evaluating link: https://spriq.jp/
2024-11-28 22:55:49,878 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,878 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2024-11-28 22:55:49,878 [DEBUG] Absolute link: https://spriq.jp/
2024-11-28 22:55:49,878 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2024-11-28 22:55:49,878 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:49,878 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,878 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:49,878 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:49,878 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 22:55:49,878 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:49,878 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,878 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:49,878 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:55:49,878 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 22:55:49,878 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:55:49,878 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,878 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:55:49,878 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:55:49,878 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 22:55:49,878 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:55:49,878 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,878 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:55:49,878 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:55:49,878 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 22:55:49,878 [DEBUG] 
Evaluating link: ./contact
2024-11-28 22:55:49,878 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,878 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-11-28 22:55:49,878 [DEBUG] Absolute link: https://other-domain.com/contact
2024-11-28 22:55:49,878 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,878 [DEBUG] 
Evaluating link: ./
2024-11-28 22:55:49,878 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,878 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-28 22:55:49,878 [DEBUG] Absolute link: https://other-domain.com/
2024-11-28 22:55:49,878 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,878 [DEBUG] 
Evaluating link: ./hiyou.php
2024-11-28 22:55:49,878 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,878 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-11-28 22:55:49,878 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-11-28 22:55:49,879 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,879 [DEBUG] 
Evaluating link: ./date.php
2024-11-28 22:55:49,879 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,879 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-11-28 22:55:49,879 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-11-28 22:55:49,879 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,879 [DEBUG] 
Evaluating link: #u
2024-11-28 22:55:49,879 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,879 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-11-28 22:55:49,879 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,879 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,879 [DEBUG] 
Evaluating link: #v
2024-11-28 22:55:49,879 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,879 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-11-28 22:55:49,879 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,879 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,879 [DEBUG] 
Evaluating link: #w
2024-11-28 22:55:49,879 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,879 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-11-28 22:55:49,879 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,879 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,879 [DEBUG] 
Evaluating link: #x
2024-11-28 22:55:49,879 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,879 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-11-28 22:55:49,879 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,879 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,879 [DEBUG] 
Evaluating link: #y
2024-11-28 22:55:49,879 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,879 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-11-28 22:55:49,879 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,879 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,879 [DEBUG] 
Evaluating link: #p
2024-11-28 22:55:49,879 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,879 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-11-28 22:55:49,879 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,879 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,879 [DEBUG] 
Evaluating link: #q
2024-11-28 22:55:49,879 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,879 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-11-28 22:55:49,879 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,879 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,879 [DEBUG] 
Evaluating link: #r
2024-11-28 22:55:49,879 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,879 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-11-28 22:55:49,879 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,879 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,879 [DEBUG] 
Evaluating link: #s
2024-11-28 22:55:49,880 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,880 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-11-28 22:55:49,880 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,880 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,880 [DEBUG] 
Evaluating link: #t
2024-11-28 22:55:49,880 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,880 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-11-28 22:55:49,880 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,880 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,880 [DEBUG] 
Evaluating link: #k
2024-11-28 22:55:49,880 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,880 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-11-28 22:55:49,880 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,880 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,880 [DEBUG] 
Evaluating link: #l
2024-11-28 22:55:49,880 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,880 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-11-28 22:55:49,880 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,880 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,880 [DEBUG] 
Evaluating link: #m
2024-11-28 22:55:49,880 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,880 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-11-28 22:55:49,880 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,880 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,880 [DEBUG] 
Evaluating link: #n
2024-11-28 22:55:49,880 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,880 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-11-28 22:55:49,880 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,880 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,880 [DEBUG] 
Evaluating link: #o
2024-11-28 22:55:49,880 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,880 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-11-28 22:55:49,880 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,880 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,880 [DEBUG] 
Evaluating link: #f
2024-11-28 22:55:49,880 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,880 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-11-28 22:55:49,880 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,880 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,880 [DEBUG] 
Evaluating link: #g
2024-11-28 22:55:49,880 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,880 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-11-28 22:55:49,880 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,880 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,880 [DEBUG] 
Evaluating link: #h
2024-11-28 22:55:49,880 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,880 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-11-28 22:55:49,881 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,881 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,881 [DEBUG] 
Evaluating link: #i
2024-11-28 22:55:49,881 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,881 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-11-28 22:55:49,881 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,881 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,881 [DEBUG] 
Evaluating link: #j
2024-11-28 22:55:49,881 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,881 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-11-28 22:55:49,881 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,881 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,881 [DEBUG] 
Evaluating link: #b
2024-11-28 22:55:49,881 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,881 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-11-28 22:55:49,881 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,881 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,881 [DEBUG] 
Evaluating link: #c
2024-11-28 22:55:49,881 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,881 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-11-28 22:55:49,881 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,881 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,881 [DEBUG] 
Evaluating link: #d
2024-11-28 22:55:49,881 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,881 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-11-28 22:55:49,881 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,881 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,881 [DEBUG] 
Evaluating link: #e
2024-11-28 22:55:49,881 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,881 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-11-28 22:55:49,881 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,881 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,881 [DEBUG] 
Evaluating link: #site
2024-11-28 22:55:49,881 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:55:49,881 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2024-11-28 22:55:49,881 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:55:49,881 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:55:49,881 [INFO] Successfully crawled https://other-domain.com/page
2024-11-28 22:55:49,884 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:49,886 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:49,886 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:49,886 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-11-28 22:55:49,886 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-11-28 22:55:49,886 [INFO] Starting crawl of URL: https://example.com/page0
2024-11-28 22:55:50,386 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:50,386 [DEBUG] Base URL: https://example.com/page0
2024-11-28 22:55:50,386 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:50,386 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:50,386 [DEBUG] Initial domain set to: example.com
2024-11-28 22:55:50,386 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:50,386 [INFO] Successfully crawled https://example.com/page0
2024-11-28 22:55:50,386 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 22:55:50,386 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 22:55:50,386 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 22:55:50,519 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:50,519 [DEBUG] Base URL: https://example.com/page1
2024-11-28 22:55:50,519 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:50,519 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:50,519 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:50,520 [INFO] Successfully crawled https://example.com/page1
2024-11-28 22:55:50,520 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 22:55:50,520 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 22:55:50,520 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 22:55:51,021 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:51,021 [DEBUG] Base URL: https://example.com/page2
2024-11-28 22:55:51,021 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:51,021 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:51,021 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:51,021 [INFO] Successfully crawled https://example.com/page2
2024-11-28 22:55:51,021 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-28 22:55:51,021 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-28 22:55:51,021 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-28 22:55:51,522 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:51,522 [DEBUG] Base URL: https://example.com/page3
2024-11-28 22:55:51,522 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:51,522 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:51,522 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:51,522 [INFO] Successfully crawled https://example.com/page3
2024-11-28 22:55:51,523 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-11-28 22:55:51,523 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-11-28 22:55:51,523 [INFO] Starting crawl of URL: https://example.com/page4
2024-11-28 22:55:52,023 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x113335410>
2024-11-28 22:55:52,023 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1131abcb0>, 281847.031812541)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1131a8980>, 281848.106353166)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1131eda20>, 281849.037311583)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1131edb00>, 281850.401851583)])']
connector: <aiohttp.connector.TCPConnector object at 0x113335e90>
2024-11-28 22:55:52,025 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:52,025 [DEBUG] Base URL: https://example.com/page4
2024-11-28 22:55:52,025 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:52,025 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:52,025 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:52,025 [INFO] Successfully crawled https://example.com/page4
2024-11-28 22:55:52,025 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:55:52,025 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-11-28 22:55:52,025 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-11-28 22:55:52,025 [INFO] Starting crawl of URL: https://example.com/page0
2024-11-28 22:55:52,514 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:52,514 [DEBUG] Base URL: https://example.com/page0
2024-11-28 22:55:52,514 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:52,514 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:52,514 [DEBUG] Initial domain set to: example.com
2024-11-28 22:55:52,514 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:52,514 [INFO] Successfully crawled https://example.com/page0
2024-11-28 22:55:52,514 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 22:55:52,514 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 22:55:52,514 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 22:55:52,660 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:52,660 [DEBUG] Base URL: https://example.com/page1
2024-11-28 22:55:52,660 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:52,660 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:52,660 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:52,660 [INFO] Successfully crawled https://example.com/page1
2024-11-28 22:55:52,660 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 22:55:52,660 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 22:55:52,660 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 22:55:53,118 [WARNING] Retry 1/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:55:53,162 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:53,162 [DEBUG] Base URL: https://example.com/page2
2024-11-28 22:55:53,162 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:53,162 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:53,162 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:53,162 [INFO] Successfully crawled https://example.com/page2
2024-11-28 22:55:53,162 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-28 22:55:53,162 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-28 22:55:53,162 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-28 22:55:53,662 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:53,662 [DEBUG] Base URL: https://example.com/page3
2024-11-28 22:55:53,662 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:53,662 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:53,662 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:53,662 [INFO] Successfully crawled https://example.com/page3
2024-11-28 22:55:53,662 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-11-28 22:55:53,662 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-11-28 22:55:53,662 [INFO] Starting crawl of URL: https://example.com/page4
2024-11-28 22:55:54,162 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:55:54,162 [DEBUG] Base URL: https://example.com/page4
2024-11-28 22:55:54,162 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:55:54,162 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:55:54,162 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:55:54,162 [INFO] Successfully crawled https://example.com/page4
2024-11-28 22:55:54,164 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:54,165 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:54,166 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:55:54,169 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:54,170 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:55:54,171 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:55:54,171 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:55:54,171 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:55:54,171 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:55:56,172 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:56:01,118 [WARNING] Retry 2/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:56:01,118 [ERROR] Error crawling https://example.com/page1: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:56:04,172 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:56:04,172 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:56:04,176 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:56:04,181 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:56:04,182 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:56:04,182 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 22:56:04,182 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 22:56:04,182 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 22:56:04,182 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 22:56:04,182 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 22:56:04,182 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 22:56:06,182 [WARNING] Retry 1/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:56:11,119 [WARNING] Retry 2/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:56:11,119 [ERROR] Error crawling https://example.com/page2: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:56:11,124 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:56:11,124 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:56:11,125 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:56:11,125 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 22:56:11,125 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 22:56:11,125 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 22:56:11,125 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 22:56:11,125 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 22:56:11,125 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 22:56:11,125 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-28 22:56:11,126 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-28 22:56:11,126 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-28 22:56:13,127 [WARNING] Retry 1/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:56:16,184 [WARNING] Retry 1/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:56:23,127 [WARNING] Retry 1/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:56:24,184 [WARNING] Retry 2/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:56:24,184 [ERROR] Error crawling https://example.com/page1: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:56:33,129 [WARNING] Retry 1/2 for https://example.com/page3: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:56:34,185 [WARNING] Retry 2/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:56:34,186 [ERROR] Error crawling https://example.com/page2: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:56:34,195 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:56:34,196 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:56:34,197 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:56:34,198 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 22:56:34,198 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 22:56:34,198 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 22:56:34,198 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 22:56:34,198 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 22:56:34,198 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 22:56:34,198 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-28 22:56:34,198 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-28 22:56:34,198 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-28 22:56:36,199 [WARNING] Retry 1/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:56:41,129 [WARNING] Retry 2/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:56:41,129 [ERROR] Error crawling https://example.com/page1: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:56:46,198 [WARNING] Retry 1/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:56:51,130 [WARNING] Retry 2/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:56:51,131 [ERROR] Error crawling https://example.com/page2: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:56:56,200 [WARNING] Retry 1/2 for https://example.com/page3: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:57:01,130 [WARNING] Retry 2/2 for https://example.com/page3: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:57:01,130 [ERROR] Error crawling https://example.com/page3: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:57:01,132 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:01,133 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:01,134 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:57:01,141 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:01,142 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:01,143 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:57:01,143 [DEBUG] Using raw URL string: https://nonexistent.example.com (type: <class 'str'>)
2024-11-28 22:57:01,143 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='nonexistent.example.com', path='', params='', query='', fragment='')
2024-11-28 22:57:01,143 [INFO] Starting crawl of URL: https://nonexistent.example.com
2024-11-28 22:57:03,145 [WARNING] Retry 1/2 for https://nonexistent.example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:57:04,200 [WARNING] Retry 2/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:57:04,200 [ERROR] Error crawling https://example.com/page1: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:57:11,145 [WARNING] Retry 2/2 for https://nonexistent.example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:57:11,145 [ERROR] Error crawling https://nonexistent.example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:57:11,150 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:11,151 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:11,152 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:57:11,156 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:11,157 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:11,158 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:57:11,158 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:57:11,158 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:57:11,158 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:57:13,160 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:57:14,201 [WARNING] Retry 2/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:57:14,202 [ERROR] Error crawling https://example.com/page2: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:57:21,159 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:57:21,160 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:57:21,168 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:21,169 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:21,170 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:57:21,171 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:57:21,171 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:57:21,171 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:57:23,173 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:57:24,202 [WARNING] Retry 2/2 for https://example.com/page3: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:57:24,203 [ERROR] Error crawling https://example.com/page3: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:57:24,204 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:24,205 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:24,206 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:57:24,213 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:24,214 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:24,215 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:57:24,216 [DEBUG] Using raw URL string: https://nonexistent.example.com (type: <class 'str'>)
2024-11-28 22:57:24,216 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='nonexistent.example.com', path='', params='', query='', fragment='')
2024-11-28 22:57:24,216 [INFO] Starting crawl of URL: https://nonexistent.example.com
2024-11-28 22:57:26,217 [WARNING] Retry 1/2 for https://nonexistent.example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:57:31,173 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:57:31,173 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:57:31,182 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:31,204 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:31,208 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:31,212 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:31,215 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:31,219 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:31,223 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:31,226 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:31,230 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:31,233 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:31,236 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:31,240 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:31,254 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:31,259 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:31,259 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:31,260 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:31,264 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:31,264 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:31,265 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:31,268 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:31,271 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:31,301 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x127a5b310>
2024-11-28 22:57:31,303 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x127a59b90>
2024-11-28 22:57:31,303 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x127a1a890>, 281829.473683625)]']
connector: <aiohttp.connector.TCPConnector object at 0x127a5bd50>
2024-11-28 22:57:31,303 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x127a8bb60>, 281831.62285125)]']
connector: <aiohttp.connector.TCPConnector object at 0x127bbec10>
2024-11-28 22:57:31,325 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:31,337 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:31,343 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:31,350 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:31,355 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:31,362 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:31,368 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:31,375 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:31,382 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:31,443 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:31,592 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:31,593 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:57:31,594 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:57:31,594 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:57:31,594 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:57:32,079 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:57:32,079 [DEBUG] Base URL: https://example.com
2024-11-28 22:57:32,079 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:57:32,079 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:57:32,080 [DEBUG] Initial domain set to: example.com
2024-11-28 22:57:32,080 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:57:32,080 [INFO] Successfully crawled https://example.com
2024-11-28 22:57:32,080 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x168b32450>
2024-11-28 22:57:32,080 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x168b2e5f0>, 281952.614882916)]']
connector: <aiohttp.connector.TCPConnector object at 0x168b31a90>
2024-11-28 22:57:32,083 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2024-11-28 22:57:33,404 [INFO] Operation took 0.11 seconds
2024-11-28 22:57:34,217 [WARNING] Retry 2/2 for https://nonexistent.example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:57:34,217 [ERROR] Error crawling https://nonexistent.example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:57:34,222 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:34,223 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:34,224 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:57:34,229 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:34,229 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:34,230 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:57:34,231 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:57:34,231 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:57:34,231 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:57:36,231 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:57:44,231 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:57:44,232 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:57:44,249 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x11335ae90>
2024-11-28 22:57:44,250 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1131ee7b0>, 281852.559287458)])']
connector: <aiohttp.connector.TCPConnector object at 0x11335b6d0>
2024-11-28 22:57:44,250 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1133725d0>
2024-11-28 22:57:44,250 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1131aaac0>, 281854.698295375)])']
connector: <aiohttp.connector.TCPConnector object at 0x113391d90>
2024-11-28 22:57:44,253 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:44,253 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:44,254 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:57:44,254 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:57:44,254 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:57:44,255 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:57:46,255 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:57:54,256 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:57:54,256 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:57:54,259 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:54,272 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:54,274 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:54,277 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:54,280 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:54,282 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:54,285 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:54,287 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:54,289 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:54,292 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:54,294 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:54,297 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:54,307 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:54,310 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:54,311 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:54,311 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:54,314 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:54,314 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:54,315 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:54,318 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:54,321 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:54,329 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:54,335 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:54,342 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:54,348 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:54,354 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:54,360 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:54,366 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:54,372 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:54,378 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:57:54,466 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:54,600 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:57:54,600 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:57:54,600 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:57:54,601 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:57:54,601 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:57:55,088 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:57:55,088 [DEBUG] Base URL: https://example.com
2024-11-28 22:57:55,088 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:57:55,088 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:57:55,088 [DEBUG] Initial domain set to: example.com
2024-11-28 22:57:55,088 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:57:55,088 [INFO] Successfully crawled https://example.com
2024-11-28 22:57:55,088 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x117e4af90>
2024-11-28 22:57:55,088 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x117c43f50>, 281975.625800958)])']
connector: <aiohttp.connector.TCPConnector object at 0x117e48210>
2024-11-28 22:57:55,089 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2024-11-28 22:57:56,264 [INFO] Operation took 0.11 seconds
2024-11-28 22:59:00,146 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 22:59:00,406 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:00,407 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:00,407 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:00,408 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:00,408 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:00,409 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:00,409 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:59:00,427 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:00,428 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:00,428 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:59:00,430 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:00,431 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:00,448 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:00,448 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:00,451 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:00,452 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:00,452 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:00,453 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:00,455 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:00,456 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:00,456 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:59:00,458 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:00,489 [WARNING] Content exceeds size limit (1048577 > 1048576)
2024-11-28 22:59:00,891 [WARNING] Content too small (80 < 100)
2024-11-28 22:59:00,899 [WARNING] Content too small (97 < 100)
2024-11-28 22:59:00,981 [WARNING] Content too small (104 < 100)
2024-11-28 22:59:01,006 [WARNING] Content too small (15 < 100)
2024-11-28 22:59:01,099 [WARNING] Content too small (96 < 100)
2024-11-28 22:59:01,104 [WARNING] Content too small (89 < 100)
2024-11-28 22:59:01,173 [INFO] Operation took 0.06 seconds
2024-11-28 22:59:01,178 [WARNING] Content too small (21 < 100)
2024-11-28 22:59:01,453 [INFO] test_operation took 0.10 seconds
2024-11-28 22:59:01,660 [INFO] inner took 0.11 seconds
2024-11-28 22:59:01,660 [INFO] outer took 0.21 seconds
2024-11-28 22:59:01,660 [INFO] error_operation took 0.00 seconds
2024-11-28 22:59:01,706 [INFO] test_operation took 0.00 seconds
2024-11-28 22:59:01,713 [WARNING] Content too small (0 < 100)
2024-11-28 22:59:01,716 [WARNING] Content too small (25 < 100)
2024-11-28 22:59:01,727 [WARNING] Content too small (49 < 100)
2024-11-28 22:59:01,822 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:59:01,822 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:01,822 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:59:01,822 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:59:01,822 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:59:02,309 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:59:02,310 [DEBUG] Base URL: https://example.com
2024-11-28 22:59:02,310 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:59:02,310 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:59:02,310 [DEBUG] Initial domain set to: example.com
2024-11-28 22:59:02,310 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:59:02,310 [INFO] Successfully crawled https://example.com
2024-11-28 22:59:02,312 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:02,313 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x105256f50>
2024-11-28 22:59:02,313 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10529cec0>, 282042.846423541)])']
connector: <aiohttp.connector.TCPConnector object at 0x1045ad810>
2024-11-28 22:59:02,313 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:59:02,313 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:02,314 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:59:02,314 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:59:02,314 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:59:02,337 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-11-28 22:59:02,337 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-11-28 22:59:02,337 [INFO] Starting crawl of URL: https://example.org
2024-11-28 22:59:02,816 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:59:02,817 [DEBUG] Base URL: https://example.com
2024-11-28 22:59:02,817 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:59:02,817 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:59:02,817 [DEBUG] Initial domain set to: example.com
2024-11-28 22:59:02,817 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:59:02,817 [INFO] Successfully crawled https://example.com
2024-11-28 22:59:03,313 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:59:03,314 [DEBUG] Base URL: https://example.org
2024-11-28 22:59:03,314 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:59:03,314 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:59:03,314 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:59:03,314 [INFO] Successfully crawled https://example.org
2024-11-28 22:59:03,315 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:03,317 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x10521ec10>
2024-11-28 22:59:03,317 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10529c0c0>, 282043.353772375)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x10529c590>, 282043.851177208)])']
connector: <aiohttp.connector.TCPConnector object at 0x10521f410>
2024-11-28 22:59:03,318 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:59:03,318 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:03,319 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:59:03,319 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:59:03,319 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:59:03,810 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:59:03,810 [DEBUG] Base URL: https://example.com
2024-11-28 22:59:03,810 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:59:03,810 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:59:03,810 [DEBUG] Initial domain set to: example.com
2024-11-28 22:59:03,810 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:59:03,811 [INFO] Successfully crawled https://example.com
2024-11-28 22:59:03,812 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:03,813 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x10521c210>
2024-11-28 22:59:03,813 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10529c210>, 282044.347069)])']
connector: <aiohttp.connector.TCPConnector object at 0x10521ecd0>
2024-11-28 22:59:03,814 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:59:03,814 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:03,815 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:59:03,815 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:59:03,815 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:59:04,313 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:59:04,313 [DEBUG] Base URL: https://example.com
2024-11-28 22:59:04,313 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:59:04,313 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:59:04,313 [DEBUG] Initial domain set to: example.com
2024-11-28 22:59:04,314 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:59:04,314 [INFO] Successfully crawled https://example.com
2024-11-28 22:59:04,319 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:59:04,320 [DEBUG] Base URL: https://example.com
2024-11-28 22:59:04,320 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:59:04,320 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:59:04,320 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:59:04,325 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:04,326 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:59:04,326 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:04,329 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:04,330 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:59:04,330 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:04,331 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:59:04,331 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:59:04,331 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:59:04,818 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:59:04,818 [DEBUG] Base URL: https://example.com
2024-11-28 22:59:04,818 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:59:04,818 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:59:04,818 [DEBUG] Initial domain set to: example.com
2024-11-28 22:59:04,818 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:59:04,818 [INFO] Successfully crawled https://example.com
2024-11-28 22:59:04,819 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:04,821 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x105200390>
2024-11-28 22:59:04,821 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10529c4b0>, 282045.355733625)])']
connector: <aiohttp.connector.TCPConnector object at 0x105201b90>
2024-11-28 22:59:04,822 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:59:04,822 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:04,823 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2024-11-28 22:59:04,824 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2024-11-28 22:59:04,824 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2024-11-28 22:59:06,848 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-28 22:59:06,851 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-28 22:59:06,852 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-28 22:59:06,860 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:06,861 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:59:06,861 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:06,862 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:59:06,862 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:59:06,862 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:59:06,885 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-11-28 22:59:06,885 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-11-28 22:59:06,885 [INFO] Starting crawl of URL: https://example.org
2024-11-28 22:59:06,885 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2024-11-28 22:59:06,885 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2024-11-28 22:59:06,886 [INFO] Starting crawl of URL: https://example.net
2024-11-28 22:59:07,364 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:59:07,365 [DEBUG] Base URL: https://example.com
2024-11-28 22:59:07,365 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:59:07,365 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:59:07,366 [DEBUG] Initial domain set to: example.com
2024-11-28 22:59:07,366 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:59:07,366 [INFO] Successfully crawled https://example.com
2024-11-28 22:59:07,865 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:59:07,865 [DEBUG] Base URL: https://example.org
2024-11-28 22:59:07,865 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:59:07,865 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:59:07,865 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:59:07,866 [INFO] Successfully crawled https://example.org
2024-11-28 22:59:08,355 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:59:08,356 [DEBUG] Base URL: https://example.net
2024-11-28 22:59:08,356 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:59:08,356 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:59:08,356 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:59:08,356 [INFO] Successfully crawled https://example.net
2024-11-28 22:59:08,358 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:08,359 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1052008d0>
2024-11-28 22:59:08,359 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10529db00>, 282047.90253775)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x10529df60>, 282048.402472583)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x10529e350>, 282048.893820333)])']
connector: <aiohttp.connector.TCPConnector object at 0x1052011d0>
2024-11-28 22:59:08,360 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:59:08,360 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:08,361 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:59:08,361 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:59:08,361 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:59:08,854 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:59:08,855 [DEBUG] Base URL: https://example.com
2024-11-28 22:59:08,855 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:59:08,855 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:59:08,855 [DEBUG] Initial domain set to: example.com
2024-11-28 22:59:08,855 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:59:08,855 [INFO] Successfully crawled https://example.com
2024-11-28 22:59:08,857 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:08,866 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:59:08,866 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:08,866 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 22:59:08,892 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:59:08,913 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:08,914 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:59:08,914 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:08,915 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:59:08,933 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:08,934 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:59:08,934 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:08,934 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:59:08,934 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2024-11-28 22:59:08,934 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2024-11-28 22:59:08,934 [INFO] Starting crawl of URL: https://example.com/page
2024-11-28 22:59:09,422 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:59:09,422 [DEBUG] Base URL: https://example.com/page
2024-11-28 22:59:09,422 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:59:09,422 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:59:09,422 [DEBUG] Initial domain set to: example.com
2024-11-28 22:59:09,422 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:59:09,423 [INFO] Successfully crawled https://example.com/page
2024-11-28 22:59:09,423 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2024-11-28 22:59:09,423 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2024-11-28 22:59:09,423 [INFO] Starting crawl of URL: https://other-domain.com/page
2024-11-28 22:59:12,967 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x105270c50>
2024-11-28 22:59:12,980 [DEBUG] 
Evaluating link: ./
2024-11-28 22:59:12,980 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,980 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-28 22:59:12,980 [DEBUG] Absolute link: https://other-domain.com/
2024-11-28 22:59:12,980 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,980 [DEBUG] 
Evaluating link: ./
2024-11-28 22:59:12,980 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,981 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-28 22:59:12,981 [DEBUG] Absolute link: https://other-domain.com/
2024-11-28 22:59:12,981 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,981 [DEBUG] 
Evaluating link: ./hiyou.php
2024-11-28 22:59:12,981 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,981 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-11-28 22:59:12,981 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-11-28 22:59:12,981 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,981 [DEBUG] 
Evaluating link: ./date.php
2024-11-28 22:59:12,981 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,981 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-11-28 22:59:12,981 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-11-28 22:59:12,981 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,981 [DEBUG] 
Evaluating link: ./contact.php
2024-11-28 22:59:12,981 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,981 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2024-11-28 22:59:12,981 [DEBUG] Absolute link: https://other-domain.com/contact.php
2024-11-28 22:59:12,981 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,981 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:59:12,981 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,981 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:59:12,981 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:59:12,981 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-28 22:59:12,981 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:59:12,981 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,981 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:59:12,981 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:59:12,981 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 22:59:12,981 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:59:12,981 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,981 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:59:12,982 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:59:12,982 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 22:59:12,982 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:59:12,982 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,982 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:59:12,982 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:59:12,982 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 22:59:12,982 [DEBUG] 
Evaluating link: ./contact
2024-11-28 22:59:12,982 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,982 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-11-28 22:59:12,982 [DEBUG] Absolute link: https://other-domain.com/contact
2024-11-28 22:59:12,982 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,982 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:59:12,982 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,982 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:59:12,982 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:59:12,982 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 22:59:12,982 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:59:12,982 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,982 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:59:12,982 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:59:12,982 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 22:59:12,982 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:59:12,982 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,982 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:59:12,982 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:59:12,982 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 22:59:12,982 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-11-28 22:59:12,982 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,982 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:59:12,982 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:59:12,982 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-11-28 22:59:12,982 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:59:12,983 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,983 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:59:12,983 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:59:12,983 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-28 22:59:12,983 [DEBUG] 
Evaluating link: #indexes
2024-11-28 22:59:12,983 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,983 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2024-11-28 22:59:12,983 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,983 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,983 [DEBUG] 
Evaluating link: #a
2024-11-28 22:59:12,983 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,983 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2024-11-28 22:59:12,983 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,983 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,983 [DEBUG] 
Evaluating link: #b
2024-11-28 22:59:12,983 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,983 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-11-28 22:59:12,983 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,983 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,983 [DEBUG] 
Evaluating link: #c
2024-11-28 22:59:12,983 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,983 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-11-28 22:59:12,983 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,983 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,983 [DEBUG] 
Evaluating link: #d
2024-11-28 22:59:12,983 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,983 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-11-28 22:59:12,983 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,983 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,983 [DEBUG] 
Evaluating link: #e
2024-11-28 22:59:12,983 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,984 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-11-28 22:59:12,984 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,984 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,984 [DEBUG] 
Evaluating link: #f
2024-11-28 22:59:12,984 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,984 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-11-28 22:59:12,984 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,984 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,984 [DEBUG] 
Evaluating link: #g
2024-11-28 22:59:12,984 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,984 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-11-28 22:59:12,984 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,984 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,984 [DEBUG] 
Evaluating link: #h
2024-11-28 22:59:12,984 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,984 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-11-28 22:59:12,984 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,984 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,984 [DEBUG] 
Evaluating link: #i
2024-11-28 22:59:12,984 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,985 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-11-28 22:59:12,985 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,985 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,985 [DEBUG] 
Evaluating link: #j
2024-11-28 22:59:12,985 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,985 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-11-28 22:59:12,985 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,985 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,985 [DEBUG] 
Evaluating link: #k
2024-11-28 22:59:12,985 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,985 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-11-28 22:59:12,985 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,985 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,985 [DEBUG] 
Evaluating link: #l
2024-11-28 22:59:12,985 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,985 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-11-28 22:59:12,985 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,985 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,985 [DEBUG] 
Evaluating link: #m
2024-11-28 22:59:12,985 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,985 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-11-28 22:59:12,985 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,985 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,985 [DEBUG] 
Evaluating link: #n
2024-11-28 22:59:12,985 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,985 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-11-28 22:59:12,985 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,985 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,986 [DEBUG] 
Evaluating link: #o
2024-11-28 22:59:12,986 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,986 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-11-28 22:59:12,986 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,986 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,986 [DEBUG] 
Evaluating link: #p
2024-11-28 22:59:12,986 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,986 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-11-28 22:59:12,986 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,986 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,986 [DEBUG] 
Evaluating link: #q
2024-11-28 22:59:12,986 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,986 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-11-28 22:59:12,986 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,986 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,986 [DEBUG] 
Evaluating link: #r
2024-11-28 22:59:12,986 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,986 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-11-28 22:59:12,986 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,986 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,986 [DEBUG] 
Evaluating link: #s
2024-11-28 22:59:12,986 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,986 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-11-28 22:59:12,986 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,986 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,987 [DEBUG] 
Evaluating link: #t
2024-11-28 22:59:12,987 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,987 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-11-28 22:59:12,987 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,987 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,987 [DEBUG] 
Evaluating link: #u
2024-11-28 22:59:12,987 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,987 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-11-28 22:59:12,987 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,987 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,987 [DEBUG] 
Evaluating link: #v
2024-11-28 22:59:12,987 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,987 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-11-28 22:59:12,987 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,987 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,987 [DEBUG] 
Evaluating link: #w
2024-11-28 22:59:12,987 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,987 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-11-28 22:59:12,987 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,987 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,987 [DEBUG] 
Evaluating link: #x
2024-11-28 22:59:12,987 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,987 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-11-28 22:59:12,987 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,987 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,987 [DEBUG] 
Evaluating link: #y
2024-11-28 22:59:12,987 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,987 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-11-28 22:59:12,987 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,987 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,987 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:59:12,988 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,988 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:59:12,988 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:59:12,988 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 22:59:12,988 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:59:12,988 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,988 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:59:12,988 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:59:12,988 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 22:59:12,988 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:59:12,988 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,988 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:59:12,988 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:59:12,988 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 22:59:12,988 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-11-28 22:59:12,988 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,988 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:59:12,988 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:59:12,988 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-11-28 22:59:12,988 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:59:12,988 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,989 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:59:12,989 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 22:59:12,989 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-28 22:59:12,989 [DEBUG] 
Evaluating link: https://spriq.jp/
2024-11-28 22:59:12,989 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,989 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2024-11-28 22:59:12,989 [DEBUG] Absolute link: https://spriq.jp/
2024-11-28 22:59:12,989 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2024-11-28 22:59:12,989 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:59:12,989 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,989 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:59:12,989 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:59:12,989 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 22:59:12,989 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:59:12,989 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,989 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:59:12,989 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 22:59:12,989 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 22:59:12,989 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:59:12,989 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,989 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:59:12,989 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 22:59:12,989 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 22:59:12,989 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:59:12,989 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,989 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:59:12,989 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 22:59:12,989 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 22:59:12,989 [DEBUG] 
Evaluating link: ./contact
2024-11-28 22:59:12,989 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,989 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-11-28 22:59:12,989 [DEBUG] Absolute link: https://other-domain.com/contact
2024-11-28 22:59:12,989 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,989 [DEBUG] 
Evaluating link: ./
2024-11-28 22:59:12,989 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,990 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-28 22:59:12,990 [DEBUG] Absolute link: https://other-domain.com/
2024-11-28 22:59:12,990 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,990 [DEBUG] 
Evaluating link: ./hiyou.php
2024-11-28 22:59:12,990 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,990 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-11-28 22:59:12,990 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-11-28 22:59:12,990 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,990 [DEBUG] 
Evaluating link: ./date.php
2024-11-28 22:59:12,990 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,990 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-11-28 22:59:12,990 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-11-28 22:59:12,990 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,990 [DEBUG] 
Evaluating link: #u
2024-11-28 22:59:12,990 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,990 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-11-28 22:59:12,990 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,990 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,990 [DEBUG] 
Evaluating link: #v
2024-11-28 22:59:12,990 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,990 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-11-28 22:59:12,990 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,990 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,990 [DEBUG] 
Evaluating link: #w
2024-11-28 22:59:12,990 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,990 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-11-28 22:59:12,990 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,990 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,990 [DEBUG] 
Evaluating link: #x
2024-11-28 22:59:12,990 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,990 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-11-28 22:59:12,990 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,990 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,990 [DEBUG] 
Evaluating link: #y
2024-11-28 22:59:12,990 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,990 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-11-28 22:59:12,990 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,991 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,991 [DEBUG] 
Evaluating link: #p
2024-11-28 22:59:12,991 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,991 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-11-28 22:59:12,991 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,991 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,991 [DEBUG] 
Evaluating link: #q
2024-11-28 22:59:12,991 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,991 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-11-28 22:59:12,991 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,991 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,991 [DEBUG] 
Evaluating link: #r
2024-11-28 22:59:12,991 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,991 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-11-28 22:59:12,991 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,991 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,991 [DEBUG] 
Evaluating link: #s
2024-11-28 22:59:12,991 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,991 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-11-28 22:59:12,991 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,991 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,991 [DEBUG] 
Evaluating link: #t
2024-11-28 22:59:12,991 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,991 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-11-28 22:59:12,991 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,991 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,991 [DEBUG] 
Evaluating link: #k
2024-11-28 22:59:12,991 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,991 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-11-28 22:59:12,991 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,991 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,991 [DEBUG] 
Evaluating link: #l
2024-11-28 22:59:12,991 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,991 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-11-28 22:59:12,991 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,991 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,991 [DEBUG] 
Evaluating link: #m
2024-11-28 22:59:12,991 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,992 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-11-28 22:59:12,992 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,992 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,992 [DEBUG] 
Evaluating link: #n
2024-11-28 22:59:12,992 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,992 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-11-28 22:59:12,992 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,992 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,992 [DEBUG] 
Evaluating link: #o
2024-11-28 22:59:12,992 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,992 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-11-28 22:59:12,992 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,992 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,992 [DEBUG] 
Evaluating link: #f
2024-11-28 22:59:12,992 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,992 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-11-28 22:59:12,992 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,992 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,992 [DEBUG] 
Evaluating link: #g
2024-11-28 22:59:12,992 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,992 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-11-28 22:59:12,992 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,992 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,992 [DEBUG] 
Evaluating link: #h
2024-11-28 22:59:12,992 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,992 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-11-28 22:59:12,992 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,992 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,992 [DEBUG] 
Evaluating link: #i
2024-11-28 22:59:12,992 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,992 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-11-28 22:59:12,992 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,992 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,992 [DEBUG] 
Evaluating link: #j
2024-11-28 22:59:12,992 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,992 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-11-28 22:59:12,992 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,993 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,993 [DEBUG] 
Evaluating link: #b
2024-11-28 22:59:12,993 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,993 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-11-28 22:59:12,993 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,993 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,993 [DEBUG] 
Evaluating link: #c
2024-11-28 22:59:12,993 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,993 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-11-28 22:59:12,993 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,993 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,993 [DEBUG] 
Evaluating link: #d
2024-11-28 22:59:12,993 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,993 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-11-28 22:59:12,993 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,993 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,993 [DEBUG] 
Evaluating link: #e
2024-11-28 22:59:12,993 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,993 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-11-28 22:59:12,993 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,993 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,993 [DEBUG] 
Evaluating link: #site
2024-11-28 22:59:12,993 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 22:59:12,993 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2024-11-28 22:59:12,993 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 22:59:12,993 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 22:59:12,993 [INFO] Successfully crawled https://other-domain.com/page
2024-11-28 22:59:12,997 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:12,999 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:59:12,999 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:13,000 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-11-28 22:59:13,000 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-11-28 22:59:13,000 [INFO] Starting crawl of URL: https://example.com/page0
2024-11-28 22:59:13,513 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:59:13,514 [DEBUG] Base URL: https://example.com/page0
2024-11-28 22:59:13,514 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:59:13,514 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:59:13,514 [DEBUG] Initial domain set to: example.com
2024-11-28 22:59:13,514 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:59:13,514 [INFO] Successfully crawled https://example.com/page0
2024-11-28 22:59:13,514 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 22:59:13,514 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 22:59:13,514 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 22:59:13,644 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:59:13,644 [DEBUG] Base URL: https://example.com/page1
2024-11-28 22:59:13,644 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:59:13,644 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:59:13,644 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:59:13,645 [INFO] Successfully crawled https://example.com/page1
2024-11-28 22:59:13,645 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 22:59:13,645 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 22:59:13,645 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 22:59:14,143 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:59:14,143 [DEBUG] Base URL: https://example.com/page2
2024-11-28 22:59:14,144 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:59:14,144 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:59:14,144 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:59:14,144 [INFO] Successfully crawled https://example.com/page2
2024-11-28 22:59:14,144 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-28 22:59:14,144 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-28 22:59:14,144 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-28 22:59:14,649 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:59:14,649 [DEBUG] Base URL: https://example.com/page3
2024-11-28 22:59:14,650 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:59:14,650 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:59:14,650 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:59:14,650 [INFO] Successfully crawled https://example.com/page3
2024-11-28 22:59:14,650 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-11-28 22:59:14,650 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-11-28 22:59:14,650 [INFO] Starting crawl of URL: https://example.com/page4
2024-11-28 22:59:15,145 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:59:15,146 [DEBUG] Base URL: https://example.com/page4
2024-11-28 22:59:15,146 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:59:15,146 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:59:15,146 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:59:15,146 [INFO] Successfully crawled https://example.com/page4
2024-11-28 22:59:15,146 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 22:59:15,146 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-11-28 22:59:15,146 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-11-28 22:59:15,146 [INFO] Starting crawl of URL: https://example.com/page0
2024-11-28 22:59:15,643 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:59:15,644 [DEBUG] Base URL: https://example.com/page0
2024-11-28 22:59:15,644 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:59:15,644 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:59:15,644 [DEBUG] Initial domain set to: example.com
2024-11-28 22:59:15,644 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:59:15,644 [INFO] Successfully crawled https://example.com/page0
2024-11-28 22:59:15,644 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 22:59:15,644 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 22:59:15,644 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 22:59:15,787 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:59:15,787 [DEBUG] Base URL: https://example.com/page1
2024-11-28 22:59:15,788 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:59:15,788 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:59:15,788 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:59:15,788 [INFO] Successfully crawled https://example.com/page1
2024-11-28 22:59:15,788 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 22:59:15,788 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 22:59:15,788 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 22:59:16,289 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:59:16,289 [DEBUG] Base URL: https://example.com/page2
2024-11-28 22:59:16,289 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:59:16,289 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:59:16,289 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:59:16,289 [INFO] Successfully crawled https://example.com/page2
2024-11-28 22:59:16,289 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-28 22:59:16,290 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-28 22:59:16,290 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-28 22:59:16,793 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:59:16,794 [DEBUG] Base URL: https://example.com/page3
2024-11-28 22:59:16,794 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:59:16,794 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:59:16,794 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:59:16,794 [INFO] Successfully crawled https://example.com/page3
2024-11-28 22:59:16,794 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-11-28 22:59:16,794 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-11-28 22:59:16,794 [INFO] Starting crawl of URL: https://example.com/page4
2024-11-28 22:59:17,291 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 22:59:17,291 [DEBUG] Base URL: https://example.com/page4
2024-11-28 22:59:17,291 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 22:59:17,291 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 22:59:17,291 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 22:59:17,291 [INFO] Successfully crawled https://example.com/page4
2024-11-28 22:59:17,297 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:17,298 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:17,299 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:59:17,304 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:17,305 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:17,306 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:59:17,306 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 22:59:17,306 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 22:59:17,306 [INFO] Starting crawl of URL: https://example.com
2024-11-28 22:59:19,307 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:59:27,307 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:59:27,308 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:59:27,315 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:27,318 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:27,319 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:59:27,319 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 22:59:27,319 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 22:59:27,319 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 22:59:27,319 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 22:59:27,319 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 22:59:27,319 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 22:59:29,320 [WARNING] Retry 1/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:59:39,321 [WARNING] Retry 1/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:59:47,321 [WARNING] Retry 2/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:59:47,321 [ERROR] Error crawling https://example.com/page1: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:59:57,322 [WARNING] Retry 2/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:59:57,323 [ERROR] Error crawling https://example.com/page2: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 22:59:57,328 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:57,328 [DEBUG] Using selector: KqueueSelector
2024-11-28 22:59:57,329 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 22:59:57,329 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 22:59:57,329 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 22:59:57,329 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 22:59:57,329 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 22:59:57,329 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 22:59:57,329 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 22:59:57,329 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-28 22:59:57,329 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-28 22:59:57,329 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-28 22:59:59,329 [WARNING] Retry 1/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:00:09,331 [WARNING] Retry 1/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:00:19,332 [WARNING] Retry 1/2 for https://example.com/page3: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:00:27,332 [WARNING] Retry 2/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:00:27,333 [ERROR] Error crawling https://example.com/page1: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:00:37,332 [WARNING] Retry 2/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:00:37,333 [ERROR] Error crawling https://example.com/page2: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:00:47,333 [WARNING] Retry 2/2 for https://example.com/page3: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:00:47,334 [ERROR] Error crawling https://example.com/page3: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:00:47,335 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:00:47,335 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:00:47,336 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 23:00:47,339 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:00:47,339 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:00:47,340 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 23:00:47,340 [DEBUG] Using raw URL string: https://nonexistent.example.com (type: <class 'str'>)
2024-11-28 23:00:47,340 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='nonexistent.example.com', path='', params='', query='', fragment='')
2024-11-28 23:00:47,340 [INFO] Starting crawl of URL: https://nonexistent.example.com
2024-11-28 23:00:49,341 [WARNING] Retry 1/2 for https://nonexistent.example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:00:57,341 [WARNING] Retry 2/2 for https://nonexistent.example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:00:57,341 [ERROR] Error crawling https://nonexistent.example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:00:57,345 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:00:57,345 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:00:57,346 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 23:00:57,349 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:00:57,349 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:00:57,350 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 23:00:57,350 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 23:00:57,350 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 23:00:57,350 [INFO] Starting crawl of URL: https://example.com
2024-11-28 23:00:59,352 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:01:07,351 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:01:07,352 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:01:07,355 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:01:07,356 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:01:07,356 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 23:01:07,357 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 23:01:07,357 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 23:01:07,357 [INFO] Starting crawl of URL: https://example.com
2024-11-28 23:01:09,357 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:01:17,357 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:01:17,358 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:01:17,361 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:01:17,378 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:01:17,382 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:01:17,384 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:01:17,387 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:01:17,390 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:01:17,394 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:01:17,397 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:01:17,401 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:01:17,419 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1052442d0>
2024-11-28 23:01:17,419 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10529c6e0>, 282044.850480666)])']
connector: <aiohttp.connector.TCPConnector object at 0x105247590>
2024-11-28 23:01:17,419 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x105143210>
2024-11-28 23:01:17,419 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10529de10>, 282049.959735666)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x10529e7b0>, 282051.271444666)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x10529e350>, 282052.142416041)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x10529ea50>, 282053.501607916)])']
connector: <aiohttp.connector.TCPConnector object at 0x105142f10>
2024-11-28 23:01:17,420 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x10523e250>
2024-11-28 23:01:17,420 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10529e580>, 282055.684485208)])']
connector: <aiohttp.connector.TCPConnector object at 0x10523ecd0>
2024-11-28 23:01:17,420 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1051e2610>
2024-11-28 23:01:17,420 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1051381a0>, 282057.829161666)])']
connector: <aiohttp.connector.TCPConnector object at 0x105146550>
2024-11-28 23:01:17,425 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:01:17,428 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:01:17,431 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:01:17,445 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:01:17,448 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:01:17,448 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:01:17,448 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:01:17,451 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:01:17,452 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:01:17,452 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:01:17,455 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:01:17,457 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:01:17,464 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:01:17,469 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:01:17,477 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:01:17,483 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:01:17,490 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:01:17,496 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:01:17,502 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:01:17,508 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:01:17,515 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:01:17,594 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:01:17,744 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:01:17,746 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 23:01:17,746 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 23:01:17,746 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 23:01:17,746 [INFO] Starting crawl of URL: https://example.com
2024-11-28 23:01:18,235 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 23:01:18,236 [DEBUG] Base URL: https://example.com
2024-11-28 23:01:18,236 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 23:01:18,236 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 23:01:18,236 [DEBUG] Initial domain set to: example.com
2024-11-28 23:01:18,236 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 23:01:18,236 [INFO] Successfully crawled https://example.com
2024-11-28 23:01:18,236 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1185dd290>
2024-11-28 23:01:18,236 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x118359470>, 282178.776147583)])']
connector: <aiohttp.connector.TCPConnector object at 0x1185dd710>
2024-11-28 23:01:18,237 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2024-11-28 23:01:19,392 [INFO] Operation took 0.10 seconds
2024-11-28 23:03:00,615 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 23:03:00,757 [WARNING] Content length 64 is below minimum size 100
2024-11-28 23:03:00,768 [WARNING] Content length 42 is below minimum size 100
2024-11-28 23:03:00,770 [WARNING] Content exceeds size limit (1048577 > 1048576)
2024-11-28 23:03:00,977 [WARNING] Content length 41 is below minimum size 100
2024-11-28 23:03:01,001 [WARNING] Content length 79 is below minimum size 100
2024-11-28 23:03:01,005 [WARNING] Content length 23 is below minimum size 100
2024-11-28 23:03:01,009 [WARNING] Content length 54 is below minimum size 100
2024-11-28 23:03:01,010 [WARNING] Content length 57 is below minimum size 100
2024-11-28 23:03:01,021 [WARNING] Content length 87 is below minimum size 100
2024-11-28 23:03:01,050 [WARNING] Content length 59 is below minimum size 100
2024-11-28 23:03:01,051 [WARNING] Content length 28 is below minimum size 100
2024-11-28 23:03:01,062 [WARNING] Content length 18 is below minimum size 100
2024-11-28 23:03:01,140 [WARNING] Content length 73 is below minimum size 100
2024-11-28 23:03:01,160 [WARNING] Content length 9 is below minimum size 100
2024-11-28 23:03:01,170 [WARNING] Content length 66 is below minimum size 100
2024-11-28 23:03:01,181 [WARNING] Content length 35 is below minimum size 100
2024-11-28 23:03:01,192 [WARNING] Content length 0 is below minimum size 100
2024-11-28 23:03:01,207 [WARNING] Content length 45 is below minimum size 100
2024-11-28 23:03:01,231 [WARNING] Content length 74 is below minimum size 100
2024-11-28 23:03:01,264 [WARNING] Content too small (80 < 100)
2024-11-28 23:03:01,274 [WARNING] Content too small (97 < 100)
2024-11-28 23:03:01,386 [WARNING] Content length 76 is below minimum size 100
2024-11-28 23:03:01,397 [WARNING] Content too small (104 < 100)
2024-11-28 23:03:01,408 [WARNING] Content too small (15 < 100)
2024-11-28 23:03:01,420 [WARNING] Content length 12 is below minimum size 100
2024-11-28 23:03:01,430 [WARNING] Content length 0 is below minimum size 100
2024-11-28 23:05:06,027 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 23:05:06,294 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:06,295 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:06,296 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:06,297 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:06,297 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:06,298 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:06,298 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:05:06,316 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:06,317 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:06,317 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:05:06,320 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:06,320 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:06,337 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:06,337 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:06,340 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:06,340 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:06,341 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:06,342 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:06,344 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:06,344 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:06,345 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:05:06,347 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:06,392 [WARNING] Content length 64 is below minimum size 100
2024-11-28 23:05:06,403 [WARNING] Content too small (73 < 100)
2024-11-28 23:05:06,403 [WARNING] Content length 42 is below minimum size 100
2024-11-28 23:05:06,600 [WARNING] Content too small (72 < 100)
2024-11-28 23:05:06,601 [WARNING] Content length 41 is below minimum size 100
2024-11-28 23:05:06,636 [WARNING] Content length 79 is below minimum size 100
2024-11-28 23:05:06,639 [WARNING] Content too small (56 < 100)
2024-11-28 23:05:06,639 [WARNING] Content length 23 is below minimum size 100
2024-11-28 23:05:06,640 [WARNING] Content too small (69 < 100)
2024-11-28 23:05:06,641 [WARNING] Content length 54 is below minimum size 100
2024-11-28 23:05:06,642 [WARNING] Content too small (57 < 100)
2024-11-28 23:05:06,642 [WARNING] Content length 57 is below minimum size 100
2024-11-28 23:05:06,653 [WARNING] Content too small (87 < 100)
2024-11-28 23:05:06,653 [WARNING] Content length 87 is below minimum size 100
2024-11-28 23:05:06,665 [WARNING] Content too small (89 < 100)
2024-11-28 23:05:06,665 [WARNING] Content length 59 is below minimum size 100
2024-11-28 23:05:06,666 [WARNING] Content too small (43 < 100)
2024-11-28 23:05:06,666 [WARNING] Content length 28 is below minimum size 100
2024-11-28 23:05:06,677 [WARNING] Content too small (61 < 100)
2024-11-28 23:05:06,677 [WARNING] Content length 18 is below minimum size 100
2024-11-28 23:05:06,754 [WARNING] Content length 73 is below minimum size 100
2024-11-28 23:05:06,772 [WARNING] Content too small (21 < 100)
2024-11-28 23:05:06,772 [WARNING] Content length 9 is below minimum size 100
2024-11-28 23:05:06,773 [WARNING] Content length 66 is below minimum size 100
2024-11-28 23:05:06,784 [WARNING] Content too small (63 < 100)
2024-11-28 23:05:06,784 [WARNING] Content length 35 is below minimum size 100
2024-11-28 23:05:06,794 [WARNING] Content too small (54 < 100)
2024-11-28 23:05:06,794 [WARNING] Content length 0 is below minimum size 100
2024-11-28 23:05:06,807 [WARNING] Content length 45 is below minimum size 100
2024-11-28 23:05:06,830 [WARNING] Content too small (74 < 100)
2024-11-28 23:05:06,831 [WARNING] Content length 74 is below minimum size 100
2024-11-28 23:05:06,862 [WARNING] Content too small (0 < 100)
2024-11-28 23:05:06,863 [WARNING] Content length 0 is below minimum size 100
2024-11-28 23:05:06,872 [WARNING] Content length 46 is below minimum size 100
2024-11-28 23:05:06,939 [WARNING] Content length 76 is below minimum size 100
2024-11-28 23:05:06,949 [WARNING] Content too small (44 < 100)
2024-11-28 23:05:06,949 [WARNING] Content length 29 is below minimum size 100
2024-11-28 23:05:06,958 [WARNING] Content too small (7 < 100)
2024-11-28 23:05:06,958 [WARNING] Content length 7 is below minimum size 100
2024-11-28 23:05:06,968 [WARNING] Content too small (72 < 100)
2024-11-28 23:05:06,969 [WARNING] Content length 12 is below minimum size 100
2024-11-28 23:05:06,978 [WARNING] Content too small (0 < 100)
2024-11-28 23:05:06,978 [WARNING] Content length 0 is below minimum size 100
2024-11-28 23:05:07,025 [WARNING] Content too small (85 < 100)
2024-11-28 23:05:07,026 [WARNING] Content length 63 is below minimum size 100
2024-11-28 23:05:07,032 [WARNING] Content too small (92 < 100)
2024-11-28 23:05:07,033 [WARNING] Content length 67 is below minimum size 100
2024-11-28 23:05:07,039 [WARNING] Content length 45 is below minimum size 100
2024-11-28 23:05:07,044 [WARNING] Content too small (26 < 100)
2024-11-28 23:05:07,044 [WARNING] Content length 26 is below minimum size 100
2024-11-28 23:05:07,049 [WARNING] Content length 39 is below minimum size 100
2024-11-28 23:05:07,054 [WARNING] Content too small (70 < 100)
2024-11-28 23:05:07,054 [WARNING] Content length 0 is below minimum size 100
2024-11-28 23:05:07,059 [WARNING] Content too small (91 < 100)
2024-11-28 23:05:07,059 [WARNING] Content length 38 is below minimum size 100
2024-11-28 23:05:07,064 [WARNING] Content too small (57 < 100)
2024-11-28 23:05:07,065 [WARNING] Content length 28 is below minimum size 100
2024-11-28 23:05:07,069 [WARNING] Content too small (72 < 100)
2024-11-28 23:05:07,070 [WARNING] Content length 58 is below minimum size 100
2024-11-28 23:05:07,147 [INFO] Operation took 0.07 seconds
2024-11-28 23:05:07,152 [WARNING] Content too small (37 < 100)
2024-11-28 23:05:07,152 [WARNING] Content length 18 is below minimum size 100
2024-11-28 23:05:07,158 [WARNING] Content too small (15 < 100)
2024-11-28 23:05:07,158 [WARNING] Content length 7 is below minimum size 100
2024-11-28 23:05:07,406 [INFO] test_operation took 0.10 seconds
2024-11-28 23:05:07,610 [INFO] inner took 0.10 seconds
2024-11-28 23:05:07,610 [INFO] outer took 0.20 seconds
2024-11-28 23:05:07,610 [INFO] error_operation took 0.00 seconds
2024-11-28 23:05:07,662 [INFO] test_operation took 0.00 seconds
2024-11-28 23:05:07,667 [WARNING] Content too small (0 < 100)
2024-11-28 23:05:07,667 [WARNING] Content length 0 is below minimum size 100
2024-11-28 23:05:07,670 [WARNING] Content too small (25 < 100)
2024-11-28 23:05:07,670 [WARNING] Content length 12 is below minimum size 100
2024-11-28 23:05:07,678 [WARNING] Content too small (38 < 100)
2024-11-28 23:05:07,679 [WARNING] Content length 38 is below minimum size 100
2024-11-28 23:05:07,709 [WARNING] Content too small (12 < 100)
2024-11-28 23:05:07,711 [WARNING] Content length 12 is below minimum size 100
2024-11-28 23:05:07,721 [WARNING] Content too small (81 < 100)
2024-11-28 23:05:07,721 [WARNING] Content length 67 is below minimum size 100
2024-11-28 23:05:07,731 [WARNING] Content too small (77 < 100)
2024-11-28 23:05:07,731 [WARNING] Content length 63 is below minimum size 100
2024-11-28 23:05:07,741 [WARNING] Content too small (42 < 100)
2024-11-28 23:05:07,741 [WARNING] Content length 29 is below minimum size 100
2024-11-28 23:05:07,752 [WARNING] Content too small (63 < 100)
2024-11-28 23:05:07,752 [WARNING] Content length 50 is below minimum size 100
2024-11-28 23:05:07,776 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 23:05:07,776 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:07,776 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 23:05:07,776 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 23:05:07,776 [INFO] Starting crawl of URL: https://example.com
2024-11-28 23:05:08,266 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 23:05:08,266 [DEBUG] Base URL: https://example.com
2024-11-28 23:05:08,266 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 23:05:08,266 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 23:05:08,266 [DEBUG] Initial domain set to: example.com
2024-11-28 23:05:08,266 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 23:05:08,266 [INFO] Successfully crawled https://example.com
2024-11-28 23:05:08,267 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:08,267 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x128939310>
2024-11-28 23:05:08,267 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x12888a510>, 282408.749055083)])']
connector: <aiohttp.connector.TCPConnector object at 0x1069fdd90>
2024-11-28 23:05:08,267 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 23:05:08,267 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:08,268 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 23:05:08,268 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 23:05:08,268 [INFO] Starting crawl of URL: https://example.com
2024-11-28 23:05:08,288 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-11-28 23:05:08,288 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-11-28 23:05:08,288 [INFO] Starting crawl of URL: https://example.org
2024-11-28 23:05:08,758 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 23:05:08,759 [DEBUG] Base URL: https://example.com
2024-11-28 23:05:08,759 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 23:05:08,759 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 23:05:08,759 [DEBUG] Initial domain set to: example.com
2024-11-28 23:05:08,759 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 23:05:08,759 [INFO] Successfully crawled https://example.com
2024-11-28 23:05:09,253 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 23:05:09,253 [DEBUG] Base URL: https://example.org
2024-11-28 23:05:09,253 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 23:05:09,253 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 23:05:09,253 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 23:05:09,253 [INFO] Successfully crawled https://example.org
2024-11-28 23:05:09,253 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:09,254 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1287fae90>
2024-11-28 23:05:09,254 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x12888b150>, 282409.24069575)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x12888b5b0>, 282409.736800041)])']
connector: <aiohttp.connector.TCPConnector object at 0x1287faf90>
2024-11-28 23:05:09,254 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 23:05:09,254 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:09,254 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 23:05:09,254 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 23:05:09,254 [INFO] Starting crawl of URL: https://example.com
2024-11-28 23:05:09,741 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 23:05:09,742 [DEBUG] Base URL: https://example.com
2024-11-28 23:05:09,742 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 23:05:09,742 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 23:05:09,742 [DEBUG] Initial domain set to: example.com
2024-11-28 23:05:09,742 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 23:05:09,742 [INFO] Successfully crawled https://example.com
2024-11-28 23:05:09,743 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:09,743 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1287fa410>
2024-11-28 23:05:09,743 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x12888ac80>, 282410.225042)])']
connector: <aiohttp.connector.TCPConnector object at 0x1287fba90>
2024-11-28 23:05:09,743 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 23:05:09,743 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:09,744 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 23:05:09,744 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 23:05:09,744 [INFO] Starting crawl of URL: https://example.com
2024-11-28 23:05:10,228 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 23:05:10,228 [DEBUG] Base URL: https://example.com
2024-11-28 23:05:10,228 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 23:05:10,228 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 23:05:10,228 [DEBUG] Initial domain set to: example.com
2024-11-28 23:05:10,228 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 23:05:10,228 [INFO] Successfully crawled https://example.com
2024-11-28 23:05:10,230 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 23:05:10,231 [DEBUG] Base URL: https://example.com
2024-11-28 23:05:10,231 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 23:05:10,231 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 23:05:10,231 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 23:05:10,234 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:10,235 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:05:10,235 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:10,238 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:10,239 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 23:05:10,239 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:10,239 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 23:05:10,239 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 23:05:10,239 [INFO] Starting crawl of URL: https://example.com
2024-11-28 23:05:10,724 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 23:05:10,724 [DEBUG] Base URL: https://example.com
2024-11-28 23:05:10,724 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 23:05:10,724 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 23:05:10,724 [DEBUG] Initial domain set to: example.com
2024-11-28 23:05:10,724 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 23:05:10,724 [INFO] Successfully crawled https://example.com
2024-11-28 23:05:10,724 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:10,725 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1287f2d50>
2024-11-28 23:05:10,725 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x128834600>, 282411.207769708)])']
connector: <aiohttp.connector.TCPConnector object at 0x128826490>
2024-11-28 23:05:10,725 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 23:05:10,725 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:10,725 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2024-11-28 23:05:10,725 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2024-11-28 23:05:10,726 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2024-11-28 23:05:12,748 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-28 23:05:12,750 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-28 23:05:12,750 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-28 23:05:12,753 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x12878ca10>
2024-11-28 23:05:12,753 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x12888bc40>, 282410.711837666)])']
connector: <aiohttp.connector.TCPConnector object at 0x1287f8650>
2024-11-28 23:05:12,755 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:12,755 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 23:05:12,756 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:12,756 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 23:05:12,756 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 23:05:12,756 [INFO] Starting crawl of URL: https://example.com
2024-11-28 23:05:12,775 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-11-28 23:05:12,775 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-11-28 23:05:12,775 [INFO] Starting crawl of URL: https://example.org
2024-11-28 23:05:12,775 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2024-11-28 23:05:12,775 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2024-11-28 23:05:12,775 [INFO] Starting crawl of URL: https://example.net
2024-11-28 23:05:13,248 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 23:05:13,248 [DEBUG] Base URL: https://example.com
2024-11-28 23:05:13,249 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 23:05:13,249 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 23:05:13,249 [DEBUG] Initial domain set to: example.com
2024-11-28 23:05:13,249 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 23:05:13,250 [INFO] Successfully crawled https://example.com
2024-11-28 23:05:13,741 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 23:05:13,741 [DEBUG] Base URL: https://example.org
2024-11-28 23:05:13,741 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 23:05:13,742 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 23:05:13,742 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 23:05:13,742 [INFO] Successfully crawled https://example.org
2024-11-28 23:05:14,251 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 23:05:14,251 [DEBUG] Base URL: https://example.net
2024-11-28 23:05:14,251 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 23:05:14,251 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 23:05:14,251 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 23:05:14,251 [INFO] Successfully crawled https://example.net
2024-11-28 23:05:14,251 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:14,252 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x128891fd0>
2024-11-28 23:05:14,252 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x128834ec0>, 282413.729623125)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x128834a60>, 282414.22557525)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x128834d70>, 282414.734886208)])']
connector: <aiohttp.connector.TCPConnector object at 0x128891890>
2024-11-28 23:05:14,252 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 23:05:14,253 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:14,253 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 23:05:14,253 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 23:05:14,253 [INFO] Starting crawl of URL: https://example.com
2024-11-28 23:05:14,741 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 23:05:14,741 [DEBUG] Base URL: https://example.com
2024-11-28 23:05:14,741 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 23:05:14,741 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 23:05:14,741 [DEBUG] Initial domain set to: example.com
2024-11-28 23:05:14,741 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 23:05:14,741 [INFO] Successfully crawled https://example.com
2024-11-28 23:05:14,742 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:14,745 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 23:05:14,745 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:14,745 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:05:14,767 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 23:05:14,786 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:14,787 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 23:05:14,787 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:14,787 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 23:05:14,806 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:14,806 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 23:05:14,806 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:14,807 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 23:05:14,807 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2024-11-28 23:05:14,807 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2024-11-28 23:05:14,807 [INFO] Starting crawl of URL: https://example.com/page
2024-11-28 23:05:15,304 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 23:05:15,305 [DEBUG] Base URL: https://example.com/page
2024-11-28 23:05:15,305 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 23:05:15,305 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 23:05:15,305 [DEBUG] Initial domain set to: example.com
2024-11-28 23:05:15,305 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 23:05:15,305 [INFO] Successfully crawled https://example.com/page
2024-11-28 23:05:15,305 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2024-11-28 23:05:15,305 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2024-11-28 23:05:15,305 [INFO] Starting crawl of URL: https://other-domain.com/page
2024-11-28 23:05:18,462 [DEBUG] 
Evaluating link: ./
2024-11-28 23:05:18,462 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,462 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-28 23:05:18,462 [DEBUG] Absolute link: https://other-domain.com/
2024-11-28 23:05:18,462 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,462 [DEBUG] 
Evaluating link: ./
2024-11-28 23:05:18,462 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,462 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-28 23:05:18,462 [DEBUG] Absolute link: https://other-domain.com/
2024-11-28 23:05:18,462 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,462 [DEBUG] 
Evaluating link: ./hiyou.php
2024-11-28 23:05:18,462 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,463 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-11-28 23:05:18,463 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-11-28 23:05:18,463 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,463 [DEBUG] 
Evaluating link: ./date.php
2024-11-28 23:05:18,463 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,463 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-11-28 23:05:18,463 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-11-28 23:05:18,463 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,463 [DEBUG] 
Evaluating link: ./contact.php
2024-11-28 23:05:18,463 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,463 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2024-11-28 23:05:18,463 [DEBUG] Absolute link: https://other-domain.com/contact.php
2024-11-28 23:05:18,463 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,463 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 23:05:18,463 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,463 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 23:05:18,463 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 23:05:18,463 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-28 23:05:18,463 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 23:05:18,463 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,463 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 23:05:18,463 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 23:05:18,463 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 23:05:18,463 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 23:05:18,463 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,463 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 23:05:18,464 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 23:05:18,464 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 23:05:18,464 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 23:05:18,464 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,464 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 23:05:18,464 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 23:05:18,464 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 23:05:18,464 [DEBUG] 
Evaluating link: ./contact
2024-11-28 23:05:18,464 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,464 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-11-28 23:05:18,464 [DEBUG] Absolute link: https://other-domain.com/contact
2024-11-28 23:05:18,464 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,464 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 23:05:18,464 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,464 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 23:05:18,464 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 23:05:18,464 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 23:05:18,464 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 23:05:18,464 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,464 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 23:05:18,464 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 23:05:18,464 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 23:05:18,464 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 23:05:18,464 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,464 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 23:05:18,464 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 23:05:18,464 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 23:05:18,464 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-11-28 23:05:18,464 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,464 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 23:05:18,465 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 23:05:18,465 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-11-28 23:05:18,465 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 23:05:18,465 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,465 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 23:05:18,465 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 23:05:18,465 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-28 23:05:18,465 [DEBUG] 
Evaluating link: #indexes
2024-11-28 23:05:18,465 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,465 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2024-11-28 23:05:18,465 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,465 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,465 [DEBUG] 
Evaluating link: #a
2024-11-28 23:05:18,465 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,465 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2024-11-28 23:05:18,465 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,465 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,465 [DEBUG] 
Evaluating link: #b
2024-11-28 23:05:18,465 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,465 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-11-28 23:05:18,465 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,465 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,465 [DEBUG] 
Evaluating link: #c
2024-11-28 23:05:18,465 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,465 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-11-28 23:05:18,465 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,465 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,465 [DEBUG] 
Evaluating link: #d
2024-11-28 23:05:18,465 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,465 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-11-28 23:05:18,465 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,466 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,466 [DEBUG] 
Evaluating link: #e
2024-11-28 23:05:18,466 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,466 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-11-28 23:05:18,466 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,466 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,466 [DEBUG] 
Evaluating link: #f
2024-11-28 23:05:18,466 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,466 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-11-28 23:05:18,466 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,466 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,466 [DEBUG] 
Evaluating link: #g
2024-11-28 23:05:18,466 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,466 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-11-28 23:05:18,466 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,466 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,466 [DEBUG] 
Evaluating link: #h
2024-11-28 23:05:18,466 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,466 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-11-28 23:05:18,466 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,466 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,466 [DEBUG] 
Evaluating link: #i
2024-11-28 23:05:18,466 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,466 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-11-28 23:05:18,466 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,466 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,466 [DEBUG] 
Evaluating link: #j
2024-11-28 23:05:18,466 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,466 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-11-28 23:05:18,466 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,466 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,466 [DEBUG] 
Evaluating link: #k
2024-11-28 23:05:18,466 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,466 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-11-28 23:05:18,466 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,466 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,467 [DEBUG] 
Evaluating link: #l
2024-11-28 23:05:18,467 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,467 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-11-28 23:05:18,467 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,467 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,467 [DEBUG] 
Evaluating link: #m
2024-11-28 23:05:18,467 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,467 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-11-28 23:05:18,467 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,467 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,467 [DEBUG] 
Evaluating link: #n
2024-11-28 23:05:18,467 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,467 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-11-28 23:05:18,467 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,467 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,467 [DEBUG] 
Evaluating link: #o
2024-11-28 23:05:18,467 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,467 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-11-28 23:05:18,467 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,467 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,467 [DEBUG] 
Evaluating link: #p
2024-11-28 23:05:18,467 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,467 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-11-28 23:05:18,467 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,467 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,467 [DEBUG] 
Evaluating link: #q
2024-11-28 23:05:18,467 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,467 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-11-28 23:05:18,467 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,468 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,468 [DEBUG] 
Evaluating link: #r
2024-11-28 23:05:18,468 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,468 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-11-28 23:05:18,468 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,468 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,468 [DEBUG] 
Evaluating link: #s
2024-11-28 23:05:18,468 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,468 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-11-28 23:05:18,468 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,468 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,468 [DEBUG] 
Evaluating link: #t
2024-11-28 23:05:18,468 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,468 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-11-28 23:05:18,468 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,468 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,468 [DEBUG] 
Evaluating link: #u
2024-11-28 23:05:18,468 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,468 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-11-28 23:05:18,468 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,468 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,468 [DEBUG] 
Evaluating link: #v
2024-11-28 23:05:18,468 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,468 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-11-28 23:05:18,469 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,469 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,469 [DEBUG] 
Evaluating link: #w
2024-11-28 23:05:18,469 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,469 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-11-28 23:05:18,469 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,469 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,469 [DEBUG] 
Evaluating link: #x
2024-11-28 23:05:18,469 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,469 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-11-28 23:05:18,469 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,469 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,469 [DEBUG] 
Evaluating link: #y
2024-11-28 23:05:18,469 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,469 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-11-28 23:05:18,469 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,469 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,469 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 23:05:18,469 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,469 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 23:05:18,469 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 23:05:18,469 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 23:05:18,469 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 23:05:18,469 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,469 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 23:05:18,469 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 23:05:18,469 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 23:05:18,469 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 23:05:18,469 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,469 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 23:05:18,469 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 23:05:18,469 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 23:05:18,469 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-11-28 23:05:18,470 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,470 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 23:05:18,470 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 23:05:18,470 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-11-28 23:05:18,470 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 23:05:18,470 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,470 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 23:05:18,470 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-28 23:05:18,470 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-28 23:05:18,470 [DEBUG] 
Evaluating link: https://spriq.jp/
2024-11-28 23:05:18,470 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,470 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2024-11-28 23:05:18,470 [DEBUG] Absolute link: https://spriq.jp/
2024-11-28 23:05:18,470 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2024-11-28 23:05:18,470 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 23:05:18,470 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,470 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 23:05:18,470 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 23:05:18,470 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 23:05:18,470 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 23:05:18,470 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,470 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 23:05:18,470 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-28 23:05:18,470 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-28 23:05:18,470 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 23:05:18,470 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,470 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 23:05:18,470 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-28 23:05:18,470 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-28 23:05:18,470 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-28 23:05:18,470 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,470 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 23:05:18,471 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-28 23:05:18,471 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-28 23:05:18,471 [DEBUG] 
Evaluating link: ./contact
2024-11-28 23:05:18,471 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,471 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-11-28 23:05:18,471 [DEBUG] Absolute link: https://other-domain.com/contact
2024-11-28 23:05:18,471 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,471 [DEBUG] 
Evaluating link: ./
2024-11-28 23:05:18,471 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,471 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-28 23:05:18,471 [DEBUG] Absolute link: https://other-domain.com/
2024-11-28 23:05:18,471 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,471 [DEBUG] 
Evaluating link: ./hiyou.php
2024-11-28 23:05:18,471 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,471 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-11-28 23:05:18,471 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-11-28 23:05:18,471 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,471 [DEBUG] 
Evaluating link: ./date.php
2024-11-28 23:05:18,471 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,471 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-11-28 23:05:18,471 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-11-28 23:05:18,471 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,471 [DEBUG] 
Evaluating link: #u
2024-11-28 23:05:18,471 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,471 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-11-28 23:05:18,471 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,471 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,471 [DEBUG] 
Evaluating link: #v
2024-11-28 23:05:18,471 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,471 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-11-28 23:05:18,471 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,471 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,471 [DEBUG] 
Evaluating link: #w
2024-11-28 23:05:18,471 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,472 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-11-28 23:05:18,472 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,472 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,472 [DEBUG] 
Evaluating link: #x
2024-11-28 23:05:18,472 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,472 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-11-28 23:05:18,472 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,472 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,472 [DEBUG] 
Evaluating link: #y
2024-11-28 23:05:18,472 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,472 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-11-28 23:05:18,472 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,472 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,472 [DEBUG] 
Evaluating link: #p
2024-11-28 23:05:18,472 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,472 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-11-28 23:05:18,472 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,472 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,472 [DEBUG] 
Evaluating link: #q
2024-11-28 23:05:18,472 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,472 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-11-28 23:05:18,472 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,472 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,472 [DEBUG] 
Evaluating link: #r
2024-11-28 23:05:18,472 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,472 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-11-28 23:05:18,472 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,472 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,472 [DEBUG] 
Evaluating link: #s
2024-11-28 23:05:18,472 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,472 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-11-28 23:05:18,472 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,472 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,473 [DEBUG] 
Evaluating link: #t
2024-11-28 23:05:18,473 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,473 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-11-28 23:05:18,473 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,473 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,473 [DEBUG] 
Evaluating link: #k
2024-11-28 23:05:18,473 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,473 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-11-28 23:05:18,473 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,473 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,473 [DEBUG] 
Evaluating link: #l
2024-11-28 23:05:18,473 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,473 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-11-28 23:05:18,473 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,473 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,473 [DEBUG] 
Evaluating link: #m
2024-11-28 23:05:18,473 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,473 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-11-28 23:05:18,473 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,473 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,473 [DEBUG] 
Evaluating link: #n
2024-11-28 23:05:18,473 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,473 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-11-28 23:05:18,473 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,473 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,473 [DEBUG] 
Evaluating link: #o
2024-11-28 23:05:18,473 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,473 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-11-28 23:05:18,473 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,473 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,473 [DEBUG] 
Evaluating link: #f
2024-11-28 23:05:18,473 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,473 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-11-28 23:05:18,473 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,473 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,473 [DEBUG] 
Evaluating link: #g
2024-11-28 23:05:18,473 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,474 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-11-28 23:05:18,474 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,474 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,474 [DEBUG] 
Evaluating link: #h
2024-11-28 23:05:18,474 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,474 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-11-28 23:05:18,474 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,474 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,474 [DEBUG] 
Evaluating link: #i
2024-11-28 23:05:18,474 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,474 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-11-28 23:05:18,474 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,474 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,474 [DEBUG] 
Evaluating link: #j
2024-11-28 23:05:18,474 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,474 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-11-28 23:05:18,474 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,474 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,474 [DEBUG] 
Evaluating link: #b
2024-11-28 23:05:18,474 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,474 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-11-28 23:05:18,474 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,474 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,474 [DEBUG] 
Evaluating link: #c
2024-11-28 23:05:18,474 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,474 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-11-28 23:05:18,474 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,474 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,474 [DEBUG] 
Evaluating link: #d
2024-11-28 23:05:18,474 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,474 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-11-28 23:05:18,474 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,474 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,474 [DEBUG] 
Evaluating link: #e
2024-11-28 23:05:18,474 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,474 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-11-28 23:05:18,474 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,474 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,474 [DEBUG] 
Evaluating link: #site
2024-11-28 23:05:18,474 [DEBUG] Base URL: https://other-domain.com/page
2024-11-28 23:05:18,474 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2024-11-28 23:05:18,474 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-28 23:05:18,475 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-28 23:05:18,475 [INFO] Successfully crawled https://other-domain.com/page
2024-11-28 23:05:18,480 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:18,481 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 23:05:18,482 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:18,482 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-11-28 23:05:18,482 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-11-28 23:05:18,482 [INFO] Starting crawl of URL: https://example.com/page0
2024-11-28 23:05:18,974 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 23:05:18,974 [DEBUG] Base URL: https://example.com/page0
2024-11-28 23:05:18,974 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 23:05:18,974 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 23:05:18,974 [DEBUG] Initial domain set to: example.com
2024-11-28 23:05:18,974 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 23:05:18,974 [INFO] Successfully crawled https://example.com/page0
2024-11-28 23:05:18,974 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 23:05:18,974 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 23:05:18,974 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 23:05:19,122 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 23:05:19,122 [DEBUG] Base URL: https://example.com/page1
2024-11-28 23:05:19,122 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 23:05:19,122 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 23:05:19,122 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 23:05:19,122 [INFO] Successfully crawled https://example.com/page1
2024-11-28 23:05:19,122 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 23:05:19,122 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 23:05:19,122 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 23:05:19,621 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 23:05:19,622 [DEBUG] Base URL: https://example.com/page2
2024-11-28 23:05:19,622 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 23:05:19,622 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 23:05:19,622 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 23:05:19,622 [INFO] Successfully crawled https://example.com/page2
2024-11-28 23:05:19,622 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-28 23:05:19,622 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-28 23:05:19,622 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-28 23:05:20,126 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 23:05:20,126 [DEBUG] Base URL: https://example.com/page3
2024-11-28 23:05:20,126 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 23:05:20,126 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 23:05:20,126 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 23:05:20,126 [INFO] Successfully crawled https://example.com/page3
2024-11-28 23:05:20,126 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-11-28 23:05:20,127 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-11-28 23:05:20,127 [INFO] Starting crawl of URL: https://example.com/page4
2024-11-28 23:05:20,640 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 23:05:20,640 [DEBUG] Base URL: https://example.com/page4
2024-11-28 23:05:20,640 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 23:05:20,640 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 23:05:20,640 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 23:05:20,640 [INFO] Successfully crawled https://example.com/page4
2024-11-28 23:05:20,640 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 23:05:20,640 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-11-28 23:05:20,640 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-11-28 23:05:20,640 [INFO] Starting crawl of URL: https://example.com/page0
2024-11-28 23:05:21,148 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 23:05:21,148 [DEBUG] Base URL: https://example.com/page0
2024-11-28 23:05:21,149 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 23:05:21,149 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 23:05:21,149 [DEBUG] Initial domain set to: example.com
2024-11-28 23:05:21,149 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 23:05:21,149 [INFO] Successfully crawled https://example.com/page0
2024-11-28 23:05:21,149 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 23:05:21,149 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 23:05:21,149 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 23:05:21,281 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 23:05:21,282 [DEBUG] Base URL: https://example.com/page1
2024-11-28 23:05:21,282 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 23:05:21,282 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 23:05:21,282 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 23:05:21,282 [INFO] Successfully crawled https://example.com/page1
2024-11-28 23:05:21,282 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 23:05:21,282 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 23:05:21,282 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 23:05:21,786 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 23:05:21,786 [DEBUG] Base URL: https://example.com/page2
2024-11-28 23:05:21,786 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 23:05:21,786 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 23:05:21,786 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 23:05:21,786 [INFO] Successfully crawled https://example.com/page2
2024-11-28 23:05:21,786 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-28 23:05:21,786 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-28 23:05:21,787 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-28 23:05:22,288 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 23:05:22,288 [DEBUG] Base URL: https://example.com/page3
2024-11-28 23:05:22,288 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 23:05:22,289 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 23:05:22,289 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 23:05:22,289 [INFO] Successfully crawled https://example.com/page3
2024-11-28 23:05:22,289 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-11-28 23:05:22,289 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-11-28 23:05:22,289 [INFO] Starting crawl of URL: https://example.com/page4
2024-11-28 23:05:22,787 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 23:05:22,788 [DEBUG] Base URL: https://example.com/page4
2024-11-28 23:05:22,788 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 23:05:22,788 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 23:05:22,788 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 23:05:22,788 [INFO] Successfully crawled https://example.com/page4
2024-11-28 23:05:22,790 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:22,791 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:22,792 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 23:05:22,796 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:22,796 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:22,797 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 23:05:22,797 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 23:05:22,797 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 23:05:22,797 [INFO] Starting crawl of URL: https://example.com
2024-11-28 23:05:24,798 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:05:32,798 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:05:32,799 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:05:32,801 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1288b90d0>
2024-11-28 23:05:32,801 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x128835240>, 282421.123716375)])']
connector: <aiohttp.connector.TCPConnector object at 0x1288bb910>
2024-11-28 23:05:32,801 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1288b9c50>
2024-11-28 23:05:32,801 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x128836900>, 282423.2716135)])']
connector: <aiohttp.connector.TCPConnector object at 0x1288bb1d0>
2024-11-28 23:05:32,804 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:32,806 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:05:32,807 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 23:05:32,807 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 23:05:32,807 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 23:05:32,807 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 23:05:32,807 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 23:05:32,808 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 23:05:32,808 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 23:05:34,809 [WARNING] Retry 1/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:05:44,810 [WARNING] Retry 1/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:05:52,810 [WARNING] Retry 2/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:05:52,810 [ERROR] Error crawling https://example.com/page1: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:06:02,810 [WARNING] Retry 2/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:06:02,811 [ERROR] Error crawling https://example.com/page2: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:06:02,813 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:06:02,814 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:06:02,815 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 23:06:02,815 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-28 23:06:02,815 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-28 23:06:02,815 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-28 23:06:02,815 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-28 23:06:02,815 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-28 23:06:02,815 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-28 23:06:02,815 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-28 23:06:02,815 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-28 23:06:02,815 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-28 23:06:04,816 [WARNING] Retry 1/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:06:14,817 [WARNING] Retry 1/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:06:24,818 [WARNING] Retry 1/2 for https://example.com/page3: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:06:32,818 [WARNING] Retry 2/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:06:32,818 [ERROR] Error crawling https://example.com/page1: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:06:42,818 [WARNING] Retry 2/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:06:42,819 [ERROR] Error crawling https://example.com/page2: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:06:52,818 [WARNING] Retry 2/2 for https://example.com/page3: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:06:52,819 [ERROR] Error crawling https://example.com/page3: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:06:52,819 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:06:52,820 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:06:52,821 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 23:06:52,824 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:06:52,824 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:06:52,825 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 23:06:52,825 [DEBUG] Using raw URL string: https://nonexistent.example.com (type: <class 'str'>)
2024-11-28 23:06:52,825 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='nonexistent.example.com', path='', params='', query='', fragment='')
2024-11-28 23:06:52,825 [INFO] Starting crawl of URL: https://nonexistent.example.com
2024-11-28 23:06:54,826 [WARNING] Retry 1/2 for https://nonexistent.example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:07:02,827 [WARNING] Retry 2/2 for https://nonexistent.example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:07:02,827 [ERROR] Error crawling https://nonexistent.example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:07:02,830 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:07:02,831 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:07:02,832 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 23:07:02,835 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:07:02,836 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:07:02,836 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 23:07:02,837 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 23:07:02,837 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 23:07:02,837 [INFO] Starting crawl of URL: https://example.com
2024-11-28 23:07:04,837 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:07:12,838 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:07:12,839 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:07:12,842 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:07:12,843 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:07:12,844 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-28 23:07:12,844 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 23:07:12,844 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 23:07:12,844 [INFO] Starting crawl of URL: https://example.com
2024-11-28 23:07:14,845 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:07:22,845 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:07:22,847 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-28 23:07:22,850 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:07:22,867 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:07:22,870 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:07:22,873 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:07:22,876 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:07:22,879 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:07:22,882 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:07:22,884 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:07:22,887 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:07:22,890 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:07:22,893 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:07:22,895 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:07:22,908 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:07:22,912 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:07:22,913 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:07:22,913 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:07:22,916 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:07:22,917 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:07:22,917 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:07:22,921 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:07:22,923 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:07:22,931 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:07:22,948 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x128727f50>
2024-11-28 23:07:22,948 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x128727b50>
2024-11-28 23:07:22,948 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x128835010>, 282415.7879325)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x128835320>, 282416.733175041)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x128835550>, 282417.671413833)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x128835710>, 282418.935532625)])']
connector: <aiohttp.connector.TCPConnector object at 0x128725f90>
2024-11-28 23:07:22,954 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:07:22,960 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:07:22,965 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:07:22,971 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:07:22,977 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:07:22,982 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:07:22,987 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:07:22,993 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-28 23:07:23,048 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:07:23,179 [DEBUG] Using selector: KqueueSelector
2024-11-28 23:07:23,181 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-28 23:07:23,181 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-28 23:07:23,181 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-28 23:07:23,181 [INFO] Starting crawl of URL: https://example.com
2024-11-28 23:07:23,662 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-28 23:07:23,662 [DEBUG] Base URL: https://example.com
2024-11-28 23:07:23,662 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-28 23:07:23,662 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-28 23:07:23,662 [DEBUG] Initial domain set to: example.com
2024-11-28 23:07:23,662 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-28 23:07:23,662 [INFO] Successfully crawled https://example.com
2024-11-28 23:07:23,663 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x12c55aa10>
2024-11-28 23:07:23,663 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x12c543b60>, 282544.144903458)])']
connector: <aiohttp.connector.TCPConnector object at 0x12c559590>
2024-11-28 23:07:23,665 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2024-11-28 23:07:24,851 [INFO] Operation took 0.10 seconds
2024-11-28 23:13:19,507 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-28 23:13:19,652 [WARNING] Content length 64 is below minimum size 100
2024-11-28 23:13:19,662 [WARNING] Content too small (73 < 100)
2024-11-28 23:13:19,662 [WARNING] Content length 42 is below minimum size 100
2024-11-28 23:13:19,914 [WARNING] Content too small (72 < 100)
2024-11-28 23:13:19,915 [WARNING] Content length 41 is below minimum size 100
2024-11-28 23:13:19,937 [WARNING] Content length 79 is below minimum size 100
2024-11-28 23:13:19,940 [WARNING] Content too small (56 < 100)
2024-11-28 23:13:19,941 [WARNING] Content length 23 is below minimum size 100
2024-11-28 23:13:19,943 [WARNING] Content too small (69 < 100)
2024-11-28 23:13:19,943 [WARNING] Content length 54 is below minimum size 100
2024-11-28 23:13:19,944 [WARNING] Content too small (57 < 100)
2024-11-28 23:13:19,945 [WARNING] Content length 57 is below minimum size 100
2024-11-28 23:13:19,956 [WARNING] Content too small (87 < 100)
2024-11-28 23:13:19,956 [WARNING] Content length 87 is below minimum size 100
2024-11-28 23:13:19,987 [WARNING] Content too small (89 < 100)
2024-11-28 23:13:19,987 [WARNING] Content length 59 is below minimum size 100
2024-11-28 23:13:19,989 [WARNING] Content too small (43 < 100)
2024-11-28 23:13:19,989 [WARNING] Content length 28 is below minimum size 100
2024-11-28 23:13:20,000 [WARNING] Content too small (61 < 100)
2024-11-28 23:13:20,001 [WARNING] Content length 18 is below minimum size 100
2024-11-28 23:13:20,078 [WARNING] Content length 73 is below minimum size 100
2024-11-28 23:13:20,098 [WARNING] Content too small (21 < 100)
2024-11-28 23:13:20,098 [WARNING] Content length 9 is below minimum size 100
2024-11-28 23:13:20,100 [WARNING] Content length 66 is below minimum size 100
2024-11-28 23:13:20,111 [WARNING] Content too small (63 < 100)
2024-11-28 23:13:20,111 [WARNING] Content length 35 is below minimum size 100
2024-11-28 23:13:20,121 [WARNING] Content too small (54 < 100)
2024-11-28 23:13:20,122 [WARNING] Content length 0 is below minimum size 100
2024-11-28 23:13:20,135 [WARNING] Content length 45 is below minimum size 100
2024-11-28 23:13:20,145 [WARNING] Content too small (74 < 100)
2024-11-28 23:13:20,146 [WARNING] Content length 74 is below minimum size 100
2024-11-28 23:13:20,234 [WARNING] Content too small (0 < 100)
2024-11-28 23:13:20,235 [WARNING] Content length 0 is below minimum size 100
2024-11-28 23:13:20,245 [WARNING] Content length 46 is below minimum size 100
2024-11-28 23:13:20,321 [WARNING] Content length 76 is below minimum size 100
2024-11-28 23:13:20,333 [WARNING] Content too small (44 < 100)
2024-11-28 23:13:20,333 [WARNING] Content length 29 is below minimum size 100
2024-11-28 23:13:20,344 [WARNING] Content too small (7 < 100)
2024-11-28 23:13:20,345 [WARNING] Content length 7 is below minimum size 100
2024-11-28 23:13:20,356 [WARNING] Content too small (72 < 100)
2024-11-28 23:13:20,357 [WARNING] Content length 12 is below minimum size 100
2024-11-28 23:13:20,366 [WARNING] Content too small (0 < 100)
2024-11-28 23:13:20,367 [WARNING] Content length 0 is below minimum size 100
2024-11-29 00:21:52,033 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-29 00:21:52,266 [WARNING] Content too small (73 < 100)
2024-11-29 00:21:52,502 [WARNING] Content too small (72 < 100)
2024-11-29 00:21:52,536 [WARNING] Content too small (56 < 100)
2024-11-29 00:21:52,537 [WARNING] Content too small (69 < 100)
2024-11-29 00:21:52,538 [WARNING] Content too small (57 < 100)
2024-11-29 00:21:52,548 [WARNING] Content too small (87 < 100)
2024-11-29 00:21:52,571 [WARNING] Content too small (89 < 100)
2024-11-29 00:21:52,572 [WARNING] Content too small (43 < 100)
2024-11-29 00:21:52,582 [WARNING] Content too small (61 < 100)
2024-11-29 00:21:52,683 [WARNING] Content too small (21 < 100)
2024-11-29 00:21:52,695 [WARNING] Content too small (63 < 100)
2024-11-29 00:21:52,705 [WARNING] Content too small (54 < 100)
2024-11-29 00:21:52,749 [WARNING] Content too small (74 < 100)
2024-11-29 00:21:52,778 [WARNING] Content too small (0 < 100)
2024-11-29 00:21:52,911 [WARNING] Content too small (44 < 100)
2024-11-29 00:21:52,921 [WARNING] Content too small (7 < 100)
2024-11-29 00:21:52,943 [WARNING] Content too small (72 < 100)
2024-11-29 00:21:52,954 [WARNING] Content too small (0 < 100)
2024-11-29 00:28:12,121 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-29 00:28:12,319 [WARNING] Content too small (73 < 100)
2024-11-29 00:28:12,576 [WARNING] Content too small (72 < 100)
2024-11-29 00:28:12,613 [WARNING] Content too small (56 < 100)
2024-11-29 00:28:12,615 [WARNING] Content too small (69 < 100)
2024-11-29 00:28:12,616 [WARNING] Content too small (57 < 100)
2024-11-29 00:28:12,628 [WARNING] Content too small (87 < 100)
2024-11-29 00:28:12,653 [WARNING] Content too small (89 < 100)
2024-11-29 00:28:12,654 [WARNING] Content too small (43 < 100)
2024-11-29 00:28:12,666 [WARNING] Content too small (61 < 100)
2024-11-29 00:28:12,756 [ERROR] Error processing content from : 'NoneType' object is not callable
2024-11-29 00:28:12,777 [WARNING] Content too small (21 < 100)
2024-11-29 00:28:12,789 [WARNING] Content too small (63 < 100)
2024-11-29 00:28:12,802 [WARNING] Content too small (54 < 100)
2024-11-29 00:28:12,813 [WARNING] Content too small (0 < 100)
2024-11-29 00:28:12,824 [WARNING] Content too small (77 < 100)
2024-11-29 00:28:12,886 [WARNING] Content too small (0 < 100)
2024-11-29 00:28:12,920 [WARNING] Content too small (0 < 100)
2024-11-29 00:28:13,034 [WARNING] Content too small (44 < 100)
2024-11-29 00:28:13,046 [WARNING] Content too small (7 < 100)
2024-11-29 00:28:13,074 [WARNING] Content too small (72 < 100)
2024-11-29 00:28:13,086 [WARNING] Content too small (0 < 100)
2024-11-29 01:19:12,086 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-29 01:19:12,466 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:12,467 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:12,467 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:12,468 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:12,468 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:12,469 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:12,469 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:19:12,487 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:12,488 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:12,488 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:19:12,490 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:12,490 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:12,493 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:12,493 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:12,504 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:12,505 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:12,506 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:12,506 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:12,508 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:12,509 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:12,509 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:19:12,511 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:12,622 [WARNING] Content too small (73 < 100)
2024-11-29 01:19:12,858 [WARNING] Content too small (72 < 100)
2024-11-29 01:19:12,891 [WARNING] Content too small (56 < 100)
2024-11-29 01:19:12,892 [WARNING] Content too small (69 < 100)
2024-11-29 01:19:12,893 [WARNING] Content too small (57 < 100)
2024-11-29 01:19:12,917 [WARNING] Content too small (87 < 100)
2024-11-29 01:19:12,928 [WARNING] Content too small (89 < 100)
2024-11-29 01:19:12,929 [WARNING] Content too small (43 < 100)
2024-11-29 01:19:12,939 [WARNING] Content too small (61 < 100)
2024-11-29 01:19:17,571 [WARNING] Content too small (11 < 100)
2024-11-29 01:19:17,582 [WARNING] Content too small (87 < 100)
2024-11-29 01:19:17,584 [WARNING] Content too small (52 < 100)
2024-11-29 01:19:18,094 [WARNING] Content too small (63 < 100)
2024-11-29 01:19:18,119 [WARNING] Content too small (54 < 100)
2024-11-29 01:19:18,131 [WARNING] Content too small (0 < 100)
2024-11-29 01:19:18,142 [WARNING] Content too small (85 < 100)
2024-11-29 01:19:18,149 [WARNING] Content too small (76 < 100)
2024-11-29 01:19:18,154 [WARNING] Content too small (77 < 100)
2024-11-29 01:19:18,159 [WARNING] Content too small (0 < 100)
2024-11-29 01:19:18,168 [WARNING] Content too small (70 < 100)
2024-11-29 01:19:18,173 [WARNING] Content too small (91 < 100)
2024-11-29 01:19:18,177 [WARNING] Content too small (57 < 100)
2024-11-29 01:19:18,183 [WARNING] Content too small (72 < 100)
2024-11-29 01:19:18,230 [INFO] Operation took 0.04 seconds
2024-11-29 01:19:18,252 [WARNING] Content too small (37 < 100)
2024-11-29 01:19:18,257 [WARNING] Content too small (15 < 100)
2024-11-29 01:19:18,496 [INFO] test_operation took 0.10 seconds
2024-11-29 01:19:18,705 [INFO] inner took 0.10 seconds
2024-11-29 01:19:18,706 [INFO] outer took 0.21 seconds
2024-11-29 01:19:18,706 [INFO] error_operation took 0.00 seconds
2024-11-29 01:19:18,759 [INFO] test_operation took 0.00 seconds
2024-11-29 01:19:18,768 [WARNING] Content too small (25 < 100)
2024-11-29 01:19:18,780 [WARNING] Content too small (38 < 100)
2024-11-29 01:19:18,809 [WARNING] Content too small (12 < 100)
2024-11-29 01:19:18,820 [WARNING] Content too small (81 < 100)
2024-11-29 01:19:18,830 [WARNING] Content too small (77 < 100)
2024-11-29 01:19:18,839 [WARNING] Content too small (42 < 100)
2024-11-29 01:19:18,850 [WARNING] Content too small (63 < 100)
2024-11-29 01:19:18,876 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:19:18,876 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:18,877 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 01:19:18,877 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 01:19:18,877 [INFO] Starting crawl of URL: https://example.com
2024-11-29 01:19:19,393 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:19:19,393 [DEBUG] Base URL: https://example.com
2024-11-29 01:19:19,393 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:19:19,393 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:19:19,393 [DEBUG] Initial domain set to: example.com
2024-11-29 01:19:19,393 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:19:19,394 [INFO] Successfully crawled https://example.com
2024-11-29 01:19:19,395 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:19,395 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1056066d0>
2024-11-29 01:19:19,395 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x106391160>, 290459.934028458)])']
connector: <aiohttp.connector.TCPConnector object at 0x10636c2d0>
2024-11-29 01:19:19,396 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:19:19,396 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:19,396 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 01:19:19,396 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 01:19:19,396 [INFO] Starting crawl of URL: https://example.com
2024-11-29 01:19:19,420 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-11-29 01:19:19,420 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-11-29 01:19:19,420 [INFO] Starting crawl of URL: https://example.org
2024-11-29 01:19:19,909 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:19:19,910 [DEBUG] Base URL: https://example.com
2024-11-29 01:19:19,910 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:19:19,910 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:19:19,910 [DEBUG] Initial domain set to: example.com
2024-11-29 01:19:19,911 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:19:19,911 [INFO] Successfully crawled https://example.com
2024-11-29 01:19:20,420 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:19:20,420 [DEBUG] Base URL: https://example.org
2024-11-29 01:19:20,420 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:19:20,420 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:19:20,421 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:19:20,421 [INFO] Successfully crawled https://example.org
2024-11-29 01:19:20,422 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:20,423 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x117aa2050>
2024-11-29 01:19:20,423 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1063931c0>, 290460.451217958)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1063fc2f0>, 290460.962340625)])']
connector: <aiohttp.connector.TCPConnector object at 0x117aa1f90>
2024-11-29 01:19:20,424 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:19:20,424 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:20,426 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 01:19:20,426 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 01:19:20,426 [INFO] Starting crawl of URL: https://example.com
2024-11-29 01:19:20,935 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:19:20,935 [DEBUG] Base URL: https://example.com
2024-11-29 01:19:20,935 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:19:20,935 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:19:20,935 [DEBUG] Initial domain set to: example.com
2024-11-29 01:19:20,935 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:19:20,935 [INFO] Successfully crawled https://example.com
2024-11-29 01:19:20,936 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:20,936 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x117aa20d0>
2024-11-29 01:19:20,936 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1063fc0c0>, 290461.478011541)])']
connector: <aiohttp.connector.TCPConnector object at 0x117aa23d0>
2024-11-29 01:19:20,937 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:19:20,937 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:20,937 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 01:19:20,937 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 01:19:20,937 [INFO] Starting crawl of URL: https://example.com
2024-11-29 01:19:21,427 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:19:21,427 [DEBUG] Base URL: https://example.com
2024-11-29 01:19:21,427 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:19:21,427 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:19:21,427 [DEBUG] Initial domain set to: example.com
2024-11-29 01:19:21,427 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:19:21,427 [INFO] Successfully crawled https://example.com
2024-11-29 01:19:21,434 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:19:21,435 [DEBUG] Base URL: https://example.com
2024-11-29 01:19:21,435 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:19:21,435 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:19:21,435 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:19:21,439 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:21,440 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:19:21,440 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:21,444 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:21,445 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:19:21,445 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:21,445 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 01:19:21,445 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 01:19:21,445 [INFO] Starting crawl of URL: https://example.com
2024-11-29 01:19:21,963 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:19:21,963 [DEBUG] Base URL: https://example.com
2024-11-29 01:19:21,963 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:19:21,963 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:19:21,963 [DEBUG] Initial domain set to: example.com
2024-11-29 01:19:21,964 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:19:21,964 [INFO] Successfully crawled https://example.com
2024-11-29 01:19:21,965 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:21,966 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x122f352d0>
2024-11-29 01:19:21,966 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1063fc1a0>, 290462.504955166)])']
connector: <aiohttp.connector.TCPConnector object at 0x122f34bd0>
2024-11-29 01:19:21,967 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:19:21,968 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:21,969 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2024-11-29 01:19:21,969 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2024-11-29 01:19:21,969 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2024-11-29 01:19:24,051 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-29 01:19:24,052 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-29 01:19:24,053 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-29 01:19:24,057 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:24,058 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:19:24,059 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:24,059 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 01:19:24,059 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 01:19:24,059 [INFO] Starting crawl of URL: https://example.com
2024-11-29 01:19:24,081 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-11-29 01:19:24,081 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-11-29 01:19:24,081 [INFO] Starting crawl of URL: https://example.org
2024-11-29 01:19:24,081 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2024-11-29 01:19:24,081 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2024-11-29 01:19:24,081 [INFO] Starting crawl of URL: https://example.net
2024-11-29 01:19:24,546 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:19:24,546 [DEBUG] Base URL: https://example.com
2024-11-29 01:19:24,546 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:19:24,546 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:19:24,547 [DEBUG] Initial domain set to: example.com
2024-11-29 01:19:24,547 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:19:24,547 [INFO] Successfully crawled https://example.com
2024-11-29 01:19:25,049 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:19:25,049 [DEBUG] Base URL: https://example.org
2024-11-29 01:19:25,049 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:19:25,049 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:19:25,049 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:19:25,050 [INFO] Successfully crawled https://example.org
2024-11-29 01:19:25,680 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:19:25,680 [DEBUG] Base URL: https://example.net
2024-11-29 01:19:25,681 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:19:25,681 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:19:25,681 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:19:25,681 [INFO] Successfully crawled https://example.net
2024-11-29 01:19:25,683 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:25,684 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x106c0de50>
2024-11-29 01:19:25,685 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1063fd4e0>, 290465.08934175)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1063fd940>, 290465.592662166)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1063fdd30>, 290466.2243465)])']
connector: <aiohttp.connector.TCPConnector object at 0x122f37ad0>
2024-11-29 01:19:25,685 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:19:25,686 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:25,686 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 01:19:25,687 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 01:19:25,687 [INFO] Starting crawl of URL: https://example.com
2024-11-29 01:19:25,710 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1065f1d50>
2024-11-29 01:19:25,710 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1063fc210>, 290461.968643291)])']
connector: <aiohttp.connector.TCPConnector object at 0x1065f06d0>
2024-11-29 01:19:25,710 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x106478910>
2024-11-29 01:19:26,199 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:19:26,200 [DEBUG] Base URL: https://example.com
2024-11-29 01:19:26,200 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:19:26,200 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:19:26,200 [DEBUG] Initial domain set to: example.com
2024-11-29 01:19:26,200 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:19:26,200 [INFO] Successfully crawled https://example.com
2024-11-29 01:19:26,201 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:26,209 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:19:26,209 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:26,210 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:19:26,237 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:19:26,258 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:26,258 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:19:26,259 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:26,259 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:19:26,278 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:26,278 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:19:26,278 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:26,279 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:19:26,279 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2024-11-29 01:19:26,279 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2024-11-29 01:19:26,279 [INFO] Starting crawl of URL: https://example.com/page
2024-11-29 01:19:26,776 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:19:26,776 [DEBUG] Base URL: https://example.com/page
2024-11-29 01:19:26,776 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:19:26,777 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:19:26,777 [DEBUG] Initial domain set to: example.com
2024-11-29 01:19:26,777 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:19:26,777 [INFO] Successfully crawled https://example.com/page
2024-11-29 01:19:26,777 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2024-11-29 01:19:26,777 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2024-11-29 01:19:26,777 [INFO] Starting crawl of URL: https://other-domain.com/page
2024-11-29 01:19:32,749 [DEBUG] 
Evaluating link: ./
2024-11-29 01:19:32,750 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,750 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-29 01:19:32,750 [DEBUG] Absolute link: https://other-domain.com/
2024-11-29 01:19:32,750 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,750 [DEBUG] 
Evaluating link: ./
2024-11-29 01:19:32,750 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,750 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-29 01:19:32,750 [DEBUG] Absolute link: https://other-domain.com/
2024-11-29 01:19:32,750 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,750 [DEBUG] 
Evaluating link: ./hiyou.php
2024-11-29 01:19:32,750 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,750 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-11-29 01:19:32,750 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-11-29 01:19:32,750 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,750 [DEBUG] 
Evaluating link: ./date.php
2024-11-29 01:19:32,750 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,750 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-11-29 01:19:32,750 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-11-29 01:19:32,750 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,750 [DEBUG] 
Evaluating link: ./contact.php
2024-11-29 01:19:32,751 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,751 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2024-11-29 01:19:32,751 [DEBUG] Absolute link: https://other-domain.com/contact.php
2024-11-29 01:19:32,751 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,751 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 01:19:32,751 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,751 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 01:19:32,751 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 01:19:32,751 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-29 01:19:32,751 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:19:32,751 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,751 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:19:32,751 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:19:32,751 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-29 01:19:32,751 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 01:19:32,751 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,751 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 01:19:32,751 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 01:19:32,751 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-29 01:19:32,751 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-29 01:19:32,751 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,751 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 01:19:32,751 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 01:19:32,751 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-29 01:19:32,751 [DEBUG] 
Evaluating link: ./contact
2024-11-29 01:19:32,751 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,751 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-11-29 01:19:32,751 [DEBUG] Absolute link: https://other-domain.com/contact
2024-11-29 01:19:32,751 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,752 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:19:32,752 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,752 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:19:32,752 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:19:32,752 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-29 01:19:32,752 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 01:19:32,752 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,752 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 01:19:32,752 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 01:19:32,752 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-29 01:19:32,752 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-29 01:19:32,752 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,752 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 01:19:32,752 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 01:19:32,752 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-29 01:19:32,752 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-11-29 01:19:32,752 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,752 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:19:32,752 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:19:32,752 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-11-29 01:19:32,752 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 01:19:32,753 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,753 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 01:19:32,753 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 01:19:32,753 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-29 01:19:32,753 [DEBUG] 
Evaluating link: #indexes
2024-11-29 01:19:32,753 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,753 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2024-11-29 01:19:32,753 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,753 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,753 [DEBUG] 
Evaluating link: #a
2024-11-29 01:19:32,753 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,753 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2024-11-29 01:19:32,753 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,753 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,753 [DEBUG] 
Evaluating link: #b
2024-11-29 01:19:32,753 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,753 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-11-29 01:19:32,753 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,753 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,753 [DEBUG] 
Evaluating link: #c
2024-11-29 01:19:32,753 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,754 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-11-29 01:19:32,754 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,754 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,754 [DEBUG] 
Evaluating link: #d
2024-11-29 01:19:32,754 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,754 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-11-29 01:19:32,754 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,754 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,754 [DEBUG] 
Evaluating link: #e
2024-11-29 01:19:32,754 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,754 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-11-29 01:19:32,754 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,754 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,754 [DEBUG] 
Evaluating link: #f
2024-11-29 01:19:32,754 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,754 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-11-29 01:19:32,754 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,754 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,754 [DEBUG] 
Evaluating link: #g
2024-11-29 01:19:32,754 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,754 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-11-29 01:19:32,754 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,754 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,754 [DEBUG] 
Evaluating link: #h
2024-11-29 01:19:32,754 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,754 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-11-29 01:19:32,754 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,754 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,754 [DEBUG] 
Evaluating link: #i
2024-11-29 01:19:32,754 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,754 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-11-29 01:19:32,754 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,754 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,754 [DEBUG] 
Evaluating link: #j
2024-11-29 01:19:32,754 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,754 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-11-29 01:19:32,755 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,755 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,755 [DEBUG] 
Evaluating link: #k
2024-11-29 01:19:32,755 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,755 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-11-29 01:19:32,755 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,755 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,755 [DEBUG] 
Evaluating link: #l
2024-11-29 01:19:32,755 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,755 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-11-29 01:19:32,755 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,755 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,755 [DEBUG] 
Evaluating link: #m
2024-11-29 01:19:32,755 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,755 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-11-29 01:19:32,755 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,755 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,755 [DEBUG] 
Evaluating link: #n
2024-11-29 01:19:32,755 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,755 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-11-29 01:19:32,755 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,755 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,755 [DEBUG] 
Evaluating link: #o
2024-11-29 01:19:32,755 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,755 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-11-29 01:19:32,755 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,755 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,755 [DEBUG] 
Evaluating link: #p
2024-11-29 01:19:32,755 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,755 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-11-29 01:19:32,755 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,755 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,755 [DEBUG] 
Evaluating link: #q
2024-11-29 01:19:32,755 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,755 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-11-29 01:19:32,755 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,755 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,756 [DEBUG] 
Evaluating link: #r
2024-11-29 01:19:32,756 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,756 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-11-29 01:19:32,756 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,756 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,756 [DEBUG] 
Evaluating link: #s
2024-11-29 01:19:32,756 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,756 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-11-29 01:19:32,756 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,756 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,756 [DEBUG] 
Evaluating link: #t
2024-11-29 01:19:32,756 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,756 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-11-29 01:19:32,756 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,756 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,756 [DEBUG] 
Evaluating link: #u
2024-11-29 01:19:32,756 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,756 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-11-29 01:19:32,756 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,756 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,756 [DEBUG] 
Evaluating link: #v
2024-11-29 01:19:32,756 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,756 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-11-29 01:19:32,756 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,756 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,756 [DEBUG] 
Evaluating link: #w
2024-11-29 01:19:32,756 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,756 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-11-29 01:19:32,757 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,757 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,757 [DEBUG] 
Evaluating link: #x
2024-11-29 01:19:32,757 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,757 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-11-29 01:19:32,757 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,757 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,757 [DEBUG] 
Evaluating link: #y
2024-11-29 01:19:32,757 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,757 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-11-29 01:19:32,757 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,757 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,757 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:19:32,757 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,757 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:19:32,757 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:19:32,757 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-29 01:19:32,757 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 01:19:32,757 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,757 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 01:19:32,757 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 01:19:32,757 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-29 01:19:32,757 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-29 01:19:32,757 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,757 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 01:19:32,757 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 01:19:32,757 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-29 01:19:32,757 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-11-29 01:19:32,757 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,757 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:19:32,757 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:19:32,757 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-11-29 01:19:32,757 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 01:19:32,757 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,757 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 01:19:32,757 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 01:19:32,757 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-29 01:19:32,758 [DEBUG] 
Evaluating link: https://spriq.jp/
2024-11-29 01:19:32,758 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,758 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2024-11-29 01:19:32,758 [DEBUG] Absolute link: https://spriq.jp/
2024-11-29 01:19:32,758 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2024-11-29 01:19:32,758 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:19:32,758 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,758 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:19:32,758 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:19:32,758 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-29 01:19:32,758 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:19:32,758 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,758 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:19:32,758 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:19:32,758 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-29 01:19:32,758 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 01:19:32,758 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,758 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 01:19:32,758 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 01:19:32,758 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-29 01:19:32,758 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-29 01:19:32,758 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,758 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 01:19:32,758 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 01:19:32,758 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-29 01:19:32,758 [DEBUG] 
Evaluating link: ./contact
2024-11-29 01:19:32,758 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,758 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-11-29 01:19:32,758 [DEBUG] Absolute link: https://other-domain.com/contact
2024-11-29 01:19:32,758 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,758 [DEBUG] 
Evaluating link: ./
2024-11-29 01:19:32,758 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,758 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-29 01:19:32,758 [DEBUG] Absolute link: https://other-domain.com/
2024-11-29 01:19:32,758 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,759 [DEBUG] 
Evaluating link: ./hiyou.php
2024-11-29 01:19:32,759 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,759 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-11-29 01:19:32,759 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-11-29 01:19:32,759 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,759 [DEBUG] 
Evaluating link: ./date.php
2024-11-29 01:19:32,759 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,759 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-11-29 01:19:32,759 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-11-29 01:19:32,759 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,759 [DEBUG] 
Evaluating link: #u
2024-11-29 01:19:32,759 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,759 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-11-29 01:19:32,759 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,759 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,759 [DEBUG] 
Evaluating link: #v
2024-11-29 01:19:32,759 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,759 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-11-29 01:19:32,759 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,759 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,759 [DEBUG] 
Evaluating link: #w
2024-11-29 01:19:32,759 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,759 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-11-29 01:19:32,759 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,759 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,759 [DEBUG] 
Evaluating link: #x
2024-11-29 01:19:32,759 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,759 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-11-29 01:19:32,759 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,759 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,759 [DEBUG] 
Evaluating link: #y
2024-11-29 01:19:32,759 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,759 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-11-29 01:19:32,759 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,759 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,759 [DEBUG] 
Evaluating link: #p
2024-11-29 01:19:32,759 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,759 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-11-29 01:19:32,760 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,760 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,760 [DEBUG] 
Evaluating link: #q
2024-11-29 01:19:32,760 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,760 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-11-29 01:19:32,760 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,760 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,760 [DEBUG] 
Evaluating link: #r
2024-11-29 01:19:32,760 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,760 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-11-29 01:19:32,760 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,760 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,760 [DEBUG] 
Evaluating link: #s
2024-11-29 01:19:32,760 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,760 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-11-29 01:19:32,760 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,760 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,760 [DEBUG] 
Evaluating link: #t
2024-11-29 01:19:32,760 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,760 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-11-29 01:19:32,760 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,760 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,760 [DEBUG] 
Evaluating link: #k
2024-11-29 01:19:32,760 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,760 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-11-29 01:19:32,760 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,760 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,760 [DEBUG] 
Evaluating link: #l
2024-11-29 01:19:32,760 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,760 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-11-29 01:19:32,760 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,760 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,760 [DEBUG] 
Evaluating link: #m
2024-11-29 01:19:32,760 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,760 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-11-29 01:19:32,760 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,761 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,761 [DEBUG] 
Evaluating link: #n
2024-11-29 01:19:32,761 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,761 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-11-29 01:19:32,761 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,761 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,761 [DEBUG] 
Evaluating link: #o
2024-11-29 01:19:32,761 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,761 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-11-29 01:19:32,761 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,761 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,761 [DEBUG] 
Evaluating link: #f
2024-11-29 01:19:32,761 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,761 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-11-29 01:19:32,761 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,761 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,761 [DEBUG] 
Evaluating link: #g
2024-11-29 01:19:32,761 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,761 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-11-29 01:19:32,761 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,761 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,761 [DEBUG] 
Evaluating link: #h
2024-11-29 01:19:32,761 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,761 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-11-29 01:19:32,761 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,761 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,761 [DEBUG] 
Evaluating link: #i
2024-11-29 01:19:32,761 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,761 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-11-29 01:19:32,761 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,761 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,761 [DEBUG] 
Evaluating link: #j
2024-11-29 01:19:32,761 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,761 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-11-29 01:19:32,761 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,761 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,761 [DEBUG] 
Evaluating link: #b
2024-11-29 01:19:32,761 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,761 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-11-29 01:19:32,761 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,761 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,761 [DEBUG] 
Evaluating link: #c
2024-11-29 01:19:32,761 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,761 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-11-29 01:19:32,761 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,761 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,762 [DEBUG] 
Evaluating link: #d
2024-11-29 01:19:32,762 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,762 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-11-29 01:19:32,762 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,762 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,762 [DEBUG] 
Evaluating link: #e
2024-11-29 01:19:32,762 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,762 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-11-29 01:19:32,762 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,762 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,762 [DEBUG] 
Evaluating link: #site
2024-11-29 01:19:32,762 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:19:32,762 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2024-11-29 01:19:32,762 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:19:32,762 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:19:32,762 [INFO] Successfully crawled https://other-domain.com/page
2024-11-29 01:19:32,766 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:32,768 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:19:32,768 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:32,768 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-11-29 01:19:32,768 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-11-29 01:19:32,768 [INFO] Starting crawl of URL: https://example.com/page0
2024-11-29 01:19:33,264 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:19:33,264 [DEBUG] Base URL: https://example.com/page0
2024-11-29 01:19:33,264 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:19:33,264 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:19:33,264 [DEBUG] Initial domain set to: example.com
2024-11-29 01:19:33,264 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:19:33,265 [INFO] Successfully crawled https://example.com/page0
2024-11-29 01:19:33,265 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-29 01:19:33,265 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-29 01:19:33,265 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-29 01:19:33,412 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:19:33,412 [DEBUG] Base URL: https://example.com/page1
2024-11-29 01:19:33,412 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:19:33,412 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:19:33,412 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:19:33,412 [INFO] Successfully crawled https://example.com/page1
2024-11-29 01:19:33,412 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-29 01:19:33,413 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-29 01:19:33,413 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-29 01:19:33,906 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:19:33,906 [DEBUG] Base URL: https://example.com/page2
2024-11-29 01:19:33,907 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:19:33,907 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:19:33,907 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:19:33,907 [INFO] Successfully crawled https://example.com/page2
2024-11-29 01:19:33,907 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-29 01:19:33,907 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-29 01:19:33,907 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-29 01:19:34,414 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:19:34,415 [DEBUG] Base URL: https://example.com/page3
2024-11-29 01:19:34,415 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:19:34,415 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:19:34,415 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:19:34,415 [INFO] Successfully crawled https://example.com/page3
2024-11-29 01:19:34,415 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-11-29 01:19:34,415 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-11-29 01:19:34,415 [INFO] Starting crawl of URL: https://example.com/page4
2024-11-29 01:19:34,908 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:19:34,908 [DEBUG] Base URL: https://example.com/page4
2024-11-29 01:19:34,908 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:19:34,909 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:19:34,909 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:19:34,909 [INFO] Successfully crawled https://example.com/page4
2024-11-29 01:19:34,909 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:19:34,909 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-11-29 01:19:34,909 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-11-29 01:19:34,909 [INFO] Starting crawl of URL: https://example.com/page0
2024-11-29 01:19:35,421 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:19:35,421 [DEBUG] Base URL: https://example.com/page0
2024-11-29 01:19:35,421 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:19:35,421 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:19:35,421 [DEBUG] Initial domain set to: example.com
2024-11-29 01:19:35,421 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:19:35,422 [INFO] Successfully crawled https://example.com/page0
2024-11-29 01:19:35,422 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-29 01:19:35,422 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-29 01:19:35,422 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-29 01:19:35,552 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:19:35,553 [DEBUG] Base URL: https://example.com/page1
2024-11-29 01:19:35,553 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:19:35,553 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:19:35,553 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:19:35,553 [INFO] Successfully crawled https://example.com/page1
2024-11-29 01:19:35,553 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-29 01:19:35,553 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-29 01:19:35,553 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-29 01:19:36,058 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:19:36,058 [DEBUG] Base URL: https://example.com/page2
2024-11-29 01:19:36,058 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:19:36,058 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:19:36,058 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:19:36,058 [INFO] Successfully crawled https://example.com/page2
2024-11-29 01:19:36,058 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-29 01:19:36,058 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-29 01:19:36,059 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-29 01:19:36,559 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:19:36,559 [DEBUG] Base URL: https://example.com/page3
2024-11-29 01:19:36,559 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:19:36,559 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:19:36,559 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:19:36,560 [INFO] Successfully crawled https://example.com/page3
2024-11-29 01:19:36,560 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-11-29 01:19:36,560 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-11-29 01:19:36,560 [INFO] Starting crawl of URL: https://example.com/page4
2024-11-29 01:19:37,062 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:19:37,063 [DEBUG] Base URL: https://example.com/page4
2024-11-29 01:19:37,063 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:19:37,063 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:19:37,063 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:19:37,064 [INFO] Successfully crawled https://example.com/page4
2024-11-29 01:19:37,070 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:37,072 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:37,073 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 01:19:37,078 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:37,079 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:37,080 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 01:19:37,081 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 01:19:37,081 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 01:19:37,081 [INFO] Starting crawl of URL: https://example.com
2024-11-29 01:19:39,082 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:19:47,081 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:19:47,082 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:19:47,089 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:47,092 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:19:47,092 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 01:19:47,093 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-29 01:19:47,093 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-29 01:19:47,093 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-29 01:19:47,093 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-29 01:19:47,093 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-29 01:19:47,093 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-29 01:19:49,093 [WARNING] Retry 1/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:19:59,094 [WARNING] Retry 1/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:20:07,095 [WARNING] Retry 2/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:20:07,096 [ERROR] Error crawling https://example.com/page1: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:20:17,095 [WARNING] Retry 2/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:20:17,096 [ERROR] Error crawling https://example.com/page2: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:20:17,100 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x105560090>
2024-11-29 01:20:17,101 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1063fdef0>, 290475.453368916)])']
connector: <aiohttp.connector.TCPConnector object at 0x1055606d0>
2024-11-29 01:20:17,101 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x105560f10>
2024-11-29 01:20:17,101 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1063ff4d0>, 290477.607790625)])']
connector: <aiohttp.connector.TCPConnector object at 0x122fff6d0>
2024-11-29 01:20:17,106 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:20:17,106 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:20:17,107 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 01:20:17,107 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-29 01:20:17,107 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-29 01:20:17,107 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-29 01:20:17,107 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-29 01:20:17,108 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-29 01:20:17,108 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-29 01:20:17,108 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-29 01:20:17,108 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-29 01:20:17,108 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-29 01:20:19,109 [WARNING] Retry 1/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:20:29,108 [WARNING] Retry 1/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:20:39,110 [WARNING] Retry 1/2 for https://example.com/page3: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:20:47,109 [WARNING] Retry 2/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:20:47,110 [ERROR] Error crawling https://example.com/page1: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:20:57,110 [WARNING] Retry 2/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:20:57,111 [ERROR] Error crawling https://example.com/page2: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:21:07,111 [WARNING] Retry 2/2 for https://example.com/page3: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:21:07,112 [ERROR] Error crawling https://example.com/page3: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:21:07,112 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:07,113 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:07,113 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 01:21:07,116 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:07,117 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:07,117 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 01:21:07,118 [DEBUG] Using raw URL string: https://nonexistent.example.com (type: <class 'str'>)
2024-11-29 01:21:07,118 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='nonexistent.example.com', path='', params='', query='', fragment='')
2024-11-29 01:21:07,118 [INFO] Starting crawl of URL: https://nonexistent.example.com
2024-11-29 01:21:09,119 [WARNING] Retry 1/2 for https://nonexistent.example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:21:17,119 [WARNING] Retry 2/2 for https://nonexistent.example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:21:17,120 [ERROR] Error crawling https://nonexistent.example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:21:17,129 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:17,130 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:17,131 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 01:21:17,135 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:17,136 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:17,137 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 01:21:17,137 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 01:21:17,137 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 01:21:17,137 [INFO] Starting crawl of URL: https://example.com
2024-11-29 01:21:19,139 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:21:27,138 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:21:27,138 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:21:27,146 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:27,147 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:27,148 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 01:21:27,148 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 01:21:27,148 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 01:21:27,148 [INFO] Starting crawl of URL: https://example.com
2024-11-29 01:21:29,149 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:21:37,148 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:21:37,149 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:21:37,157 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,179 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:21:37,180 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,181 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,181 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:21:37,182 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,182 [DEBUG] 
Evaluating link: https://example.com/doc1
2024-11-29 01:21:37,182 [DEBUG] Target domain: example.com
2024-11-29 01:21:37,182 [DEBUG] URL domain: example.com
2024-11-29 01:21:37,182 [DEBUG] URL approved for crawling: https://example.com/doc1
2024-11-29 01:21:37,182 [DEBUG] 
Evaluating link: https://example.com/excluded/page
2024-11-29 01:21:37,182 [DEBUG] Target domain: example.com
2024-11-29 01:21:37,182 [DEBUG] URL domain: example.com
2024-11-29 01:21:37,182 [DEBUG] URL approved for crawling: https://example.com/excluded/page
2024-11-29 01:21:37,189 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,190 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:21:37,190 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,195 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,195 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:21:37,196 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,196 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-29 01:21:37,200 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,201 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:21:37,201 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,206 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,207 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:21:37,207 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,207 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-29 01:21:37,208 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,208 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:21:37,209 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,209 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-29 01:21:37,213 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,213 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:21:37,213 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,214 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-29 01:21:37,218 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,219 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:21:37,219 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,220 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-29 01:21:37,223 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,223 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:21:37,224 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,224 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-29 01:21:37,224 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,226 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:21:37,227 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,231 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-29 01:21:37,248 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,281 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x106c0f950>
2024-11-29 01:21:37,281 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1063fdb00>, 290467.31844025)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1063fe0b0>, 290468.940634333)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1063fe190>, 290470.584970208)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1063fe200>, 290473.282703)])']
connector: <aiohttp.connector.TCPConnector object at 0x1065f1ad0>
2024-11-29 01:21:37,284 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,288 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,288 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,289 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:21:37,292 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,292 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,293 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:21:37,296 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,298 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:21:37,305 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:21:37,311 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:21:37,317 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:21:37,323 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:21:37,329 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:21:37,335 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:21:37,340 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:21:37,346 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:21:37,351 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:21:37,408 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,540 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:21:37,542 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:21:37,542 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 01:21:37,542 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 01:21:37,542 [INFO] Starting crawl of URL: https://example.com
2024-11-29 01:21:38,044 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:21:38,044 [DEBUG] Base URL: https://example.com
2024-11-29 01:21:38,044 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:21:38,045 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:21:38,045 [DEBUG] Initial domain set to: example.com
2024-11-29 01:21:38,045 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:21:38,045 [INFO] Successfully crawled https://example.com
2024-11-29 01:21:38,045 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x10616e590>
2024-11-29 01:21:38,046 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10612ca60>, 290598.59278)])']
connector: <aiohttp.connector.TCPConnector object at 0x10616dbd0>
2024-11-29 01:21:38,047 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2024-11-29 01:21:39,272 [INFO] Operation took 0.11 seconds
2024-11-29 01:36:43,515 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-29 01:56:05,199 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-29 01:57:11,986 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-29 01:57:12,514 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:12,514 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:12,515 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:12,517 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:12,517 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:12,518 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:12,518 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:57:12,543 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:12,544 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:12,544 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:57:12,547 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:12,547 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:12,550 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:12,550 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:12,561 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:12,562 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:12,563 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:12,563 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:12,566 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:12,566 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:12,567 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:57:12,569 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:13,394 [INFO] Operation took 0.06 seconds
2024-11-29 01:57:13,653 [INFO] test_operation took 0.10 seconds
2024-11-29 01:57:13,863 [INFO] inner took 0.10 seconds
2024-11-29 01:57:13,863 [INFO] outer took 0.21 seconds
2024-11-29 01:57:13,863 [INFO] error_operation took 0.00 seconds
2024-11-29 01:57:13,913 [INFO] test_operation took 0.00 seconds
2024-11-29 01:57:14,042 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:57:14,042 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:14,042 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 01:57:14,043 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 01:57:14,043 [INFO] Starting crawl of URL: https://example.com
2024-11-29 01:57:14,575 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:57:14,575 [DEBUG] Base URL: https://example.com
2024-11-29 01:57:14,575 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:57:14,575 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:57:14,575 [DEBUG] Initial domain set to: example.com
2024-11-29 01:57:14,575 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:57:14,575 [INFO] Successfully crawled https://example.com
2024-11-29 01:57:14,576 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:14,576 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1379215d0>
2024-11-29 01:57:14,576 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1378e5390>, 292735.125392333)]']
connector: <aiohttp.connector.TCPConnector object at 0x137922010>
2024-11-29 01:57:14,576 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:57:14,576 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:14,577 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 01:57:14,577 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 01:57:14,577 [INFO] Starting crawl of URL: https://example.com
2024-11-29 01:57:14,595 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-11-29 01:57:14,595 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-11-29 01:57:14,595 [INFO] Starting crawl of URL: https://example.org
2024-11-29 01:57:15,063 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:57:15,063 [DEBUG] Base URL: https://example.com
2024-11-29 01:57:15,063 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:57:15,063 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:57:15,063 [DEBUG] Initial domain set to: example.com
2024-11-29 01:57:15,063 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:57:15,063 [INFO] Successfully crawled https://example.com
2024-11-29 01:57:15,575 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:57:15,575 [DEBUG] Base URL: https://example.org
2024-11-29 01:57:15,575 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:57:15,576 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:57:15,576 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:57:15,576 [INFO] Successfully crawled https://example.org
2024-11-29 01:57:15,577 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:15,578 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1379ec850>
2024-11-29 01:57:15,578 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1378cc750>, 292735.611869333)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x1378cc6e0>, 292736.124443416)]']
connector: <aiohttp.connector.TCPConnector object at 0x1379ed790>
2024-11-29 01:57:15,579 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:57:15,579 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:15,580 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 01:57:15,580 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 01:57:15,580 [INFO] Starting crawl of URL: https://example.com
2024-11-29 01:57:16,083 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:57:16,083 [DEBUG] Base URL: https://example.com
2024-11-29 01:57:16,083 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:57:16,083 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:57:16,083 [DEBUG] Initial domain set to: example.com
2024-11-29 01:57:16,083 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:57:16,083 [INFO] Successfully crawled https://example.com
2024-11-29 01:57:16,084 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:16,084 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1379ef590>
2024-11-29 01:57:16,084 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1378e7d20>, 292736.632635333)]']
connector: <aiohttp.connector.TCPConnector object at 0x1379eee10>
2024-11-29 01:57:16,085 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:57:16,085 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:16,086 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 01:57:16,086 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 01:57:16,086 [INFO] Starting crawl of URL: https://example.com
2024-11-29 01:57:16,579 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:57:16,579 [DEBUG] Base URL: https://example.com
2024-11-29 01:57:16,579 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:57:16,579 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:57:16,579 [DEBUG] Initial domain set to: example.com
2024-11-29 01:57:16,579 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:57:16,579 [INFO] Successfully crawled https://example.com
2024-11-29 01:57:16,580 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:57:16,580 [DEBUG] Base URL: https://example.com
2024-11-29 01:57:16,580 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:57:16,580 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:57:16,580 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:57:16,587 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:16,587 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:57:16,588 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:16,591 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:16,592 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:57:16,592 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:16,592 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 01:57:16,592 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 01:57:16,592 [INFO] Starting crawl of URL: https://example.com
2024-11-29 01:57:17,087 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:57:17,087 [DEBUG] Base URL: https://example.com
2024-11-29 01:57:17,087 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:57:17,087 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:57:17,087 [DEBUG] Initial domain set to: example.com
2024-11-29 01:57:17,087 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:57:17,087 [INFO] Successfully crawled https://example.com
2024-11-29 01:57:17,088 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:17,088 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1375c41d0>
2024-11-29 01:57:17,088 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1378ccad0>, 292737.637280708)]']
connector: <aiohttp.connector.TCPConnector object at 0x1375c4810>
2024-11-29 01:57:17,089 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:57:17,089 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:17,089 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2024-11-29 01:57:17,089 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2024-11-29 01:57:17,089 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2024-11-29 01:57:19,165 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-29 01:57:19,166 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-29 01:57:19,166 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-29 01:57:19,171 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:19,172 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:57:19,172 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:19,172 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 01:57:19,172 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 01:57:19,172 [INFO] Starting crawl of URL: https://example.com
2024-11-29 01:57:19,189 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-11-29 01:57:19,189 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-11-29 01:57:19,189 [INFO] Starting crawl of URL: https://example.org
2024-11-29 01:57:19,189 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2024-11-29 01:57:19,189 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2024-11-29 01:57:19,189 [INFO] Starting crawl of URL: https://example.net
2024-11-29 01:57:19,688 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:57:19,688 [DEBUG] Base URL: https://example.com
2024-11-29 01:57:19,688 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:57:19,688 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:57:19,688 [DEBUG] Initial domain set to: example.com
2024-11-29 01:57:19,688 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:57:19,688 [INFO] Successfully crawled https://example.com
2024-11-29 01:57:20,165 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:57:20,165 [DEBUG] Base URL: https://example.org
2024-11-29 01:57:20,165 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:57:20,165 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:57:20,165 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:57:20,165 [INFO] Successfully crawled https://example.org
2024-11-29 01:57:20,772 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:57:20,772 [DEBUG] Base URL: https://example.net
2024-11-29 01:57:20,772 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:57:20,772 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:57:20,772 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:57:20,772 [INFO] Successfully crawled https://example.net
2024-11-29 01:57:20,773 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:20,773 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1375c5810>
2024-11-29 01:57:20,773 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1378cc7c0>, 292740.238399583)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x1378cd080>, 292740.715283083)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x1378cd470>, 292741.322540125)]']
connector: <aiohttp.connector.TCPConnector object at 0x1375c5250>
2024-11-29 01:57:20,774 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:57:20,774 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:20,774 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 01:57:20,775 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 01:57:20,775 [INFO] Starting crawl of URL: https://example.com
2024-11-29 01:57:21,267 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:57:21,267 [DEBUG] Base URL: https://example.com
2024-11-29 01:57:21,267 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:57:21,267 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:57:21,267 [DEBUG] Initial domain set to: example.com
2024-11-29 01:57:21,267 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:57:21,267 [INFO] Successfully crawled https://example.com
2024-11-29 01:57:21,268 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:21,271 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1374e1850>
2024-11-29 01:57:21,271 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1378cf2a0>, 292737.129654416)]']
connector: <aiohttp.connector.TCPConnector object at 0x1374e2c90>
2024-11-29 01:57:21,271 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x137951810>
2024-11-29 01:57:21,277 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:57:21,278 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:21,278 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:57:21,299 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:57:21,317 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:21,317 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:57:21,318 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:21,318 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:57:21,334 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:21,334 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:57:21,334 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:21,335 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:57:21,335 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2024-11-29 01:57:21,335 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2024-11-29 01:57:21,335 [INFO] Starting crawl of URL: https://example.com/page
2024-11-29 01:57:21,822 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:57:21,823 [DEBUG] Base URL: https://example.com/page
2024-11-29 01:57:21,823 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:57:21,823 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:57:21,823 [DEBUG] Initial domain set to: example.com
2024-11-29 01:57:21,823 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:57:21,823 [INFO] Successfully crawled https://example.com/page
2024-11-29 01:57:21,823 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2024-11-29 01:57:21,823 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2024-11-29 01:57:21,823 [INFO] Starting crawl of URL: https://other-domain.com/page
2024-11-29 01:57:25,783 [DEBUG] 
Evaluating link: ./
2024-11-29 01:57:25,783 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,783 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-29 01:57:25,783 [DEBUG] Absolute link: https://other-domain.com/
2024-11-29 01:57:25,783 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,783 [DEBUG] 
Evaluating link: ./
2024-11-29 01:57:25,784 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,784 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-29 01:57:25,784 [DEBUG] Absolute link: https://other-domain.com/
2024-11-29 01:57:25,784 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,784 [DEBUG] 
Evaluating link: ./hiyou.php
2024-11-29 01:57:25,784 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,784 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-11-29 01:57:25,784 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-11-29 01:57:25,784 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,784 [DEBUG] 
Evaluating link: ./date.php
2024-11-29 01:57:25,784 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,784 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-11-29 01:57:25,784 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-11-29 01:57:25,784 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,784 [DEBUG] 
Evaluating link: ./contact.php
2024-11-29 01:57:25,784 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,784 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2024-11-29 01:57:25,784 [DEBUG] Absolute link: https://other-domain.com/contact.php
2024-11-29 01:57:25,784 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,784 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 01:57:25,784 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,784 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 01:57:25,785 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 01:57:25,785 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-29 01:57:25,785 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:57:25,785 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,785 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:57:25,785 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:57:25,785 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-29 01:57:25,785 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 01:57:25,785 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,785 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 01:57:25,785 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 01:57:25,785 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-29 01:57:25,785 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-29 01:57:25,785 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,785 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 01:57:25,785 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 01:57:25,785 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-29 01:57:25,785 [DEBUG] 
Evaluating link: ./contact
2024-11-29 01:57:25,785 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,785 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-11-29 01:57:25,785 [DEBUG] Absolute link: https://other-domain.com/contact
2024-11-29 01:57:25,785 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,785 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:57:25,785 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,785 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:57:25,785 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:57:25,785 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-29 01:57:25,785 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 01:57:25,786 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,786 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 01:57:25,786 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 01:57:25,786 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-29 01:57:25,786 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-29 01:57:25,786 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,786 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 01:57:25,786 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 01:57:25,786 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-29 01:57:25,786 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-11-29 01:57:25,786 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,786 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:57:25,786 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:57:25,786 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-11-29 01:57:25,786 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 01:57:25,786 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,787 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 01:57:25,787 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 01:57:25,787 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-29 01:57:25,787 [DEBUG] 
Evaluating link: #indexes
2024-11-29 01:57:25,787 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,787 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2024-11-29 01:57:25,787 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,787 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,787 [DEBUG] 
Evaluating link: #a
2024-11-29 01:57:25,787 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,787 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2024-11-29 01:57:25,787 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,787 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,787 [DEBUG] 
Evaluating link: #b
2024-11-29 01:57:25,787 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,787 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-11-29 01:57:25,787 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,787 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,787 [DEBUG] 
Evaluating link: #c
2024-11-29 01:57:25,787 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,787 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-11-29 01:57:25,787 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,787 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,787 [DEBUG] 
Evaluating link: #d
2024-11-29 01:57:25,787 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,787 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-11-29 01:57:25,787 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,787 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,787 [DEBUG] 
Evaluating link: #e
2024-11-29 01:57:25,788 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,788 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-11-29 01:57:25,788 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,788 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,788 [DEBUG] 
Evaluating link: #f
2024-11-29 01:57:25,788 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,788 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-11-29 01:57:25,788 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,788 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,788 [DEBUG] 
Evaluating link: #g
2024-11-29 01:57:25,788 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,788 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-11-29 01:57:25,788 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,788 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,788 [DEBUG] 
Evaluating link: #h
2024-11-29 01:57:25,788 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,788 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-11-29 01:57:25,788 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,788 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,788 [DEBUG] 
Evaluating link: #i
2024-11-29 01:57:25,788 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,788 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-11-29 01:57:25,788 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,788 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,788 [DEBUG] 
Evaluating link: #j
2024-11-29 01:57:25,788 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,788 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-11-29 01:57:25,788 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,788 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,788 [DEBUG] 
Evaluating link: #k
2024-11-29 01:57:25,788 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,788 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-11-29 01:57:25,788 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,788 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,788 [DEBUG] 
Evaluating link: #l
2024-11-29 01:57:25,789 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,789 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-11-29 01:57:25,789 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,789 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,789 [DEBUG] 
Evaluating link: #m
2024-11-29 01:57:25,789 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,789 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-11-29 01:57:25,789 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,789 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,789 [DEBUG] 
Evaluating link: #n
2024-11-29 01:57:25,789 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,789 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-11-29 01:57:25,789 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,789 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,789 [DEBUG] 
Evaluating link: #o
2024-11-29 01:57:25,789 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,789 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-11-29 01:57:25,789 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,789 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,789 [DEBUG] 
Evaluating link: #p
2024-11-29 01:57:25,789 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,789 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-11-29 01:57:25,789 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,789 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,789 [DEBUG] 
Evaluating link: #q
2024-11-29 01:57:25,789 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,789 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-11-29 01:57:25,789 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,789 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,789 [DEBUG] 
Evaluating link: #r
2024-11-29 01:57:25,789 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,789 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-11-29 01:57:25,789 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,789 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,790 [DEBUG] 
Evaluating link: #s
2024-11-29 01:57:25,790 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,790 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-11-29 01:57:25,790 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,790 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,790 [DEBUG] 
Evaluating link: #t
2024-11-29 01:57:25,790 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,790 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-11-29 01:57:25,790 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,790 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,790 [DEBUG] 
Evaluating link: #u
2024-11-29 01:57:25,790 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,790 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-11-29 01:57:25,790 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,790 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,790 [DEBUG] 
Evaluating link: #v
2024-11-29 01:57:25,790 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,790 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-11-29 01:57:25,790 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,790 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,790 [DEBUG] 
Evaluating link: #w
2024-11-29 01:57:25,790 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,790 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-11-29 01:57:25,790 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,790 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,790 [DEBUG] 
Evaluating link: #x
2024-11-29 01:57:25,790 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,790 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-11-29 01:57:25,790 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,791 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,791 [DEBUG] 
Evaluating link: #y
2024-11-29 01:57:25,791 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,791 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-11-29 01:57:25,791 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,791 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,791 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:57:25,791 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,791 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:57:25,791 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:57:25,791 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-29 01:57:25,791 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 01:57:25,791 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,791 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 01:57:25,791 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 01:57:25,791 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-29 01:57:25,791 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-29 01:57:25,791 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,791 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 01:57:25,791 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 01:57:25,791 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-29 01:57:25,791 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-11-29 01:57:25,791 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,791 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:57:25,791 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:57:25,791 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-11-29 01:57:25,791 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 01:57:25,791 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,791 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 01:57:25,791 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 01:57:25,791 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-29 01:57:25,792 [DEBUG] 
Evaluating link: https://spriq.jp/
2024-11-29 01:57:25,792 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,792 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2024-11-29 01:57:25,792 [DEBUG] Absolute link: https://spriq.jp/
2024-11-29 01:57:25,792 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2024-11-29 01:57:25,792 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:57:25,792 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,792 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:57:25,792 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:57:25,792 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-29 01:57:25,792 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:57:25,792 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,792 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:57:25,792 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 01:57:25,792 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-29 01:57:25,792 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 01:57:25,792 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,792 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 01:57:25,792 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 01:57:25,792 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-29 01:57:25,792 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-29 01:57:25,792 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,792 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 01:57:25,792 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 01:57:25,792 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-29 01:57:25,792 [DEBUG] 
Evaluating link: ./contact
2024-11-29 01:57:25,792 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,792 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-11-29 01:57:25,792 [DEBUG] Absolute link: https://other-domain.com/contact
2024-11-29 01:57:25,792 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,792 [DEBUG] 
Evaluating link: ./
2024-11-29 01:57:25,792 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,792 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-29 01:57:25,792 [DEBUG] Absolute link: https://other-domain.com/
2024-11-29 01:57:25,793 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,793 [DEBUG] 
Evaluating link: ./hiyou.php
2024-11-29 01:57:25,793 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,793 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-11-29 01:57:25,793 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-11-29 01:57:25,793 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,793 [DEBUG] 
Evaluating link: ./date.php
2024-11-29 01:57:25,793 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,793 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-11-29 01:57:25,793 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-11-29 01:57:25,793 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,793 [DEBUG] 
Evaluating link: #u
2024-11-29 01:57:25,793 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,793 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-11-29 01:57:25,793 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,793 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,793 [DEBUG] 
Evaluating link: #v
2024-11-29 01:57:25,793 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,793 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-11-29 01:57:25,793 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,793 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,793 [DEBUG] 
Evaluating link: #w
2024-11-29 01:57:25,793 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,793 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-11-29 01:57:25,793 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,793 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,793 [DEBUG] 
Evaluating link: #x
2024-11-29 01:57:25,793 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,793 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-11-29 01:57:25,793 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,793 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,793 [DEBUG] 
Evaluating link: #y
2024-11-29 01:57:25,793 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,793 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-11-29 01:57:25,794 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,794 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,794 [DEBUG] 
Evaluating link: #p
2024-11-29 01:57:25,794 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,794 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-11-29 01:57:25,794 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,794 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,794 [DEBUG] 
Evaluating link: #q
2024-11-29 01:57:25,794 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,794 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-11-29 01:57:25,794 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,794 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,794 [DEBUG] 
Evaluating link: #r
2024-11-29 01:57:25,794 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,794 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-11-29 01:57:25,794 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,794 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,794 [DEBUG] 
Evaluating link: #s
2024-11-29 01:57:25,794 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,794 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-11-29 01:57:25,794 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,794 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,794 [DEBUG] 
Evaluating link: #t
2024-11-29 01:57:25,794 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,794 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-11-29 01:57:25,794 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,794 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,795 [DEBUG] 
Evaluating link: #k
2024-11-29 01:57:25,795 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,795 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-11-29 01:57:25,795 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,795 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,795 [DEBUG] 
Evaluating link: #l
2024-11-29 01:57:25,795 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,795 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-11-29 01:57:25,795 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,795 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,795 [DEBUG] 
Evaluating link: #m
2024-11-29 01:57:25,795 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,795 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-11-29 01:57:25,795 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,795 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,795 [DEBUG] 
Evaluating link: #n
2024-11-29 01:57:25,795 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,795 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-11-29 01:57:25,795 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,795 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,795 [DEBUG] 
Evaluating link: #o
2024-11-29 01:57:25,795 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,795 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-11-29 01:57:25,795 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,795 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,795 [DEBUG] 
Evaluating link: #f
2024-11-29 01:57:25,795 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,795 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-11-29 01:57:25,795 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,795 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,795 [DEBUG] 
Evaluating link: #g
2024-11-29 01:57:25,795 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,795 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-11-29 01:57:25,795 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,795 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,796 [DEBUG] 
Evaluating link: #h
2024-11-29 01:57:25,796 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,796 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-11-29 01:57:25,796 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,796 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,796 [DEBUG] 
Evaluating link: #i
2024-11-29 01:57:25,796 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,796 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-11-29 01:57:25,796 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,796 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,796 [DEBUG] 
Evaluating link: #j
2024-11-29 01:57:25,796 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,796 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-11-29 01:57:25,796 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,796 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,796 [DEBUG] 
Evaluating link: #b
2024-11-29 01:57:25,796 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,796 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-11-29 01:57:25,796 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,796 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,796 [DEBUG] 
Evaluating link: #c
2024-11-29 01:57:25,796 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,796 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-11-29 01:57:25,796 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,796 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,796 [DEBUG] 
Evaluating link: #d
2024-11-29 01:57:25,796 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,796 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-11-29 01:57:25,796 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,796 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,796 [DEBUG] 
Evaluating link: #e
2024-11-29 01:57:25,796 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,796 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-11-29 01:57:25,796 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,796 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,796 [DEBUG] 
Evaluating link: #site
2024-11-29 01:57:25,796 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 01:57:25,797 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2024-11-29 01:57:25,797 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 01:57:25,797 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 01:57:25,797 [INFO] Successfully crawled https://other-domain.com/page
2024-11-29 01:57:25,801 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:25,803 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:57:25,803 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:25,803 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-11-29 01:57:25,803 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-11-29 01:57:25,803 [INFO] Starting crawl of URL: https://example.com/page0
2024-11-29 01:57:26,299 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:57:26,300 [DEBUG] Base URL: https://example.com/page0
2024-11-29 01:57:26,300 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:57:26,300 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:57:26,300 [DEBUG] Initial domain set to: example.com
2024-11-29 01:57:26,300 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:57:26,300 [INFO] Successfully crawled https://example.com/page0
2024-11-29 01:57:26,300 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-29 01:57:26,300 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-29 01:57:26,300 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-29 01:57:26,447 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:57:26,448 [DEBUG] Base URL: https://example.com/page1
2024-11-29 01:57:26,448 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:57:26,448 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:57:26,448 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:57:26,448 [INFO] Successfully crawled https://example.com/page1
2024-11-29 01:57:26,448 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-29 01:57:26,448 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-29 01:57:26,448 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-29 01:57:26,948 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:57:26,948 [DEBUG] Base URL: https://example.com/page2
2024-11-29 01:57:26,948 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:57:26,948 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:57:26,948 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:57:26,948 [INFO] Successfully crawled https://example.com/page2
2024-11-29 01:57:26,948 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-29 01:57:26,948 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-29 01:57:26,948 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-29 01:57:27,457 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:57:27,457 [DEBUG] Base URL: https://example.com/page3
2024-11-29 01:57:27,457 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:57:27,457 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:57:27,458 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:57:27,458 [INFO] Successfully crawled https://example.com/page3
2024-11-29 01:57:27,458 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-11-29 01:57:27,458 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-11-29 01:57:27,458 [INFO] Starting crawl of URL: https://example.com/page4
2024-11-29 01:57:27,951 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:57:27,951 [DEBUG] Base URL: https://example.com/page4
2024-11-29 01:57:27,952 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:57:27,952 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:57:27,952 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:57:27,952 [INFO] Successfully crawled https://example.com/page4
2024-11-29 01:57:27,952 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:57:27,952 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-11-29 01:57:27,952 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-11-29 01:57:27,952 [INFO] Starting crawl of URL: https://example.com/page0
2024-11-29 01:57:28,471 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:57:28,471 [DEBUG] Base URL: https://example.com/page0
2024-11-29 01:57:28,471 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:57:28,471 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:57:28,471 [DEBUG] Initial domain set to: example.com
2024-11-29 01:57:28,472 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:57:28,472 [INFO] Successfully crawled https://example.com/page0
2024-11-29 01:57:28,472 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-29 01:57:28,472 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-29 01:57:28,472 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-29 01:57:28,599 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:57:28,600 [DEBUG] Base URL: https://example.com/page1
2024-11-29 01:57:28,600 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:57:28,600 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:57:28,600 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:57:28,600 [INFO] Successfully crawled https://example.com/page1
2024-11-29 01:57:28,600 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-29 01:57:28,600 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-29 01:57:28,600 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-29 01:57:29,100 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:57:29,100 [DEBUG] Base URL: https://example.com/page2
2024-11-29 01:57:29,100 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:57:29,100 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:57:29,100 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:57:29,100 [INFO] Successfully crawled https://example.com/page2
2024-11-29 01:57:29,100 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-29 01:57:29,100 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-29 01:57:29,100 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-29 01:57:29,601 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:57:29,601 [DEBUG] Base URL: https://example.com/page3
2024-11-29 01:57:29,601 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:57:29,601 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:57:29,601 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:57:29,602 [INFO] Successfully crawled https://example.com/page3
2024-11-29 01:57:29,602 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-11-29 01:57:29,602 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-11-29 01:57:29,602 [INFO] Starting crawl of URL: https://example.com/page4
2024-11-29 01:57:30,102 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x137758bd0>
2024-11-29 01:57:30,102 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1378cda90>, 292742.373242125)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x1378cde80>, 292743.721013583)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x1378cdf60>, 292745.085578458)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x1378cdfd0>, 292746.318270958)]']
connector: <aiohttp.connector.TCPConnector object at 0x13775b5d0>
2024-11-29 01:57:30,104 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:57:30,104 [DEBUG] Base URL: https://example.com/page4
2024-11-29 01:57:30,104 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:57:30,104 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:57:30,104 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:57:30,104 [INFO] Successfully crawled https://example.com/page4
2024-11-29 01:57:30,108 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:30,110 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:30,111 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 01:57:30,115 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:30,116 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:30,117 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 01:57:30,118 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 01:57:30,118 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 01:57:30,118 [INFO] Starting crawl of URL: https://example.com
2024-11-29 01:57:32,119 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:57:40,118 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:57:40,119 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:57:40,124 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:40,131 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:57:40,132 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 01:57:40,132 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-29 01:57:40,132 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-29 01:57:40,132 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-29 01:57:40,132 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-29 01:57:40,132 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-29 01:57:40,132 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-29 01:57:42,133 [WARNING] Retry 1/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:57:52,134 [WARNING] Retry 1/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:58:00,134 [WARNING] Retry 2/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:58:00,134 [ERROR] Error crawling https://example.com/page1: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:58:10,135 [WARNING] Retry 2/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:58:10,136 [ERROR] Error crawling https://example.com/page2: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:58:10,145 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:58:10,146 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:58:10,147 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 01:58:10,148 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-29 01:58:10,148 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-29 01:58:10,148 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-29 01:58:10,148 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-29 01:58:10,148 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-29 01:58:10,148 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-29 01:58:10,148 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-29 01:58:10,148 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-29 01:58:10,148 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-29 01:58:12,149 [WARNING] Retry 1/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:58:22,150 [WARNING] Retry 1/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:58:32,151 [WARNING] Retry 1/2 for https://example.com/page3: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:58:40,151 [WARNING] Retry 2/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:58:40,152 [ERROR] Error crawling https://example.com/page1: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:58:50,151 [WARNING] Retry 2/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:58:50,153 [ERROR] Error crawling https://example.com/page2: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:59:00,152 [WARNING] Retry 2/2 for https://example.com/page3: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:59:00,153 [ERROR] Error crawling https://example.com/page3: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:59:00,155 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:00,156 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:00,157 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 01:59:00,163 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:00,164 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:00,164 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 01:59:00,165 [DEBUG] Using raw URL string: https://nonexistent.example.com (type: <class 'str'>)
2024-11-29 01:59:00,165 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='nonexistent.example.com', path='', params='', query='', fragment='')
2024-11-29 01:59:00,165 [INFO] Starting crawl of URL: https://nonexistent.example.com
2024-11-29 01:59:02,166 [WARNING] Retry 1/2 for https://nonexistent.example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:59:10,166 [WARNING] Retry 2/2 for https://nonexistent.example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:59:10,167 [ERROR] Error crawling https://nonexistent.example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:59:10,176 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:10,177 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:10,178 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 01:59:10,182 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:10,183 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:10,184 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 01:59:10,184 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 01:59:10,184 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 01:59:10,184 [INFO] Starting crawl of URL: https://example.com
2024-11-29 01:59:12,185 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:59:20,185 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:59:20,186 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:59:20,193 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:20,195 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:20,195 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 01:59:20,196 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 01:59:20,196 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 01:59:20,196 [INFO] Starting crawl of URL: https://example.com
2024-11-29 01:59:22,197 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:59:30,197 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:59:30,198 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 01:59:30,202 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,222 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:59:30,222 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,223 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,224 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:59:30,224 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,225 [DEBUG] 
Evaluating link: https://example.com/doc1
2024-11-29 01:59:30,225 [DEBUG] Target domain: example.com
2024-11-29 01:59:30,225 [DEBUG] URL domain: example.com
2024-11-29 01:59:30,225 [DEBUG] URL approved for crawling: https://example.com/doc1
2024-11-29 01:59:30,225 [DEBUG] 
Evaluating link: https://example.com/excluded/page
2024-11-29 01:59:30,225 [DEBUG] Target domain: example.com
2024-11-29 01:59:30,225 [DEBUG] URL domain: example.com
2024-11-29 01:59:30,225 [DEBUG] URL approved for crawling: https://example.com/excluded/page
2024-11-29 01:59:30,231 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,231 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:59:30,231 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,235 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,236 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:59:30,236 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,236 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-29 01:59:30,240 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,240 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:59:30,240 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,244 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,244 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:59:30,245 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,245 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-29 01:59:30,245 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,246 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:59:30,246 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,247 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-29 01:59:30,250 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,251 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:59:30,251 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,252 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-29 01:59:30,256 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,256 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:59:30,257 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,257 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-29 01:59:30,261 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,261 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:59:30,261 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,262 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-29 01:59:30,262 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,263 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:59:30,263 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,264 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-29 01:59:30,267 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,296 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x13770f610>
2024-11-29 01:59:30,296 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x13770d590>
2024-11-29 01:59:30,296 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1378cdcc0>, 292748.501495666)]']
connector: <aiohttp.connector.TCPConnector object at 0x13770f8d0>
2024-11-29 01:59:30,296 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1378ced60>, 292750.652025041)]']
connector: <aiohttp.connector.TCPConnector object at 0x13770fed0>
2024-11-29 01:59:30,300 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,304 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,305 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,305 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:59:30,308 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,308 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,309 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:59:30,312 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,314 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:59:30,321 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:59:30,328 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:59:30,350 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:59:30,375 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:59:30,383 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:59:30,390 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:59:30,397 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:59:30,402 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:59:30,409 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 01:59:30,469 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,619 [DEBUG] Using selector: KqueueSelector
2024-11-29 01:59:30,620 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 01:59:30,620 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 01:59:30,621 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 01:59:30,621 [INFO] Starting crawl of URL: https://example.com
2024-11-29 01:59:31,114 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 01:59:31,114 [DEBUG] Base URL: https://example.com
2024-11-29 01:59:31,114 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 01:59:31,114 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 01:59:31,114 [DEBUG] Initial domain set to: example.com
2024-11-29 01:59:31,114 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 01:59:31,114 [INFO] Successfully crawled https://example.com
2024-11-29 01:59:31,114 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1466ad150>
2024-11-29 01:59:31,115 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x14660bb60>, 292871.6654065)]']
connector: <aiohttp.connector.TCPConnector object at 0x1466acad0>
2024-11-29 01:59:31,116 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2024-11-29 01:59:32,475 [INFO] Operation took 0.10 seconds
2024-11-29 02:00:51,433 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-29 02:00:51,920 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:51,921 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:51,921 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:51,923 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:51,924 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:51,924 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:51,925 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:00:51,949 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:51,950 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:51,950 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:00:51,953 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:51,954 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:51,957 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:51,957 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:51,969 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:51,969 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:51,970 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:51,971 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:51,973 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:51,974 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:51,974 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:00:51,977 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:52,761 [INFO] Operation took 0.06 seconds
2024-11-29 02:00:53,051 [INFO] test_operation took 0.11 seconds
2024-11-29 02:00:53,261 [INFO] inner took 0.10 seconds
2024-11-29 02:00:53,261 [INFO] outer took 0.21 seconds
2024-11-29 02:00:53,261 [INFO] error_operation took 0.00 seconds
2024-11-29 02:00:53,290 [INFO] test_operation took 0.00 seconds
2024-11-29 02:00:53,478 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:00:53,478 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:53,479 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 02:00:53,479 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 02:00:53,479 [INFO] Starting crawl of URL: https://example.com
2024-11-29 02:00:53,969 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:00:53,969 [DEBUG] Base URL: https://example.com
2024-11-29 02:00:53,969 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:00:53,969 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:00:53,969 [DEBUG] Initial domain set to: example.com
2024-11-29 02:00:53,969 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:00:53,969 [INFO] Successfully crawled https://example.com
2024-11-29 02:00:53,970 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:53,970 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x12ee85cd0>
2024-11-29 02:00:53,970 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x12ecdacf0>, 292954.521055875)]']
connector: <aiohttp.connector.TCPConnector object at 0x12edad110>
2024-11-29 02:00:53,971 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:00:53,971 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:53,971 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 02:00:53,971 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 02:00:53,971 [INFO] Starting crawl of URL: https://example.com
2024-11-29 02:00:53,987 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-11-29 02:00:53,987 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-11-29 02:00:53,987 [INFO] Starting crawl of URL: https://example.org
2024-11-29 02:00:54,459 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:00:54,459 [DEBUG] Base URL: https://example.com
2024-11-29 02:00:54,459 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:00:54,459 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:00:54,459 [DEBUG] Initial domain set to: example.com
2024-11-29 02:00:54,459 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:00:54,459 [INFO] Successfully crawled https://example.com
2024-11-29 02:00:54,957 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:00:54,957 [DEBUG] Base URL: https://example.org
2024-11-29 02:00:54,957 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:00:54,958 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:00:54,958 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:00:54,958 [INFO] Successfully crawled https://example.org
2024-11-29 02:00:54,958 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:54,959 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x12ec16a50>
2024-11-29 02:00:54,959 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x12ecdb0e0>, 292955.011534958)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x12ecdaac0>, 292955.509807041)]']
connector: <aiohttp.connector.TCPConnector object at 0x12ec15950>
2024-11-29 02:00:54,959 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:00:54,959 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:54,959 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 02:00:54,960 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 02:00:54,960 [INFO] Starting crawl of URL: https://example.com
2024-11-29 02:00:55,448 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:00:55,448 [DEBUG] Base URL: https://example.com
2024-11-29 02:00:55,448 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:00:55,448 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:00:55,448 [DEBUG] Initial domain set to: example.com
2024-11-29 02:00:55,448 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:00:55,448 [INFO] Successfully crawled https://example.com
2024-11-29 02:00:55,449 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:55,449 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x12edaccd0>
2024-11-29 02:00:55,449 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x12ecdb4d0>, 292956.000236541)]']
connector: <aiohttp.connector.TCPConnector object at 0x12edaf810>
2024-11-29 02:00:55,449 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:00:55,449 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:55,450 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 02:00:55,450 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 02:00:55,450 [INFO] Starting crawl of URL: https://example.com
2024-11-29 02:00:55,928 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:00:55,928 [DEBUG] Base URL: https://example.com
2024-11-29 02:00:55,928 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:00:55,928 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:00:55,928 [DEBUG] Initial domain set to: example.com
2024-11-29 02:00:55,928 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:00:55,928 [INFO] Successfully crawled https://example.com
2024-11-29 02:00:55,928 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:00:55,928 [DEBUG] Base URL: https://example.com
2024-11-29 02:00:55,928 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:00:55,928 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:00:55,928 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:00:55,933 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:55,934 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:00:55,934 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:55,936 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:55,937 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:00:55,937 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:55,937 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 02:00:55,937 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 02:00:55,937 [INFO] Starting crawl of URL: https://example.com
2024-11-29 02:00:56,417 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:00:56,417 [DEBUG] Base URL: https://example.com
2024-11-29 02:00:56,417 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:00:56,418 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:00:56,418 [DEBUG] Initial domain set to: example.com
2024-11-29 02:00:56,418 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:00:56,418 [INFO] Successfully crawled https://example.com
2024-11-29 02:00:56,418 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:56,419 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x12ec2e650>
2024-11-29 02:00:56,419 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x12ecacde0>, 292956.969784458)]']
connector: <aiohttp.connector.TCPConnector object at 0x12ed0a590>
2024-11-29 02:00:56,419 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:00:56,419 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:56,420 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2024-11-29 02:00:56,420 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2024-11-29 02:00:56,420 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2024-11-29 02:00:58,437 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-29 02:00:58,438 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-29 02:00:58,438 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-29 02:00:58,442 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:58,442 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:00:58,442 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:58,443 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 02:00:58,443 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 02:00:58,443 [INFO] Starting crawl of URL: https://example.com
2024-11-29 02:00:58,458 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-11-29 02:00:58,458 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-11-29 02:00:58,458 [INFO] Starting crawl of URL: https://example.org
2024-11-29 02:00:58,458 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2024-11-29 02:00:58,458 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2024-11-29 02:00:58,458 [INFO] Starting crawl of URL: https://example.net
2024-11-29 02:00:58,924 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:00:58,925 [DEBUG] Base URL: https://example.com
2024-11-29 02:00:58,925 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:00:58,925 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:00:58,925 [DEBUG] Initial domain set to: example.com
2024-11-29 02:00:58,925 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:00:58,925 [INFO] Successfully crawled https://example.com
2024-11-29 02:00:59,424 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:00:59,424 [DEBUG] Base URL: https://example.org
2024-11-29 02:00:59,424 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:00:59,424 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:00:59,424 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:00:59,424 [INFO] Successfully crawled https://example.org
2024-11-29 02:00:59,938 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:00:59,938 [DEBUG] Base URL: https://example.net
2024-11-29 02:00:59,938 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:00:59,938 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:00:59,938 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:00:59,938 [INFO] Successfully crawled https://example.net
2024-11-29 02:00:59,939 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:59,939 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x12ee4b890>
2024-11-29 02:00:59,939 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x12ecacc20>, 292959.4769615)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x12ecac6e0>, 292959.976207583)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x12ecac8a0>, 292960.490432125)]']
connector: <aiohttp.connector.TCPConnector object at 0x12ee49790>
2024-11-29 02:00:59,939 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:00:59,940 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:00:59,940 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 02:00:59,940 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 02:00:59,940 [INFO] Starting crawl of URL: https://example.com
2024-11-29 02:01:00,429 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:01:00,429 [DEBUG] Base URL: https://example.com
2024-11-29 02:01:00,429 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:01:00,429 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:01:00,429 [DEBUG] Initial domain set to: example.com
2024-11-29 02:01:00,429 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:01:00,429 [INFO] Successfully crawled https://example.com
2024-11-29 02:01:00,429 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:01:00,431 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x12ed0a4d0>
2024-11-29 02:01:00,431 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x12ecd9fd0>, 292956.480169625)]']
connector: <aiohttp.connector.TCPConnector object at 0x12ec16a90>
2024-11-29 02:01:00,431 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x12ed1c450>
2024-11-29 02:01:00,436 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:01:00,436 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:01:00,436 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:01:00,454 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:01:00,470 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:01:00,470 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:01:00,470 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:01:00,471 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:01:00,487 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:01:00,488 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:01:00,488 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:01:00,488 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:01:00,488 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2024-11-29 02:01:00,488 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2024-11-29 02:01:00,488 [INFO] Starting crawl of URL: https://example.com/page
2024-11-29 02:01:00,986 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:01:00,987 [DEBUG] Base URL: https://example.com/page
2024-11-29 02:01:00,987 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:01:00,987 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:01:00,987 [DEBUG] Initial domain set to: example.com
2024-11-29 02:01:00,987 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:01:00,987 [INFO] Successfully crawled https://example.com/page
2024-11-29 02:01:00,987 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2024-11-29 02:01:00,987 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2024-11-29 02:01:00,987 [INFO] Starting crawl of URL: https://other-domain.com/page
2024-11-29 02:01:05,426 [DEBUG] 
Evaluating link: ./
2024-11-29 02:01:05,427 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,427 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-29 02:01:05,427 [DEBUG] Absolute link: https://other-domain.com/
2024-11-29 02:01:05,427 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,427 [DEBUG] 
Evaluating link: ./
2024-11-29 02:01:05,427 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,427 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-29 02:01:05,427 [DEBUG] Absolute link: https://other-domain.com/
2024-11-29 02:01:05,427 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,427 [DEBUG] 
Evaluating link: ./hiyou.php
2024-11-29 02:01:05,427 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,427 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-11-29 02:01:05,427 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-11-29 02:01:05,427 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,427 [DEBUG] 
Evaluating link: ./date.php
2024-11-29 02:01:05,427 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,427 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-11-29 02:01:05,427 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-11-29 02:01:05,427 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,427 [DEBUG] 
Evaluating link: ./contact.php
2024-11-29 02:01:05,427 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,427 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2024-11-29 02:01:05,428 [DEBUG] Absolute link: https://other-domain.com/contact.php
2024-11-29 02:01:05,428 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,428 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 02:01:05,428 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,428 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 02:01:05,428 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 02:01:05,428 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-29 02:01:05,428 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:01:05,428 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,428 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:01:05,428 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:01:05,428 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-29 02:01:05,428 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 02:01:05,428 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,428 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 02:01:05,428 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 02:01:05,428 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-29 02:01:05,428 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-29 02:01:05,428 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,428 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 02:01:05,428 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 02:01:05,428 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-29 02:01:05,428 [DEBUG] 
Evaluating link: ./contact
2024-11-29 02:01:05,428 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,428 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-11-29 02:01:05,428 [DEBUG] Absolute link: https://other-domain.com/contact
2024-11-29 02:01:05,428 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,428 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:01:05,428 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,428 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:01:05,428 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:01:05,428 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-29 02:01:05,428 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 02:01:05,428 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,428 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 02:01:05,428 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 02:01:05,428 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-29 02:01:05,428 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-29 02:01:05,428 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,428 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 02:01:05,428 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 02:01:05,428 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-29 02:01:05,429 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-11-29 02:01:05,429 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,429 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:01:05,429 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:01:05,429 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-11-29 02:01:05,429 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 02:01:05,429 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,429 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 02:01:05,429 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 02:01:05,429 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-29 02:01:05,429 [DEBUG] 
Evaluating link: #indexes
2024-11-29 02:01:05,429 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,429 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2024-11-29 02:01:05,429 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,429 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,429 [DEBUG] 
Evaluating link: #a
2024-11-29 02:01:05,429 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,429 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2024-11-29 02:01:05,429 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,429 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,429 [DEBUG] 
Evaluating link: #b
2024-11-29 02:01:05,429 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,429 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-11-29 02:01:05,429 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,429 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,429 [DEBUG] 
Evaluating link: #c
2024-11-29 02:01:05,429 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,429 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-11-29 02:01:05,429 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,429 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,429 [DEBUG] 
Evaluating link: #d
2024-11-29 02:01:05,429 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,429 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-11-29 02:01:05,429 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,429 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,429 [DEBUG] 
Evaluating link: #e
2024-11-29 02:01:05,429 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,429 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-11-29 02:01:05,429 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,429 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,430 [DEBUG] 
Evaluating link: #f
2024-11-29 02:01:05,430 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,430 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-11-29 02:01:05,430 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,430 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,430 [DEBUG] 
Evaluating link: #g
2024-11-29 02:01:05,430 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,430 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-11-29 02:01:05,430 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,430 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,430 [DEBUG] 
Evaluating link: #h
2024-11-29 02:01:05,430 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,430 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-11-29 02:01:05,430 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,430 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,430 [DEBUG] 
Evaluating link: #i
2024-11-29 02:01:05,430 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,430 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-11-29 02:01:05,430 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,430 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,430 [DEBUG] 
Evaluating link: #j
2024-11-29 02:01:05,430 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,430 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-11-29 02:01:05,430 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,430 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,430 [DEBUG] 
Evaluating link: #k
2024-11-29 02:01:05,430 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,430 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-11-29 02:01:05,430 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,430 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,430 [DEBUG] 
Evaluating link: #l
2024-11-29 02:01:05,430 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,430 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-11-29 02:01:05,430 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,430 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,430 [DEBUG] 
Evaluating link: #m
2024-11-29 02:01:05,430 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,430 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-11-29 02:01:05,430 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,430 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,430 [DEBUG] 
Evaluating link: #n
2024-11-29 02:01:05,430 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,430 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-11-29 02:01:05,430 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,430 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,431 [DEBUG] 
Evaluating link: #o
2024-11-29 02:01:05,431 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,431 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-11-29 02:01:05,431 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,431 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,431 [DEBUG] 
Evaluating link: #p
2024-11-29 02:01:05,431 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,431 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-11-29 02:01:05,431 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,431 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,431 [DEBUG] 
Evaluating link: #q
2024-11-29 02:01:05,431 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,431 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-11-29 02:01:05,431 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,431 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,431 [DEBUG] 
Evaluating link: #r
2024-11-29 02:01:05,431 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,431 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-11-29 02:01:05,431 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,431 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,431 [DEBUG] 
Evaluating link: #s
2024-11-29 02:01:05,431 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,431 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-11-29 02:01:05,431 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,431 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,431 [DEBUG] 
Evaluating link: #t
2024-11-29 02:01:05,431 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,431 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-11-29 02:01:05,431 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,431 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,431 [DEBUG] 
Evaluating link: #u
2024-11-29 02:01:05,431 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,431 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-11-29 02:01:05,431 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,431 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,431 [DEBUG] 
Evaluating link: #v
2024-11-29 02:01:05,431 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,431 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-11-29 02:01:05,431 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,431 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,431 [DEBUG] 
Evaluating link: #w
2024-11-29 02:01:05,431 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,431 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-11-29 02:01:05,431 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,431 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,432 [DEBUG] 
Evaluating link: #x
2024-11-29 02:01:05,432 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,432 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-11-29 02:01:05,432 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,432 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,432 [DEBUG] 
Evaluating link: #y
2024-11-29 02:01:05,432 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,432 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-11-29 02:01:05,432 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,432 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,432 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:01:05,432 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,432 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:01:05,432 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:01:05,432 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-29 02:01:05,432 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 02:01:05,432 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,432 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 02:01:05,432 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 02:01:05,432 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-29 02:01:05,432 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-29 02:01:05,432 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,432 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 02:01:05,432 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 02:01:05,432 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-29 02:01:05,432 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-11-29 02:01:05,432 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,432 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:01:05,432 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:01:05,432 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-11-29 02:01:05,432 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 02:01:05,432 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,432 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 02:01:05,432 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 02:01:05,432 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-29 02:01:05,432 [DEBUG] 
Evaluating link: https://spriq.jp/
2024-11-29 02:01:05,432 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,432 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2024-11-29 02:01:05,432 [DEBUG] Absolute link: https://spriq.jp/
2024-11-29 02:01:05,432 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2024-11-29 02:01:05,432 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:01:05,432 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,432 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:01:05,433 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:01:05,433 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-29 02:01:05,433 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:01:05,433 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,433 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:01:05,433 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:01:05,433 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-29 02:01:05,433 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 02:01:05,433 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,433 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 02:01:05,433 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 02:01:05,433 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-29 02:01:05,433 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-29 02:01:05,433 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,433 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 02:01:05,433 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 02:01:05,433 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-29 02:01:05,433 [DEBUG] 
Evaluating link: ./contact
2024-11-29 02:01:05,433 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,433 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-11-29 02:01:05,433 [DEBUG] Absolute link: https://other-domain.com/contact
2024-11-29 02:01:05,433 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,433 [DEBUG] 
Evaluating link: ./
2024-11-29 02:01:05,433 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,433 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-29 02:01:05,433 [DEBUG] Absolute link: https://other-domain.com/
2024-11-29 02:01:05,433 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,433 [DEBUG] 
Evaluating link: ./hiyou.php
2024-11-29 02:01:05,433 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,433 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-11-29 02:01:05,433 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-11-29 02:01:05,433 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,433 [DEBUG] 
Evaluating link: ./date.php
2024-11-29 02:01:05,433 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,433 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-11-29 02:01:05,433 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-11-29 02:01:05,433 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,433 [DEBUG] 
Evaluating link: #u
2024-11-29 02:01:05,433 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,433 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-11-29 02:01:05,433 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,433 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,433 [DEBUG] 
Evaluating link: #v
2024-11-29 02:01:05,433 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,433 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-11-29 02:01:05,434 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,434 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,434 [DEBUG] 
Evaluating link: #w
2024-11-29 02:01:05,434 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,434 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-11-29 02:01:05,434 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,434 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,434 [DEBUG] 
Evaluating link: #x
2024-11-29 02:01:05,434 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,434 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-11-29 02:01:05,434 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,434 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,434 [DEBUG] 
Evaluating link: #y
2024-11-29 02:01:05,434 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,434 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-11-29 02:01:05,434 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,434 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,434 [DEBUG] 
Evaluating link: #p
2024-11-29 02:01:05,434 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,434 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-11-29 02:01:05,434 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,434 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,434 [DEBUG] 
Evaluating link: #q
2024-11-29 02:01:05,434 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,434 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-11-29 02:01:05,434 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,434 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,434 [DEBUG] 
Evaluating link: #r
2024-11-29 02:01:05,434 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,434 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-11-29 02:01:05,434 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,434 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,434 [DEBUG] 
Evaluating link: #s
2024-11-29 02:01:05,434 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,434 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-11-29 02:01:05,434 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,434 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,434 [DEBUG] 
Evaluating link: #t
2024-11-29 02:01:05,434 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,434 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-11-29 02:01:05,434 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,434 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,434 [DEBUG] 
Evaluating link: #k
2024-11-29 02:01:05,434 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,434 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-11-29 02:01:05,434 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,434 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,434 [DEBUG] 
Evaluating link: #l
2024-11-29 02:01:05,434 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,434 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-11-29 02:01:05,435 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,435 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,435 [DEBUG] 
Evaluating link: #m
2024-11-29 02:01:05,435 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,435 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-11-29 02:01:05,435 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,435 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,435 [DEBUG] 
Evaluating link: #n
2024-11-29 02:01:05,435 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,435 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-11-29 02:01:05,435 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,435 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,435 [DEBUG] 
Evaluating link: #o
2024-11-29 02:01:05,435 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,435 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-11-29 02:01:05,435 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,435 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,435 [DEBUG] 
Evaluating link: #f
2024-11-29 02:01:05,435 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,435 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-11-29 02:01:05,435 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,435 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,435 [DEBUG] 
Evaluating link: #g
2024-11-29 02:01:05,435 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,435 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-11-29 02:01:05,435 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,435 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,435 [DEBUG] 
Evaluating link: #h
2024-11-29 02:01:05,435 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,435 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-11-29 02:01:05,435 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,435 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,435 [DEBUG] 
Evaluating link: #i
2024-11-29 02:01:05,435 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,435 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-11-29 02:01:05,435 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,435 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,435 [DEBUG] 
Evaluating link: #j
2024-11-29 02:01:05,435 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,435 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-11-29 02:01:05,435 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,435 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,435 [DEBUG] 
Evaluating link: #b
2024-11-29 02:01:05,435 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,435 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-11-29 02:01:05,435 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,436 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,436 [DEBUG] 
Evaluating link: #c
2024-11-29 02:01:05,436 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,436 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-11-29 02:01:05,436 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,436 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,436 [DEBUG] 
Evaluating link: #d
2024-11-29 02:01:05,436 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,436 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-11-29 02:01:05,436 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,436 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,436 [DEBUG] 
Evaluating link: #e
2024-11-29 02:01:05,436 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,436 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-11-29 02:01:05,436 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,436 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,436 [DEBUG] 
Evaluating link: #site
2024-11-29 02:01:05,436 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:01:05,436 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2024-11-29 02:01:05,436 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:01:05,436 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:01:05,436 [INFO] Successfully crawled https://other-domain.com/page
2024-11-29 02:01:05,439 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:01:05,440 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:01:05,440 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:01:05,441 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-11-29 02:01:05,441 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-11-29 02:01:05,441 [INFO] Starting crawl of URL: https://example.com/page0
2024-11-29 02:01:05,921 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:01:05,921 [DEBUG] Base URL: https://example.com/page0
2024-11-29 02:01:05,921 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:01:05,921 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:01:05,921 [DEBUG] Initial domain set to: example.com
2024-11-29 02:01:05,921 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:01:05,921 [INFO] Successfully crawled https://example.com/page0
2024-11-29 02:01:05,921 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-29 02:01:05,921 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-29 02:01:05,921 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-29 02:01:06,077 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:01:06,077 [DEBUG] Base URL: https://example.com/page1
2024-11-29 02:01:06,078 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:01:06,078 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:01:06,078 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:01:06,078 [INFO] Successfully crawled https://example.com/page1
2024-11-29 02:01:06,078 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-29 02:01:06,078 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-29 02:01:06,078 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-29 02:01:06,575 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:01:06,575 [DEBUG] Base URL: https://example.com/page2
2024-11-29 02:01:06,575 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:01:06,575 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:01:06,575 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:01:06,575 [INFO] Successfully crawled https://example.com/page2
2024-11-29 02:01:06,575 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-29 02:01:06,575 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-29 02:01:06,575 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-29 02:01:07,079 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:01:07,079 [DEBUG] Base URL: https://example.com/page3
2024-11-29 02:01:07,080 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:01:07,080 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:01:07,080 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:01:07,080 [INFO] Successfully crawled https://example.com/page3
2024-11-29 02:01:07,080 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-11-29 02:01:07,080 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-11-29 02:01:07,080 [INFO] Starting crawl of URL: https://example.com/page4
2024-11-29 02:01:07,578 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:01:07,578 [DEBUG] Base URL: https://example.com/page4
2024-11-29 02:01:07,578 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:01:07,578 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:01:07,578 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:01:07,578 [INFO] Successfully crawled https://example.com/page4
2024-11-29 02:01:07,578 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:01:07,578 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-11-29 02:01:07,578 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-11-29 02:01:07,579 [INFO] Starting crawl of URL: https://example.com/page0
2024-11-29 02:01:08,072 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:01:08,072 [DEBUG] Base URL: https://example.com/page0
2024-11-29 02:01:08,072 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:01:08,072 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:01:08,072 [DEBUG] Initial domain set to: example.com
2024-11-29 02:01:08,072 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:01:08,072 [INFO] Successfully crawled https://example.com/page0
2024-11-29 02:01:08,072 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-29 02:01:08,072 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-29 02:01:08,072 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-29 02:01:08,216 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:01:08,216 [DEBUG] Base URL: https://example.com/page1
2024-11-29 02:01:08,216 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:01:08,216 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:01:08,216 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:01:08,216 [INFO] Successfully crawled https://example.com/page1
2024-11-29 02:01:08,216 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-29 02:01:08,216 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-29 02:01:08,216 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-29 02:01:08,715 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:01:08,715 [DEBUG] Base URL: https://example.com/page2
2024-11-29 02:01:08,715 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:01:08,715 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:01:08,715 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:01:08,715 [INFO] Successfully crawled https://example.com/page2
2024-11-29 02:01:08,715 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-29 02:01:08,715 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-29 02:01:08,715 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-29 02:01:09,216 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:01:09,216 [DEBUG] Base URL: https://example.com/page3
2024-11-29 02:01:09,216 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:01:09,216 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:01:09,216 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:01:09,216 [INFO] Successfully crawled https://example.com/page3
2024-11-29 02:01:09,216 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-11-29 02:01:09,216 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-11-29 02:01:09,216 [INFO] Starting crawl of URL: https://example.com/page4
2024-11-29 02:01:09,716 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x12ecb2e10>
2024-11-29 02:01:09,716 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x12ecac130>, 292961.538954041)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x12ecacf30>, 292962.524824041)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x12ecad010>, 292964.763536416)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x12ecad160>, 292965.972306083)]']
connector: <aiohttp.connector.TCPConnector object at 0x12ed355d0>
2024-11-29 02:01:09,718 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:01:09,718 [DEBUG] Base URL: https://example.com/page4
2024-11-29 02:01:09,718 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:01:09,718 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:01:09,718 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:01:09,718 [INFO] Successfully crawled https://example.com/page4
2024-11-29 02:01:09,721 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:01:09,722 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:01:09,722 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 02:01:09,726 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:01:09,726 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:01:09,727 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 02:01:09,727 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 02:01:09,727 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 02:01:09,727 [INFO] Starting crawl of URL: https://example.com
2024-11-29 02:01:11,729 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:01:19,744 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:01:19,745 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:01:19,748 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:01:19,751 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:01:19,752 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 02:01:19,752 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-29 02:01:19,752 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-29 02:01:19,752 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-29 02:01:19,752 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-29 02:01:19,752 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-29 02:01:19,752 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-29 02:01:21,756 [WARNING] Retry 1/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:01:31,768 [WARNING] Retry 1/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:01:39,771 [WARNING] Retry 2/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:01:39,771 [ERROR] Error crawling https://example.com/page1: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:01:49,775 [WARNING] Retry 2/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:01:49,776 [ERROR] Error crawling https://example.com/page2: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:01:49,784 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:01:49,784 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:01:49,785 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 02:01:49,786 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-29 02:01:49,786 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-29 02:01:49,786 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-29 02:01:49,786 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-29 02:01:49,786 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-29 02:01:49,786 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-29 02:01:49,786 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-29 02:01:49,786 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-29 02:01:49,786 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-29 02:01:51,788 [WARNING] Retry 1/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:02:01,790 [WARNING] Retry 1/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:02:11,790 [WARNING] Retry 1/2 for https://example.com/page3: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:02:19,791 [WARNING] Retry 2/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:02:19,791 [ERROR] Error crawling https://example.com/page1: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:02:29,792 [WARNING] Retry 2/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:02:29,793 [ERROR] Error crawling https://example.com/page2: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:02:39,793 [WARNING] Retry 2/2 for https://example.com/page3: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:02:39,794 [ERROR] Error crawling https://example.com/page3: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:02:39,794 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:02:39,795 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:02:39,796 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 02:02:39,799 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:02:39,800 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:02:39,800 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 02:02:39,800 [DEBUG] Using raw URL string: https://nonexistent.example.com (type: <class 'str'>)
2024-11-29 02:02:39,800 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='nonexistent.example.com', path='', params='', query='', fragment='')
2024-11-29 02:02:39,800 [INFO] Starting crawl of URL: https://nonexistent.example.com
2024-11-29 02:02:41,802 [WARNING] Retry 1/2 for https://nonexistent.example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:02:49,802 [WARNING] Retry 2/2 for https://nonexistent.example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:02:49,802 [ERROR] Error crawling https://nonexistent.example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:02:49,805 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:02:49,806 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:02:49,807 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 02:02:49,809 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:02:49,810 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:02:49,810 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 02:02:49,811 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 02:02:49,811 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 02:02:49,811 [INFO] Starting crawl of URL: https://example.com
2024-11-29 02:02:51,811 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:02:59,812 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:02:59,812 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:02:59,815 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:02:59,816 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:02:59,817 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 02:02:59,817 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 02:02:59,817 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 02:02:59,817 [INFO] Starting crawl of URL: https://example.com
2024-11-29 02:03:01,817 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:03:09,818 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:03:09,819 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:03:09,825 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:09,847 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:03:09,847 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:09,848 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:09,849 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:03:09,849 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:09,850 [DEBUG] 
Evaluating link: https://example.com/doc1
2024-11-29 02:03:09,850 [DEBUG] Target domain: example.com
2024-11-29 02:03:09,850 [DEBUG] URL domain: example.com
2024-11-29 02:03:09,850 [DEBUG] URL approved for crawling: https://example.com/doc1
2024-11-29 02:03:09,850 [DEBUG] 
Evaluating link: https://example.com/excluded/page
2024-11-29 02:03:09,850 [DEBUG] Target domain: example.com
2024-11-29 02:03:09,850 [DEBUG] URL domain: example.com
2024-11-29 02:03:09,850 [DEBUG] URL approved for crawling: https://example.com/excluded/page
2024-11-29 02:03:09,877 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x12ec4ea10>
2024-11-29 02:03:09,877 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x12ecacc90>, 292968.130749291)]']
connector: <aiohttp.connector.TCPConnector object at 0x12ec4d5d0>
2024-11-29 02:03:09,878 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x12ef0d110>
2024-11-29 02:03:09,878 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x12ecae040>, 292970.268404208)]']
connector: <aiohttp.connector.TCPConnector object at 0x12ec41050>
2024-11-29 02:03:09,881 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:09,882 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:03:09,882 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:09,887 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:09,887 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:03:09,888 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:09,888 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-29 02:03:09,891 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:09,892 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:03:09,892 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:09,896 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:09,897 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:03:09,897 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:09,897 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-29 02:03:09,898 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:09,898 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:03:09,899 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:09,899 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-29 02:03:09,902 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:09,903 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:03:09,903 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:09,903 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-29 02:03:09,907 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:09,907 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:03:09,908 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:09,908 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-29 02:03:09,911 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:09,911 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:03:09,912 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:09,912 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-29 02:03:09,912 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:09,913 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:03:09,913 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:09,913 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-29 02:03:09,920 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:09,966 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:09,970 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:09,970 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:09,971 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:03:09,977 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:09,977 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:09,978 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:03:09,983 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:09,985 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:03:09,992 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:03:09,999 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:03:10,006 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:03:10,013 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:03:10,020 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:03:10,027 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:03:10,034 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:03:10,041 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:03:10,047 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:03:10,135 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:10,292 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:03:10,294 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:03:10,294 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 02:03:10,294 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 02:03:10,294 [INFO] Starting crawl of URL: https://example.com
2024-11-29 02:03:10,789 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:03:10,789 [DEBUG] Base URL: https://example.com
2024-11-29 02:03:10,789 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:03:10,789 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:03:10,789 [DEBUG] Initial domain set to: example.com
2024-11-29 02:03:10,789 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:03:10,789 [INFO] Successfully crawled https://example.com
2024-11-29 02:03:10,789 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x13a7f7110>
2024-11-29 02:03:10,789 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x13f00f4d0>, 293091.299976)]']
connector: <aiohttp.connector.TCPConnector object at 0x13a7f47d0>
2024-11-29 02:03:10,791 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2024-11-29 02:03:12,114 [INFO] Operation took 0.10 seconds
2024-11-29 02:26:58,422 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-29 02:28:23,612 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-29 02:29:38,297 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-11-29 02:29:38,798 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:38,798 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:38,799 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:38,801 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:38,802 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:38,802 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:38,802 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:29:38,826 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:38,826 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:38,827 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:29:38,829 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:38,830 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:38,832 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:38,833 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:38,844 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:38,844 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:38,845 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:38,846 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:38,849 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:38,849 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:38,849 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:29:38,852 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:39,130 [ERROR] Error processing content: 'NoneType' object has no attribute 'strip'
2024-11-29 02:29:39,607 [INFO] Operation took 0.05 seconds
2024-11-29 02:29:39,883 [INFO] test_operation took 0.10 seconds
2024-11-29 02:29:40,087 [INFO] inner took 0.10 seconds
2024-11-29 02:29:40,087 [INFO] outer took 0.20 seconds
2024-11-29 02:29:40,087 [INFO] error_operation took 0.00 seconds
2024-11-29 02:29:40,113 [INFO] test_operation took 0.00 seconds
2024-11-29 02:29:40,254 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:29:40,255 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:40,258 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 02:29:40,259 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 02:29:40,259 [INFO] Starting crawl of URL: https://example.com
2024-11-29 02:29:40,786 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:29:40,786 [DEBUG] Base URL: https://example.com
2024-11-29 02:29:40,787 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:29:40,787 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:29:40,787 [DEBUG] Initial domain set to: example.com
2024-11-29 02:29:40,787 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:29:40,787 [INFO] Successfully crawled https://example.com
2024-11-29 02:29:40,787 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:40,788 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1229ae4d0>
2024-11-29 02:29:40,788 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1228ce270>, 294681.346492458)]']
connector: <aiohttp.connector.TCPConnector object at 0x12260b6d0>
2024-11-29 02:29:40,788 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:29:40,788 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:40,789 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 02:29:40,789 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 02:29:40,789 [INFO] Starting crawl of URL: https://example.com
2024-11-29 02:29:40,806 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-11-29 02:29:40,806 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-11-29 02:29:40,806 [INFO] Starting crawl of URL: https://example.org
2024-11-29 02:29:41,280 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:29:41,280 [DEBUG] Base URL: https://example.com
2024-11-29 02:29:41,280 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:29:41,280 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:29:41,280 [DEBUG] Initial domain set to: example.com
2024-11-29 02:29:41,280 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:29:41,280 [INFO] Successfully crawled https://example.com
2024-11-29 02:29:41,776 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:29:41,776 [DEBUG] Base URL: https://example.org
2024-11-29 02:29:41,776 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:29:41,776 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:29:41,776 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:29:41,776 [INFO] Successfully crawled https://example.org
2024-11-29 02:29:41,776 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:41,777 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1229f19d0>
2024-11-29 02:29:41,777 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1228ce4a0>, 294681.840055291)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x1228cd390>, 294682.335905375)]']
connector: <aiohttp.connector.TCPConnector object at 0x1229f16d0>
2024-11-29 02:29:41,777 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:29:41,777 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:41,778 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 02:29:41,778 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 02:29:41,778 [INFO] Starting crawl of URL: https://example.com
2024-11-29 02:29:42,268 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:29:42,268 [DEBUG] Base URL: https://example.com
2024-11-29 02:29:42,268 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:29:42,268 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:29:42,268 [DEBUG] Initial domain set to: example.com
2024-11-29 02:29:42,269 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:29:42,269 [INFO] Successfully crawled https://example.com
2024-11-29 02:29:42,269 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:42,270 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1228481d0>
2024-11-29 02:29:42,270 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1228cea50>, 294682.828364833)]']
connector: <aiohttp.connector.TCPConnector object at 0x1229af210>
2024-11-29 02:29:42,270 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:29:42,271 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:42,271 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 02:29:42,271 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 02:29:42,271 [INFO] Starting crawl of URL: https://example.com
2024-11-29 02:29:42,766 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:29:42,766 [DEBUG] Base URL: https://example.com
2024-11-29 02:29:42,766 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:29:42,766 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:29:42,767 [DEBUG] Initial domain set to: example.com
2024-11-29 02:29:42,767 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:29:42,767 [INFO] Successfully crawled https://example.com
2024-11-29 02:29:42,767 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:29:42,767 [DEBUG] Base URL: https://example.com
2024-11-29 02:29:42,767 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:29:42,767 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:29:42,767 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:29:42,772 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:42,773 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:29:42,774 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:42,777 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:42,777 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:29:42,777 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:42,778 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 02:29:42,778 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 02:29:42,778 [INFO] Starting crawl of URL: https://example.com
2024-11-29 02:29:43,262 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:29:43,262 [DEBUG] Base URL: https://example.com
2024-11-29 02:29:43,262 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:29:43,262 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:29:43,262 [DEBUG] Initial domain set to: example.com
2024-11-29 02:29:43,262 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:29:43,262 [INFO] Successfully crawled https://example.com
2024-11-29 02:29:43,263 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:43,263 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x122a11810>
2024-11-29 02:29:43,263 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x12280c3d0>, 294683.822011708)]']
connector: <aiohttp.connector.TCPConnector object at 0x122912e10>
2024-11-29 02:29:43,264 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:29:43,264 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:43,264 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2024-11-29 02:29:43,265 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2024-11-29 02:29:43,265 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2024-11-29 02:29:45,345 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-29 02:29:45,347 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-29 02:29:45,347 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-11-29 02:29:45,357 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:45,357 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:29:45,358 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:45,358 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 02:29:45,358 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 02:29:45,358 [INFO] Starting crawl of URL: https://example.com
2024-11-29 02:29:45,381 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-11-29 02:29:45,381 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-11-29 02:29:45,381 [INFO] Starting crawl of URL: https://example.org
2024-11-29 02:29:45,382 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2024-11-29 02:29:45,382 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2024-11-29 02:29:45,382 [INFO] Starting crawl of URL: https://example.net
2024-11-29 02:29:45,847 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:29:45,847 [DEBUG] Base URL: https://example.com
2024-11-29 02:29:45,848 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:29:45,848 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:29:45,848 [DEBUG] Initial domain set to: example.com
2024-11-29 02:29:45,848 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:29:45,848 [INFO] Successfully crawled https://example.com
2024-11-29 02:29:46,359 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:29:46,359 [DEBUG] Base URL: https://example.org
2024-11-29 02:29:46,359 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:29:46,359 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:29:46,359 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:29:46,359 [INFO] Successfully crawled https://example.org
2024-11-29 02:29:46,850 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:29:46,850 [DEBUG] Base URL: https://example.net
2024-11-29 02:29:46,850 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:29:46,850 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:29:46,850 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:29:46,850 [INFO] Successfully crawled https://example.net
2024-11-29 02:29:46,851 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:46,852 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x122a10350>
2024-11-29 02:29:46,852 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x12280fee0>, 294686.407560083)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x12280d550>, 294686.918502625)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x12280c0c0>, 294687.410357041)]']
connector: <aiohttp.connector.TCPConnector object at 0x122a10810>
2024-11-29 02:29:46,852 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:29:46,852 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:46,853 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 02:29:46,853 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 02:29:46,853 [INFO] Starting crawl of URL: https://example.com
2024-11-29 02:29:47,350 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:29:47,350 [DEBUG] Base URL: https://example.com
2024-11-29 02:29:47,350 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:29:47,350 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:29:47,350 [DEBUG] Initial domain set to: example.com
2024-11-29 02:29:47,350 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:29:47,350 [INFO] Successfully crawled https://example.com
2024-11-29 02:29:47,351 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:47,355 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x122b5fb50>
2024-11-29 02:29:47,355 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x1228ce270>, 294683.326425083)]']
connector: <aiohttp.connector.TCPConnector object at 0x122b5ec50>
2024-11-29 02:29:47,355 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x122a11ad0>
2024-11-29 02:29:47,360 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:29:47,360 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:47,361 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:29:47,383 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:29:47,400 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:47,401 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:29:47,401 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:47,401 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:29:47,418 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:47,418 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:29:47,418 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:47,419 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:29:47,419 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2024-11-29 02:29:47,419 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2024-11-29 02:29:47,419 [INFO] Starting crawl of URL: https://example.com/page
2024-11-29 02:29:47,910 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:29:47,910 [DEBUG] Base URL: https://example.com/page
2024-11-29 02:29:47,910 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:29:47,910 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:29:47,911 [DEBUG] Initial domain set to: example.com
2024-11-29 02:29:47,911 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:29:47,911 [INFO] Successfully crawled https://example.com/page
2024-11-29 02:29:47,911 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2024-11-29 02:29:47,911 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2024-11-29 02:29:47,911 [INFO] Starting crawl of URL: https://other-domain.com/page
2024-11-29 02:29:51,862 [DEBUG] 
Evaluating link: ./
2024-11-29 02:29:51,862 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,862 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-29 02:29:51,862 [DEBUG] Absolute link: https://other-domain.com/
2024-11-29 02:29:51,863 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,863 [DEBUG] 
Evaluating link: ./
2024-11-29 02:29:51,863 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,863 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-29 02:29:51,863 [DEBUG] Absolute link: https://other-domain.com/
2024-11-29 02:29:51,863 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,863 [DEBUG] 
Evaluating link: ./hiyou.php
2024-11-29 02:29:51,863 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,863 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-11-29 02:29:51,863 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-11-29 02:29:51,863 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,863 [DEBUG] 
Evaluating link: ./date.php
2024-11-29 02:29:51,863 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,863 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-11-29 02:29:51,863 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-11-29 02:29:51,863 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,863 [DEBUG] 
Evaluating link: ./contact.php
2024-11-29 02:29:51,863 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,863 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2024-11-29 02:29:51,863 [DEBUG] Absolute link: https://other-domain.com/contact.php
2024-11-29 02:29:51,863 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,863 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 02:29:51,863 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,863 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 02:29:51,863 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 02:29:51,863 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-29 02:29:51,863 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:29:51,863 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,863 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:29:51,863 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:29:51,863 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-29 02:29:51,863 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 02:29:51,863 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,864 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 02:29:51,864 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 02:29:51,864 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-29 02:29:51,864 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-29 02:29:51,864 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,864 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 02:29:51,864 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 02:29:51,864 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-29 02:29:51,864 [DEBUG] 
Evaluating link: ./contact
2024-11-29 02:29:51,864 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,864 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-11-29 02:29:51,864 [DEBUG] Absolute link: https://other-domain.com/contact
2024-11-29 02:29:51,864 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,864 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:29:51,864 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,864 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:29:51,864 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:29:51,864 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-29 02:29:51,864 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 02:29:51,864 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,864 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 02:29:51,864 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 02:29:51,864 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-29 02:29:51,864 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-29 02:29:51,864 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,864 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 02:29:51,864 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 02:29:51,864 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-29 02:29:51,864 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-11-29 02:29:51,864 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,864 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:29:51,864 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:29:51,864 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-11-29 02:29:51,864 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 02:29:51,864 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,865 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 02:29:51,865 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 02:29:51,865 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-29 02:29:51,865 [DEBUG] 
Evaluating link: #indexes
2024-11-29 02:29:51,865 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,865 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2024-11-29 02:29:51,865 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,865 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,865 [DEBUG] 
Evaluating link: #a
2024-11-29 02:29:51,865 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,865 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2024-11-29 02:29:51,865 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,865 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,865 [DEBUG] 
Evaluating link: #b
2024-11-29 02:29:51,865 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,865 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-11-29 02:29:51,865 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,865 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,865 [DEBUG] 
Evaluating link: #c
2024-11-29 02:29:51,865 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,865 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-11-29 02:29:51,865 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,865 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,865 [DEBUG] 
Evaluating link: #d
2024-11-29 02:29:51,865 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,865 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-11-29 02:29:51,865 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,865 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,865 [DEBUG] 
Evaluating link: #e
2024-11-29 02:29:51,865 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,865 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-11-29 02:29:51,865 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,865 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,865 [DEBUG] 
Evaluating link: #f
2024-11-29 02:29:51,865 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,865 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-11-29 02:29:51,865 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,865 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,865 [DEBUG] 
Evaluating link: #g
2024-11-29 02:29:51,865 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,865 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-11-29 02:29:51,865 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,866 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,866 [DEBUG] 
Evaluating link: #h
2024-11-29 02:29:51,866 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,866 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-11-29 02:29:51,866 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,866 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,866 [DEBUG] 
Evaluating link: #i
2024-11-29 02:29:51,866 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,866 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-11-29 02:29:51,866 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,866 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,866 [DEBUG] 
Evaluating link: #j
2024-11-29 02:29:51,866 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,866 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-11-29 02:29:51,866 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,866 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,866 [DEBUG] 
Evaluating link: #k
2024-11-29 02:29:51,866 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,866 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-11-29 02:29:51,866 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,866 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,866 [DEBUG] 
Evaluating link: #l
2024-11-29 02:29:51,866 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,866 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-11-29 02:29:51,866 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,866 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,866 [DEBUG] 
Evaluating link: #m
2024-11-29 02:29:51,866 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,866 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-11-29 02:29:51,866 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,866 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,866 [DEBUG] 
Evaluating link: #n
2024-11-29 02:29:51,866 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,866 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-11-29 02:29:51,866 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,866 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,866 [DEBUG] 
Evaluating link: #o
2024-11-29 02:29:51,866 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,866 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-11-29 02:29:51,866 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,866 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,866 [DEBUG] 
Evaluating link: #p
2024-11-29 02:29:51,867 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,867 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-11-29 02:29:51,867 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,867 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,867 [DEBUG] 
Evaluating link: #q
2024-11-29 02:29:51,867 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,867 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-11-29 02:29:51,867 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,867 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,867 [DEBUG] 
Evaluating link: #r
2024-11-29 02:29:51,867 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,867 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-11-29 02:29:51,867 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,867 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,867 [DEBUG] 
Evaluating link: #s
2024-11-29 02:29:51,867 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,867 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-11-29 02:29:51,867 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,867 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,867 [DEBUG] 
Evaluating link: #t
2024-11-29 02:29:51,867 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,867 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-11-29 02:29:51,867 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,867 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,867 [DEBUG] 
Evaluating link: #u
2024-11-29 02:29:51,867 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,867 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-11-29 02:29:51,867 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,867 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,867 [DEBUG] 
Evaluating link: #v
2024-11-29 02:29:51,867 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,867 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-11-29 02:29:51,867 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,867 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,867 [DEBUG] 
Evaluating link: #w
2024-11-29 02:29:51,867 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,867 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-11-29 02:29:51,867 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,867 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,867 [DEBUG] 
Evaluating link: #x
2024-11-29 02:29:51,867 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,867 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-11-29 02:29:51,867 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,867 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,867 [DEBUG] 
Evaluating link: #y
2024-11-29 02:29:51,868 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,868 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-11-29 02:29:51,868 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,868 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,868 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:29:51,868 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,868 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:29:51,868 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:29:51,868 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-29 02:29:51,868 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 02:29:51,868 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,868 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 02:29:51,868 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 02:29:51,868 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-29 02:29:51,868 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-29 02:29:51,868 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,868 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 02:29:51,868 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 02:29:51,868 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-29 02:29:51,868 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-11-29 02:29:51,868 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,868 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:29:51,868 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:29:51,868 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-11-29 02:29:51,868 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 02:29:51,868 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,868 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 02:29:51,868 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-11-29 02:29:51,868 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-11-29 02:29:51,868 [DEBUG] 
Evaluating link: https://spriq.jp/
2024-11-29 02:29:51,868 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,868 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2024-11-29 02:29:51,868 [DEBUG] Absolute link: https://spriq.jp/
2024-11-29 02:29:51,868 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2024-11-29 02:29:51,868 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:29:51,868 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,868 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:29:51,868 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:29:51,868 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-29 02:29:51,868 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:29:51,868 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,868 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:29:51,869 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-11-29 02:29:51,869 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-11-29 02:29:51,869 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 02:29:51,869 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,869 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 02:29:51,869 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-11-29 02:29:51,869 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-11-29 02:29:51,869 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-11-29 02:29:51,869 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,869 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 02:29:51,869 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-11-29 02:29:51,869 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-11-29 02:29:51,869 [DEBUG] 
Evaluating link: ./contact
2024-11-29 02:29:51,869 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,869 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-11-29 02:29:51,869 [DEBUG] Absolute link: https://other-domain.com/contact
2024-11-29 02:29:51,869 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,869 [DEBUG] 
Evaluating link: ./
2024-11-29 02:29:51,869 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,869 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-11-29 02:29:51,869 [DEBUG] Absolute link: https://other-domain.com/
2024-11-29 02:29:51,869 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,869 [DEBUG] 
Evaluating link: ./hiyou.php
2024-11-29 02:29:51,869 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,869 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-11-29 02:29:51,869 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-11-29 02:29:51,869 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,869 [DEBUG] 
Evaluating link: ./date.php
2024-11-29 02:29:51,869 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,869 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-11-29 02:29:51,869 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-11-29 02:29:51,869 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,869 [DEBUG] 
Evaluating link: #u
2024-11-29 02:29:51,869 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,869 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-11-29 02:29:51,869 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,869 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,869 [DEBUG] 
Evaluating link: #v
2024-11-29 02:29:51,869 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,869 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-11-29 02:29:51,869 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,869 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,869 [DEBUG] 
Evaluating link: #w
2024-11-29 02:29:51,869 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,870 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-11-29 02:29:51,870 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,870 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,870 [DEBUG] 
Evaluating link: #x
2024-11-29 02:29:51,870 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,870 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-11-29 02:29:51,870 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,870 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,870 [DEBUG] 
Evaluating link: #y
2024-11-29 02:29:51,870 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,870 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-11-29 02:29:51,870 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,870 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,870 [DEBUG] 
Evaluating link: #p
2024-11-29 02:29:51,870 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,870 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-11-29 02:29:51,870 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,870 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,870 [DEBUG] 
Evaluating link: #q
2024-11-29 02:29:51,870 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,870 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-11-29 02:29:51,870 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,870 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,870 [DEBUG] 
Evaluating link: #r
2024-11-29 02:29:51,870 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,870 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-11-29 02:29:51,870 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,870 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,870 [DEBUG] 
Evaluating link: #s
2024-11-29 02:29:51,870 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,870 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-11-29 02:29:51,870 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,870 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,870 [DEBUG] 
Evaluating link: #t
2024-11-29 02:29:51,870 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,870 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-11-29 02:29:51,870 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,870 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,870 [DEBUG] 
Evaluating link: #k
2024-11-29 02:29:51,870 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,870 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-11-29 02:29:51,870 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,871 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,871 [DEBUG] 
Evaluating link: #l
2024-11-29 02:29:51,871 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,871 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-11-29 02:29:51,871 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,871 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,871 [DEBUG] 
Evaluating link: #m
2024-11-29 02:29:51,871 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,871 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-11-29 02:29:51,871 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,871 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,871 [DEBUG] 
Evaluating link: #n
2024-11-29 02:29:51,871 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,871 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-11-29 02:29:51,871 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,871 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,871 [DEBUG] 
Evaluating link: #o
2024-11-29 02:29:51,871 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,871 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-11-29 02:29:51,871 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,871 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,871 [DEBUG] 
Evaluating link: #f
2024-11-29 02:29:51,871 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,871 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-11-29 02:29:51,871 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,871 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,871 [DEBUG] 
Evaluating link: #g
2024-11-29 02:29:51,871 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,871 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-11-29 02:29:51,871 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,871 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,871 [DEBUG] 
Evaluating link: #h
2024-11-29 02:29:51,871 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,871 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-11-29 02:29:51,871 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,871 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,871 [DEBUG] 
Evaluating link: #i
2024-11-29 02:29:51,871 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,871 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-11-29 02:29:51,871 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,871 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,871 [DEBUG] 
Evaluating link: #j
2024-11-29 02:29:51,871 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,872 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-11-29 02:29:51,872 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,872 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,872 [DEBUG] 
Evaluating link: #b
2024-11-29 02:29:51,872 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,872 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-11-29 02:29:51,872 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,872 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,872 [DEBUG] 
Evaluating link: #c
2024-11-29 02:29:51,872 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,872 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-11-29 02:29:51,872 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,872 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,872 [DEBUG] 
Evaluating link: #d
2024-11-29 02:29:51,872 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,872 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-11-29 02:29:51,872 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,872 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,872 [DEBUG] 
Evaluating link: #e
2024-11-29 02:29:51,872 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,872 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-11-29 02:29:51,872 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,872 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,872 [DEBUG] 
Evaluating link: #site
2024-11-29 02:29:51,872 [DEBUG] Base URL: https://other-domain.com/page
2024-11-29 02:29:51,872 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2024-11-29 02:29:51,872 [DEBUG] Absolute link: https://other-domain.com/page
2024-11-29 02:29:51,872 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-11-29 02:29:51,872 [INFO] Successfully crawled https://other-domain.com/page
2024-11-29 02:29:51,875 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:51,877 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:29:51,877 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:51,877 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-11-29 02:29:51,877 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-11-29 02:29:51,877 [INFO] Starting crawl of URL: https://example.com/page0
2024-11-29 02:29:52,380 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:29:52,380 [DEBUG] Base URL: https://example.com/page0
2024-11-29 02:29:52,381 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:29:52,381 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:29:52,381 [DEBUG] Initial domain set to: example.com
2024-11-29 02:29:52,381 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:29:52,381 [INFO] Successfully crawled https://example.com/page0
2024-11-29 02:29:52,381 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-29 02:29:52,381 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-29 02:29:52,381 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-29 02:29:52,523 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:29:52,524 [DEBUG] Base URL: https://example.com/page1
2024-11-29 02:29:52,524 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:29:52,524 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:29:52,524 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:29:52,524 [INFO] Successfully crawled https://example.com/page1
2024-11-29 02:29:52,524 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-29 02:29:52,524 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-29 02:29:52,525 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-29 02:29:53,027 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:29:53,027 [DEBUG] Base URL: https://example.com/page2
2024-11-29 02:29:53,027 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:29:53,027 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:29:53,027 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:29:53,027 [INFO] Successfully crawled https://example.com/page2
2024-11-29 02:29:53,027 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-29 02:29:53,027 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-29 02:29:53,027 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-29 02:29:53,521 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:29:53,521 [DEBUG] Base URL: https://example.com/page3
2024-11-29 02:29:53,522 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:29:53,522 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:29:53,522 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:29:53,522 [INFO] Successfully crawled https://example.com/page3
2024-11-29 02:29:53,522 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-11-29 02:29:53,522 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-11-29 02:29:53,522 [INFO] Starting crawl of URL: https://example.com/page4
2024-11-29 02:29:54,022 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:29:54,022 [DEBUG] Base URL: https://example.com/page4
2024-11-29 02:29:54,022 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:29:54,022 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:29:54,022 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:29:54,023 [INFO] Successfully crawled https://example.com/page4
2024-11-29 02:29:54,023 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:29:54,023 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-11-29 02:29:54,023 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-11-29 02:29:54,023 [INFO] Starting crawl of URL: https://example.com/page0
2024-11-29 02:29:54,530 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:29:54,531 [DEBUG] Base URL: https://example.com/page0
2024-11-29 02:29:54,531 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:29:54,531 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:29:54,531 [DEBUG] Initial domain set to: example.com
2024-11-29 02:29:54,531 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:29:54,531 [INFO] Successfully crawled https://example.com/page0
2024-11-29 02:29:54,531 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-29 02:29:54,531 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-29 02:29:54,531 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-29 02:29:54,673 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:29:54,673 [DEBUG] Base URL: https://example.com/page1
2024-11-29 02:29:54,673 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:29:54,674 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:29:54,674 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:29:54,674 [INFO] Successfully crawled https://example.com/page1
2024-11-29 02:29:54,674 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-29 02:29:54,674 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-29 02:29:54,674 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-29 02:29:55,171 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:29:55,171 [DEBUG] Base URL: https://example.com/page2
2024-11-29 02:29:55,171 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:29:55,171 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:29:55,171 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:29:55,171 [INFO] Successfully crawled https://example.com/page2
2024-11-29 02:29:55,171 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-29 02:29:55,171 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-29 02:29:55,171 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-29 02:29:55,669 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:29:55,670 [DEBUG] Base URL: https://example.com/page3
2024-11-29 02:29:55,670 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:29:55,670 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:29:55,670 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:29:55,670 [INFO] Successfully crawled https://example.com/page3
2024-11-29 02:29:55,670 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-11-29 02:29:55,670 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-11-29 02:29:55,670 [INFO] Starting crawl of URL: https://example.com/page4
2024-11-29 02:29:56,172 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:29:56,172 [DEBUG] Base URL: https://example.com/page4
2024-11-29 02:29:56,172 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:29:56,172 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:29:56,172 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:29:56,172 [INFO] Successfully crawled https://example.com/page4
2024-11-29 02:29:56,176 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x122937dd0>
2024-11-29 02:29:56,176 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x12280f8c0>, 294688.470682333)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x12280c520>, 294689.763066625)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x12280c600>, 294691.056972458)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x12280c750>, 294692.415047416)]']
connector: <aiohttp.connector.TCPConnector object at 0x122935cd0>
2024-11-29 02:29:56,179 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:56,181 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:56,181 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 02:29:56,195 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:56,196 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:29:56,197 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 02:29:56,197 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 02:29:56,197 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 02:29:56,197 [INFO] Starting crawl of URL: https://example.com
2024-11-29 02:29:58,198 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:30:06,198 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:30:06,199 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:30:06,206 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:30:06,212 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:30:06,213 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 02:30:06,213 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-29 02:30:06,213 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-29 02:30:06,213 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-29 02:30:06,213 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-29 02:30:06,213 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-29 02:30:06,213 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-29 02:30:08,214 [WARNING] Retry 1/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:30:18,214 [WARNING] Retry 1/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:30:26,214 [WARNING] Retry 2/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:30:26,214 [ERROR] Error crawling https://example.com/page1: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:30:36,214 [WARNING] Retry 2/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:30:36,214 [ERROR] Error crawling https://example.com/page2: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:30:36,223 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:30:36,224 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:30:36,225 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 02:30:36,226 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-11-29 02:30:36,226 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-11-29 02:30:36,226 [INFO] Starting crawl of URL: https://example.com/page1
2024-11-29 02:30:36,226 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-11-29 02:30:36,226 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-11-29 02:30:36,226 [INFO] Starting crawl of URL: https://example.com/page2
2024-11-29 02:30:36,226 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-11-29 02:30:36,226 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-11-29 02:30:36,226 [INFO] Starting crawl of URL: https://example.com/page3
2024-11-29 02:30:38,227 [WARNING] Retry 1/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:30:48,227 [WARNING] Retry 1/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:30:58,229 [WARNING] Retry 1/2 for https://example.com/page3: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:31:06,228 [WARNING] Retry 2/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:31:06,229 [ERROR] Error crawling https://example.com/page1: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:31:16,229 [WARNING] Retry 2/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:31:16,229 [ERROR] Error crawling https://example.com/page2: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:31:26,229 [WARNING] Retry 2/2 for https://example.com/page3: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:31:26,230 [ERROR] Error crawling https://example.com/page3: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:31:26,231 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:26,231 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:26,232 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 02:31:26,255 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1229fe610>
2024-11-29 02:31:26,255 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x12280c280>, 294694.581789125)]']
connector: <aiohttp.connector.TCPConnector object at 0x12284e410>
2024-11-29 02:31:26,255 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1229fdc50>
2024-11-29 02:31:26,255 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x12280def0>, 294696.731837708)]']
connector: <aiohttp.connector.TCPConnector object at 0x1229f4150>
2024-11-29 02:31:26,259 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:26,259 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:26,260 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 02:31:26,260 [DEBUG] Using raw URL string: https://nonexistent.example.com (type: <class 'str'>)
2024-11-29 02:31:26,260 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='nonexistent.example.com', path='', params='', query='', fragment='')
2024-11-29 02:31:26,260 [INFO] Starting crawl of URL: https://nonexistent.example.com
2024-11-29 02:31:28,261 [WARNING] Retry 1/2 for https://nonexistent.example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:31:36,261 [WARNING] Retry 2/2 for https://nonexistent.example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:31:36,262 [ERROR] Error crawling https://nonexistent.example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:31:36,266 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:36,266 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:36,267 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 02:31:36,270 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:36,271 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:36,271 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 02:31:36,272 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 02:31:36,272 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 02:31:36,272 [INFO] Starting crawl of URL: https://example.com
2024-11-29 02:31:38,273 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:31:46,272 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:31:46,273 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:31:46,276 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:46,277 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:46,277 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-11-29 02:31:46,278 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 02:31:46,278 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 02:31:46,278 [INFO] Starting crawl of URL: https://example.com
2024-11-29 02:31:48,278 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:31:56,279 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:31:56,279 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-11-29 02:31:56,282 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,295 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:31:56,295 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,296 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,296 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:31:56,297 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,297 [DEBUG] 
Evaluating link: https://example.com/doc1
2024-11-29 02:31:56,297 [DEBUG] Target domain: example.com
2024-11-29 02:31:56,297 [DEBUG] URL domain: example.com
2024-11-29 02:31:56,297 [DEBUG] URL approved for crawling: https://example.com/doc1
2024-11-29 02:31:56,297 [DEBUG] 
Evaluating link: https://example.com/excluded/page
2024-11-29 02:31:56,297 [DEBUG] Target domain: example.com
2024-11-29 02:31:56,297 [DEBUG] URL domain: example.com
2024-11-29 02:31:56,297 [DEBUG] URL approved for crawling: https://example.com/excluded/page
2024-11-29 02:31:56,301 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,302 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:31:56,302 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,305 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,306 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:31:56,306 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,306 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-29 02:31:56,315 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,316 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:31:56,316 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,319 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,320 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:31:56,320 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,320 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-29 02:31:56,329 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,329 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:31:56,329 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,330 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-29 02:31:56,339 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,340 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:31:56,340 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,341 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-29 02:31:56,349 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,350 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:31:56,350 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,350 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-29 02:31:56,359 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,359 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:31:56,360 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,360 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-29 02:31:56,392 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,393 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:31:56,393 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,393 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-11-29 02:31:56,403 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,413 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,416 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,417 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,417 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:31:56,421 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,421 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,422 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:31:56,425 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,428 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:31:56,436 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:31:56,442 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:31:56,448 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:31:56,455 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:31:56,462 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:31:56,469 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:31:56,511 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:31:56,524 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:31:56,531 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-11-29 02:31:56,591 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,748 [DEBUG] Using selector: KqueueSelector
2024-11-29 02:31:56,751 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-11-29 02:31:56,751 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-11-29 02:31:56,751 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-11-29 02:31:56,751 [INFO] Starting crawl of URL: https://example.com
2024-11-29 02:31:57,364 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-11-29 02:31:57,364 [DEBUG] Base URL: https://example.com
2024-11-29 02:31:57,364 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-11-29 02:31:57,364 [DEBUG] Absolute link: https://iana.org/domains/example
2024-11-29 02:31:57,364 [DEBUG] Initial domain set to: example.com
2024-11-29 02:31:57,364 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-11-29 02:31:57,364 [INFO] Successfully crawled https://example.com
2024-11-29 02:31:57,364 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x16e535d90>
2024-11-29 02:31:57,364 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x16e438ad0>, 294817.925260458)]']
connector: <aiohttp.connector.TCPConnector object at 0x16e534c50>
2024-11-29 02:31:57,365 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2024-11-29 02:31:58,723 [INFO] Operation took 0.10 seconds
2024-11-29 02:35:56,506 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 14:21:00,761 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 14:21:01,274 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:01,274 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:01,275 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:01,277 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:01,277 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:01,278 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:01,278 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:21:01,304 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:01,304 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:01,305 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:21:01,307 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:01,308 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:01,311 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:01,311 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:01,314 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:01,315 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:01,315 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:01,316 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:01,318 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:01,319 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:01,319 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:21:01,322 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:02,247 [INFO] Operation took 0.00 seconds
2024-12-02 14:21:02,512 [INFO] test_operation took 0.10 seconds
2024-12-02 14:21:02,723 [INFO] inner took 0.10 seconds
2024-12-02 14:21:02,723 [INFO] outer took 0.21 seconds
2024-12-02 14:21:02,723 [INFO] error_operation took 0.00 seconds
2024-12-02 14:21:02,774 [INFO] test_operation took 0.00 seconds
2024-12-02 14:21:02,932 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:21:02,932 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:02,933 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-02 14:21:02,933 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-02 14:21:02,933 [INFO] Starting crawl of URL: https://example.com
2024-12-02 14:21:03,473 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:21:03,473 [DEBUG] Base URL: https://example.com
2024-12-02 14:21:03,473 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:21:03,473 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:21:03,473 [DEBUG] Initial domain set to: example.com
2024-12-02 14:21:03,473 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:21:03,473 [INFO] Successfully crawled https://example.com
2024-12-02 14:21:03,473 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:03,474 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x168dc9890>
2024-12-02 14:21:03,474 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x168fc3e70>, 420087.793509833)]']
connector: <aiohttp.connector.TCPConnector object at 0x168f5bf50>
2024-12-02 14:21:03,474 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:21:03,474 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:03,475 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-02 14:21:03,475 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-02 14:21:03,475 [INFO] Starting crawl of URL: https://example.com
2024-12-02 14:21:03,520 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-12-02 14:21:03,520 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-12-02 14:21:03,520 [INFO] Starting crawl of URL: https://example.org
2024-12-02 14:21:04,079 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:21:04,079 [DEBUG] Base URL: https://example.com
2024-12-02 14:21:04,079 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:21:04,079 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:21:04,079 [DEBUG] Initial domain set to: example.com
2024-12-02 14:21:04,079 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:21:04,079 [INFO] Successfully crawled https://example.com
2024-12-02 14:21:04,803 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:21:04,803 [DEBUG] Base URL: https://example.org
2024-12-02 14:21:04,803 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:21:04,803 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:21:04,804 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:21:04,804 [INFO] Successfully crawled https://example.org
2024-12-02 14:21:04,804 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:04,805 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1690006d0>
2024-12-02 14:21:04,805 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x168fc3e70>, 420088.399741041)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x16913d470>, 420089.123345833)]']
connector: <aiohttp.connector.TCPConnector object at 0x1690012d0>
2024-12-02 14:21:04,805 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:21:04,805 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:04,806 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-02 14:21:04,806 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-02 14:21:04,806 [INFO] Starting crawl of URL: https://example.com
2024-12-02 14:21:05,534 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:21:05,535 [DEBUG] Base URL: https://example.com
2024-12-02 14:21:05,535 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:21:05,535 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:21:05,535 [DEBUG] Initial domain set to: example.com
2024-12-02 14:21:05,535 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:21:05,535 [INFO] Successfully crawled https://example.com
2024-12-02 14:21:05,535 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:05,536 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x168f586d0>
2024-12-02 14:21:05,536 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x16913ed60>, 420089.855342333)]']
connector: <aiohttp.connector.TCPConnector object at 0x168f58c10>
2024-12-02 14:21:05,536 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:21:05,536 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:05,537 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-02 14:21:05,537 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-02 14:21:05,537 [INFO] Starting crawl of URL: https://example.com
2024-12-02 14:21:06,369 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:21:06,369 [DEBUG] Base URL: https://example.com
2024-12-02 14:21:06,369 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:21:06,369 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:21:06,369 [DEBUG] Initial domain set to: example.com
2024-12-02 14:21:06,369 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:21:06,369 [INFO] Successfully crawled https://example.com
2024-12-02 14:21:06,373 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:21:06,373 [DEBUG] Base URL: https://example.com
2024-12-02 14:21:06,373 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:21:06,373 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:21:06,373 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:21:06,377 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:06,377 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:21:06,378 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:06,380 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:06,381 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:21:06,381 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:06,381 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-02 14:21:06,381 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-02 14:21:06,381 [INFO] Starting crawl of URL: https://example.com
2024-12-02 14:21:07,023 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:21:07,024 [DEBUG] Base URL: https://example.com
2024-12-02 14:21:07,024 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:21:07,024 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:21:07,024 [DEBUG] Initial domain set to: example.com
2024-12-02 14:21:07,024 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:21:07,024 [INFO] Successfully crawled https://example.com
2024-12-02 14:21:07,024 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:07,024 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x168f3bf50>
2024-12-02 14:21:07,025 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x16913dbe0>, 420091.344320666)]']
connector: <aiohttp.connector.TCPConnector object at 0x168f6ec10>
2024-12-02 14:21:07,025 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:21:07,025 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:07,025 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2024-12-02 14:21:07,025 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2024-12-02 14:21:07,025 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2024-12-02 14:21:09,118 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-02 14:21:09,126 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-02 14:21:09,126 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-02 14:21:09,133 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1691ef250>
2024-12-02 14:21:09,133 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x16913ecf0>, 420090.688917875)]']
connector: <aiohttp.connector.TCPConnector object at 0x1691eecd0>
2024-12-02 14:21:09,136 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:09,137 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:21:09,137 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:09,137 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-02 14:21:09,138 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-02 14:21:09,138 [INFO] Starting crawl of URL: https://example.com
2024-12-02 14:21:09,155 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-12-02 14:21:09,155 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-12-02 14:21:09,155 [INFO] Starting crawl of URL: https://example.org
2024-12-02 14:21:09,155 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2024-12-02 14:21:09,155 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2024-12-02 14:21:09,155 [INFO] Starting crawl of URL: https://example.net
2024-12-02 14:21:09,646 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:21:09,646 [DEBUG] Base URL: https://example.com
2024-12-02 14:21:09,646 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:21:09,646 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:21:09,646 [DEBUG] Initial domain set to: example.com
2024-12-02 14:21:09,646 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:21:09,646 [INFO] Successfully crawled https://example.com
2024-12-02 14:21:10,147 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:21:10,147 [DEBUG] Base URL: https://example.org
2024-12-02 14:21:10,147 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:21:10,147 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:21:10,147 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:21:10,147 [INFO] Successfully crawled https://example.org
2024-12-02 14:21:10,742 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:21:10,742 [DEBUG] Base URL: https://example.net
2024-12-02 14:21:10,742 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:21:10,743 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:21:10,743 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:21:10,743 [INFO] Successfully crawled https://example.net
2024-12-02 14:21:10,743 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:10,744 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x169158950>
2024-12-02 14:21:10,744 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x16913def0>, 420093.966656)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x16913d6a0>, 420094.467919208)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x16913d860>, 420095.06299625)]']
connector: <aiohttp.connector.TCPConnector object at 0x16915b450>
2024-12-02 14:21:10,744 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:21:10,744 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:10,745 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-02 14:21:10,745 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-02 14:21:10,745 [INFO] Starting crawl of URL: https://example.com
2024-12-02 14:21:11,242 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:21:11,242 [DEBUG] Base URL: https://example.com
2024-12-02 14:21:11,242 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:21:11,242 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:21:11,242 [DEBUG] Initial domain set to: example.com
2024-12-02 14:21:11,242 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:21:11,242 [INFO] Successfully crawled https://example.com
2024-12-02 14:21:11,243 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:11,246 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:21:11,246 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:11,247 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:21:11,272 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:21:11,290 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:11,291 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:21:11,291 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:11,292 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:21:11,309 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:11,311 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:21:11,311 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:11,312 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:21:11,313 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2024-12-02 14:21:11,314 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2024-12-02 14:21:11,314 [INFO] Starting crawl of URL: https://example.com/page
2024-12-02 14:21:11,827 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:21:11,827 [DEBUG] Base URL: https://example.com/page
2024-12-02 14:21:11,827 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:21:11,827 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:21:11,827 [DEBUG] Initial domain set to: example.com
2024-12-02 14:21:11,827 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:21:11,827 [INFO] Successfully crawled https://example.com/page
2024-12-02 14:21:11,827 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2024-12-02 14:21:11,827 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2024-12-02 14:21:11,827 [INFO] Starting crawl of URL: https://other-domain.com/page
2024-12-02 14:21:18,450 [DEBUG] 
Evaluating link: ./
2024-12-02 14:21:18,451 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,451 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-02 14:21:18,451 [DEBUG] Absolute link: https://other-domain.com/
2024-12-02 14:21:18,451 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,451 [DEBUG] 
Evaluating link: ./
2024-12-02 14:21:18,451 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,451 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-02 14:21:18,451 [DEBUG] Absolute link: https://other-domain.com/
2024-12-02 14:21:18,451 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,451 [DEBUG] 
Evaluating link: ./hiyou.php
2024-12-02 14:21:18,451 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,451 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-12-02 14:21:18,451 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-12-02 14:21:18,451 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,451 [DEBUG] 
Evaluating link: ./date.php
2024-12-02 14:21:18,451 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,451 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-12-02 14:21:18,451 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-12-02 14:21:18,451 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,451 [DEBUG] 
Evaluating link: ./contact.php
2024-12-02 14:21:18,451 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,451 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2024-12-02 14:21:18,451 [DEBUG] Absolute link: https://other-domain.com/contact.php
2024-12-02 14:21:18,451 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,451 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-02 14:21:18,451 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,451 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-02 14:21:18,451 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-02 14:21:18,452 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-02 14:21:18,452 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:21:18,452 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,452 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:21:18,452 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:21:18,452 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-02 14:21:18,452 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-02 14:21:18,452 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,452 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-02 14:21:18,452 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-02 14:21:18,452 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-02 14:21:18,452 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-02 14:21:18,452 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,452 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-02 14:21:18,452 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-02 14:21:18,452 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-02 14:21:18,452 [DEBUG] 
Evaluating link: ./contact
2024-12-02 14:21:18,452 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,452 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-12-02 14:21:18,452 [DEBUG] Absolute link: https://other-domain.com/contact
2024-12-02 14:21:18,452 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,452 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:21:18,452 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,452 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:21:18,452 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:21:18,452 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-02 14:21:18,452 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-02 14:21:18,452 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,452 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-02 14:21:18,453 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-02 14:21:18,453 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-02 14:21:18,453 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-02 14:21:18,453 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,453 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-02 14:21:18,453 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-02 14:21:18,453 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-02 14:21:18,453 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-12-02 14:21:18,453 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,453 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:21:18,453 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:21:18,453 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-12-02 14:21:18,453 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-02 14:21:18,453 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,453 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-02 14:21:18,453 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-02 14:21:18,453 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-02 14:21:18,453 [DEBUG] 
Evaluating link: #indexes
2024-12-02 14:21:18,453 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,453 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2024-12-02 14:21:18,453 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,453 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,453 [DEBUG] 
Evaluating link: #a
2024-12-02 14:21:18,453 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,453 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2024-12-02 14:21:18,453 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,453 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,453 [DEBUG] 
Evaluating link: #b
2024-12-02 14:21:18,453 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,453 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-12-02 14:21:18,453 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,453 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,453 [DEBUG] 
Evaluating link: #c
2024-12-02 14:21:18,453 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,453 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-12-02 14:21:18,453 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,453 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,453 [DEBUG] 
Evaluating link: #d
2024-12-02 14:21:18,453 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,453 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-12-02 14:21:18,453 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,453 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,454 [DEBUG] 
Evaluating link: #e
2024-12-02 14:21:18,454 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,454 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-12-02 14:21:18,454 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,454 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,454 [DEBUG] 
Evaluating link: #f
2024-12-02 14:21:18,454 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,454 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-12-02 14:21:18,454 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,454 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,454 [DEBUG] 
Evaluating link: #g
2024-12-02 14:21:18,454 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,454 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-12-02 14:21:18,454 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,454 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,454 [DEBUG] 
Evaluating link: #h
2024-12-02 14:21:18,454 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,454 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-12-02 14:21:18,454 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,454 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,454 [DEBUG] 
Evaluating link: #i
2024-12-02 14:21:18,454 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,454 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-12-02 14:21:18,454 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,454 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,454 [DEBUG] 
Evaluating link: #j
2024-12-02 14:21:18,454 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,454 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-12-02 14:21:18,454 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,454 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,454 [DEBUG] 
Evaluating link: #k
2024-12-02 14:21:18,454 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,454 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-12-02 14:21:18,454 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,454 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,454 [DEBUG] 
Evaluating link: #l
2024-12-02 14:21:18,454 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,454 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-12-02 14:21:18,454 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,455 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,455 [DEBUG] 
Evaluating link: #m
2024-12-02 14:21:18,455 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,455 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-12-02 14:21:18,455 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,455 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,455 [DEBUG] 
Evaluating link: #n
2024-12-02 14:21:18,455 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,455 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-12-02 14:21:18,455 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,455 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,455 [DEBUG] 
Evaluating link: #o
2024-12-02 14:21:18,455 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,455 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-12-02 14:21:18,455 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,455 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,455 [DEBUG] 
Evaluating link: #p
2024-12-02 14:21:18,455 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,455 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-12-02 14:21:18,455 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,455 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,455 [DEBUG] 
Evaluating link: #q
2024-12-02 14:21:18,455 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,455 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-12-02 14:21:18,455 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,455 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,455 [DEBUG] 
Evaluating link: #r
2024-12-02 14:21:18,455 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,456 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-12-02 14:21:18,456 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,456 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,456 [DEBUG] 
Evaluating link: #s
2024-12-02 14:21:18,456 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,456 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-12-02 14:21:18,456 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,456 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,456 [DEBUG] 
Evaluating link: #t
2024-12-02 14:21:18,456 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,456 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-12-02 14:21:18,456 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,456 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,456 [DEBUG] 
Evaluating link: #u
2024-12-02 14:21:18,456 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,456 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-12-02 14:21:18,456 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,456 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,456 [DEBUG] 
Evaluating link: #v
2024-12-02 14:21:18,456 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,456 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-12-02 14:21:18,456 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,456 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,456 [DEBUG] 
Evaluating link: #w
2024-12-02 14:21:18,456 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,456 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-12-02 14:21:18,456 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,456 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,456 [DEBUG] 
Evaluating link: #x
2024-12-02 14:21:18,456 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,456 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-12-02 14:21:18,456 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,456 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,456 [DEBUG] 
Evaluating link: #y
2024-12-02 14:21:18,456 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,456 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-12-02 14:21:18,456 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,456 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,456 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:21:18,456 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,457 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:21:18,457 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:21:18,457 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-02 14:21:18,457 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-02 14:21:18,457 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,457 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-02 14:21:18,457 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-02 14:21:18,457 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-02 14:21:18,457 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-02 14:21:18,457 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,457 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-02 14:21:18,457 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-02 14:21:18,457 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-02 14:21:18,457 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-12-02 14:21:18,457 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,457 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:21:18,457 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:21:18,457 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-12-02 14:21:18,457 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-02 14:21:18,457 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,457 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-02 14:21:18,457 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-02 14:21:18,457 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-02 14:21:18,457 [DEBUG] 
Evaluating link: https://spriq.jp/
2024-12-02 14:21:18,457 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,457 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2024-12-02 14:21:18,457 [DEBUG] Absolute link: https://spriq.jp/
2024-12-02 14:21:18,457 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2024-12-02 14:21:18,457 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:21:18,457 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,457 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:21:18,457 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:21:18,457 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-02 14:21:18,457 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:21:18,457 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,457 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:21:18,457 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:21:18,457 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-02 14:21:18,457 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-02 14:21:18,457 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,457 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-02 14:21:18,457 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-02 14:21:18,458 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-02 14:21:18,458 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-02 14:21:18,458 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,458 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-02 14:21:18,458 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-02 14:21:18,458 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-02 14:21:18,458 [DEBUG] 
Evaluating link: ./contact
2024-12-02 14:21:18,458 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,458 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-12-02 14:21:18,458 [DEBUG] Absolute link: https://other-domain.com/contact
2024-12-02 14:21:18,458 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,458 [DEBUG] 
Evaluating link: ./
2024-12-02 14:21:18,458 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,458 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-02 14:21:18,458 [DEBUG] Absolute link: https://other-domain.com/
2024-12-02 14:21:18,458 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,458 [DEBUG] 
Evaluating link: ./hiyou.php
2024-12-02 14:21:18,458 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,458 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-12-02 14:21:18,458 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-12-02 14:21:18,458 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,458 [DEBUG] 
Evaluating link: ./date.php
2024-12-02 14:21:18,458 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,458 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-12-02 14:21:18,458 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-12-02 14:21:18,458 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,458 [DEBUG] 
Evaluating link: #u
2024-12-02 14:21:18,458 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,458 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-12-02 14:21:18,458 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,458 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,458 [DEBUG] 
Evaluating link: #v
2024-12-02 14:21:18,458 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,458 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-12-02 14:21:18,458 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,458 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,458 [DEBUG] 
Evaluating link: #w
2024-12-02 14:21:18,458 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,458 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-12-02 14:21:18,458 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,458 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,458 [DEBUG] 
Evaluating link: #x
2024-12-02 14:21:18,458 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,458 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-12-02 14:21:18,458 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,458 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,459 [DEBUG] 
Evaluating link: #y
2024-12-02 14:21:18,459 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,459 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-12-02 14:21:18,459 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,459 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,459 [DEBUG] 
Evaluating link: #p
2024-12-02 14:21:18,459 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,459 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-12-02 14:21:18,459 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,459 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,459 [DEBUG] 
Evaluating link: #q
2024-12-02 14:21:18,459 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,459 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-12-02 14:21:18,459 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,459 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,459 [DEBUG] 
Evaluating link: #r
2024-12-02 14:21:18,459 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,459 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-12-02 14:21:18,459 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,459 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,459 [DEBUG] 
Evaluating link: #s
2024-12-02 14:21:18,459 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,459 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-12-02 14:21:18,459 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,459 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,459 [DEBUG] 
Evaluating link: #t
2024-12-02 14:21:18,459 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,459 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-12-02 14:21:18,459 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,459 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,459 [DEBUG] 
Evaluating link: #k
2024-12-02 14:21:18,459 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,459 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-12-02 14:21:18,459 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,459 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,459 [DEBUG] 
Evaluating link: #l
2024-12-02 14:21:18,459 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,459 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-12-02 14:21:18,459 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,459 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,459 [DEBUG] 
Evaluating link: #m
2024-12-02 14:21:18,459 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,459 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-12-02 14:21:18,459 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,459 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,459 [DEBUG] 
Evaluating link: #n
2024-12-02 14:21:18,460 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,460 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-12-02 14:21:18,460 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,460 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,460 [DEBUG] 
Evaluating link: #o
2024-12-02 14:21:18,460 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,460 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-12-02 14:21:18,460 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,460 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,460 [DEBUG] 
Evaluating link: #f
2024-12-02 14:21:18,460 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,460 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-12-02 14:21:18,460 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,460 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,460 [DEBUG] 
Evaluating link: #g
2024-12-02 14:21:18,460 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,460 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-12-02 14:21:18,460 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,460 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,460 [DEBUG] 
Evaluating link: #h
2024-12-02 14:21:18,460 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,460 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-12-02 14:21:18,460 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,460 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,460 [DEBUG] 
Evaluating link: #i
2024-12-02 14:21:18,460 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,460 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-12-02 14:21:18,460 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,460 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,460 [DEBUG] 
Evaluating link: #j
2024-12-02 14:21:18,460 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,460 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-12-02 14:21:18,460 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,460 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,460 [DEBUG] 
Evaluating link: #b
2024-12-02 14:21:18,460 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,460 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-12-02 14:21:18,460 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,460 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,460 [DEBUG] 
Evaluating link: #c
2024-12-02 14:21:18,460 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,460 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-12-02 14:21:18,460 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,460 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,460 [DEBUG] 
Evaluating link: #d
2024-12-02 14:21:18,460 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,461 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-12-02 14:21:18,461 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,461 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,461 [DEBUG] 
Evaluating link: #e
2024-12-02 14:21:18,461 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,461 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-12-02 14:21:18,461 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,461 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,461 [DEBUG] 
Evaluating link: #site
2024-12-02 14:21:18,461 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:21:18,461 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2024-12-02 14:21:18,461 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:21:18,461 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:21:18,461 [INFO] Successfully crawled https://other-domain.com/page
2024-12-02 14:21:18,466 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:18,468 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:21:18,468 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:18,468 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-12-02 14:21:18,468 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-12-02 14:21:18,468 [INFO] Starting crawl of URL: https://example.com/page0
2024-12-02 14:21:19,038 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:21:19,038 [DEBUG] Base URL: https://example.com/page0
2024-12-02 14:21:19,038 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:21:19,038 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:21:19,038 [DEBUG] Initial domain set to: example.com
2024-12-02 14:21:19,038 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:21:19,038 [INFO] Successfully crawled https://example.com/page0
2024-12-02 14:21:19,038 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-12-02 14:21:19,038 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-12-02 14:21:19,038 [INFO] Starting crawl of URL: https://example.com/page1
2024-12-02 14:21:19,164 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:21:19,164 [DEBUG] Base URL: https://example.com/page1
2024-12-02 14:21:19,164 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:21:19,164 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:21:19,164 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:21:19,164 [INFO] Successfully crawled https://example.com/page1
2024-12-02 14:21:19,164 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-12-02 14:21:19,164 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-12-02 14:21:19,165 [INFO] Starting crawl of URL: https://example.com/page2
2024-12-02 14:21:19,663 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:21:19,663 [DEBUG] Base URL: https://example.com/page2
2024-12-02 14:21:19,663 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:21:19,663 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:21:19,663 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:21:19,663 [INFO] Successfully crawled https://example.com/page2
2024-12-02 14:21:19,663 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-12-02 14:21:19,663 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-12-02 14:21:19,663 [INFO] Starting crawl of URL: https://example.com/page3
2024-12-02 14:21:20,170 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:21:20,171 [DEBUG] Base URL: https://example.com/page3
2024-12-02 14:21:20,171 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:21:20,171 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:21:20,171 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:21:20,171 [INFO] Successfully crawled https://example.com/page3
2024-12-02 14:21:20,171 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-12-02 14:21:20,171 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-12-02 14:21:20,171 [INFO] Starting crawl of URL: https://example.com/page4
2024-12-02 14:21:20,667 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:21:20,667 [DEBUG] Base URL: https://example.com/page4
2024-12-02 14:21:20,668 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:21:20,668 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:21:20,668 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:21:20,668 [INFO] Successfully crawled https://example.com/page4
2024-12-02 14:21:20,668 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:21:20,668 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-12-02 14:21:20,668 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-12-02 14:21:20,668 [INFO] Starting crawl of URL: https://example.com/page0
2024-12-02 14:21:21,165 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:21:21,166 [DEBUG] Base URL: https://example.com/page0
2024-12-02 14:21:21,166 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:21:21,166 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:21:21,166 [DEBUG] Initial domain set to: example.com
2024-12-02 14:21:21,166 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:21:21,166 [INFO] Successfully crawled https://example.com/page0
2024-12-02 14:21:21,166 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-12-02 14:21:21,166 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-12-02 14:21:21,166 [INFO] Starting crawl of URL: https://example.com/page1
2024-12-02 14:21:21,307 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:21:21,307 [DEBUG] Base URL: https://example.com/page1
2024-12-02 14:21:21,307 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:21:21,307 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:21:21,307 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:21:21,307 [INFO] Successfully crawled https://example.com/page1
2024-12-02 14:21:21,307 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-12-02 14:21:21,307 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-12-02 14:21:21,307 [INFO] Starting crawl of URL: https://example.com/page2
2024-12-02 14:21:21,808 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:21:21,808 [DEBUG] Base URL: https://example.com/page2
2024-12-02 14:21:21,808 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:21:21,808 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:21:21,808 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:21:21,808 [INFO] Successfully crawled https://example.com/page2
2024-12-02 14:21:21,808 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-12-02 14:21:21,808 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-12-02 14:21:21,808 [INFO] Starting crawl of URL: https://example.com/page3
2024-12-02 14:21:22,314 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:21:22,314 [DEBUG] Base URL: https://example.com/page3
2024-12-02 14:21:22,314 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:21:22,314 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:21:22,314 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:21:22,314 [INFO] Successfully crawled https://example.com/page3
2024-12-02 14:21:22,314 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-12-02 14:21:22,314 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-12-02 14:21:22,314 [INFO] Starting crawl of URL: https://example.com/page4
2024-12-02 14:21:22,813 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:21:22,813 [DEBUG] Base URL: https://example.com/page4
2024-12-02 14:21:22,813 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:21:22,813 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:21:22,813 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:21:22,813 [INFO] Successfully crawled https://example.com/page4
2024-12-02 14:21:22,816 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:22,816 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:22,817 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-12-02 14:21:22,822 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:22,822 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:22,823 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-12-02 14:21:22,823 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-02 14:21:22,824 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-02 14:21:22,824 [INFO] Starting crawl of URL: https://example.com
2024-12-02 14:21:24,824 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:21:32,824 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:21:32,825 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:21:32,830 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x169019590>
2024-12-02 14:21:32,830 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x16913dd30>, 420104.98675775)]']
connector: <aiohttp.connector.TCPConnector object at 0x1690192d0>
2024-12-02 14:21:32,831 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x169068350>
2024-12-02 14:21:32,831 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x16913f0e0>, 420107.132964541)]']
connector: <aiohttp.connector.TCPConnector object at 0x16906d650>
2024-12-02 14:21:32,835 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:32,837 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:21:32,838 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-12-02 14:21:32,838 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-12-02 14:21:32,838 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-12-02 14:21:32,838 [INFO] Starting crawl of URL: https://example.com/page1
2024-12-02 14:21:32,838 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-12-02 14:21:32,838 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-12-02 14:21:32,838 [INFO] Starting crawl of URL: https://example.com/page2
2024-12-02 14:21:34,840 [WARNING] Retry 1/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:21:44,841 [WARNING] Retry 1/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:21:52,841 [WARNING] Retry 2/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:21:52,842 [ERROR] Error crawling https://example.com/page1: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:22:02,842 [WARNING] Retry 2/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:22:02,843 [ERROR] Error crawling https://example.com/page2: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:22:02,846 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:22:02,846 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:22:02,847 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-12-02 14:22:02,847 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-12-02 14:22:02,848 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-12-02 14:22:02,848 [INFO] Starting crawl of URL: https://example.com/page1
2024-12-02 14:22:02,848 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-12-02 14:22:02,848 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-12-02 14:22:02,848 [INFO] Starting crawl of URL: https://example.com/page2
2024-12-02 14:22:02,848 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-12-02 14:22:02,848 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-12-02 14:22:02,848 [INFO] Starting crawl of URL: https://example.com/page3
2024-12-02 14:22:04,848 [WARNING] Retry 1/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:22:14,848 [WARNING] Retry 1/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:22:24,849 [WARNING] Retry 1/2 for https://example.com/page3: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:22:32,849 [WARNING] Retry 2/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:22:32,850 [ERROR] Error crawling https://example.com/page1: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:22:42,850 [WARNING] Retry 2/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:22:42,850 [ERROR] Error crawling https://example.com/page2: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:22:52,851 [WARNING] Retry 2/2 for https://example.com/page3: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:22:52,851 [ERROR] Error crawling https://example.com/page3: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:22:52,852 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:22:52,853 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:22:52,854 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-12-02 14:22:52,857 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:22:52,857 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:22:52,858 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-12-02 14:22:52,858 [DEBUG] Using raw URL string: https://nonexistent.example.com (type: <class 'str'>)
2024-12-02 14:22:52,858 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='nonexistent.example.com', path='', params='', query='', fragment='')
2024-12-02 14:22:52,858 [INFO] Starting crawl of URL: https://nonexistent.example.com
2024-12-02 14:22:54,859 [WARNING] Retry 1/2 for https://nonexistent.example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:23:02,859 [WARNING] Retry 2/2 for https://nonexistent.example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:23:02,859 [ERROR] Error crawling https://nonexistent.example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:23:02,863 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:02,863 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:02,864 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-12-02 14:23:02,867 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:02,868 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:02,868 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-12-02 14:23:02,869 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-02 14:23:02,869 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-02 14:23:02,869 [INFO] Starting crawl of URL: https://example.com
2024-12-02 14:23:04,870 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:23:12,869 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:23:12,869 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:23:12,872 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:12,873 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:12,873 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-12-02 14:23:12,873 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-02 14:23:12,874 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-02 14:23:12,874 [INFO] Starting crawl of URL: https://example.com
2024-12-02 14:23:14,874 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:23:22,874 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:23:22,874 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:23:22,878 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:22,893 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:23:22,893 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:22,894 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:22,895 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:23:22,895 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:22,895 [DEBUG] 
Evaluating link: https://example.com/doc1
2024-12-02 14:23:22,896 [DEBUG] Target domain: example.com
2024-12-02 14:23:22,896 [DEBUG] URL domain: example.com
2024-12-02 14:23:22,896 [DEBUG] URL approved for crawling: https://example.com/doc1
2024-12-02 14:23:22,896 [DEBUG] 
Evaluating link: https://example.com/excluded/page
2024-12-02 14:23:22,896 [DEBUG] Target domain: example.com
2024-12-02 14:23:22,896 [DEBUG] URL domain: example.com
2024-12-02 14:23:22,896 [DEBUG] URL approved for crawling: https://example.com/excluded/page
2024-12-02 14:23:22,900 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:22,901 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:23:22,901 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:22,904 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:22,905 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:23:22,905 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:22,906 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-12-02 14:23:22,914 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:22,915 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:23:22,915 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:22,919 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:22,919 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:23:22,920 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:22,920 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-12-02 14:23:22,929 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:22,930 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:23:22,930 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:22,930 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-12-02 14:23:22,954 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x16915a310>
2024-12-02 14:23:22,954 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x169305cd0>
2024-12-02 14:23:22,954 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x16913da20>, 420096.147187791)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x16913ecf0>, 420098.64857375)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x16913de10>, 420100.265513)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x16913df60>, 420102.758717583)]']
connector: <aiohttp.connector.TCPConnector object at 0x169306ed0>
2024-12-02 14:23:22,960 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:22,961 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:23:22,962 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:22,962 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-12-02 14:23:22,971 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:22,971 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:23:22,972 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:22,972 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-12-02 14:23:22,981 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:22,982 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:23:22,982 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:22,982 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-12-02 14:23:22,990 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:22,991 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:23:22,991 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:22,992 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-12-02 14:23:23,000 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:23,009 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:23,013 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:23,013 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:23,014 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:23:23,017 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:23,017 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:23,018 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:23:23,021 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:23,027 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:23:23,064 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:23:23,073 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:23:23,083 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:23:23,090 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:23:23,096 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:23:23,126 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:23:23,132 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:23:23,138 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:23:23,144 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:23:23,204 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:23,354 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:23:23,356 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:23:23,356 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-02 14:23:23,356 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-02 14:23:23,356 [INFO] Starting crawl of URL: https://example.com
2024-12-02 14:23:23,865 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:23:23,865 [DEBUG] Base URL: https://example.com
2024-12-02 14:23:23,865 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:23:23,865 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:23:23,865 [DEBUG] Initial domain set to: example.com
2024-12-02 14:23:23,865 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:23:23,865 [INFO] Successfully crawled https://example.com
2024-12-02 14:23:23,865 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x16bc1ef90>
2024-12-02 14:23:23,865 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x16bc25b70>, 420228.183700583)]']
connector: <aiohttp.connector.TCPConnector object at 0x16bc1dd10>
2024-12-02 14:23:23,867 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2024-12-02 14:23:25,192 [INFO] Operation took 0.10 seconds
2024-12-02 14:26:24,717 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 14:26:25,192 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:25,192 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:25,193 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:25,195 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:25,195 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:25,196 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:25,196 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:26:25,219 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:25,220 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:25,220 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:26:25,222 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:25,223 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:25,226 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:25,226 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:25,229 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:25,229 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:25,230 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:25,230 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:25,233 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:25,233 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:25,233 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:26:25,236 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:26,523 [INFO] Operation took 0.00 seconds
2024-12-02 14:26:26,802 [INFO] test_operation took 0.10 seconds
2024-12-02 14:26:27,009 [INFO] inner took 0.10 seconds
2024-12-02 14:26:27,010 [INFO] outer took 0.21 seconds
2024-12-02 14:26:27,010 [INFO] error_operation took 0.00 seconds
2024-12-02 14:26:27,039 [INFO] test_operation took 0.00 seconds
2024-12-02 14:26:27,186 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:26:27,186 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:27,186 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-02 14:26:27,186 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-02 14:26:27,186 [INFO] Starting crawl of URL: https://example.com
2024-12-02 14:26:27,708 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:26:27,708 [DEBUG] Base URL: https://example.com
2024-12-02 14:26:27,708 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:26:27,708 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:26:27,708 [DEBUG] Initial domain set to: example.com
2024-12-02 14:26:27,708 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:26:27,708 [INFO] Successfully crawled https://example.com
2024-12-02 14:26:27,709 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:27,709 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x146fd4490>
2024-12-02 14:26:27,709 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x146f4d5c0>, 420412.024618541)]']
connector: <aiohttp.connector.TCPConnector object at 0x146b5e390>
2024-12-02 14:26:27,710 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:26:27,710 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:27,710 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-02 14:26:27,710 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-02 14:26:27,710 [INFO] Starting crawl of URL: https://example.com
2024-12-02 14:26:27,725 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-12-02 14:26:27,725 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-12-02 14:26:27,725 [INFO] Starting crawl of URL: https://example.org
2024-12-02 14:26:28,200 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:26:28,200 [DEBUG] Base URL: https://example.com
2024-12-02 14:26:28,200 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:26:28,200 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:26:28,200 [DEBUG] Initial domain set to: example.com
2024-12-02 14:26:28,200 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:26:28,200 [INFO] Successfully crawled https://example.com
2024-12-02 14:26:28,718 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:26:28,718 [DEBUG] Base URL: https://example.org
2024-12-02 14:26:28,718 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:26:28,718 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:26:28,718 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:26:28,718 [INFO] Successfully crawled https://example.org
2024-12-02 14:26:28,718 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:28,719 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x147040650>
2024-12-02 14:26:28,719 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x146f4d7f0>, 420412.51635825)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x146f4fd20>, 420413.034424416)]']
connector: <aiohttp.connector.TCPConnector object at 0x1470422d0>
2024-12-02 14:26:28,719 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:26:28,719 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:28,720 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-02 14:26:28,720 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-02 14:26:28,720 [INFO] Starting crawl of URL: https://example.com
2024-12-02 14:26:29,213 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:26:29,213 [DEBUG] Base URL: https://example.com
2024-12-02 14:26:29,213 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:26:29,213 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:26:29,213 [DEBUG] Initial domain set to: example.com
2024-12-02 14:26:29,213 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:26:29,213 [INFO] Successfully crawled https://example.com
2024-12-02 14:26:29,213 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:29,214 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x146e3e610>
2024-12-02 14:26:29,214 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x146f4d9b0>, 420413.52938625)]']
connector: <aiohttp.connector.TCPConnector object at 0x146e3fc90>
2024-12-02 14:26:29,214 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:26:29,214 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:29,215 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-02 14:26:29,215 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-02 14:26:29,215 [INFO] Starting crawl of URL: https://example.com
2024-12-02 14:26:29,709 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:26:29,709 [DEBUG] Base URL: https://example.com
2024-12-02 14:26:29,709 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:26:29,709 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:26:29,709 [DEBUG] Initial domain set to: example.com
2024-12-02 14:26:29,709 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:26:29,709 [INFO] Successfully crawled https://example.com
2024-12-02 14:26:29,709 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:26:29,709 [DEBUG] Base URL: https://example.com
2024-12-02 14:26:29,709 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:26:29,709 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:26:29,709 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:26:29,714 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:29,715 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:26:29,715 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:29,718 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:29,719 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:26:29,719 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:29,719 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-02 14:26:29,719 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-02 14:26:29,719 [INFO] Starting crawl of URL: https://example.com
2024-12-02 14:26:30,209 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:26:30,210 [DEBUG] Base URL: https://example.com
2024-12-02 14:26:30,210 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:26:30,210 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:26:30,210 [DEBUG] Initial domain set to: example.com
2024-12-02 14:26:30,210 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:26:30,210 [INFO] Successfully crawled https://example.com
2024-12-02 14:26:30,210 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:30,211 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x146d7c750>
2024-12-02 14:26:30,211 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x146f92900>, 420414.5262065)]']
connector: <aiohttp.connector.TCPConnector object at 0x146d7fd10>
2024-12-02 14:26:30,211 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:26:30,211 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:30,212 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2024-12-02 14:26:30,212 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2024-12-02 14:26:30,212 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2024-12-02 14:26:32,230 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-02 14:26:32,232 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-02 14:26:32,232 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-02 14:26:32,238 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x146f7dfd0>
2024-12-02 14:26:32,239 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x146f4fe00>, 420414.025301125)]']
connector: <aiohttp.connector.TCPConnector object at 0x146f7fc90>
2024-12-02 14:26:32,265 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:32,266 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:26:32,267 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:32,267 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-02 14:26:32,267 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-02 14:26:32,267 [INFO] Starting crawl of URL: https://example.com
2024-12-02 14:26:32,283 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-12-02 14:26:32,283 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-12-02 14:26:32,283 [INFO] Starting crawl of URL: https://example.org
2024-12-02 14:26:32,283 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2024-12-02 14:26:32,283 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2024-12-02 14:26:32,283 [INFO] Starting crawl of URL: https://example.net
2024-12-02 14:26:32,768 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:26:32,768 [DEBUG] Base URL: https://example.com
2024-12-02 14:26:32,769 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:26:32,769 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:26:32,769 [DEBUG] Initial domain set to: example.com
2024-12-02 14:26:32,769 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:26:32,769 [INFO] Successfully crawled https://example.com
2024-12-02 14:26:33,247 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:26:33,247 [DEBUG] Base URL: https://example.org
2024-12-02 14:26:33,247 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:26:33,247 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:26:33,247 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:26:33,247 [INFO] Successfully crawled https://example.org
2024-12-02 14:26:33,775 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:26:33,776 [DEBUG] Base URL: https://example.net
2024-12-02 14:26:33,776 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:26:33,776 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:26:33,776 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:26:33,776 [INFO] Successfully crawled https://example.net
2024-12-02 14:26:33,776 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:33,777 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x146f1cd10>
2024-12-02 14:26:33,777 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x146f4f0e0>, 420417.085132666)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x146f4dcc0>, 420417.563489166)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x146f4cd00>, 420418.091989416)]']
connector: <aiohttp.connector.TCPConnector object at 0x146f1d590>
2024-12-02 14:26:33,777 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:26:33,777 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:33,778 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-02 14:26:33,778 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-02 14:26:33,778 [INFO] Starting crawl of URL: https://example.com
2024-12-02 14:26:34,287 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:26:34,287 [DEBUG] Base URL: https://example.com
2024-12-02 14:26:34,287 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:26:34,287 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:26:34,287 [DEBUG] Initial domain set to: example.com
2024-12-02 14:26:34,287 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:26:34,287 [INFO] Successfully crawled https://example.com
2024-12-02 14:26:34,288 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:34,292 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:26:34,293 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:34,293 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:26:34,312 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:26:34,328 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:34,328 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:26:34,328 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:34,329 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:26:34,344 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:34,345 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:26:34,345 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:34,345 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:26:34,345 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2024-12-02 14:26:34,345 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2024-12-02 14:26:34,346 [INFO] Starting crawl of URL: https://example.com/page
2024-12-02 14:26:34,846 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:26:34,846 [DEBUG] Base URL: https://example.com/page
2024-12-02 14:26:34,846 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:26:34,846 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:26:34,846 [DEBUG] Initial domain set to: example.com
2024-12-02 14:26:34,847 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:26:34,847 [INFO] Successfully crawled https://example.com/page
2024-12-02 14:26:34,847 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2024-12-02 14:26:34,847 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2024-12-02 14:26:34,847 [INFO] Starting crawl of URL: https://other-domain.com/page
2024-12-02 14:26:38,003 [DEBUG] 
Evaluating link: ./
2024-12-02 14:26:38,004 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,004 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-02 14:26:38,004 [DEBUG] Absolute link: https://other-domain.com/
2024-12-02 14:26:38,004 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,004 [DEBUG] 
Evaluating link: ./
2024-12-02 14:26:38,004 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,004 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-02 14:26:38,004 [DEBUG] Absolute link: https://other-domain.com/
2024-12-02 14:26:38,004 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,004 [DEBUG] 
Evaluating link: ./hiyou.php
2024-12-02 14:26:38,004 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,004 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-12-02 14:26:38,004 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-12-02 14:26:38,004 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,004 [DEBUG] 
Evaluating link: ./date.php
2024-12-02 14:26:38,004 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,004 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-12-02 14:26:38,004 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-12-02 14:26:38,004 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,004 [DEBUG] 
Evaluating link: ./contact.php
2024-12-02 14:26:38,004 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,004 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2024-12-02 14:26:38,005 [DEBUG] Absolute link: https://other-domain.com/contact.php
2024-12-02 14:26:38,005 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,005 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-02 14:26:38,005 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,005 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-02 14:26:38,005 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-02 14:26:38,005 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-02 14:26:38,005 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:26:38,005 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,005 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:26:38,005 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:26:38,005 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-02 14:26:38,005 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-02 14:26:38,005 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,005 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-02 14:26:38,005 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-02 14:26:38,005 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-02 14:26:38,005 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-02 14:26:38,005 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,005 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-02 14:26:38,005 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-02 14:26:38,005 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-02 14:26:38,005 [DEBUG] 
Evaluating link: ./contact
2024-12-02 14:26:38,005 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,005 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-12-02 14:26:38,005 [DEBUG] Absolute link: https://other-domain.com/contact
2024-12-02 14:26:38,005 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,005 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:26:38,005 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,005 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:26:38,005 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:26:38,005 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-02 14:26:38,005 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-02 14:26:38,005 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,006 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-02 14:26:38,006 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-02 14:26:38,006 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-02 14:26:38,006 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-02 14:26:38,006 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,006 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-02 14:26:38,006 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-02 14:26:38,006 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-02 14:26:38,006 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-12-02 14:26:38,006 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,006 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:26:38,006 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:26:38,006 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-12-02 14:26:38,006 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-02 14:26:38,006 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,006 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-02 14:26:38,006 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-02 14:26:38,006 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-02 14:26:38,006 [DEBUG] 
Evaluating link: #indexes
2024-12-02 14:26:38,006 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,006 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2024-12-02 14:26:38,006 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,006 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,006 [DEBUG] 
Evaluating link: #a
2024-12-02 14:26:38,006 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,006 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2024-12-02 14:26:38,006 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,006 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,006 [DEBUG] 
Evaluating link: #b
2024-12-02 14:26:38,006 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,006 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-12-02 14:26:38,006 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,006 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,006 [DEBUG] 
Evaluating link: #c
2024-12-02 14:26:38,006 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,006 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-12-02 14:26:38,006 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,006 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,006 [DEBUG] 
Evaluating link: #d
2024-12-02 14:26:38,006 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,006 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-12-02 14:26:38,006 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,006 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,006 [DEBUG] 
Evaluating link: #e
2024-12-02 14:26:38,006 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,006 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-12-02 14:26:38,006 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,007 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,007 [DEBUG] 
Evaluating link: #f
2024-12-02 14:26:38,007 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,007 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-12-02 14:26:38,007 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,007 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,007 [DEBUG] 
Evaluating link: #g
2024-12-02 14:26:38,007 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,007 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-12-02 14:26:38,007 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,007 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,007 [DEBUG] 
Evaluating link: #h
2024-12-02 14:26:38,007 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,007 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-12-02 14:26:38,007 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,007 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,007 [DEBUG] 
Evaluating link: #i
2024-12-02 14:26:38,007 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,007 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-12-02 14:26:38,007 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,007 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,007 [DEBUG] 
Evaluating link: #j
2024-12-02 14:26:38,007 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,007 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-12-02 14:26:38,007 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,007 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,007 [DEBUG] 
Evaluating link: #k
2024-12-02 14:26:38,007 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,007 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-12-02 14:26:38,007 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,007 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,007 [DEBUG] 
Evaluating link: #l
2024-12-02 14:26:38,007 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,007 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-12-02 14:26:38,007 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,007 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,007 [DEBUG] 
Evaluating link: #m
2024-12-02 14:26:38,007 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,007 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-12-02 14:26:38,007 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,007 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,007 [DEBUG] 
Evaluating link: #n
2024-12-02 14:26:38,007 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,007 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-12-02 14:26:38,007 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,007 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,007 [DEBUG] 
Evaluating link: #o
2024-12-02 14:26:38,008 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,008 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-12-02 14:26:38,008 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,008 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,008 [DEBUG] 
Evaluating link: #p
2024-12-02 14:26:38,008 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,008 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-12-02 14:26:38,008 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,008 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,008 [DEBUG] 
Evaluating link: #q
2024-12-02 14:26:38,008 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,008 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-12-02 14:26:38,008 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,008 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,008 [DEBUG] 
Evaluating link: #r
2024-12-02 14:26:38,008 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,008 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-12-02 14:26:38,008 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,008 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,008 [DEBUG] 
Evaluating link: #s
2024-12-02 14:26:38,008 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,008 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-12-02 14:26:38,008 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,008 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,008 [DEBUG] 
Evaluating link: #t
2024-12-02 14:26:38,008 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,008 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-12-02 14:26:38,008 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,008 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,008 [DEBUG] 
Evaluating link: #u
2024-12-02 14:26:38,008 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,008 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-12-02 14:26:38,008 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,008 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,008 [DEBUG] 
Evaluating link: #v
2024-12-02 14:26:38,008 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,008 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-12-02 14:26:38,008 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,008 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,008 [DEBUG] 
Evaluating link: #w
2024-12-02 14:26:38,008 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,008 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-12-02 14:26:38,008 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,008 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,008 [DEBUG] 
Evaluating link: #x
2024-12-02 14:26:38,009 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,009 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-12-02 14:26:38,009 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,009 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,009 [DEBUG] 
Evaluating link: #y
2024-12-02 14:26:38,009 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,009 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-12-02 14:26:38,009 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,009 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,009 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:26:38,009 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,009 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:26:38,009 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:26:38,009 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-02 14:26:38,009 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-02 14:26:38,009 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,009 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-02 14:26:38,009 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-02 14:26:38,009 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-02 14:26:38,009 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-02 14:26:38,009 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,009 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-02 14:26:38,009 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-02 14:26:38,009 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-02 14:26:38,009 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-12-02 14:26:38,009 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,009 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:26:38,009 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:26:38,009 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-12-02 14:26:38,009 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-02 14:26:38,009 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,009 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-02 14:26:38,009 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-02 14:26:38,010 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-02 14:26:38,010 [DEBUG] 
Evaluating link: https://spriq.jp/
2024-12-02 14:26:38,010 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,010 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2024-12-02 14:26:38,010 [DEBUG] Absolute link: https://spriq.jp/
2024-12-02 14:26:38,010 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2024-12-02 14:26:38,010 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:26:38,010 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,010 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:26:38,010 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:26:38,010 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-02 14:26:38,010 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:26:38,010 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,010 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:26:38,010 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-02 14:26:38,010 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-02 14:26:38,010 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-02 14:26:38,010 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,010 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-02 14:26:38,010 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-02 14:26:38,010 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-02 14:26:38,010 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-02 14:26:38,010 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,010 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-02 14:26:38,010 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-02 14:26:38,010 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-02 14:26:38,010 [DEBUG] 
Evaluating link: ./contact
2024-12-02 14:26:38,010 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,010 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-12-02 14:26:38,010 [DEBUG] Absolute link: https://other-domain.com/contact
2024-12-02 14:26:38,010 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,010 [DEBUG] 
Evaluating link: ./
2024-12-02 14:26:38,010 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,010 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-02 14:26:38,010 [DEBUG] Absolute link: https://other-domain.com/
2024-12-02 14:26:38,011 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,011 [DEBUG] 
Evaluating link: ./hiyou.php
2024-12-02 14:26:38,011 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,011 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-12-02 14:26:38,011 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-12-02 14:26:38,011 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,011 [DEBUG] 
Evaluating link: ./date.php
2024-12-02 14:26:38,011 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,011 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-12-02 14:26:38,011 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-12-02 14:26:38,011 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,011 [DEBUG] 
Evaluating link: #u
2024-12-02 14:26:38,011 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,011 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-12-02 14:26:38,011 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,011 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,011 [DEBUG] 
Evaluating link: #v
2024-12-02 14:26:38,011 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,011 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-12-02 14:26:38,011 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,011 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,011 [DEBUG] 
Evaluating link: #w
2024-12-02 14:26:38,011 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,011 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-12-02 14:26:38,011 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,011 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,011 [DEBUG] 
Evaluating link: #x
2024-12-02 14:26:38,011 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,011 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-12-02 14:26:38,011 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,011 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,011 [DEBUG] 
Evaluating link: #y
2024-12-02 14:26:38,011 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,011 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-12-02 14:26:38,011 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,012 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,012 [DEBUG] 
Evaluating link: #p
2024-12-02 14:26:38,012 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,012 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-12-02 14:26:38,012 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,012 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,012 [DEBUG] 
Evaluating link: #q
2024-12-02 14:26:38,012 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,012 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-12-02 14:26:38,012 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,012 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,012 [DEBUG] 
Evaluating link: #r
2024-12-02 14:26:38,012 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,012 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-12-02 14:26:38,012 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,012 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,012 [DEBUG] 
Evaluating link: #s
2024-12-02 14:26:38,012 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,012 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-12-02 14:26:38,012 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,012 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,012 [DEBUG] 
Evaluating link: #t
2024-12-02 14:26:38,012 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,012 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-12-02 14:26:38,012 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,012 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,012 [DEBUG] 
Evaluating link: #k
2024-12-02 14:26:38,012 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,012 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-12-02 14:26:38,012 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,012 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,012 [DEBUG] 
Evaluating link: #l
2024-12-02 14:26:38,012 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,012 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-12-02 14:26:38,012 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,012 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,013 [DEBUG] 
Evaluating link: #m
2024-12-02 14:26:38,013 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,013 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-12-02 14:26:38,013 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,013 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,013 [DEBUG] 
Evaluating link: #n
2024-12-02 14:26:38,013 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,013 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-12-02 14:26:38,013 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,013 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,013 [DEBUG] 
Evaluating link: #o
2024-12-02 14:26:38,013 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,013 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-12-02 14:26:38,013 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,013 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,013 [DEBUG] 
Evaluating link: #f
2024-12-02 14:26:38,013 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,013 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-12-02 14:26:38,013 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,013 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,013 [DEBUG] 
Evaluating link: #g
2024-12-02 14:26:38,013 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,013 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-12-02 14:26:38,013 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,013 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,013 [DEBUG] 
Evaluating link: #h
2024-12-02 14:26:38,013 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,013 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-12-02 14:26:38,013 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,013 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,013 [DEBUG] 
Evaluating link: #i
2024-12-02 14:26:38,013 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,013 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-12-02 14:26:38,013 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,013 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,014 [DEBUG] 
Evaluating link: #j
2024-12-02 14:26:38,014 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,014 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-12-02 14:26:38,014 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,014 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,014 [DEBUG] 
Evaluating link: #b
2024-12-02 14:26:38,014 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,014 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-12-02 14:26:38,014 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,014 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,014 [DEBUG] 
Evaluating link: #c
2024-12-02 14:26:38,014 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,014 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-12-02 14:26:38,014 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,014 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,014 [DEBUG] 
Evaluating link: #d
2024-12-02 14:26:38,014 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,014 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-12-02 14:26:38,014 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,014 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,014 [DEBUG] 
Evaluating link: #e
2024-12-02 14:26:38,014 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,014 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-12-02 14:26:38,014 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,014 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,014 [DEBUG] 
Evaluating link: #site
2024-12-02 14:26:38,014 [DEBUG] Base URL: https://other-domain.com/page
2024-12-02 14:26:38,014 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2024-12-02 14:26:38,014 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-02 14:26:38,014 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-02 14:26:38,014 [INFO] Successfully crawled https://other-domain.com/page
2024-12-02 14:26:38,019 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:38,021 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:26:38,021 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:38,021 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-12-02 14:26:38,022 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-12-02 14:26:38,022 [INFO] Starting crawl of URL: https://example.com/page0
2024-12-02 14:26:38,528 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:26:38,528 [DEBUG] Base URL: https://example.com/page0
2024-12-02 14:26:38,528 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:26:38,528 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:26:38,528 [DEBUG] Initial domain set to: example.com
2024-12-02 14:26:38,528 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:26:38,528 [INFO] Successfully crawled https://example.com/page0
2024-12-02 14:26:38,528 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-12-02 14:26:38,528 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-12-02 14:26:38,528 [INFO] Starting crawl of URL: https://example.com/page1
2024-12-02 14:26:38,668 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:26:38,668 [DEBUG] Base URL: https://example.com/page1
2024-12-02 14:26:38,668 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:26:38,668 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:26:38,668 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:26:38,668 [INFO] Successfully crawled https://example.com/page1
2024-12-02 14:26:38,668 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-12-02 14:26:38,668 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-12-02 14:26:38,668 [INFO] Starting crawl of URL: https://example.com/page2
2024-12-02 14:26:39,163 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:26:39,163 [DEBUG] Base URL: https://example.com/page2
2024-12-02 14:26:39,163 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:26:39,163 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:26:39,163 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:26:39,163 [INFO] Successfully crawled https://example.com/page2
2024-12-02 14:26:39,163 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-12-02 14:26:39,163 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-12-02 14:26:39,163 [INFO] Starting crawl of URL: https://example.com/page3
2024-12-02 14:26:39,665 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:26:39,666 [DEBUG] Base URL: https://example.com/page3
2024-12-02 14:26:39,667 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:26:39,667 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:26:39,667 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:26:39,667 [INFO] Successfully crawled https://example.com/page3
2024-12-02 14:26:39,667 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-12-02 14:26:39,668 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-12-02 14:26:39,668 [INFO] Starting crawl of URL: https://example.com/page4
2024-12-02 14:26:40,165 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:26:40,165 [DEBUG] Base URL: https://example.com/page4
2024-12-02 14:26:40,166 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:26:40,166 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:26:40,167 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:26:40,167 [INFO] Successfully crawled https://example.com/page4
2024-12-02 14:26:40,167 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:26:40,167 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-12-02 14:26:40,167 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-12-02 14:26:40,167 [INFO] Starting crawl of URL: https://example.com/page0
2024-12-02 14:26:40,673 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:26:40,673 [DEBUG] Base URL: https://example.com/page0
2024-12-02 14:26:40,674 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:26:40,674 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:26:40,674 [DEBUG] Initial domain set to: example.com
2024-12-02 14:26:40,674 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:26:40,674 [INFO] Successfully crawled https://example.com/page0
2024-12-02 14:26:40,674 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-12-02 14:26:40,674 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-12-02 14:26:40,674 [INFO] Starting crawl of URL: https://example.com/page1
2024-12-02 14:26:40,807 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:26:40,807 [DEBUG] Base URL: https://example.com/page1
2024-12-02 14:26:40,807 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:26:40,807 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:26:40,807 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:26:40,807 [INFO] Successfully crawled https://example.com/page1
2024-12-02 14:26:40,807 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-12-02 14:26:40,807 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-12-02 14:26:40,807 [INFO] Starting crawl of URL: https://example.com/page2
2024-12-02 14:26:41,334 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:26:41,334 [DEBUG] Base URL: https://example.com/page2
2024-12-02 14:26:41,334 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:26:41,334 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:26:41,334 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:26:41,334 [INFO] Successfully crawled https://example.com/page2
2024-12-02 14:26:41,334 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-12-02 14:26:41,334 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-12-02 14:26:41,335 [INFO] Starting crawl of URL: https://example.com/page3
2024-12-02 14:26:41,809 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:26:41,809 [DEBUG] Base URL: https://example.com/page3
2024-12-02 14:26:41,809 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:26:41,809 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:26:41,810 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:26:41,810 [INFO] Successfully crawled https://example.com/page3
2024-12-02 14:26:41,810 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-12-02 14:26:41,810 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-12-02 14:26:41,810 [INFO] Starting crawl of URL: https://example.com/page4
2024-12-02 14:26:42,311 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:26:42,311 [DEBUG] Base URL: https://example.com/page4
2024-12-02 14:26:42,311 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:26:42,311 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:26:42,311 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:26:42,311 [INFO] Successfully crawled https://example.com/page4
2024-12-02 14:26:42,314 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:42,315 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:42,316 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-12-02 14:26:42,320 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:42,320 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:42,321 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-12-02 14:26:42,321 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-02 14:26:42,322 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-02 14:26:42,322 [INFO] Starting crawl of URL: https://example.com
2024-12-02 14:26:44,322 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:26:52,322 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:26:52,323 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:26:52,325 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x147063510>
2024-12-02 14:26:52,326 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x146f4e0b0>, 420424.480934958)]']
connector: <aiohttp.connector.TCPConnector object at 0x146f76290>
2024-12-02 14:26:52,326 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x147061910>
2024-12-02 14:26:52,326 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x146f68130>, 420426.627123291)]']
connector: <aiohttp.connector.TCPConnector object at 0x147060d10>
2024-12-02 14:26:52,329 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:52,331 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:26:52,332 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-12-02 14:26:52,332 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-12-02 14:26:52,332 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-12-02 14:26:52,332 [INFO] Starting crawl of URL: https://example.com/page1
2024-12-02 14:26:52,332 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-12-02 14:26:52,333 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-12-02 14:26:52,333 [INFO] Starting crawl of URL: https://example.com/page2
2024-12-02 14:26:54,333 [WARNING] Retry 1/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:27:04,333 [WARNING] Retry 1/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:27:12,334 [WARNING] Retry 2/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:27:12,336 [ERROR] Error crawling https://example.com/page1: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:27:22,335 [WARNING] Retry 2/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:27:22,336 [ERROR] Error crawling https://example.com/page2: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:27:22,341 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:27:22,342 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:27:22,342 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-12-02 14:27:22,343 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-12-02 14:27:22,343 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-12-02 14:27:22,343 [INFO] Starting crawl of URL: https://example.com/page1
2024-12-02 14:27:22,343 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-12-02 14:27:22,343 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-12-02 14:27:22,343 [INFO] Starting crawl of URL: https://example.com/page2
2024-12-02 14:27:22,343 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-12-02 14:27:22,343 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-12-02 14:27:22,343 [INFO] Starting crawl of URL: https://example.com/page3
2024-12-02 14:27:24,343 [WARNING] Retry 1/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:27:34,345 [WARNING] Retry 1/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:27:44,346 [WARNING] Retry 1/2 for https://example.com/page3: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:27:52,347 [WARNING] Retry 2/2 for https://example.com/page1: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:27:52,347 [ERROR] Error crawling https://example.com/page1: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:28:02,348 [WARNING] Retry 2/2 for https://example.com/page2: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:28:02,348 [ERROR] Error crawling https://example.com/page2: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:28:12,349 [WARNING] Retry 2/2 for https://example.com/page3: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:28:12,349 [ERROR] Error crawling https://example.com/page3: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:28:12,350 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:12,352 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:12,353 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-12-02 14:28:12,358 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:12,358 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:12,359 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-12-02 14:28:12,359 [DEBUG] Using raw URL string: https://nonexistent.example.com (type: <class 'str'>)
2024-12-02 14:28:12,359 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='nonexistent.example.com', path='', params='', query='', fragment='')
2024-12-02 14:28:12,359 [INFO] Starting crawl of URL: https://nonexistent.example.com
2024-12-02 14:28:14,360 [WARNING] Retry 1/2 for https://nonexistent.example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:28:22,360 [WARNING] Retry 2/2 for https://nonexistent.example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:28:22,360 [ERROR] Error crawling https://nonexistent.example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:28:22,365 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:22,366 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:22,367 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-12-02 14:28:22,370 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:22,370 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:22,371 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-12-02 14:28:22,371 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-02 14:28:22,371 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-02 14:28:22,371 [INFO] Starting crawl of URL: https://example.com
2024-12-02 14:28:24,371 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:28:32,372 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:28:32,372 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:28:32,375 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:32,376 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:32,376 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=3, max_pages=100, follow_links=True, rate_limit=0.1, verify_ssl=False, concurrent_requests=2)
2024-12-02 14:28:32,377 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-02 14:28:32,377 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-02 14:28:32,377 [INFO] Starting crawl of URL: https://example.com
2024-12-02 14:28:34,378 [WARNING] Retry 1/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:28:42,377 [WARNING] Retry 2/2 for https://example.com: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:28:42,378 [ERROR] Error crawling https://example.com: Failed after 2 retries: 'coroutine' object does not support the asynchronous context manager protocol
2024-12-02 14:28:42,381 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,395 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:28:42,395 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,396 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,397 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:28:42,397 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,398 [DEBUG] 
Evaluating link: https://example.com/doc1
2024-12-02 14:28:42,398 [DEBUG] Target domain: example.com
2024-12-02 14:28:42,398 [DEBUG] URL domain: example.com
2024-12-02 14:28:42,398 [DEBUG] URL approved for crawling: https://example.com/doc1
2024-12-02 14:28:42,398 [DEBUG] 
Evaluating link: https://example.com/excluded/page
2024-12-02 14:28:42,398 [DEBUG] Target domain: example.com
2024-12-02 14:28:42,398 [DEBUG] URL domain: example.com
2024-12-02 14:28:42,398 [DEBUG] URL approved for crawling: https://example.com/excluded/page
2024-12-02 14:28:42,402 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,403 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:28:42,403 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,407 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,408 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:28:42,408 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,408 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-12-02 14:28:42,416 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,417 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:28:42,417 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,421 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,421 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:28:42,422 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,422 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-12-02 14:28:42,431 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,431 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:28:42,432 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,432 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-12-02 14:28:42,440 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,441 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:28:42,441 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,441 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-12-02 14:28:42,469 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x146f94850>
2024-12-02 14:28:42,470 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1470d8e90>
2024-12-02 14:28:42,470 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x146f4db70>, 420419.162642458)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x146f68050>, 420420.111896958)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x146f681a0>, 420421.083587083)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x146f68210>, 420422.31191025)]']
connector: <aiohttp.connector.TCPConnector object at 0x1470d9b90>
2024-12-02 14:28:42,473 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,474 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:28:42,474 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,475 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-12-02 14:28:42,483 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,484 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:28:42,484 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,485 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-12-02 14:28:42,493 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,493 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:28:42,493 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,494 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2024-12-02 14:28:42,502 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,512 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,516 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,516 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,517 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:28:42,520 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,520 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,520 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:28:42,524 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,534 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:28:42,567 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:28:42,576 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:28:42,583 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:28:42,590 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:28:42,596 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:28:42,603 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:28:42,609 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:28:42,616 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:28:42,623 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-02 14:28:42,709 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,866 [DEBUG] Using selector: KqueueSelector
2024-12-02 14:28:42,869 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-02 14:28:42,869 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-02 14:28:42,869 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-02 14:28:42,869 [INFO] Starting crawl of URL: https://example.com
2024-12-02 14:28:43,382 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-02 14:28:43,382 [DEBUG] Base URL: https://example.com
2024-12-02 14:28:43,383 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-02 14:28:43,383 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-02 14:28:43,383 [DEBUG] Initial domain set to: example.com
2024-12-02 14:28:43,383 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-02 14:28:43,383 [INFO] Successfully crawled https://example.com
2024-12-02 14:28:43,383 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1470eb810>
2024-12-02 14:28:43,384 [ERROR] Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x147e3f7e0>, 420547.693298291)]']
connector: <aiohttp.connector.TCPConnector object at 0x147e48b50>
2024-12-02 14:28:43,388 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2024-12-02 14:28:44,686 [INFO] Operation took 0.10 seconds
2024-12-02 14:30:38,373 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 14:32:35,897 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 14:37:12,876 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 14:49:58,201 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 14:54:04,007 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 15:36:13,198 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 15:36:37,218 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 15:45:44,036 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 15:45:52,185 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 15:47:40,265 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 15:47:45,464 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 15:49:13,942 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 16:05:03,864 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 16:12:04,071 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 16:22:24,724 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 16:27:15,950 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 16:27:16,154 [ERROR] Error processing content: URLProcessor() takes no arguments
2024-12-02 16:27:16,163 [WARNING] Content length exceeds the maximum limit.
2024-12-02 16:27:16,372 [ERROR] Error processing content: URLProcessor() takes no arguments
2024-12-02 16:27:16,422 [ERROR] Error processing content: URLProcessor() takes no arguments
2024-12-02 16:27:16,431 [ERROR] Error processing content: URLProcessor() takes no arguments
2024-12-02 16:27:16,442 [ERROR] Error processing content: URLProcessor() takes no arguments
2024-12-02 16:27:16,451 [ERROR] Error processing content: URLProcessor() takes no arguments
2024-12-02 16:27:16,471 [ERROR] Error processing content: URLProcessor() takes no arguments
2024-12-02 16:27:16,481 [ERROR] Error processing content: URLProcessor() takes no arguments
2024-12-02 16:27:16,490 [ERROR] Error processing content: URLProcessor() takes no arguments
2024-12-02 16:27:16,499 [ERROR] Error processing content: URLProcessor() takes no arguments
2024-12-02 16:27:16,508 [ERROR] Error processing content: URLProcessor() takes no arguments
2024-12-02 16:27:16,517 [ERROR] Error processing content: URLProcessor() takes no arguments
2024-12-02 16:32:18,752 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 16:32:18,953 [ERROR] Error processing content: URLProcessor() takes no arguments
2024-12-02 16:32:18,963 [WARNING] Content length exceeds the maximum limit.
2024-12-02 16:32:19,187 [ERROR] Error processing content: URLProcessor() takes no arguments
2024-12-02 16:32:19,238 [ERROR] Error processing content: URLProcessor() takes no arguments
2024-12-02 16:32:19,248 [ERROR] Error processing content: URLProcessor() takes no arguments
2024-12-02 16:32:19,258 [ERROR] Error processing content: URLProcessor() takes no arguments
2024-12-02 16:32:19,267 [ERROR] Error processing content: URLProcessor() takes no arguments
2024-12-02 16:32:19,287 [ERROR] Error processing content: URLProcessor() takes no arguments
2024-12-02 16:32:19,297 [ERROR] Error processing content: URLProcessor() takes no arguments
2024-12-02 16:32:19,306 [ERROR] Error processing content: URLProcessor() takes no arguments
2024-12-02 16:32:19,315 [ERROR] Error processing content: URLProcessor() takes no arguments
2024-12-02 16:32:19,325 [ERROR] Error processing content: URLProcessor() takes no arguments
2024-12-02 16:32:19,336 [ERROR] Error processing content: URLProcessor() takes no arguments
2024-12-02 16:52:44,126 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 16:52:44,349 [ERROR] Error processing content: 'NoneType' object has no attribute 'strip'
2024-12-02 16:53:13,251 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 16:53:13,482 [ERROR] Error processing content: 'NoneType' object has no attribute 'strip'
2024-12-02 16:55:38,927 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 16:55:39,133 [ERROR] Error processing content: 'NoneType' object has no attribute 'strip'
2024-12-02 16:58:49,812 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 16:58:50,008 [ERROR] Error processing content: 'NoneType' object has no attribute 'strip'
2024-12-02 16:58:59,051 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 16:58:59,134 [ERROR] Error processing content: Tag.find_all() got multiple values for argument 'name'
2024-12-02 16:58:59,218 [ERROR] Error processing content: Tag.find_all() got multiple values for argument 'name'
2024-12-02 16:58:59,259 [ERROR] Error processing content: 'NoneType' object has no attribute 'strip'
2024-12-02 16:58:59,268 [ERROR] Error processing content: Tag.find_all() got multiple values for argument 'name'
2024-12-02 16:58:59,278 [ERROR] Error processing content: Tag.find_all() got multiple values for argument 'name'
2024-12-02 16:58:59,287 [ERROR] Error processing content: Tag.find_all() got multiple values for argument 'name'
2024-12-02 16:58:59,297 [ERROR] Error processing content: Tag.find_all() got multiple values for argument 'name'
2024-12-02 16:58:59,307 [ERROR] Error processing content: Tag.find_all() got multiple values for argument 'name'
2024-12-02 16:58:59,325 [ERROR] Error processing content: Tag.find_all() got multiple values for argument 'name'
2024-12-02 16:58:59,334 [ERROR] Error processing content: Tag.find_all() got multiple values for argument 'name'
2024-12-02 16:58:59,343 [ERROR] Error processing content: Tag.find_all() got multiple values for argument 'name'
2024-12-02 16:58:59,352 [ERROR] Error processing content: Tag.find_all() got multiple values for argument 'name'
2024-12-02 16:58:59,417 [ERROR] Error processing content: Tag.find_all() got multiple values for argument 'name'
2024-12-02 16:58:59,667 [ERROR] Error processing content: Tag.find_all() got multiple values for argument 'name'
2024-12-02 16:58:59,703 [ERROR] Error processing content: Tag.find_all() got multiple values for argument 'name'
2024-12-02 16:58:59,712 [ERROR] Error processing content: Tag.find_all() got multiple values for argument 'name'
2024-12-02 16:58:59,721 [ERROR] Error processing content: Tag.find_all() got multiple values for argument 'name'
2024-12-02 17:02:09,304 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 17:02:09,500 [ERROR] Error processing content: 'NoneType' object has no attribute 'strip'
2024-12-02 17:04:02,101 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 17:04:02,303 [ERROR] Error processing content: 'NoneType' object has no attribute 'strip'
2024-12-02 17:05:23,499 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 17:05:23,691 [ERROR] Error processing content: 'NoneType' object has no attribute 'strip'
2024-12-02 17:06:33,646 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 17:06:33,844 [ERROR] Error processing content: 'NoneType' object has no attribute 'strip'
2024-12-02 17:07:33,730 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 17:07:33,932 [ERROR] Error processing content: 'NoneType' object has no attribute 'strip'
2024-12-02 17:10:40,794 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 17:10:40,993 [ERROR] Error processing content: 'NoneType' object has no attribute 'strip'
2024-12-02 17:12:24,239 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 17:12:24,434 [ERROR] Error processing content: 'NoneType' object has no attribute 'strip'
2024-12-02 17:13:01,847 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 17:13:02,032 [ERROR] Error processing content: 'NoneType' object has no attribute 'strip'
2024-12-02 17:15:05,527 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 17:27:38,073 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 17:27:38,267 [ERROR] Error processing content: 'NoneType' object has no attribute 'strip'
2024-12-02 17:38:19,050 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 17:38:19,127 [ERROR] Error processing content: name 'content' is not defined
2024-12-02 17:38:19,217 [ERROR] Error processing content: name 'content' is not defined
2024-12-02 17:38:19,258 [ERROR] Error processing content: 'NoneType' object has no attribute 'strip'
2024-12-02 17:38:19,267 [ERROR] Error processing content: name 'content' is not defined
2024-12-02 17:38:19,276 [ERROR] Error processing content: name 'content' is not defined
2024-12-02 17:38:19,285 [ERROR] Error processing content: name 'content' is not defined
2024-12-02 17:38:19,294 [ERROR] Error processing content: name 'content' is not defined
2024-12-02 17:38:19,303 [ERROR] Error processing content: name 'content' is not defined
2024-12-02 17:38:19,322 [ERROR] Error processing content: name 'content' is not defined
2024-12-02 17:38:19,332 [ERROR] Error processing content: name 'content' is not defined
2024-12-02 17:38:19,342 [ERROR] Error processing content: name 'content' is not defined
2024-12-02 17:38:19,350 [ERROR] Error processing content: name 'content' is not defined
2024-12-02 17:38:19,400 [ERROR] Error processing content: name 'content' is not defined
2024-12-02 17:38:19,614 [ERROR] Error processing content: name 'content' is not defined
2024-12-02 17:38:19,651 [ERROR] Error processing content: name 'content' is not defined
2024-12-02 17:38:19,660 [ERROR] Error processing content: name 'content' is not defined
2024-12-02 17:38:19,670 [ERROR] Error processing content: name 'content' is not defined
2024-12-02 17:39:03,340 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 17:39:03,567 [ERROR] Error processing content: 'NoneType' object has no attribute 'strip'
2024-12-02 17:44:16,401 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 17:44:16,657 [ERROR] Error processing content: 'NoneType' object has no attribute 'strip'
2024-12-02 17:48:36,284 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 17:48:36,483 [ERROR] Error processing content: 'NoneType' object has no attribute 'strip'
2024-12-02 17:50:36,526 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 17:50:36,722 [ERROR] Error processing content: 'NoneType' object has no attribute 'strip'
2024-12-02 17:54:01,506 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 18:35:11,377 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 21:29:26,129 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 21:31:03,830 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 21:33:03,321 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 21:36:25,966 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 21:37:47,948 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 21:52:06,730 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 22:44:20,807 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 23:01:51,201 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-02 23:01:51,357 [ERROR] Error processing content: name 'Comment' is not defined
2024-12-02 23:01:51,491 [ERROR] Error processing content: name 'Comment' is not defined
2024-12-02 23:01:51,521 [ERROR] Error processing content: name 'Comment' is not defined
2024-12-02 23:01:51,531 [ERROR] Error processing content: name 'Comment' is not defined
2024-12-02 23:01:51,543 [ERROR] Error processing content: name 'Comment' is not defined
2024-12-02 23:01:51,553 [ERROR] Error processing content: name 'Comment' is not defined
2024-12-02 23:01:51,574 [ERROR] Error processing content: name 'Comment' is not defined
2024-12-02 23:01:51,584 [ERROR] Error processing content: name 'Comment' is not defined
2024-12-02 23:01:51,593 [ERROR] Error processing content: name 'Comment' is not defined
2024-12-02 23:01:51,604 [ERROR] Error processing content: name 'Comment' is not defined
2024-12-02 23:01:51,614 [ERROR] Error processing content: name 'Comment' is not defined
2024-12-02 23:01:51,624 [ERROR] Error processing content: name 'Comment' is not defined
2024-12-02 23:01:51,703 [ERROR] Error processing content: name 'Comment' is not defined
2024-12-02 23:01:52,099 [ERROR] Error processing content: name 'Comment' is not defined
2024-12-02 23:01:52,108 [ERROR] Error processing content: name 'Comment' is not defined
2024-12-02 23:01:52,126 [ERROR] Error processing content: name 'Comment' is not defined
2024-12-02 23:05:16,769 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 00:08:51,374 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 00:08:51,455 [ERROR] Error processing content: argument of type 'NoneType' is not iterable
2024-12-03 00:08:51,476 [ERROR] Error processing content: argument of type 'NoneType' is not iterable
2024-12-03 00:08:51,573 [ERROR] Error processing content: argument of type 'NoneType' is not iterable
2024-12-03 00:08:51,648 [ERROR] Error processing content: object of type 'bool' has no len()
2024-12-03 00:35:51,041 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 00:35:51,080 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,101 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,107 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,115 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,121 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,127 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,134 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,152 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,160 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,174 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,181 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,187 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,193 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,199 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,205 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,211 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,218 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,236 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,259 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,268 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,276 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,298 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,305 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,310 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,326 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,332 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,338 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,345 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,351 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,383 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,448 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,454 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,462 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,468 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,480 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,495 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,502 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,508 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,515 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,523 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,537 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,543 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,550 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,556 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,562 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,580 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,587 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,593 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,600 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,606 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,614 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,621 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 00:35:51,627 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,085 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 01:12:48,217 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,242 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,253 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,273 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,295 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,306 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,316 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,327 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,337 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,348 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,421 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,431 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,441 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,460 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,470 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,481 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,495 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,575 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,589 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,602 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,612 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,622 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,642 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,653 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,663 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,677 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,687 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,698 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,719 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,729 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,739 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,749 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,759 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,778 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,797 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,808 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,820 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,830 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,839 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,849 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,871 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,882 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,891 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,901 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,913 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,922 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,932 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:48,972 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,004 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,024 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,058 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,068 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,079 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,091 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,111 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,142 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,151 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,176 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,214 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,228 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,241 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,254 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,268 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,288 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,297 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,307 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,317 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,329 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,343 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,368 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,383 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,408 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,429 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,438 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,458 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,479 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,488 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,498 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,508 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,518 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,539 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,549 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,560 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,571 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,581 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,595 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,618 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,632 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,644 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,654 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,666 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,677 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,687 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,705 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,715 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,725 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,736 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,747 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,756 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,774 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:12:49,784 [ERROR] Error processing content: argument of type 'bool' is not iterable
2024-12-03 01:16:01,580 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 01:16:02,129 [ERROR] Error processing content: Empty content
2024-12-03 01:17:21,391 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 01:17:21,923 [ERROR] Error processing content: Empty content
2024-12-03 01:18:28,277 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 01:18:28,794 [ERROR] Error processing content: Empty content
2024-12-03 01:18:28,858 [ERROR] Error processing content: 'bool' object has no attribute 'find_all'
2024-12-03 01:18:28,914 [ERROR] Error processing content: 'bool' object has no attribute 'find_all'
2024-12-03 01:20:20,016 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 01:20:20,532 [ERROR] Error processing content: Empty content
2024-12-03 01:20:20,594 [ERROR] Error processing content: 'bool' object has no attribute 'find_all'
2024-12-03 01:20:20,646 [ERROR] Error processing content: 'bool' object has no attribute 'find_all'
2024-12-03 01:22:19,296 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 01:22:19,812 [ERROR] Error processing content: Empty content
2024-12-03 01:22:19,914 [ERROR] Error processing content: 'bool' object has no attribute 'find_all'
2024-12-03 01:22:19,969 [ERROR] Error processing content: 'bool' object has no attribute 'find_all'
2024-12-03 01:23:48,036 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 01:23:48,538 [ERROR] Error processing content: Empty content
2024-12-03 01:23:48,598 [ERROR] Error processing content: 'bool' object has no attribute 'find_all'
2024-12-03 01:23:48,651 [ERROR] Error processing content: 'bool' object has no attribute 'find_all'
2024-12-03 01:25:38,255 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 01:25:38,786 [ERROR] Error processing content: Empty content
2024-12-03 01:25:38,882 [ERROR] Error processing content: 'bool' object has no attribute 'find_all'
2024-12-03 01:25:38,936 [ERROR] Error processing content: 'bool' object has no attribute 'find_all'
2024-12-03 01:28:56,022 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 01:28:56,539 [ERROR] Error processing content: Empty content
2024-12-03 01:28:56,602 [ERROR] Error processing content: 'bool' object has no attribute 'find_all'
2024-12-03 01:28:56,655 [ERROR] Error processing content: 'bool' object has no attribute 'find_all'
2024-12-03 01:31:06,779 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 01:32:51,277 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 01:36:10,470 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 01:38:28,860 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 01:40:27,539 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 01:45:08,886 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 01:46:51,542 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 01:49:12,696 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 01:51:23,166 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 01:53:48,229 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 01:56:34,569 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 01:59:49,358 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 02:02:45,038 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 02:05:23,471 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 02:07:24,847 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 02:09:18,013 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 02:10:56,749 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 02:13:56,901 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 02:16:21,766 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 02:18:21,071 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 02:21:25,019 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 02:21:25,502 [ERROR] Error processing content: Cannot replace one element with another when the element to be replaced is not part of a tree.
2024-12-03 02:23:52,165 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 02:26:55,589 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 02:29:26,098 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 02:29:26,151 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,175 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,185 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,216 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,227 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,237 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,247 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,257 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,267 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,277 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,300 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,350 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,370 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,380 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,390 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,400 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,409 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,423 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,437 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,473 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,487 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,500 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,513 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,523 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,542 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,553 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,564 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,574 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,584 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,595 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,605 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,626 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,637 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,649 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,660 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,669 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,679 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,698 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,698 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,708 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,717 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,718 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,729 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,738 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,749 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,758 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,777 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,787 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,797 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,807 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,837 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,857 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,871 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,881 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,899 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,909 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,928 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,938 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,948 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,957 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,966 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:26,976 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,015 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,025 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,035 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,044 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,057 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,080 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,093 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,107 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,120 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,133 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,144 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,154 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,175 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,186 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,195 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,208 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,221 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,231 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,255 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,270 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,280 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,289 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,298 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,308 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,317 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,337 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,347 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,357 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,366 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,367 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,377 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,389 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,410 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,420 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,429 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,440 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,453 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,467 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,481 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,505 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,517 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,529 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,540 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,550 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,559 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,580 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,590 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,615 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,625 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,635 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,646 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:29:27,665 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_inline_code'
2024-12-03 02:34:01,568 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 02:35:45,422 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 02:35:45,472 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,497 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,508 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,518 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,538 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,548 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,560 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,570 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,580 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,589 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,599 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,616 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,626 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,636 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,645 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,655 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,666 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,685 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,695 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,706 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,715 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,724 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,738 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,751 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,774 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,787 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,801 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,814 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,828 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,843 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,862 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,872 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,882 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,891 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,901 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,910 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,920 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,941 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,952 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,963 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,973 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,982 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:45,992 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,014 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,024 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,025 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,036 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,046 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,046 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,057 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,066 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,076 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,095 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,105 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,115 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,124 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,134 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,143 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,154 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,173 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,182 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,196 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,205 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,223 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,247 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,258 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,268 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,280 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,289 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,299 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,300 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,309 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,329 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,340 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,349 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,358 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,387 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,406 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,431 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,446 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,461 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,476 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,490 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,505 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,519 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,538 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,547 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,558 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,568 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,579 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,593 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,617 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,628 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,643 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,658 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,668 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,679 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,689 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,710 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,722 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,733 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,744 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,754 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,764 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,764 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,765 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,787 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,797 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,806 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,817 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,829 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,844 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,859 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,884 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,894 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,905 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,916 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,927 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,938 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,958 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,968 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,978 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:46,988 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:47,000 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:47,001 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:47,012 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:47,022 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:35:47,041 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_extract_title'
2024-12-03 02:37:00,033 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 02:39:47,077 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 02:41:09,739 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 02:42:09,751 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 02:43:55,499 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 09:34:56,866 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 09:34:57,061 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 09:34:57,131 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 09:36:32,473 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 09:36:32,661 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 09:36:32,727 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 09:38:37,545 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 09:38:37,742 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 09:38:37,813 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 09:40:32,228 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 09:40:32,418 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 09:40:32,485 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 09:41:19,010 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 09:41:19,191 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 09:41:19,255 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 09:43:27,110 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 09:43:27,297 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 09:43:27,364 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 09:45:31,563 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 09:45:31,762 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 09:45:31,830 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 09:46:19,280 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 09:46:19,464 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 09:46:19,530 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 09:47:38,829 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 09:50:35,952 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 09:54:04,657 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 09:56:48,830 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 09:56:48,886 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:48,913 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:48,924 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:48,934 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:48,955 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:48,966 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:48,976 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:48,985 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:48,995 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,005 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,015 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,035 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,045 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,056 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,066 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,075 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,086 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,106 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,115 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,125 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,135 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,145 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,158 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,173 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,196 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,209 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,222 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,236 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,250 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,264 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,286 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,296 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,307 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,316 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,326 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,336 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,345 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,364 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,374 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,384 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,394 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,403 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,413 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,432 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,442 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,443 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,452 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,462 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,463 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,472 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,482 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,491 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,512 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,523 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,534 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,544 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,553 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,563 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,584 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,595 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,605 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,628 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,638 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,666 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,679 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,689 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,699 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,709 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,718 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,727 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,728 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,747 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,756 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,767 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,776 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,786 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,796 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,814 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,828 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,842 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,856 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,870 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,884 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,907 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,920 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,930 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,939 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,949 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,959 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,969 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:49,992 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,005 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,015 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,028 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,044 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,053 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,073 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,082 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,092 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,102 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,112 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,121 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,131 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,151 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,151 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,152 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,166 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,176 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,185 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,194 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,205 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,230 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,244 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,258 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,269 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,278 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,288 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,298 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,317 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,327 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,339 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,348 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,359 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,374 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,375 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,386 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,404 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:56:50,415 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_links'
2024-12-03 09:59:11,538 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 10:03:15,293 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 10:04:45,894 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 10:06:34,963 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 10:08:53,344 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 10:10:25,666 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 10:12:11,616 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 10:13:01,683 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 10:16:48,296 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 10:19:09,657 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 10:20:08,037 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 10:22:57,981 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 10:25:27,061 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 10:29:34,432 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 10:35:12,455 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 10:36:20,790 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 10:36:48,296 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 10:36:48,492 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 10:36:48,556 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 10:41:42,620 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 10:41:42,809 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 10:41:42,877 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 10:41:56,987 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 10:41:57,175 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 10:41:57,242 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 10:45:32,399 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 10:45:32,591 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 10:45:32,659 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 10:53:49,927 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 10:53:50,118 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 10:53:50,185 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 10:54:05,609 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 10:54:05,666 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:05,690 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:05,700 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:05,710 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:05,731 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:05,741 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:05,752 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:05,762 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:05,772 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:05,781 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:05,792 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:05,811 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:05,820 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:05,830 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:05,840 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:05,850 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:05,861 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:05,880 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:05,889 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:05,899 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:05,908 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:05,919 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:05,932 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:05,946 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:05,968 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:05,982 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:05,995 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,009 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,023 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,037 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 10:54:06,055 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,066 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,076 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,085 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,095 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,105 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,115 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,137 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,147 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,157 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,166 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,177 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 10:54:06,187 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,210 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,221 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,222 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,232 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,242 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,242 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,253 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,262 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,272 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,290 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,300 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,310 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,320 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,329 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,340 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,360 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,370 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,380 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,404 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,414 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,442 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,455 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,465 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,474 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,484 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,493 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,503 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,504 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,523 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,533 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,544 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,555 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,565 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,575 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,595 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,608 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,622 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,634 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,647 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,661 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,674 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,699 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,709 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,720 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,729 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,739 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,748 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,770 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,784 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,796 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,810 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,823 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,833 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,855 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,865 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,874 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,884 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,893 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,904 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,913 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,932 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,932 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,933 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,947 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,957 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,966 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,975 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,985 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:06,998 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:07,023 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:07,037 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:07,047 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:07,057 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:07,066 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:07,076 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:07,095 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:07,105 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:07,115 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:07,125 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:07,136 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:07,149 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:07,150 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:07,159 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:07,178 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:07,189 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:18,687 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 10:54:18,739 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:18,764 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:18,774 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:18,785 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:18,809 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:18,819 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:18,829 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:18,839 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:18,849 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:18,859 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:18,869 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:18,890 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:18,900 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:18,910 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:18,920 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:18,930 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:18,940 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:18,960 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:18,970 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:18,980 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:18,989 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:18,998 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,013 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,026 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,049 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,062 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,076 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,090 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,103 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,116 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 10:54:19,135 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,145 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,155 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,164 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,174 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,183 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,193 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,214 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,224 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,234 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,245 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,255 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 10:54:19,264 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,284 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,294 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,294 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,304 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,314 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,315 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,325 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,334 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,344 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,365 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,375 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,384 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,394 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,403 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,414 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,434 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,444 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,455 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,479 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,488 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,515 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,527 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,537 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,546 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,555 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,566 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,575 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,576 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,594 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,604 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,614 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,623 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,633 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,642 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,661 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,675 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,688 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,702 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,715 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,728 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,742 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,766 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,776 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,786 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,797 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,807 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,817 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,841 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,855 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,865 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,878 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,891 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,901 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,920 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,930 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,939 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,949 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,959 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,968 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,978 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,996 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,997 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:19,997 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:20,011 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:20,020 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:20,030 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:20,039 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:20,050 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:20,062 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:20,085 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:20,099 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:20,110 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:20,119 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:20,129 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:20,140 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:20,159 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:20,169 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:20,179 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:20,189 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:20,198 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:20,212 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:20,212 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:20,222 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:20,242 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:54:20,253 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:40,825 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 10:55:40,880 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:40,907 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:40,918 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:40,929 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:40,963 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:40,974 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:40,985 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:40,995 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,004 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,014 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,024 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,044 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,055 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,064 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,075 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,084 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,094 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,113 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,123 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,133 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,143 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,152 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,168 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,183 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,205 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,219 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,234 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,247 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,261 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,275 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 10:55:41,296 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,308 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,318 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,327 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,337 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,346 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,356 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,376 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,386 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,395 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,404 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,414 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 10:55:41,423 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,444 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,454 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,455 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,466 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,476 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,477 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,487 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,498 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,508 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,527 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,537 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,547 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,557 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,567 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,576 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,595 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,605 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,615 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,638 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,648 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,676 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,690 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,699 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,708 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,718 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,728 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,738 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,739 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,758 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,767 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,778 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,789 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,800 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,810 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,832 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,848 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,861 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,876 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,890 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,903 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,917 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,941 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,951 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,961 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,972 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,983 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:41,993 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,018 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,032 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,042 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,056 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,070 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,079 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,101 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,112 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,122 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,131 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,142 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,152 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,161 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,180 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,180 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,181 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,197 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,208 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,217 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,228 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,239 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,252 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,276 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,290 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,300 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,310 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,320 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,330 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,350 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,359 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,369 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,378 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,388 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,403 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,404 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,414 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,432 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:55:42,443 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:18,653 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 10:58:18,710 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:18,734 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:18,744 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:18,754 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:18,774 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:18,784 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:18,795 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:18,805 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:18,815 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:18,826 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:18,836 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:18,856 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:18,866 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:18,875 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:18,884 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:18,894 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:18,904 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:18,922 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:18,933 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:18,943 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:18,954 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:18,964 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:18,977 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:18,990 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,011 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,025 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,040 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,054 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,068 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,083 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 10:58:19,104 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,114 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,124 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,134 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,144 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,155 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,164 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,184 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,194 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,204 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,214 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,224 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 10:58:19,234 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,254 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,263 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,264 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,274 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,283 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,284 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,294 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,305 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,314 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,336 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,346 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,357 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,368 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,378 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,389 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,409 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,419 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,430 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,454 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,464 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,496 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,510 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,521 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,530 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,540 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,549 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,559 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,559 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,580 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,591 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,601 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,611 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,621 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,630 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,650 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,665 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,679 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,693 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,705 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,719 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,732 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,758 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,768 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,778 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,788 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,801 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,813 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,837 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,855 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,889 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,905 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,919 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,928 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,947 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,957 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,967 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,978 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:19,991 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:20,002 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:20,013 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:20,044 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:20,044 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:20,045 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:20,059 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:20,070 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:20,082 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:20,093 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:20,104 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:20,119 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:20,144 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:20,159 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:20,170 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:20,181 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:20,191 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:20,202 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:20,224 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:20,235 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:20,246 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:20,259 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:20,269 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:20,286 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:20,286 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:20,298 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:20,320 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:58:20,331 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_lists'
2024-12-03 10:59:49,112 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 11:01:54,865 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 11:03:00,048 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 11:23:41,010 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 11:23:41,199 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 11:23:41,264 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 11:24:31,549 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 11:24:31,734 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 11:24:31,808 [ERROR] Error processing content: 'ContentProcessor' object has no attribute 'assets'
2024-12-03 11:55:05,536 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 11:55:05,594 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:05,621 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:05,632 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:05,643 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:05,667 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:05,678 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:05,689 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:05,700 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:05,710 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:05,720 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:05,730 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:05,750 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:05,761 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:05,773 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:05,783 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:05,794 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:05,804 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:05,824 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:05,836 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:05,846 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:05,855 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:05,866 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:05,879 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:05,893 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:05,916 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:05,930 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:05,945 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:05,959 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:05,972 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:05,985 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,006 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,016 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,027 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,036 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,047 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,057 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,068 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,090 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,100 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,111 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,122 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,132 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,143 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,163 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,173 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,174 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,184 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,195 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,195 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,205 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,215 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,225 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,247 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,257 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,268 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,279 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,289 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,299 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,310 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,330 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,342 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,356 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,366 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,375 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,406 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,421 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,433 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,443 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,453 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,463 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,472 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,473 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,493 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,504 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,515 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,525 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,536 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,546 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,565 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,580 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,595 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,610 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,625 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,639 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,652 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,677 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,688 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,699 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,709 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,719 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,729 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,751 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,766 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,778 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,792 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,806 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,816 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,827 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,847 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,858 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,868 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,878 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,887 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,897 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,918 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,918 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,919 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,935 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,945 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,954 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,964 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,973 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:06,988 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:07,014 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:07,028 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:07,039 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:07,049 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:07,061 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:07,071 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:07,090 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:07,100 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:07,110 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:07,120 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:07,130 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:07,143 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:07,143 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:07,152 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:07,171 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 11:55:07,181 [ERROR] Error processing content: 'ContentProcessor' object has no attribute '_process_definition_lists'
2024-12-03 12:08:25,804 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 12:08:31,668 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 12:11:34,280 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 12:15:41,569 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 12:26:20,005 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 12:29:04,270 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 12:29:12,004 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 12:30:56,218 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 12:31:47,438 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 12:39:21,021 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 12:42:15,384 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 12:44:26,359 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 12:45:40,379 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 12:45:47,465 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 12:46:16,488 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 12:48:40,506 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 12:57:16,417 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 13:00:23,051 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 17:56:46,032 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 17:57:46,244 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 18:02:46,759 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 18:03:33,269 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 18:04:57,352 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 18:07:06,910 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 18:07:12,222 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 18:07:48,125 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 18:07:48,397 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:48,398 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:48,399 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:48,400 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:48,400 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:48,401 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:48,401 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:07:48,433 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:48,433 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:48,434 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:07:48,436 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:48,436 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:48,438 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:48,438 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:48,441 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:48,441 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:48,442 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:48,442 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:48,445 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:48,445 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:48,446 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:07:48,448 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:49,976 [INFO] Operation took 0.03 seconds
2024-12-03 18:07:50,224 [INFO] test_operation took 0.11 seconds
2024-12-03 18:07:50,427 [INFO] inner took 0.10 seconds
2024-12-03 18:07:50,428 [INFO] outer took 0.20 seconds
2024-12-03 18:07:50,428 [INFO] error_operation took 0.00 seconds
2024-12-03 18:07:50,481 [INFO] test_operation took 0.00 seconds
2024-12-03 18:07:50,507 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:07:50,507 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:50,507 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:07:50,507 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:07:50,507 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:07:51,028 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:07:51,029 [DEBUG] Base URL: https://example.com
2024-12-03 18:07:51,029 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:07:51,029 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:07:51,029 [DEBUG] Initial domain set to: example.com
2024-12-03 18:07:51,029 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:07:51,029 [INFO] Successfully crawled https://example.com
2024-12-03 18:07:51,031 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:51,031 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x107c26950>
2024-12-03 18:07:51,032 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1079b8fa0>, 520095.552212166)])']
connector: <aiohttp.connector.TCPConnector object at 0x107c24410>
2024-12-03 18:07:51,032 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:07:51,032 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:51,033 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:07:51,034 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:07:51,034 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:07:51,055 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-12-03 18:07:51,055 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-12-03 18:07:51,055 [INFO] Starting crawl of URL: https://example.org
2024-12-03 18:07:51,531 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:07:51,531 [DEBUG] Base URL: https://example.com
2024-12-03 18:07:51,531 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:07:51,531 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:07:51,531 [DEBUG] Initial domain set to: example.com
2024-12-03 18:07:51,531 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:07:51,532 [INFO] Successfully crawled https://example.com
2024-12-03 18:07:52,053 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:07:52,054 [DEBUG] Base URL: https://example.org
2024-12-03 18:07:52,054 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:07:52,054 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:07:52,054 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:07:52,054 [INFO] Successfully crawled https://example.org
2024-12-03 18:07:52,056 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:52,056 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x107c681d0>
2024-12-03 18:07:52,056 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1079b92b0>, 520096.055545666)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1079b87c0>, 520096.578164)])']
connector: <aiohttp.connector.TCPConnector object at 0x107c6bb10>
2024-12-03 18:07:52,057 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:07:52,057 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:52,058 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:07:52,058 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:07:52,058 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:07:52,565 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:07:52,565 [DEBUG] Base URL: https://example.com
2024-12-03 18:07:52,565 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:07:52,565 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:07:52,565 [DEBUG] Initial domain set to: example.com
2024-12-03 18:07:52,565 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:07:52,566 [INFO] Successfully crawled https://example.com
2024-12-03 18:07:52,566 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:52,567 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x107c686d0>
2024-12-03 18:07:52,567 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1079b9b00>, 520097.089120291)])']
connector: <aiohttp.connector.TCPConnector object at 0x107c6a790>
2024-12-03 18:07:52,568 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:07:52,568 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:52,569 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:07:52,569 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:07:52,569 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:07:53,088 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:07:53,088 [DEBUG] Base URL: https://example.com
2024-12-03 18:07:53,088 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:07:53,088 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:07:53,089 [DEBUG] Initial domain set to: example.com
2024-12-03 18:07:53,089 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:07:53,089 [INFO] Successfully crawled https://example.com
2024-12-03 18:07:53,090 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:07:53,090 [DEBUG] Base URL: https://example.com
2024-12-03 18:07:53,090 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:07:53,090 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:07:53,090 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:07:53,098 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:53,099 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:07:53,099 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:53,101 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:53,101 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:07:53,101 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:53,101 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:07:53,101 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:07:53,101 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:07:53,607 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:07:53,607 [DEBUG] Base URL: https://example.com
2024-12-03 18:07:53,607 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:07:53,607 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:07:53,607 [DEBUG] Initial domain set to: example.com
2024-12-03 18:07:53,608 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:07:53,608 [INFO] Successfully crawled https://example.com
2024-12-03 18:07:53,609 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:53,610 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x107a9d1d0>
2024-12-03 18:07:53,610 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1079b8a60>, 520098.130382416)])']
connector: <aiohttp.connector.TCPConnector object at 0x107a9ebd0>
2024-12-03 18:07:53,611 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:07:53,611 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:53,612 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2024-12-03 18:07:53,612 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2024-12-03 18:07:53,612 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2024-12-03 18:07:55,666 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-03 18:07:55,668 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-03 18:07:55,668 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-03 18:07:55,672 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:55,672 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:07:55,672 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:55,673 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:07:55,673 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:07:55,673 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:07:55,688 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-12-03 18:07:55,688 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-12-03 18:07:55,688 [INFO] Starting crawl of URL: https://example.org
2024-12-03 18:07:55,688 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2024-12-03 18:07:55,689 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2024-12-03 18:07:55,689 [INFO] Starting crawl of URL: https://example.net
2024-12-03 18:07:56,189 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:07:56,189 [DEBUG] Base URL: https://example.com
2024-12-03 18:07:56,189 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:07:56,190 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:07:56,190 [DEBUG] Initial domain set to: example.com
2024-12-03 18:07:56,190 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:07:56,190 [INFO] Successfully crawled https://example.com
2024-12-03 18:07:56,678 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:07:56,678 [DEBUG] Base URL: https://example.org
2024-12-03 18:07:56,678 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:07:56,678 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:07:56,678 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:07:56,678 [INFO] Successfully crawled https://example.org
2024-12-03 18:07:57,307 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:07:57,307 [DEBUG] Base URL: https://example.net
2024-12-03 18:07:57,308 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:07:57,308 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:07:57,308 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:07:57,308 [INFO] Successfully crawled https://example.net
2024-12-03 18:07:57,310 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:57,311 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x106630d10>
2024-12-03 18:07:57,311 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x107a00830>, 520100.713130875)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x107a00590>, 520101.202972375)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x107a009f0>, 520101.831277)])']
connector: <aiohttp.connector.TCPConnector object at 0x107c40850>
2024-12-03 18:07:57,311 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:07:57,312 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:57,312 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:07:57,313 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:07:57,313 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:07:57,817 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:07:57,818 [DEBUG] Base URL: https://example.com
2024-12-03 18:07:57,818 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:07:57,818 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:07:57,818 [DEBUG] Initial domain set to: example.com
2024-12-03 18:07:57,818 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:07:57,818 [INFO] Successfully crawled https://example.com
2024-12-03 18:07:57,820 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:57,827 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:07:57,827 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:57,828 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:07:57,852 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:07:57,867 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:57,868 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:07:57,868 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:57,868 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:07:57,884 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:57,885 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:07:57,885 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:07:57,885 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:07:57,885 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2024-12-03 18:07:57,885 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2024-12-03 18:07:57,885 [INFO] Starting crawl of URL: https://example.com/page
2024-12-03 18:07:58,391 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:07:58,391 [DEBUG] Base URL: https://example.com/page
2024-12-03 18:07:58,391 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:07:58,391 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:07:58,391 [DEBUG] Initial domain set to: example.com
2024-12-03 18:07:58,392 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:07:58,392 [INFO] Successfully crawled https://example.com/page
2024-12-03 18:07:58,392 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2024-12-03 18:07:58,392 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2024-12-03 18:07:58,392 [INFO] Starting crawl of URL: https://other-domain.com/page
2024-12-03 18:08:04,853 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x107a9c190>
2024-12-03 18:08:04,859 [DEBUG] 
Evaluating link: ./
2024-12-03 18:08:04,859 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,859 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-03 18:08:04,859 [DEBUG] Absolute link: https://other-domain.com/
2024-12-03 18:08:04,859 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,859 [DEBUG] 
Evaluating link: ./
2024-12-03 18:08:04,859 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,859 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-03 18:08:04,859 [DEBUG] Absolute link: https://other-domain.com/
2024-12-03 18:08:04,859 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,859 [DEBUG] 
Evaluating link: ./hiyou.php
2024-12-03 18:08:04,859 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,859 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-12-03 18:08:04,859 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-12-03 18:08:04,859 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,859 [DEBUG] 
Evaluating link: ./date.php
2024-12-03 18:08:04,859 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,859 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-12-03 18:08:04,859 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-12-03 18:08:04,859 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,859 [DEBUG] 
Evaluating link: ./contact.php
2024-12-03 18:08:04,859 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,859 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2024-12-03 18:08:04,859 [DEBUG] Absolute link: https://other-domain.com/contact.php
2024-12-03 18:08:04,859 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,859 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:08:04,859 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,860 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:08:04,860 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:08:04,860 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-03 18:08:04,860 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:04,860 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,860 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:04,860 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:04,860 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-03 18:08:04,860 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:08:04,860 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,860 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:08:04,860 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:08:04,860 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-03 18:08:04,860 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:08:04,860 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,860 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:08:04,860 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:08:04,860 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-03 18:08:04,860 [DEBUG] 
Evaluating link: ./contact
2024-12-03 18:08:04,860 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,860 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-12-03 18:08:04,860 [DEBUG] Absolute link: https://other-domain.com/contact
2024-12-03 18:08:04,860 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,860 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:04,860 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,860 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:04,860 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:04,860 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-03 18:08:04,860 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:08:04,860 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,860 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:08:04,860 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:08:04,860 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-03 18:08:04,860 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:08:04,860 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,860 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:08:04,860 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:08:04,860 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-03 18:08:04,860 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-12-03 18:08:04,860 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,860 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:04,860 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:04,861 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-12-03 18:08:04,861 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:08:04,861 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,861 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:08:04,861 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:08:04,861 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-03 18:08:04,861 [DEBUG] 
Evaluating link: #indexes
2024-12-03 18:08:04,861 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,861 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2024-12-03 18:08:04,861 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,861 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,861 [DEBUG] 
Evaluating link: #a
2024-12-03 18:08:04,861 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,861 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2024-12-03 18:08:04,861 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,861 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,861 [DEBUG] 
Evaluating link: #b
2024-12-03 18:08:04,861 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,861 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-12-03 18:08:04,861 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,861 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,861 [DEBUG] 
Evaluating link: #c
2024-12-03 18:08:04,861 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,861 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-12-03 18:08:04,861 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,861 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,861 [DEBUG] 
Evaluating link: #d
2024-12-03 18:08:04,861 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,861 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-12-03 18:08:04,861 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,861 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,861 [DEBUG] 
Evaluating link: #e
2024-12-03 18:08:04,861 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,861 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-12-03 18:08:04,861 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,861 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,861 [DEBUG] 
Evaluating link: #f
2024-12-03 18:08:04,861 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,861 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-12-03 18:08:04,861 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,861 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,861 [DEBUG] 
Evaluating link: #g
2024-12-03 18:08:04,861 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,861 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-12-03 18:08:04,861 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,861 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,862 [DEBUG] 
Evaluating link: #h
2024-12-03 18:08:04,862 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,862 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-12-03 18:08:04,862 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,862 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,862 [DEBUG] 
Evaluating link: #i
2024-12-03 18:08:04,862 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,862 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-12-03 18:08:04,862 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,862 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,862 [DEBUG] 
Evaluating link: #j
2024-12-03 18:08:04,862 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,862 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-12-03 18:08:04,862 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,862 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,862 [DEBUG] 
Evaluating link: #k
2024-12-03 18:08:04,862 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,862 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-12-03 18:08:04,862 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,862 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,862 [DEBUG] 
Evaluating link: #l
2024-12-03 18:08:04,862 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,862 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-12-03 18:08:04,862 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,862 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,862 [DEBUG] 
Evaluating link: #m
2024-12-03 18:08:04,862 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,862 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-12-03 18:08:04,862 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,862 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,862 [DEBUG] 
Evaluating link: #n
2024-12-03 18:08:04,862 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,862 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-12-03 18:08:04,862 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,862 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,862 [DEBUG] 
Evaluating link: #o
2024-12-03 18:08:04,862 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,862 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-12-03 18:08:04,862 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,862 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,862 [DEBUG] 
Evaluating link: #p
2024-12-03 18:08:04,862 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,862 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-12-03 18:08:04,862 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,862 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,862 [DEBUG] 
Evaluating link: #q
2024-12-03 18:08:04,862 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,863 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-12-03 18:08:04,863 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,863 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,863 [DEBUG] 
Evaluating link: #r
2024-12-03 18:08:04,863 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,863 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-12-03 18:08:04,863 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,863 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,863 [DEBUG] 
Evaluating link: #s
2024-12-03 18:08:04,863 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,863 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-12-03 18:08:04,863 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,863 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,863 [DEBUG] 
Evaluating link: #t
2024-12-03 18:08:04,863 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,863 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-12-03 18:08:04,863 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,863 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,863 [DEBUG] 
Evaluating link: #u
2024-12-03 18:08:04,863 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,863 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-12-03 18:08:04,863 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,863 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,863 [DEBUG] 
Evaluating link: #v
2024-12-03 18:08:04,863 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,863 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-12-03 18:08:04,863 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,863 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,863 [DEBUG] 
Evaluating link: #w
2024-12-03 18:08:04,863 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,863 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-12-03 18:08:04,863 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,863 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,863 [DEBUG] 
Evaluating link: #x
2024-12-03 18:08:04,863 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,863 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-12-03 18:08:04,863 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,863 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,863 [DEBUG] 
Evaluating link: #y
2024-12-03 18:08:04,863 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,863 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-12-03 18:08:04,863 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,863 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,863 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:04,863 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,863 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:04,863 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:04,864 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-03 18:08:04,864 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:08:04,864 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,864 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:08:04,864 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:08:04,864 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-03 18:08:04,864 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:08:04,864 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,864 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:08:04,864 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:08:04,864 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-03 18:08:04,864 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-12-03 18:08:04,864 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,864 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:04,864 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:04,864 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-12-03 18:08:04,864 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:08:04,864 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,864 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:08:04,864 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:08:04,864 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-03 18:08:04,864 [DEBUG] 
Evaluating link: https://spriq.jp/
2024-12-03 18:08:04,864 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,864 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2024-12-03 18:08:04,864 [DEBUG] Absolute link: https://spriq.jp/
2024-12-03 18:08:04,864 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2024-12-03 18:08:04,864 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:04,864 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,864 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:04,864 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:04,864 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-03 18:08:04,864 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:04,864 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,864 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:04,864 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:04,864 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-03 18:08:04,864 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:08:04,864 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,864 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:08:04,864 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:08:04,864 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-03 18:08:04,864 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:08:04,865 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,865 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:08:04,865 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:08:04,865 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-03 18:08:04,865 [DEBUG] 
Evaluating link: ./contact
2024-12-03 18:08:04,865 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,865 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-12-03 18:08:04,865 [DEBUG] Absolute link: https://other-domain.com/contact
2024-12-03 18:08:04,865 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,865 [DEBUG] 
Evaluating link: ./
2024-12-03 18:08:04,865 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,865 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-03 18:08:04,865 [DEBUG] Absolute link: https://other-domain.com/
2024-12-03 18:08:04,865 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,865 [DEBUG] 
Evaluating link: ./hiyou.php
2024-12-03 18:08:04,865 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,865 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-12-03 18:08:04,865 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-12-03 18:08:04,865 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,865 [DEBUG] 
Evaluating link: ./date.php
2024-12-03 18:08:04,865 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,865 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-12-03 18:08:04,865 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-12-03 18:08:04,865 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,865 [DEBUG] 
Evaluating link: #u
2024-12-03 18:08:04,865 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,865 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-12-03 18:08:04,865 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,865 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,865 [DEBUG] 
Evaluating link: #v
2024-12-03 18:08:04,865 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,865 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-12-03 18:08:04,865 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,865 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,865 [DEBUG] 
Evaluating link: #w
2024-12-03 18:08:04,865 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,865 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-12-03 18:08:04,865 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,865 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,865 [DEBUG] 
Evaluating link: #x
2024-12-03 18:08:04,865 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,866 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-12-03 18:08:04,866 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,866 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,866 [DEBUG] 
Evaluating link: #y
2024-12-03 18:08:04,866 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,866 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-12-03 18:08:04,866 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,866 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,866 [DEBUG] 
Evaluating link: #p
2024-12-03 18:08:04,866 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,866 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-12-03 18:08:04,866 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,866 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,866 [DEBUG] 
Evaluating link: #q
2024-12-03 18:08:04,866 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,866 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-12-03 18:08:04,866 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,866 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,866 [DEBUG] 
Evaluating link: #r
2024-12-03 18:08:04,866 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,866 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-12-03 18:08:04,866 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,866 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,866 [DEBUG] 
Evaluating link: #s
2024-12-03 18:08:04,866 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,866 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-12-03 18:08:04,866 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,866 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,866 [DEBUG] 
Evaluating link: #t
2024-12-03 18:08:04,866 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,866 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-12-03 18:08:04,866 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,866 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,866 [DEBUG] 
Evaluating link: #k
2024-12-03 18:08:04,866 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,866 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-12-03 18:08:04,866 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,866 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,866 [DEBUG] 
Evaluating link: #l
2024-12-03 18:08:04,866 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,866 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-12-03 18:08:04,867 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,867 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,867 [DEBUG] 
Evaluating link: #m
2024-12-03 18:08:04,867 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,867 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-12-03 18:08:04,867 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,867 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,867 [DEBUG] 
Evaluating link: #n
2024-12-03 18:08:04,867 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,867 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-12-03 18:08:04,867 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,867 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,867 [DEBUG] 
Evaluating link: #o
2024-12-03 18:08:04,867 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,867 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-12-03 18:08:04,867 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,867 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,867 [DEBUG] 
Evaluating link: #f
2024-12-03 18:08:04,867 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,867 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-12-03 18:08:04,867 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,867 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,867 [DEBUG] 
Evaluating link: #g
2024-12-03 18:08:04,867 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,867 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-12-03 18:08:04,867 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,867 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,867 [DEBUG] 
Evaluating link: #h
2024-12-03 18:08:04,867 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,867 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-12-03 18:08:04,867 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,867 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,867 [DEBUG] 
Evaluating link: #i
2024-12-03 18:08:04,867 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,867 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-12-03 18:08:04,867 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,867 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,867 [DEBUG] 
Evaluating link: #j
2024-12-03 18:08:04,867 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,867 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-12-03 18:08:04,867 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,867 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,867 [DEBUG] 
Evaluating link: #b
2024-12-03 18:08:04,867 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,867 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-12-03 18:08:04,867 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,868 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,868 [DEBUG] 
Evaluating link: #c
2024-12-03 18:08:04,868 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,868 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-12-03 18:08:04,868 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,868 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,868 [DEBUG] 
Evaluating link: #d
2024-12-03 18:08:04,868 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,868 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-12-03 18:08:04,868 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,868 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,868 [DEBUG] 
Evaluating link: #e
2024-12-03 18:08:04,868 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,868 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-12-03 18:08:04,868 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,868 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,868 [DEBUG] 
Evaluating link: #site
2024-12-03 18:08:04,868 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:04,868 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2024-12-03 18:08:04,868 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:04,868 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:04,868 [INFO] Successfully crawled https://other-domain.com/page
2024-12-03 18:08:04,871 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:04,871 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:08:04,871 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:04,872 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-12-03 18:08:04,872 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-12-03 18:08:04,872 [INFO] Starting crawl of URL: https://example.com/page0
2024-12-03 18:08:05,386 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:05,387 [DEBUG] Base URL: https://example.com/page0
2024-12-03 18:08:05,387 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:05,387 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:05,387 [DEBUG] Initial domain set to: example.com
2024-12-03 18:08:05,387 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:05,387 [INFO] Successfully crawled https://example.com/page0
2024-12-03 18:08:05,388 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-12-03 18:08:05,388 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-12-03 18:08:05,388 [INFO] Starting crawl of URL: https://example.com/page1
2024-12-03 18:08:05,521 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:05,521 [DEBUG] Base URL: https://example.com/page1
2024-12-03 18:08:05,521 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:05,521 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:05,521 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:05,521 [INFO] Successfully crawled https://example.com/page1
2024-12-03 18:08:05,521 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-12-03 18:08:05,522 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-12-03 18:08:05,522 [INFO] Starting crawl of URL: https://example.com/page2
2024-12-03 18:08:06,023 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:06,023 [DEBUG] Base URL: https://example.com/page2
2024-12-03 18:08:06,023 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:06,023 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:06,023 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:06,023 [INFO] Successfully crawled https://example.com/page2
2024-12-03 18:08:06,023 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-12-03 18:08:06,024 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-12-03 18:08:06,024 [INFO] Starting crawl of URL: https://example.com/page3
2024-12-03 18:08:06,521 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:06,521 [DEBUG] Base URL: https://example.com/page3
2024-12-03 18:08:06,521 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:06,521 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:06,521 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:06,521 [INFO] Successfully crawled https://example.com/page3
2024-12-03 18:08:06,521 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-12-03 18:08:06,521 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-12-03 18:08:06,521 [INFO] Starting crawl of URL: https://example.com/page4
2024-12-03 18:08:07,018 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:07,019 [DEBUG] Base URL: https://example.com/page4
2024-12-03 18:08:07,019 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:07,019 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:07,019 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:07,019 [INFO] Successfully crawled https://example.com/page4
2024-12-03 18:08:07,019 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:08:07,019 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-12-03 18:08:07,020 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-12-03 18:08:07,020 [INFO] Starting crawl of URL: https://example.com/page0
2024-12-03 18:08:07,537 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:07,537 [DEBUG] Base URL: https://example.com/page0
2024-12-03 18:08:07,538 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:07,538 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:07,538 [DEBUG] Initial domain set to: example.com
2024-12-03 18:08:07,538 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:07,538 [INFO] Successfully crawled https://example.com/page0
2024-12-03 18:08:07,538 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-12-03 18:08:07,538 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-12-03 18:08:07,538 [INFO] Starting crawl of URL: https://example.com/page1
2024-12-03 18:08:07,687 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:07,687 [DEBUG] Base URL: https://example.com/page1
2024-12-03 18:08:07,688 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:07,688 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:07,688 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:07,688 [INFO] Successfully crawled https://example.com/page1
2024-12-03 18:08:07,688 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-12-03 18:08:07,688 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-12-03 18:08:07,688 [INFO] Starting crawl of URL: https://example.com/page2
2024-12-03 18:08:08,189 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:08,189 [DEBUG] Base URL: https://example.com/page2
2024-12-03 18:08:08,190 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:08,190 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:08,190 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:08,190 [INFO] Successfully crawled https://example.com/page2
2024-12-03 18:08:08,190 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-12-03 18:08:08,190 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-12-03 18:08:08,190 [INFO] Starting crawl of URL: https://example.com/page3
2024-12-03 18:08:08,669 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:08,670 [DEBUG] Base URL: https://example.com/page3
2024-12-03 18:08:08,670 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:08,670 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:08,670 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:08,670 [INFO] Successfully crawled https://example.com/page3
2024-12-03 18:08:08,670 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-12-03 18:08:08,671 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-12-03 18:08:08,671 [INFO] Starting crawl of URL: https://example.com/page4
2024-12-03 18:08:09,174 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:09,175 [DEBUG] Base URL: https://example.com/page4
2024-12-03 18:08:09,175 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:09,175 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:09,175 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:09,175 [INFO] Successfully crawled https://example.com/page4
2024-12-03 18:08:09,181 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:09,182 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:09,185 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:09,186 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:09,188 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:09,189 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:09,192 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:09,192 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:09,196 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:09,196 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:09,199 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:09,200 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:09,203 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:09,204 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:09,206 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:09,207 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:09,210 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:09,210 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:09,213 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:09,226 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:09,229 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:09,231 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:09,234 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:09,236 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:09,239 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:09,242 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:09,244 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:09,247 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:09,250 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:09,252 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:09,277 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x107ba7190>
2024-12-03 18:08:09,277 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1079b83d0>, 520097.61311625)])']
connector: <aiohttp.connector.TCPConnector object at 0x107ba52d0>
2024-12-03 18:08:09,277 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x107b14d10>
2024-12-03 18:08:09,277 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x107a00980>, 520102.916052458)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x107a00c90>, 520105.516878291)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x107a00f30>, 520106.77855075)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x107a00a60>, 520109.370989083)])']
connector: <aiohttp.connector.TCPConnector object at 0x107b14310>
2024-12-03 18:08:09,277 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x107b0a610>
2024-12-03 18:08:09,277 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x107a01b00>, 520111.5421335)])']
connector: <aiohttp.connector.TCPConnector object at 0x107b0b350>
2024-12-03 18:08:09,277 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x107a1ef10>
2024-12-03 18:08:09,277 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x107a02190>, 520113.700015458)])']
connector: <aiohttp.connector.TCPConnector object at 0x107b089d0>
2024-12-03 18:08:09,283 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:09,286 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:09,287 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:09,287 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:09,291 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:09,291 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:09,291 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:09,294 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:09,297 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:09,304 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:09,309 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:09,314 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:09,319 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:09,325 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:09,330 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:09,336 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:09,341 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:09,347 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:09,417 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:09,536 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:09,538 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:08:09,538 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:08:09,538 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:08:09,538 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:08:10,051 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:10,051 [DEBUG] Base URL: https://example.com
2024-12-03 18:08:10,052 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:10,052 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:10,052 [DEBUG] Initial domain set to: example.com
2024-12-03 18:08:10,052 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:10,052 [INFO] Successfully crawled https://example.com
2024-12-03 18:08:10,052 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x11adc9fd0>
2024-12-03 18:08:10,053 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x11ad6b3f0>, 520114.575281666)])']
connector: <aiohttp.connector.TCPConnector object at 0x11adcb190>
2024-12-03 18:08:10,055 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2024-12-03 18:08:11,233 [INFO] Operation took 0.11 seconds
2024-12-03 18:08:21,408 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 18:08:21,670 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:21,671 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:21,672 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:21,673 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:21,674 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:21,674 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:21,674 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:21,705 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:21,705 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:21,706 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:21,708 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:21,709 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:21,711 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:21,711 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:21,714 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:21,714 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:21,715 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:21,715 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:21,718 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:21,718 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:21,718 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:21,720 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:23,274 [INFO] Operation took 0.02 seconds
2024-12-03 18:08:23,521 [INFO] test_operation took 0.10 seconds
2024-12-03 18:08:23,722 [INFO] inner took 0.10 seconds
2024-12-03 18:08:23,722 [INFO] outer took 0.20 seconds
2024-12-03 18:08:23,722 [INFO] error_operation took 0.00 seconds
2024-12-03 18:08:23,774 [INFO] test_operation took 0.00 seconds
2024-12-03 18:08:23,798 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:08:23,798 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:23,799 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:08:23,799 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:08:23,799 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:08:24,311 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:24,311 [DEBUG] Base URL: https://example.com
2024-12-03 18:08:24,311 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:24,311 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:24,311 [DEBUG] Initial domain set to: example.com
2024-12-03 18:08:24,312 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:24,312 [INFO] Successfully crawled https://example.com
2024-12-03 18:08:24,313 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:24,314 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1065d6dd0>
2024-12-03 18:08:24,314 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10641c130>, 520128.834617125)])']
connector: <aiohttp.connector.TCPConnector object at 0x10661a2d0>
2024-12-03 18:08:24,315 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:08:24,315 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:24,316 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:08:24,316 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:08:24,316 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:08:24,336 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-12-03 18:08:24,336 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-12-03 18:08:24,336 [INFO] Starting crawl of URL: https://example.org
2024-12-03 18:08:24,825 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:24,826 [DEBUG] Base URL: https://example.com
2024-12-03 18:08:24,826 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:24,826 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:24,826 [DEBUG] Initial domain set to: example.com
2024-12-03 18:08:24,826 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:24,826 [INFO] Successfully crawled https://example.com
2024-12-03 18:08:25,329 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:25,329 [DEBUG] Base URL: https://example.org
2024-12-03 18:08:25,330 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:25,330 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:25,330 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:25,330 [INFO] Successfully crawled https://example.org
2024-12-03 18:08:25,332 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:25,333 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x10661bd50>
2024-12-03 18:08:25,334 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10641c210>, 520129.34971825)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x10641c670>, 520129.853553666)])']
connector: <aiohttp.connector.TCPConnector object at 0x10661aa90>
2024-12-03 18:08:25,335 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:08:25,335 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:25,337 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:08:25,337 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:08:25,337 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:08:25,845 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:25,845 [DEBUG] Base URL: https://example.com
2024-12-03 18:08:25,846 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:25,846 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:25,846 [DEBUG] Initial domain set to: example.com
2024-12-03 18:08:25,846 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:25,846 [INFO] Successfully crawled https://example.com
2024-12-03 18:08:25,847 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:25,848 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1066aaed0>
2024-12-03 18:08:25,848 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10641c360>, 520130.370052708)])']
connector: <aiohttp.connector.TCPConnector object at 0x106619690>
2024-12-03 18:08:25,849 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:08:25,849 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:25,850 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:08:25,850 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:08:25,850 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:08:26,363 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:26,364 [DEBUG] Base URL: https://example.com
2024-12-03 18:08:26,364 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:26,364 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:26,364 [DEBUG] Initial domain set to: example.com
2024-12-03 18:08:26,364 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:26,365 [INFO] Successfully crawled https://example.com
2024-12-03 18:08:26,366 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:26,366 [DEBUG] Base URL: https://example.com
2024-12-03 18:08:26,366 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:26,366 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:26,366 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:26,374 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:26,375 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:26,375 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:26,377 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:26,377 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:08:26,378 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:26,378 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:08:26,378 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:08:26,378 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:08:26,891 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:26,891 [DEBUG] Base URL: https://example.com
2024-12-03 18:08:26,892 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:26,892 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:26,892 [DEBUG] Initial domain set to: example.com
2024-12-03 18:08:26,892 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:26,892 [INFO] Successfully crawled https://example.com
2024-12-03 18:08:26,894 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:26,894 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x106511a90>
2024-12-03 18:08:26,895 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10674b070>, 520131.416660875)])']
connector: <aiohttp.connector.TCPConnector object at 0x106510190>
2024-12-03 18:08:26,895 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:08:26,896 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:26,897 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2024-12-03 18:08:26,897 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2024-12-03 18:08:26,898 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2024-12-03 18:08:28,917 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-03 18:08:28,920 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-03 18:08:28,920 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-03 18:08:28,927 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:28,928 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:08:28,928 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:28,928 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:08:28,928 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:08:28,928 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:08:28,944 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-12-03 18:08:28,944 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-12-03 18:08:28,944 [INFO] Starting crawl of URL: https://example.org
2024-12-03 18:08:28,944 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2024-12-03 18:08:28,944 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2024-12-03 18:08:28,944 [INFO] Starting crawl of URL: https://example.net
2024-12-03 18:08:29,412 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:29,412 [DEBUG] Base URL: https://example.com
2024-12-03 18:08:29,412 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:29,412 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:29,412 [DEBUG] Initial domain set to: example.com
2024-12-03 18:08:29,413 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:29,413 [INFO] Successfully crawled https://example.com
2024-12-03 18:08:29,932 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:29,932 [DEBUG] Base URL: https://example.org
2024-12-03 18:08:29,932 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:29,932 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:29,932 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:29,933 [INFO] Successfully crawled https://example.org
2024-12-03 18:08:30,421 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:30,422 [DEBUG] Base URL: https://example.net
2024-12-03 18:08:30,422 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:30,422 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:30,422 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:30,422 [INFO] Successfully crawled https://example.net
2024-12-03 18:08:30,424 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:30,425 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1066479d0>
2024-12-03 18:08:30,425 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10641c910>, 520133.938126291)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x10641d4e0>, 520134.456644)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x10641d940>, 520134.945813833)])']
connector: <aiohttp.connector.TCPConnector object at 0x106645c50>
2024-12-03 18:08:30,426 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:08:30,426 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:30,427 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:08:30,427 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:08:30,427 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:08:30,925 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:30,925 [DEBUG] Base URL: https://example.com
2024-12-03 18:08:30,925 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:30,925 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:30,925 [DEBUG] Initial domain set to: example.com
2024-12-03 18:08:30,925 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:30,925 [INFO] Successfully crawled https://example.com
2024-12-03 18:08:30,926 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:30,929 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:08:30,929 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:30,929 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:30,950 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:08:30,967 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:30,967 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:08:30,967 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:30,968 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:08:30,984 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:30,984 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:08:30,984 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:30,984 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:08:30,985 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2024-12-03 18:08:30,985 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2024-12-03 18:08:30,985 [INFO] Starting crawl of URL: https://example.com/page
2024-12-03 18:08:31,492 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:31,493 [DEBUG] Base URL: https://example.com/page
2024-12-03 18:08:31,493 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:31,493 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:31,493 [DEBUG] Initial domain set to: example.com
2024-12-03 18:08:31,493 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:31,493 [INFO] Successfully crawled https://example.com/page
2024-12-03 18:08:31,493 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2024-12-03 18:08:31,493 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2024-12-03 18:08:31,493 [INFO] Starting crawl of URL: https://other-domain.com/page
2024-12-03 18:08:36,204 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1065db8d0>
2024-12-03 18:08:36,210 [DEBUG] 
Evaluating link: ./
2024-12-03 18:08:36,210 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,210 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-03 18:08:36,211 [DEBUG] Absolute link: https://other-domain.com/
2024-12-03 18:08:36,211 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,211 [DEBUG] 
Evaluating link: ./
2024-12-03 18:08:36,211 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,211 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-03 18:08:36,211 [DEBUG] Absolute link: https://other-domain.com/
2024-12-03 18:08:36,211 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,211 [DEBUG] 
Evaluating link: ./hiyou.php
2024-12-03 18:08:36,211 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,211 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-12-03 18:08:36,211 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-12-03 18:08:36,211 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,211 [DEBUG] 
Evaluating link: ./date.php
2024-12-03 18:08:36,211 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,211 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-12-03 18:08:36,211 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-12-03 18:08:36,211 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,211 [DEBUG] 
Evaluating link: ./contact.php
2024-12-03 18:08:36,211 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,211 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2024-12-03 18:08:36,211 [DEBUG] Absolute link: https://other-domain.com/contact.php
2024-12-03 18:08:36,211 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,211 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:08:36,211 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,211 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:08:36,211 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:08:36,211 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-03 18:08:36,211 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:36,211 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,211 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:36,211 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:36,211 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-03 18:08:36,211 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:08:36,211 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,211 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:08:36,211 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:08:36,211 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-03 18:08:36,211 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:08:36,211 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,212 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:08:36,212 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:08:36,212 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-03 18:08:36,212 [DEBUG] 
Evaluating link: ./contact
2024-12-03 18:08:36,212 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,212 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-12-03 18:08:36,212 [DEBUG] Absolute link: https://other-domain.com/contact
2024-12-03 18:08:36,212 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,212 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:36,212 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,212 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:36,212 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:36,212 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-03 18:08:36,212 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:08:36,212 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,212 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:08:36,212 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:08:36,212 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-03 18:08:36,212 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:08:36,212 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,212 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:08:36,212 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:08:36,212 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-03 18:08:36,212 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-12-03 18:08:36,212 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,212 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:36,212 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:36,212 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-12-03 18:08:36,212 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:08:36,212 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,212 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:08:36,212 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:08:36,212 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-03 18:08:36,212 [DEBUG] 
Evaluating link: #indexes
2024-12-03 18:08:36,212 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,212 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2024-12-03 18:08:36,212 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,212 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,212 [DEBUG] 
Evaluating link: #a
2024-12-03 18:08:36,213 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,213 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2024-12-03 18:08:36,213 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,213 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,213 [DEBUG] 
Evaluating link: #b
2024-12-03 18:08:36,213 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,213 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-12-03 18:08:36,213 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,213 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,213 [DEBUG] 
Evaluating link: #c
2024-12-03 18:08:36,213 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,213 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-12-03 18:08:36,213 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,213 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,213 [DEBUG] 
Evaluating link: #d
2024-12-03 18:08:36,213 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,213 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-12-03 18:08:36,213 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,213 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,213 [DEBUG] 
Evaluating link: #e
2024-12-03 18:08:36,213 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,213 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-12-03 18:08:36,213 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,213 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,213 [DEBUG] 
Evaluating link: #f
2024-12-03 18:08:36,213 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,213 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-12-03 18:08:36,213 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,213 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,213 [DEBUG] 
Evaluating link: #g
2024-12-03 18:08:36,213 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,213 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-12-03 18:08:36,213 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,213 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,213 [DEBUG] 
Evaluating link: #h
2024-12-03 18:08:36,213 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,213 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-12-03 18:08:36,213 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,213 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,214 [DEBUG] 
Evaluating link: #i
2024-12-03 18:08:36,214 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,214 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-12-03 18:08:36,214 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,214 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,214 [DEBUG] 
Evaluating link: #j
2024-12-03 18:08:36,214 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,214 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-12-03 18:08:36,214 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,214 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,214 [DEBUG] 
Evaluating link: #k
2024-12-03 18:08:36,214 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,214 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-12-03 18:08:36,214 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,214 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,214 [DEBUG] 
Evaluating link: #l
2024-12-03 18:08:36,214 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,214 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-12-03 18:08:36,214 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,214 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,214 [DEBUG] 
Evaluating link: #m
2024-12-03 18:08:36,214 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,214 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-12-03 18:08:36,214 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,214 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,214 [DEBUG] 
Evaluating link: #n
2024-12-03 18:08:36,214 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,214 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-12-03 18:08:36,214 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,214 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,214 [DEBUG] 
Evaluating link: #o
2024-12-03 18:08:36,214 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,214 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-12-03 18:08:36,214 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,214 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,214 [DEBUG] 
Evaluating link: #p
2024-12-03 18:08:36,214 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,214 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-12-03 18:08:36,214 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,214 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,214 [DEBUG] 
Evaluating link: #q
2024-12-03 18:08:36,215 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,215 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-12-03 18:08:36,215 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,215 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,215 [DEBUG] 
Evaluating link: #r
2024-12-03 18:08:36,215 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,215 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-12-03 18:08:36,215 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,215 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,215 [DEBUG] 
Evaluating link: #s
2024-12-03 18:08:36,215 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,215 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-12-03 18:08:36,215 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,215 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,215 [DEBUG] 
Evaluating link: #t
2024-12-03 18:08:36,215 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,215 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-12-03 18:08:36,215 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,215 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,215 [DEBUG] 
Evaluating link: #u
2024-12-03 18:08:36,215 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,215 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-12-03 18:08:36,215 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,215 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,215 [DEBUG] 
Evaluating link: #v
2024-12-03 18:08:36,215 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,215 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-12-03 18:08:36,215 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,215 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,215 [DEBUG] 
Evaluating link: #w
2024-12-03 18:08:36,215 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,215 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-12-03 18:08:36,215 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,215 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,215 [DEBUG] 
Evaluating link: #x
2024-12-03 18:08:36,215 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,215 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-12-03 18:08:36,215 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,215 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,215 [DEBUG] 
Evaluating link: #y
2024-12-03 18:08:36,215 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,215 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-12-03 18:08:36,215 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,215 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,215 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:36,215 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,216 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:36,216 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:36,216 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-03 18:08:36,216 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:08:36,216 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,216 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:08:36,216 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:08:36,216 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-03 18:08:36,216 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:08:36,216 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,216 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:08:36,216 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:08:36,216 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-03 18:08:36,216 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-12-03 18:08:36,216 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,216 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:36,216 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:36,216 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-12-03 18:08:36,216 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:08:36,216 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,216 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:08:36,216 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:08:36,216 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-03 18:08:36,216 [DEBUG] 
Evaluating link: https://spriq.jp/
2024-12-03 18:08:36,216 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,216 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2024-12-03 18:08:36,216 [DEBUG] Absolute link: https://spriq.jp/
2024-12-03 18:08:36,216 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2024-12-03 18:08:36,216 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:36,216 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,216 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:36,216 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:36,216 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-03 18:08:36,216 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:36,216 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,216 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:36,216 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:08:36,216 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-03 18:08:36,216 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:08:36,216 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,217 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:08:36,217 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:08:36,217 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-03 18:08:36,217 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:08:36,217 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,217 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:08:36,217 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:08:36,217 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-03 18:08:36,217 [DEBUG] 
Evaluating link: ./contact
2024-12-03 18:08:36,217 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,217 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-12-03 18:08:36,217 [DEBUG] Absolute link: https://other-domain.com/contact
2024-12-03 18:08:36,217 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,217 [DEBUG] 
Evaluating link: ./
2024-12-03 18:08:36,217 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,217 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-03 18:08:36,217 [DEBUG] Absolute link: https://other-domain.com/
2024-12-03 18:08:36,217 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,217 [DEBUG] 
Evaluating link: ./hiyou.php
2024-12-03 18:08:36,217 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,217 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-12-03 18:08:36,217 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-12-03 18:08:36,217 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,217 [DEBUG] 
Evaluating link: ./date.php
2024-12-03 18:08:36,217 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,217 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-12-03 18:08:36,217 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-12-03 18:08:36,217 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,217 [DEBUG] 
Evaluating link: #u
2024-12-03 18:08:36,217 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,217 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-12-03 18:08:36,217 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,217 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,217 [DEBUG] 
Evaluating link: #v
2024-12-03 18:08:36,217 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,217 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-12-03 18:08:36,217 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,217 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,217 [DEBUG] 
Evaluating link: #w
2024-12-03 18:08:36,217 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,217 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-12-03 18:08:36,217 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,218 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,218 [DEBUG] 
Evaluating link: #x
2024-12-03 18:08:36,218 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,218 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-12-03 18:08:36,218 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,218 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,218 [DEBUG] 
Evaluating link: #y
2024-12-03 18:08:36,218 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,218 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-12-03 18:08:36,218 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,218 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,218 [DEBUG] 
Evaluating link: #p
2024-12-03 18:08:36,218 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,218 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-12-03 18:08:36,218 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,218 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,218 [DEBUG] 
Evaluating link: #q
2024-12-03 18:08:36,218 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,218 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-12-03 18:08:36,218 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,218 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,218 [DEBUG] 
Evaluating link: #r
2024-12-03 18:08:36,218 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,218 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-12-03 18:08:36,218 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,218 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,218 [DEBUG] 
Evaluating link: #s
2024-12-03 18:08:36,218 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,218 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-12-03 18:08:36,218 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,218 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,218 [DEBUG] 
Evaluating link: #t
2024-12-03 18:08:36,218 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,218 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-12-03 18:08:36,218 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,218 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,218 [DEBUG] 
Evaluating link: #k
2024-12-03 18:08:36,218 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,218 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-12-03 18:08:36,218 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,218 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,219 [DEBUG] 
Evaluating link: #l
2024-12-03 18:08:36,219 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,219 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-12-03 18:08:36,219 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,219 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,219 [DEBUG] 
Evaluating link: #m
2024-12-03 18:08:36,219 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,219 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-12-03 18:08:36,219 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,219 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,219 [DEBUG] 
Evaluating link: #n
2024-12-03 18:08:36,219 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,219 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-12-03 18:08:36,219 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,219 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,219 [DEBUG] 
Evaluating link: #o
2024-12-03 18:08:36,219 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,219 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-12-03 18:08:36,219 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,219 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,219 [DEBUG] 
Evaluating link: #f
2024-12-03 18:08:36,219 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,219 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-12-03 18:08:36,219 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,219 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,219 [DEBUG] 
Evaluating link: #g
2024-12-03 18:08:36,219 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,219 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-12-03 18:08:36,219 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,219 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,219 [DEBUG] 
Evaluating link: #h
2024-12-03 18:08:36,219 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,219 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-12-03 18:08:36,219 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,219 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,219 [DEBUG] 
Evaluating link: #i
2024-12-03 18:08:36,219 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,219 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-12-03 18:08:36,219 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,219 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,219 [DEBUG] 
Evaluating link: #j
2024-12-03 18:08:36,220 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,220 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-12-03 18:08:36,220 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,220 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,220 [DEBUG] 
Evaluating link: #b
2024-12-03 18:08:36,220 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,220 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-12-03 18:08:36,220 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,220 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,220 [DEBUG] 
Evaluating link: #c
2024-12-03 18:08:36,220 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,220 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-12-03 18:08:36,220 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,220 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,220 [DEBUG] 
Evaluating link: #d
2024-12-03 18:08:36,220 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,220 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-12-03 18:08:36,220 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,220 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,220 [DEBUG] 
Evaluating link: #e
2024-12-03 18:08:36,220 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,220 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-12-03 18:08:36,220 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,220 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,220 [DEBUG] 
Evaluating link: #site
2024-12-03 18:08:36,220 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:08:36,220 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2024-12-03 18:08:36,220 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:08:36,220 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:08:36,220 [INFO] Successfully crawled https://other-domain.com/page
2024-12-03 18:08:36,223 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:36,224 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:08:36,224 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:36,224 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-12-03 18:08:36,224 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-12-03 18:08:36,224 [INFO] Starting crawl of URL: https://example.com/page0
2024-12-03 18:08:36,723 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:36,723 [DEBUG] Base URL: https://example.com/page0
2024-12-03 18:08:36,723 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:36,723 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:36,723 [DEBUG] Initial domain set to: example.com
2024-12-03 18:08:36,723 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:36,723 [INFO] Successfully crawled https://example.com/page0
2024-12-03 18:08:36,724 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-12-03 18:08:36,724 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-12-03 18:08:36,724 [INFO] Starting crawl of URL: https://example.com/page1
2024-12-03 18:08:36,865 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:36,866 [DEBUG] Base URL: https://example.com/page1
2024-12-03 18:08:36,866 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:36,866 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:36,866 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:36,866 [INFO] Successfully crawled https://example.com/page1
2024-12-03 18:08:36,866 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-12-03 18:08:36,866 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-12-03 18:08:36,866 [INFO] Starting crawl of URL: https://example.com/page2
2024-12-03 18:08:37,368 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:37,368 [DEBUG] Base URL: https://example.com/page2
2024-12-03 18:08:37,368 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:37,368 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:37,368 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:37,369 [INFO] Successfully crawled https://example.com/page2
2024-12-03 18:08:37,369 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-12-03 18:08:37,369 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-12-03 18:08:37,369 [INFO] Starting crawl of URL: https://example.com/page3
2024-12-03 18:08:37,870 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:37,871 [DEBUG] Base URL: https://example.com/page3
2024-12-03 18:08:37,871 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:37,871 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:37,871 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:37,871 [INFO] Successfully crawled https://example.com/page3
2024-12-03 18:08:37,871 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-12-03 18:08:37,871 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-12-03 18:08:37,871 [INFO] Starting crawl of URL: https://example.com/page4
2024-12-03 18:08:38,369 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:38,370 [DEBUG] Base URL: https://example.com/page4
2024-12-03 18:08:38,370 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:38,370 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:38,370 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:38,370 [INFO] Successfully crawled https://example.com/page4
2024-12-03 18:08:38,370 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:08:38,370 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-12-03 18:08:38,370 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-12-03 18:08:38,370 [INFO] Starting crawl of URL: https://example.com/page0
2024-12-03 18:08:38,877 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:38,877 [DEBUG] Base URL: https://example.com/page0
2024-12-03 18:08:38,878 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:38,878 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:38,878 [DEBUG] Initial domain set to: example.com
2024-12-03 18:08:38,878 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:38,878 [INFO] Successfully crawled https://example.com/page0
2024-12-03 18:08:38,878 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-12-03 18:08:38,878 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-12-03 18:08:38,878 [INFO] Starting crawl of URL: https://example.com/page1
2024-12-03 18:08:39,029 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:39,029 [DEBUG] Base URL: https://example.com/page1
2024-12-03 18:08:39,029 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:39,030 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:39,030 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:39,030 [INFO] Successfully crawled https://example.com/page1
2024-12-03 18:08:39,030 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-12-03 18:08:39,030 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-12-03 18:08:39,030 [INFO] Starting crawl of URL: https://example.com/page2
2024-12-03 18:08:39,511 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:39,511 [DEBUG] Base URL: https://example.com/page2
2024-12-03 18:08:39,511 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:39,511 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:39,511 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:39,512 [INFO] Successfully crawled https://example.com/page2
2024-12-03 18:08:39,512 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-12-03 18:08:39,512 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-12-03 18:08:39,512 [INFO] Starting crawl of URL: https://example.com/page3
2024-12-03 18:08:40,015 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:40,015 [DEBUG] Base URL: https://example.com/page3
2024-12-03 18:08:40,015 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:40,016 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:40,016 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:40,016 [INFO] Successfully crawled https://example.com/page3
2024-12-03 18:08:40,016 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-12-03 18:08:40,016 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-12-03 18:08:40,016 [INFO] Starting crawl of URL: https://example.com/page4
2024-12-03 18:08:40,519 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:40,519 [DEBUG] Base URL: https://example.com/page4
2024-12-03 18:08:40,520 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:40,520 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:40,520 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:40,520 [INFO] Successfully crawled https://example.com/page4
2024-12-03 18:08:40,526 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:40,527 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:40,530 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:40,531 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:40,533 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:40,534 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:40,537 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:40,537 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:40,540 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:40,540 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:40,543 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:40,544 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:40,546 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:40,547 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:40,550 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:40,551 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:40,553 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:40,554 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:40,556 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:40,569 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:40,571 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:40,574 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:40,577 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:40,580 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:40,583 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:40,585 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:40,588 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:40,590 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:40,593 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:40,596 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:40,621 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x106776410>
2024-12-03 18:08:40,621 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10641c590>, 520130.888197208)])']
connector: <aiohttp.connector.TCPConnector object at 0x1065d40d0>
2024-12-03 18:08:40,621 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x106644450>
2024-12-03 18:08:40,621 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10641d8d0>, 520136.018605791)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x10641dbe0>, 520138.308207)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x10641de80>, 520139.536545125)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x10641d9b0>, 520140.723582708)])']
connector: <aiohttp.connector.TCPConnector object at 0x106644850>
2024-12-03 18:08:40,621 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x106620110>
2024-12-03 18:08:40,621 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10641ea50>, 520142.89555375)])']
connector: <aiohttp.connector.TCPConnector object at 0x106621d90>
2024-12-03 18:08:40,621 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x106623cd0>
2024-12-03 18:08:40,621 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10641f0e0>, 520145.044234125)])']
connector: <aiohttp.connector.TCPConnector object at 0x1066fca50>
2024-12-03 18:08:40,627 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:40,630 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:40,630 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:40,631 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:40,634 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:40,634 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:40,634 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:40,637 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:40,640 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:40,647 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:40,653 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:40,659 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:40,664 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:40,671 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:40,676 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:40,682 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:40,688 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:40,694 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:08:40,763 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:40,882 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:08:40,882 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:08:40,882 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:08:40,883 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:08:40,883 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:08:41,386 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:08:41,386 [DEBUG] Base URL: https://example.com
2024-12-03 18:08:41,386 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:08:41,386 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:08:41,386 [DEBUG] Initial domain set to: example.com
2024-12-03 18:08:41,387 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:08:41,387 [INFO] Successfully crawled https://example.com
2024-12-03 18:08:41,387 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x10a172ad0>
2024-12-03 18:08:41,387 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10a1a43d0>, 520145.910681875)])']
connector: <aiohttp.connector.TCPConnector object at 0x10a1703d0>
2024-12-03 18:08:41,390 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2024-12-03 18:08:42,554 [INFO] Operation took 0.10 seconds
2024-12-03 18:09:32,182 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 18:10:40,498 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 18:10:40,916 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:40,918 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:40,919 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:40,922 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:40,922 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:40,923 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:40,923 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:10:40,957 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:40,957 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:40,958 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:10:40,961 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:40,961 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:40,964 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:40,964 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:40,966 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:40,966 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:40,967 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:40,967 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:40,970 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:40,970 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:40,971 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:10:40,973 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:42,491 [INFO] Operation took 0.04 seconds
2024-12-03 18:10:42,740 [INFO] test_operation took 0.11 seconds
2024-12-03 18:10:42,943 [INFO] inner took 0.10 seconds
2024-12-03 18:10:42,943 [INFO] outer took 0.20 seconds
2024-12-03 18:10:42,943 [INFO] error_operation took 0.00 seconds
2024-12-03 18:10:43,044 [INFO] test_operation took 0.00 seconds
2024-12-03 18:10:43,207 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:10:43,207 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:43,208 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:10:43,208 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:10:43,208 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:10:43,733 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:10:43,733 [DEBUG] Base URL: https://example.com
2024-12-03 18:10:43,734 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:10:43,734 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:10:43,734 [DEBUG] Initial domain set to: example.com
2024-12-03 18:10:43,734 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:10:43,734 [INFO] Successfully crawled https://example.com
2024-12-03 18:10:43,735 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:43,736 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x106727290>
2024-12-03 18:10:43,736 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1065c0980>, 520268.2604825)])']
connector: <aiohttp.connector.TCPConnector object at 0x1069b8c10>
2024-12-03 18:10:43,736 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:10:43,736 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:43,737 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:10:43,737 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:10:43,737 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:10:43,753 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-12-03 18:10:43,753 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-12-03 18:10:43,753 [INFO] Starting crawl of URL: https://example.org
2024-12-03 18:10:44,242 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:10:44,243 [DEBUG] Base URL: https://example.com
2024-12-03 18:10:44,243 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:10:44,243 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:10:44,243 [DEBUG] Initial domain set to: example.com
2024-12-03 18:10:44,243 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:10:44,243 [INFO] Successfully crawled https://example.com
2024-12-03 18:10:44,747 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:10:44,748 [DEBUG] Base URL: https://example.org
2024-12-03 18:10:44,748 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:10:44,748 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:10:44,748 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:10:44,748 [INFO] Successfully crawled https://example.org
2024-12-03 18:10:44,749 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:44,750 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x106993750>
2024-12-03 18:10:44,750 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1065c1a90>, 520268.770299625)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x106690440>, 520269.275036666)])']
connector: <aiohttp.connector.TCPConnector object at 0x106992f10>
2024-12-03 18:10:44,750 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:10:44,751 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:44,751 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:10:44,751 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:10:44,752 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:10:45,268 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:10:45,268 [DEBUG] Base URL: https://example.com
2024-12-03 18:10:45,268 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:10:45,268 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:10:45,268 [DEBUG] Initial domain set to: example.com
2024-12-03 18:10:45,269 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:10:45,269 [INFO] Successfully crawled https://example.com
2024-12-03 18:10:45,270 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:45,270 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1069bbc50>
2024-12-03 18:10:45,270 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x106690130>, 520269.795635458)])']
connector: <aiohttp.connector.TCPConnector object at 0x1069b9f10>
2024-12-03 18:10:45,271 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:10:45,271 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:45,272 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:10:45,272 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:10:45,272 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:10:45,769 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:10:45,770 [DEBUG] Base URL: https://example.com
2024-12-03 18:10:45,770 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:10:45,770 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:10:45,770 [DEBUG] Initial domain set to: example.com
2024-12-03 18:10:45,770 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:10:45,770 [INFO] Successfully crawled https://example.com
2024-12-03 18:10:45,771 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:10:45,772 [DEBUG] Base URL: https://example.com
2024-12-03 18:10:45,772 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:10:45,772 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:10:45,772 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:10:45,779 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:45,779 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:10:45,779 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:45,780 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1069d59d0>
2024-12-03 18:10:45,781 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x106690360>, 520270.297574083)])']
connector: <aiohttp.connector.TCPConnector object at 0x1069d6b50>
2024-12-03 18:10:45,782 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:45,782 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:10:45,783 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:45,783 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:10:45,783 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:10:45,783 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:10:46,272 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:10:46,272 [DEBUG] Base URL: https://example.com
2024-12-03 18:10:46,272 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:10:46,272 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:10:46,272 [DEBUG] Initial domain set to: example.com
2024-12-03 18:10:46,273 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:10:46,273 [INFO] Successfully crawled https://example.com
2024-12-03 18:10:46,274 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:46,274 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x10665a810>
2024-12-03 18:10:46,274 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x106672040>, 520270.799517)])']
connector: <aiohttp.connector.TCPConnector object at 0x10665b650>
2024-12-03 18:10:46,275 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:10:46,275 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:46,276 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2024-12-03 18:10:46,276 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2024-12-03 18:10:46,276 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2024-12-03 18:10:48,294 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-03 18:10:48,297 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-03 18:10:48,298 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-03 18:10:48,306 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:48,307 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:10:48,307 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:48,307 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:10:48,307 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:10:48,308 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:10:48,323 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-12-03 18:10:48,323 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-12-03 18:10:48,323 [INFO] Starting crawl of URL: https://example.org
2024-12-03 18:10:48,324 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2024-12-03 18:10:48,324 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2024-12-03 18:10:48,324 [INFO] Starting crawl of URL: https://example.net
2024-12-03 18:10:48,804 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:10:48,805 [DEBUG] Base URL: https://example.com
2024-12-03 18:10:48,805 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:10:48,805 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:10:48,805 [DEBUG] Initial domain set to: example.com
2024-12-03 18:10:48,805 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:10:48,805 [INFO] Successfully crawled https://example.com
2024-12-03 18:10:49,310 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:10:49,311 [DEBUG] Base URL: https://example.org
2024-12-03 18:10:49,311 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:10:49,311 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:10:49,311 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:10:49,311 [INFO] Successfully crawled https://example.org
2024-12-03 18:10:49,811 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:10:49,811 [DEBUG] Base URL: https://example.net
2024-12-03 18:10:49,811 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:10:49,811 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:10:49,812 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:10:49,812 [INFO] Successfully crawled https://example.net
2024-12-03 18:10:49,813 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:49,814 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1068138d0>
2024-12-03 18:10:49,814 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x106690670>, 520273.331972125)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1066915c0>, 520273.83825475)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1066917f0>, 520274.338588208)])']
connector: <aiohttp.connector.TCPConnector object at 0x106993790>
2024-12-03 18:10:49,814 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:10:49,815 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:49,816 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:10:49,816 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:10:49,816 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:10:50,317 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:10:50,318 [DEBUG] Base URL: https://example.com
2024-12-03 18:10:50,318 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:10:50,318 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:10:50,318 [DEBUG] Initial domain set to: example.com
2024-12-03 18:10:50,318 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:10:50,318 [INFO] Successfully crawled https://example.com
2024-12-03 18:10:50,319 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:50,323 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:10:50,323 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:50,324 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:10:50,345 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:10:50,361 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:50,361 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:10:50,361 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:50,362 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:10:50,379 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:50,379 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:10:50,380 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:50,380 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:10:50,380 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2024-12-03 18:10:50,380 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2024-12-03 18:10:50,380 [INFO] Starting crawl of URL: https://example.com/page
2024-12-03 18:10:50,876 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:10:50,876 [DEBUG] Base URL: https://example.com/page
2024-12-03 18:10:50,877 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:10:50,877 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:10:50,877 [DEBUG] Initial domain set to: example.com
2024-12-03 18:10:50,877 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:10:50,877 [INFO] Successfully crawled https://example.com/page
2024-12-03 18:10:50,877 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2024-12-03 18:10:50,877 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2024-12-03 18:10:50,877 [INFO] Starting crawl of URL: https://other-domain.com/page
2024-12-03 18:10:53,884 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x106811e90>
2024-12-03 18:10:53,887 [DEBUG] 
Evaluating link: ./
2024-12-03 18:10:53,887 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,887 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-03 18:10:53,887 [DEBUG] Absolute link: https://other-domain.com/
2024-12-03 18:10:53,887 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,887 [DEBUG] 
Evaluating link: ./
2024-12-03 18:10:53,887 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,887 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-03 18:10:53,887 [DEBUG] Absolute link: https://other-domain.com/
2024-12-03 18:10:53,887 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,887 [DEBUG] 
Evaluating link: ./hiyou.php
2024-12-03 18:10:53,887 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,887 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-12-03 18:10:53,887 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-12-03 18:10:53,887 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,887 [DEBUG] 
Evaluating link: ./date.php
2024-12-03 18:10:53,887 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,887 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-12-03 18:10:53,887 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-12-03 18:10:53,887 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,887 [DEBUG] 
Evaluating link: ./contact.php
2024-12-03 18:10:53,887 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,887 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2024-12-03 18:10:53,887 [DEBUG] Absolute link: https://other-domain.com/contact.php
2024-12-03 18:10:53,887 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,887 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:10:53,887 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,888 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:10:53,888 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:10:53,888 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-03 18:10:53,888 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:10:53,888 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,888 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:10:53,888 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:10:53,888 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-03 18:10:53,888 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:10:53,888 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,888 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:10:53,888 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:10:53,888 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-03 18:10:53,888 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:10:53,888 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,888 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:10:53,888 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:10:53,888 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-03 18:10:53,888 [DEBUG] 
Evaluating link: ./contact
2024-12-03 18:10:53,888 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,888 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-12-03 18:10:53,888 [DEBUG] Absolute link: https://other-domain.com/contact
2024-12-03 18:10:53,888 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,888 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:10:53,888 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,888 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:10:53,888 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:10:53,888 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-03 18:10:53,888 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:10:53,888 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,888 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:10:53,888 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:10:53,888 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-03 18:10:53,888 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:10:53,888 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,888 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:10:53,888 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:10:53,889 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-03 18:10:53,889 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-12-03 18:10:53,889 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,889 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:10:53,889 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:10:53,889 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-12-03 18:10:53,889 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:10:53,889 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,889 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:10:53,889 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:10:53,889 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-03 18:10:53,889 [DEBUG] 
Evaluating link: #indexes
2024-12-03 18:10:53,889 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,889 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2024-12-03 18:10:53,889 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,889 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,889 [DEBUG] 
Evaluating link: #a
2024-12-03 18:10:53,889 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,889 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2024-12-03 18:10:53,889 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,889 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,889 [DEBUG] 
Evaluating link: #b
2024-12-03 18:10:53,889 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,889 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-12-03 18:10:53,889 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,889 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,889 [DEBUG] 
Evaluating link: #c
2024-12-03 18:10:53,889 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,889 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-12-03 18:10:53,889 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,889 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,889 [DEBUG] 
Evaluating link: #d
2024-12-03 18:10:53,889 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,889 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-12-03 18:10:53,889 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,889 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,889 [DEBUG] 
Evaluating link: #e
2024-12-03 18:10:53,889 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,889 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-12-03 18:10:53,889 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,889 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,889 [DEBUG] 
Evaluating link: #f
2024-12-03 18:10:53,889 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,889 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-12-03 18:10:53,890 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,890 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,890 [DEBUG] 
Evaluating link: #g
2024-12-03 18:10:53,890 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,890 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-12-03 18:10:53,890 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,890 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,890 [DEBUG] 
Evaluating link: #h
2024-12-03 18:10:53,890 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,890 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-12-03 18:10:53,890 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,890 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,890 [DEBUG] 
Evaluating link: #i
2024-12-03 18:10:53,890 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,890 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-12-03 18:10:53,890 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,890 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,890 [DEBUG] 
Evaluating link: #j
2024-12-03 18:10:53,890 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,890 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-12-03 18:10:53,890 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,890 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,890 [DEBUG] 
Evaluating link: #k
2024-12-03 18:10:53,890 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,890 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-12-03 18:10:53,890 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,890 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,890 [DEBUG] 
Evaluating link: #l
2024-12-03 18:10:53,890 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,890 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-12-03 18:10:53,890 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,890 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,890 [DEBUG] 
Evaluating link: #m
2024-12-03 18:10:53,890 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,890 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-12-03 18:10:53,890 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,890 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,890 [DEBUG] 
Evaluating link: #n
2024-12-03 18:10:53,890 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,890 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-12-03 18:10:53,890 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,891 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,891 [DEBUG] 
Evaluating link: #o
2024-12-03 18:10:53,891 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,891 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-12-03 18:10:53,891 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,891 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,891 [DEBUG] 
Evaluating link: #p
2024-12-03 18:10:53,891 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,891 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-12-03 18:10:53,891 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,891 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,891 [DEBUG] 
Evaluating link: #q
2024-12-03 18:10:53,891 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,891 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-12-03 18:10:53,891 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,891 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,891 [DEBUG] 
Evaluating link: #r
2024-12-03 18:10:53,891 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,891 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-12-03 18:10:53,891 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,891 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,891 [DEBUG] 
Evaluating link: #s
2024-12-03 18:10:53,891 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,891 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-12-03 18:10:53,891 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,891 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,891 [DEBUG] 
Evaluating link: #t
2024-12-03 18:10:53,891 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,891 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-12-03 18:10:53,891 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,891 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,891 [DEBUG] 
Evaluating link: #u
2024-12-03 18:10:53,891 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,891 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-12-03 18:10:53,891 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,891 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,891 [DEBUG] 
Evaluating link: #v
2024-12-03 18:10:53,891 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,891 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-12-03 18:10:53,891 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,891 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,891 [DEBUG] 
Evaluating link: #w
2024-12-03 18:10:53,892 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,892 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-12-03 18:10:53,892 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,892 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,892 [DEBUG] 
Evaluating link: #x
2024-12-03 18:10:53,892 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,892 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-12-03 18:10:53,892 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,892 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,892 [DEBUG] 
Evaluating link: #y
2024-12-03 18:10:53,892 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,892 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-12-03 18:10:53,892 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,892 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,892 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:10:53,892 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,892 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:10:53,892 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:10:53,892 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-03 18:10:53,892 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:10:53,892 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,892 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:10:53,892 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:10:53,892 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-03 18:10:53,892 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:10:53,892 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,892 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:10:53,892 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:10:53,892 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-03 18:10:53,892 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-12-03 18:10:53,892 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,892 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:10:53,892 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:10:53,892 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-12-03 18:10:53,892 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:10:53,892 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,892 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:10:53,892 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:10:53,892 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-03 18:10:53,892 [DEBUG] 
Evaluating link: https://spriq.jp/
2024-12-03 18:10:53,892 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,892 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2024-12-03 18:10:53,892 [DEBUG] Absolute link: https://spriq.jp/
2024-12-03 18:10:53,892 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2024-12-03 18:10:53,892 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:10:53,893 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,893 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:10:53,893 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:10:53,893 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-03 18:10:53,893 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:10:53,893 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,893 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:10:53,893 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:10:53,893 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-03 18:10:53,893 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:10:53,893 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,893 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:10:53,893 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:10:53,893 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-03 18:10:53,893 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:10:53,893 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,893 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:10:53,893 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:10:53,893 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-03 18:10:53,893 [DEBUG] 
Evaluating link: ./contact
2024-12-03 18:10:53,893 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,893 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-12-03 18:10:53,893 [DEBUG] Absolute link: https://other-domain.com/contact
2024-12-03 18:10:53,893 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,893 [DEBUG] 
Evaluating link: ./
2024-12-03 18:10:53,893 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,893 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-03 18:10:53,893 [DEBUG] Absolute link: https://other-domain.com/
2024-12-03 18:10:53,893 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,893 [DEBUG] 
Evaluating link: ./hiyou.php
2024-12-03 18:10:53,893 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,893 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-12-03 18:10:53,893 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-12-03 18:10:53,893 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,893 [DEBUG] 
Evaluating link: ./date.php
2024-12-03 18:10:53,893 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,893 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-12-03 18:10:53,893 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-12-03 18:10:53,893 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,893 [DEBUG] 
Evaluating link: #u
2024-12-03 18:10:53,893 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,893 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-12-03 18:10:53,893 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,893 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,893 [DEBUG] 
Evaluating link: #v
2024-12-03 18:10:53,894 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,894 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-12-03 18:10:53,894 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,894 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,894 [DEBUG] 
Evaluating link: #w
2024-12-03 18:10:53,894 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,894 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-12-03 18:10:53,894 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,894 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,894 [DEBUG] 
Evaluating link: #x
2024-12-03 18:10:53,894 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,894 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-12-03 18:10:53,894 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,894 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,894 [DEBUG] 
Evaluating link: #y
2024-12-03 18:10:53,894 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,894 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-12-03 18:10:53,894 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,894 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,894 [DEBUG] 
Evaluating link: #p
2024-12-03 18:10:53,894 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,894 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-12-03 18:10:53,894 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,894 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,894 [DEBUG] 
Evaluating link: #q
2024-12-03 18:10:53,894 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,894 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-12-03 18:10:53,894 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,894 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,894 [DEBUG] 
Evaluating link: #r
2024-12-03 18:10:53,894 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,894 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-12-03 18:10:53,894 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,894 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,894 [DEBUG] 
Evaluating link: #s
2024-12-03 18:10:53,894 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,894 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-12-03 18:10:53,894 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,894 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,894 [DEBUG] 
Evaluating link: #t
2024-12-03 18:10:53,894 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,894 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-12-03 18:10:53,894 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,894 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,894 [DEBUG] 
Evaluating link: #k
2024-12-03 18:10:53,894 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,894 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-12-03 18:10:53,894 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,895 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,895 [DEBUG] 
Evaluating link: #l
2024-12-03 18:10:53,895 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,895 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-12-03 18:10:53,895 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,895 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,895 [DEBUG] 
Evaluating link: #m
2024-12-03 18:10:53,895 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,895 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-12-03 18:10:53,895 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,895 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,895 [DEBUG] 
Evaluating link: #n
2024-12-03 18:10:53,895 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,895 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-12-03 18:10:53,895 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,895 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,895 [DEBUG] 
Evaluating link: #o
2024-12-03 18:10:53,895 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,895 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-12-03 18:10:53,895 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,895 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,895 [DEBUG] 
Evaluating link: #f
2024-12-03 18:10:53,895 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,895 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-12-03 18:10:53,895 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,895 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,895 [DEBUG] 
Evaluating link: #g
2024-12-03 18:10:53,895 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,895 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-12-03 18:10:53,895 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,895 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,895 [DEBUG] 
Evaluating link: #h
2024-12-03 18:10:53,895 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,895 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-12-03 18:10:53,895 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,895 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,895 [DEBUG] 
Evaluating link: #i
2024-12-03 18:10:53,895 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,895 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-12-03 18:10:53,895 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,895 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,895 [DEBUG] 
Evaluating link: #j
2024-12-03 18:10:53,895 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,896 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-12-03 18:10:53,896 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,896 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,896 [DEBUG] 
Evaluating link: #b
2024-12-03 18:10:53,896 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,896 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-12-03 18:10:53,896 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,896 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,896 [DEBUG] 
Evaluating link: #c
2024-12-03 18:10:53,896 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,896 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-12-03 18:10:53,896 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,896 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,896 [DEBUG] 
Evaluating link: #d
2024-12-03 18:10:53,896 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,896 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-12-03 18:10:53,896 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,896 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,896 [DEBUG] 
Evaluating link: #e
2024-12-03 18:10:53,896 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,896 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-12-03 18:10:53,896 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,896 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,896 [DEBUG] 
Evaluating link: #site
2024-12-03 18:10:53,896 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:10:53,896 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2024-12-03 18:10:53,896 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:10:53,896 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:10:53,896 [INFO] Successfully crawled https://other-domain.com/page
2024-12-03 18:10:53,899 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:53,899 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:10:53,899 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:53,900 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-12-03 18:10:53,900 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-12-03 18:10:53,900 [INFO] Starting crawl of URL: https://example.com/page0
2024-12-03 18:10:54,468 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:10:54,469 [DEBUG] Base URL: https://example.com/page0
2024-12-03 18:10:54,469 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:10:54,469 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:10:54,469 [DEBUG] Initial domain set to: example.com
2024-12-03 18:10:54,469 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:10:54,470 [INFO] Successfully crawled https://example.com/page0
2024-12-03 18:10:54,470 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-12-03 18:10:54,470 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-12-03 18:10:54,470 [INFO] Starting crawl of URL: https://example.com/page1
2024-12-03 18:10:54,653 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:10:54,653 [DEBUG] Base URL: https://example.com/page1
2024-12-03 18:10:54,653 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:10:54,653 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:10:54,653 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:10:54,653 [INFO] Successfully crawled https://example.com/page1
2024-12-03 18:10:54,653 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-12-03 18:10:54,653 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-12-03 18:10:54,654 [INFO] Starting crawl of URL: https://example.com/page2
2024-12-03 18:10:55,099 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:10:55,099 [DEBUG] Base URL: https://example.com/page2
2024-12-03 18:10:55,099 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:10:55,099 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:10:55,099 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:10:55,100 [INFO] Successfully crawled https://example.com/page2
2024-12-03 18:10:55,100 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-12-03 18:10:55,100 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-12-03 18:10:55,100 [INFO] Starting crawl of URL: https://example.com/page3
2024-12-03 18:10:55,599 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:10:55,600 [DEBUG] Base URL: https://example.com/page3
2024-12-03 18:10:55,600 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:10:55,600 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:10:55,600 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:10:55,600 [INFO] Successfully crawled https://example.com/page3
2024-12-03 18:10:55,600 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-12-03 18:10:55,600 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-12-03 18:10:55,600 [INFO] Starting crawl of URL: https://example.com/page4
2024-12-03 18:10:56,140 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:10:56,140 [DEBUG] Base URL: https://example.com/page4
2024-12-03 18:10:56,140 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:10:56,140 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:10:56,140 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:10:56,141 [INFO] Successfully crawled https://example.com/page4
2024-12-03 18:10:56,141 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:10:56,141 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-12-03 18:10:56,141 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-12-03 18:10:56,141 [INFO] Starting crawl of URL: https://example.com/page0
2024-12-03 18:10:56,629 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:10:56,629 [DEBUG] Base URL: https://example.com/page0
2024-12-03 18:10:56,629 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:10:56,630 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:10:56,630 [DEBUG] Initial domain set to: example.com
2024-12-03 18:10:56,630 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:10:56,630 [INFO] Successfully crawled https://example.com/page0
2024-12-03 18:10:56,630 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-12-03 18:10:56,630 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-12-03 18:10:56,630 [INFO] Starting crawl of URL: https://example.com/page1
2024-12-03 18:10:56,842 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:10:56,843 [DEBUG] Base URL: https://example.com/page1
2024-12-03 18:10:56,843 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:10:56,843 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:10:56,843 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:10:56,843 [INFO] Successfully crawled https://example.com/page1
2024-12-03 18:10:56,843 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-12-03 18:10:56,843 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-12-03 18:10:56,843 [INFO] Starting crawl of URL: https://example.com/page2
2024-12-03 18:10:57,282 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:10:57,282 [DEBUG] Base URL: https://example.com/page2
2024-12-03 18:10:57,283 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:10:57,283 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:10:57,283 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:10:57,283 [INFO] Successfully crawled https://example.com/page2
2024-12-03 18:10:57,283 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-12-03 18:10:57,283 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-12-03 18:10:57,284 [INFO] Starting crawl of URL: https://example.com/page3
2024-12-03 18:10:57,787 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:10:57,788 [DEBUG] Base URL: https://example.com/page3
2024-12-03 18:10:57,788 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:10:57,788 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:10:57,788 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:10:57,788 [INFO] Successfully crawled https://example.com/page3
2024-12-03 18:10:57,788 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-12-03 18:10:57,788 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-12-03 18:10:57,788 [INFO] Starting crawl of URL: https://example.com/page4
2024-12-03 18:10:58,285 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:10:58,286 [DEBUG] Base URL: https://example.com/page4
2024-12-03 18:10:58,286 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:10:58,286 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:10:58,286 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:10:58,287 [INFO] Successfully crawled https://example.com/page4
2024-12-03 18:10:58,293 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:58,294 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:58,297 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1069d85d0>
2024-12-03 18:10:58,297 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x106691550>, 520280.668226)])']
connector: <aiohttp.connector.TCPConnector object at 0x10680bd50>
2024-12-03 18:10:58,297 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1069a8c10>
2024-12-03 18:10:58,297 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x106692f20>, 520282.8131575)])']
connector: <aiohttp.connector.TCPConnector object at 0x106823690>
2024-12-03 18:10:58,300 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:58,301 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:58,304 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:58,305 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:58,307 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:58,308 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:58,311 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:58,311 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:58,314 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:58,314 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:58,316 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:58,317 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:58,320 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:58,320 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:58,323 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:58,323 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:58,326 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:58,338 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:10:58,341 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:10:58,344 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:10:58,347 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:10:58,351 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:10:58,353 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:10:58,356 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:10:58,358 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:10:58,361 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:10:58,364 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:10:58,366 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:10:58,378 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:58,381 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:58,382 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:58,382 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:10:58,385 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:58,386 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:58,386 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:10:58,389 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:58,391 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:10:58,400 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:10:58,419 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x106813310>
2024-12-03 18:10:58,419 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x106691710>, 520275.404182958)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x106691a90>, 520276.312906)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x106691be0>, 520277.241739291)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x106691c50>, 520278.404363375)])']
connector: <aiohttp.connector.TCPConnector object at 0x106811850>
2024-12-03 18:10:58,424 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:10:58,430 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:10:58,436 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:10:58,441 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:10:58,446 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:10:58,452 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:10:58,457 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:10:58,462 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:10:58,515 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:58,652 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:10:58,654 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:10:58,654 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:10:58,654 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:10:58,654 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:10:59,162 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:10:59,162 [DEBUG] Base URL: https://example.com
2024-12-03 18:10:59,162 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:10:59,163 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:10:59,163 [DEBUG] Initial domain set to: example.com
2024-12-03 18:10:59,163 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:10:59,163 [INFO] Successfully crawled https://example.com
2024-12-03 18:10:59,163 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x121386490>
2024-12-03 18:10:59,164 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x121363e00>, 520283.68986375)])']
connector: <aiohttp.connector.TCPConnector object at 0x121385490>
2024-12-03 18:10:59,166 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2024-12-03 18:11:00,351 [INFO] Operation took 0.10 seconds
2024-12-03 18:16:02,847 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-03 18:16:03,130 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:03,131 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:03,131 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:03,132 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:03,133 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:03,133 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:03,133 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:16:03,166 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:03,167 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:03,167 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:16:03,169 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:03,170 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:03,172 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:03,172 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:03,175 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:03,175 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:03,175 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:03,176 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:03,179 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:03,179 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:03,179 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:16:03,182 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:04,724 [INFO] Operation took 0.04 seconds
2024-12-03 18:16:04,973 [INFO] test_operation took 0.11 seconds
2024-12-03 18:16:05,182 [INFO] inner took 0.10 seconds
2024-12-03 18:16:05,182 [INFO] outer took 0.21 seconds
2024-12-03 18:16:05,182 [INFO] error_operation took 0.00 seconds
2024-12-03 18:16:05,237 [INFO] test_operation took 0.00 seconds
2024-12-03 18:16:05,262 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:16:05,262 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:05,263 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:16:05,263 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:16:05,263 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:16:05,769 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:16:05,769 [DEBUG] Base URL: https://example.com
2024-12-03 18:16:05,770 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:16:05,770 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:16:05,770 [DEBUG] Initial domain set to: example.com
2024-12-03 18:16:05,770 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:16:05,770 [INFO] Successfully crawled https://example.com
2024-12-03 18:16:05,771 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:05,772 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x103e1f850>
2024-12-03 18:16:05,772 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1042189f0>, 520590.303100541)])']
connector: <aiohttp.connector.TCPConnector object at 0x1044e8250>
2024-12-03 18:16:05,773 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:16:05,773 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:05,774 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:16:05,774 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:16:05,775 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:16:05,791 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-12-03 18:16:05,791 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-12-03 18:16:05,791 [INFO] Starting crawl of URL: https://example.org
2024-12-03 18:16:06,315 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:16:06,316 [DEBUG] Base URL: https://example.com
2024-12-03 18:16:06,316 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:16:06,316 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:16:06,316 [DEBUG] Initial domain set to: example.com
2024-12-03 18:16:06,316 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:16:06,316 [INFO] Successfully crawled https://example.com
2024-12-03 18:16:06,778 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:16:06,779 [DEBUG] Base URL: https://example.org
2024-12-03 18:16:06,779 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:16:06,779 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:16:06,779 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:16:06,779 [INFO] Successfully crawled https://example.org
2024-12-03 18:16:06,781 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:06,781 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x104465050>
2024-12-03 18:16:06,781 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1042181a0>, 520590.849549041)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1042e84b0>, 520591.312496958)])']
connector: <aiohttp.connector.TCPConnector object at 0x104466750>
2024-12-03 18:16:06,782 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:16:06,782 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:06,783 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:16:06,783 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:16:06,783 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:16:07,292 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:16:07,292 [DEBUG] Base URL: https://example.com
2024-12-03 18:16:07,292 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:16:07,292 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:16:07,293 [DEBUG] Initial domain set to: example.com
2024-12-03 18:16:07,293 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:16:07,293 [INFO] Successfully crawled https://example.com
2024-12-03 18:16:07,294 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:07,294 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x104466fd0>
2024-12-03 18:16:07,294 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1042e8210>, 520591.826389125)])']
connector: <aiohttp.connector.TCPConnector object at 0x104466190>
2024-12-03 18:16:07,295 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:16:07,295 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:07,296 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:16:07,296 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:16:07,296 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:16:07,806 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:16:07,806 [DEBUG] Base URL: https://example.com
2024-12-03 18:16:07,807 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:16:07,807 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:16:07,807 [DEBUG] Initial domain set to: example.com
2024-12-03 18:16:07,807 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:16:07,807 [INFO] Successfully crawled https://example.com
2024-12-03 18:16:07,808 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:16:07,808 [DEBUG] Base URL: https://example.com
2024-12-03 18:16:07,808 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:16:07,808 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:16:07,808 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:16:07,816 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:07,816 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:16:07,817 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:07,818 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x104464490>
2024-12-03 18:16:07,818 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1042e8440>, 520592.340562708)])']
connector: <aiohttp.connector.TCPConnector object at 0x104467110>
2024-12-03 18:16:07,820 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:07,820 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:16:07,820 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:07,821 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:16:07,821 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:16:07,821 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:16:08,345 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:16:08,346 [DEBUG] Base URL: https://example.com
2024-12-03 18:16:08,346 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:16:08,346 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:16:08,346 [DEBUG] Initial domain set to: example.com
2024-12-03 18:16:08,346 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:16:08,346 [INFO] Successfully crawled https://example.com
2024-12-03 18:16:08,348 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:08,349 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x10453b650>
2024-12-03 18:16:08,349 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1042e5f60>, 520592.878764708)])']
connector: <aiohttp.connector.TCPConnector object at 0x10453a250>
2024-12-03 18:16:08,350 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:16:08,350 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:08,351 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2024-12-03 18:16:08,351 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2024-12-03 18:16:08,352 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2024-12-03 18:16:10,370 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-03 18:16:10,374 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-03 18:16:10,375 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-03 18:16:10,384 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:10,385 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:16:10,385 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:10,385 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:16:10,385 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:16:10,385 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:16:10,400 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-12-03 18:16:10,400 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-12-03 18:16:10,400 [INFO] Starting crawl of URL: https://example.org
2024-12-03 18:16:10,400 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2024-12-03 18:16:10,401 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2024-12-03 18:16:10,401 [INFO] Starting crawl of URL: https://example.net
2024-12-03 18:16:10,901 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:16:10,902 [DEBUG] Base URL: https://example.com
2024-12-03 18:16:10,902 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:16:10,902 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:16:10,902 [DEBUG] Initial domain set to: example.com
2024-12-03 18:16:10,902 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:16:10,902 [INFO] Successfully crawled https://example.com
2024-12-03 18:16:11,395 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:16:11,395 [DEBUG] Base URL: https://example.org
2024-12-03 18:16:11,396 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:16:11,396 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:16:11,396 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:16:11,396 [INFO] Successfully crawled https://example.org
2024-12-03 18:16:11,885 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:16:11,886 [DEBUG] Base URL: https://example.net
2024-12-03 18:16:11,886 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:16:11,886 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:16:11,886 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:16:11,886 [INFO] Successfully crawled https://example.net
2024-12-03 18:16:11,888 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:11,888 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x104412850>
2024-12-03 18:16:11,888 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1042e8670>, 520595.435499291)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1042e95c0>, 520595.929446208)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1042e97f0>, 520596.419729208)])']
connector: <aiohttp.connector.TCPConnector object at 0x104410250>
2024-12-03 18:16:11,889 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:16:11,889 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:11,890 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:16:11,890 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:16:11,890 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:16:12,411 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:16:12,411 [DEBUG] Base URL: https://example.com
2024-12-03 18:16:12,411 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:16:12,411 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:16:12,411 [DEBUG] Initial domain set to: example.com
2024-12-03 18:16:12,411 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:16:12,412 [INFO] Successfully crawled https://example.com
2024-12-03 18:16:12,413 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:12,416 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:16:12,416 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:12,417 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:16:12,440 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:16:12,456 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:12,456 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:16:12,456 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:12,457 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:16:12,474 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:12,474 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:16:12,474 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:12,475 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:16:12,475 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2024-12-03 18:16:12,475 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2024-12-03 18:16:12,475 [INFO] Starting crawl of URL: https://example.com/page
2024-12-03 18:16:12,972 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:16:12,972 [DEBUG] Base URL: https://example.com/page
2024-12-03 18:16:12,973 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:16:12,973 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:16:12,973 [DEBUG] Initial domain set to: example.com
2024-12-03 18:16:12,973 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:16:12,973 [INFO] Successfully crawled https://example.com/page
2024-12-03 18:16:12,973 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2024-12-03 18:16:12,973 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2024-12-03 18:16:12,973 [INFO] Starting crawl of URL: https://other-domain.com/page
2024-12-03 18:16:16,207 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x104496950>
2024-12-03 18:16:16,209 [DEBUG] 
Evaluating link: ./
2024-12-03 18:16:16,209 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,209 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-03 18:16:16,209 [DEBUG] Absolute link: https://other-domain.com/
2024-12-03 18:16:16,209 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,209 [DEBUG] 
Evaluating link: ./
2024-12-03 18:16:16,209 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,209 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-03 18:16:16,209 [DEBUG] Absolute link: https://other-domain.com/
2024-12-03 18:16:16,209 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,209 [DEBUG] 
Evaluating link: ./hiyou.php
2024-12-03 18:16:16,209 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,209 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-12-03 18:16:16,209 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-12-03 18:16:16,209 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,209 [DEBUG] 
Evaluating link: ./date.php
2024-12-03 18:16:16,209 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,209 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-12-03 18:16:16,209 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-12-03 18:16:16,209 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,209 [DEBUG] 
Evaluating link: ./contact.php
2024-12-03 18:16:16,209 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,209 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2024-12-03 18:16:16,209 [DEBUG] Absolute link: https://other-domain.com/contact.php
2024-12-03 18:16:16,209 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,209 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:16:16,209 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,209 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:16:16,209 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:16:16,209 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-03 18:16:16,209 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:16:16,209 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,210 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:16:16,210 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:16:16,210 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-03 18:16:16,210 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:16:16,210 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,210 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:16:16,210 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:16:16,210 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-03 18:16:16,210 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:16:16,210 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,210 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:16:16,210 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:16:16,210 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-03 18:16:16,210 [DEBUG] 
Evaluating link: ./contact
2024-12-03 18:16:16,210 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,210 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-12-03 18:16:16,210 [DEBUG] Absolute link: https://other-domain.com/contact
2024-12-03 18:16:16,210 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,210 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:16:16,210 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,210 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:16:16,210 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:16:16,210 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-03 18:16:16,210 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:16:16,210 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,210 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:16:16,210 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:16:16,210 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-03 18:16:16,210 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:16:16,210 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,210 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:16:16,210 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:16:16,210 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-03 18:16:16,210 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-12-03 18:16:16,210 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,210 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:16:16,210 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:16:16,210 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-12-03 18:16:16,210 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:16:16,210 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,210 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:16:16,210 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:16:16,210 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-03 18:16:16,210 [DEBUG] 
Evaluating link: #indexes
2024-12-03 18:16:16,210 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,211 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2024-12-03 18:16:16,211 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,211 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,211 [DEBUG] 
Evaluating link: #a
2024-12-03 18:16:16,211 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,211 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2024-12-03 18:16:16,211 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,211 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,211 [DEBUG] 
Evaluating link: #b
2024-12-03 18:16:16,211 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,211 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-12-03 18:16:16,211 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,211 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,211 [DEBUG] 
Evaluating link: #c
2024-12-03 18:16:16,211 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,211 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-12-03 18:16:16,211 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,211 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,211 [DEBUG] 
Evaluating link: #d
2024-12-03 18:16:16,211 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,211 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-12-03 18:16:16,211 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,211 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,211 [DEBUG] 
Evaluating link: #e
2024-12-03 18:16:16,211 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,211 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-12-03 18:16:16,211 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,211 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,211 [DEBUG] 
Evaluating link: #f
2024-12-03 18:16:16,211 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,211 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-12-03 18:16:16,211 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,211 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,211 [DEBUG] 
Evaluating link: #g
2024-12-03 18:16:16,211 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,211 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-12-03 18:16:16,211 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,211 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,211 [DEBUG] 
Evaluating link: #h
2024-12-03 18:16:16,211 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,211 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-12-03 18:16:16,211 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,211 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,211 [DEBUG] 
Evaluating link: #i
2024-12-03 18:16:16,211 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,211 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-12-03 18:16:16,211 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,212 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,212 [DEBUG] 
Evaluating link: #j
2024-12-03 18:16:16,212 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,212 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-12-03 18:16:16,212 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,212 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,212 [DEBUG] 
Evaluating link: #k
2024-12-03 18:16:16,212 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,212 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-12-03 18:16:16,212 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,212 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,212 [DEBUG] 
Evaluating link: #l
2024-12-03 18:16:16,212 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,212 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-12-03 18:16:16,212 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,212 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,212 [DEBUG] 
Evaluating link: #m
2024-12-03 18:16:16,212 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,212 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-12-03 18:16:16,212 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,212 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,212 [DEBUG] 
Evaluating link: #n
2024-12-03 18:16:16,212 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,212 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-12-03 18:16:16,212 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,212 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,212 [DEBUG] 
Evaluating link: #o
2024-12-03 18:16:16,212 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,212 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-12-03 18:16:16,212 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,212 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,212 [DEBUG] 
Evaluating link: #p
2024-12-03 18:16:16,212 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,212 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-12-03 18:16:16,212 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,212 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,212 [DEBUG] 
Evaluating link: #q
2024-12-03 18:16:16,212 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,212 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-12-03 18:16:16,212 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,212 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,212 [DEBUG] 
Evaluating link: #r
2024-12-03 18:16:16,212 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,212 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-12-03 18:16:16,212 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,212 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,213 [DEBUG] 
Evaluating link: #s
2024-12-03 18:16:16,213 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,213 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-12-03 18:16:16,213 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,213 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,213 [DEBUG] 
Evaluating link: #t
2024-12-03 18:16:16,213 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,213 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-12-03 18:16:16,213 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,213 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,213 [DEBUG] 
Evaluating link: #u
2024-12-03 18:16:16,213 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,213 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-12-03 18:16:16,213 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,213 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,213 [DEBUG] 
Evaluating link: #v
2024-12-03 18:16:16,213 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,213 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-12-03 18:16:16,213 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,213 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,213 [DEBUG] 
Evaluating link: #w
2024-12-03 18:16:16,213 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,213 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-12-03 18:16:16,213 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,213 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,213 [DEBUG] 
Evaluating link: #x
2024-12-03 18:16:16,213 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,213 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-12-03 18:16:16,213 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,213 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,213 [DEBUG] 
Evaluating link: #y
2024-12-03 18:16:16,213 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,213 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-12-03 18:16:16,213 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,213 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,213 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:16:16,213 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,213 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:16:16,213 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:16:16,213 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-03 18:16:16,213 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:16:16,213 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,213 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:16:16,213 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:16:16,213 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-03 18:16:16,213 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:16:16,213 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,213 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:16:16,214 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:16:16,214 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-03 18:16:16,214 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-12-03 18:16:16,214 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,214 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:16:16,214 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:16:16,214 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-12-03 18:16:16,214 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:16:16,214 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,214 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:16:16,214 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-03 18:16:16,214 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-03 18:16:16,214 [DEBUG] 
Evaluating link: https://spriq.jp/
2024-12-03 18:16:16,214 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,214 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2024-12-03 18:16:16,214 [DEBUG] Absolute link: https://spriq.jp/
2024-12-03 18:16:16,214 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2024-12-03 18:16:16,214 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:16:16,214 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,214 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:16:16,214 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:16:16,214 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-03 18:16:16,214 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:16:16,214 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,214 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:16:16,214 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-03 18:16:16,214 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-03 18:16:16,214 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:16:16,214 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,214 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:16:16,214 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-03 18:16:16,214 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-03 18:16:16,214 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:16:16,214 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,214 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:16:16,214 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-03 18:16:16,214 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-03 18:16:16,214 [DEBUG] 
Evaluating link: ./contact
2024-12-03 18:16:16,214 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,214 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-12-03 18:16:16,214 [DEBUG] Absolute link: https://other-domain.com/contact
2024-12-03 18:16:16,214 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,214 [DEBUG] 
Evaluating link: ./
2024-12-03 18:16:16,214 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,214 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-03 18:16:16,214 [DEBUG] Absolute link: https://other-domain.com/
2024-12-03 18:16:16,215 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,215 [DEBUG] 
Evaluating link: ./hiyou.php
2024-12-03 18:16:16,215 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,215 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-12-03 18:16:16,215 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-12-03 18:16:16,215 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,215 [DEBUG] 
Evaluating link: ./date.php
2024-12-03 18:16:16,215 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,215 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-12-03 18:16:16,215 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-12-03 18:16:16,215 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,215 [DEBUG] 
Evaluating link: #u
2024-12-03 18:16:16,215 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,215 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-12-03 18:16:16,215 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,215 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,215 [DEBUG] 
Evaluating link: #v
2024-12-03 18:16:16,215 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,215 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-12-03 18:16:16,215 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,215 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,215 [DEBUG] 
Evaluating link: #w
2024-12-03 18:16:16,215 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,215 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-12-03 18:16:16,215 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,215 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,215 [DEBUG] 
Evaluating link: #x
2024-12-03 18:16:16,215 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,215 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-12-03 18:16:16,215 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,215 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,215 [DEBUG] 
Evaluating link: #y
2024-12-03 18:16:16,215 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,215 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-12-03 18:16:16,215 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,215 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,215 [DEBUG] 
Evaluating link: #p
2024-12-03 18:16:16,215 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,215 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-12-03 18:16:16,215 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,215 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,215 [DEBUG] 
Evaluating link: #q
2024-12-03 18:16:16,216 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,216 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-12-03 18:16:16,216 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,216 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,216 [DEBUG] 
Evaluating link: #r
2024-12-03 18:16:16,216 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,216 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-12-03 18:16:16,216 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,216 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,216 [DEBUG] 
Evaluating link: #s
2024-12-03 18:16:16,216 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,216 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-12-03 18:16:16,216 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,216 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,216 [DEBUG] 
Evaluating link: #t
2024-12-03 18:16:16,216 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,216 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-12-03 18:16:16,216 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,216 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,216 [DEBUG] 
Evaluating link: #k
2024-12-03 18:16:16,216 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,216 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-12-03 18:16:16,216 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,216 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,216 [DEBUG] 
Evaluating link: #l
2024-12-03 18:16:16,216 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,216 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-12-03 18:16:16,216 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,216 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,216 [DEBUG] 
Evaluating link: #m
2024-12-03 18:16:16,216 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,216 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-12-03 18:16:16,216 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,216 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,216 [DEBUG] 
Evaluating link: #n
2024-12-03 18:16:16,216 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,216 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-12-03 18:16:16,216 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,216 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,216 [DEBUG] 
Evaluating link: #o
2024-12-03 18:16:16,217 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,217 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-12-03 18:16:16,217 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,217 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,217 [DEBUG] 
Evaluating link: #f
2024-12-03 18:16:16,217 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,217 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-12-03 18:16:16,217 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,217 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,217 [DEBUG] 
Evaluating link: #g
2024-12-03 18:16:16,217 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,217 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-12-03 18:16:16,217 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,217 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,217 [DEBUG] 
Evaluating link: #h
2024-12-03 18:16:16,217 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,217 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-12-03 18:16:16,217 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,217 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,217 [DEBUG] 
Evaluating link: #i
2024-12-03 18:16:16,217 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,217 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-12-03 18:16:16,217 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,217 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,217 [DEBUG] 
Evaluating link: #j
2024-12-03 18:16:16,217 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,217 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-12-03 18:16:16,217 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,217 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,217 [DEBUG] 
Evaluating link: #b
2024-12-03 18:16:16,217 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,217 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-12-03 18:16:16,217 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,217 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,217 [DEBUG] 
Evaluating link: #c
2024-12-03 18:16:16,217 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,217 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-12-03 18:16:16,217 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,217 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,217 [DEBUG] 
Evaluating link: #d
2024-12-03 18:16:16,217 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,217 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-12-03 18:16:16,218 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,218 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,218 [DEBUG] 
Evaluating link: #e
2024-12-03 18:16:16,218 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,218 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-12-03 18:16:16,218 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,218 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,218 [DEBUG] 
Evaluating link: #site
2024-12-03 18:16:16,218 [DEBUG] Base URL: https://other-domain.com/page
2024-12-03 18:16:16,218 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2024-12-03 18:16:16,218 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-03 18:16:16,218 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-03 18:16:16,218 [INFO] Successfully crawled https://other-domain.com/page
2024-12-03 18:16:16,221 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:16,221 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:16:16,221 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:16,222 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-12-03 18:16:16,222 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-12-03 18:16:16,222 [INFO] Starting crawl of URL: https://example.com/page0
2024-12-03 18:16:16,729 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:16:16,729 [DEBUG] Base URL: https://example.com/page0
2024-12-03 18:16:16,729 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:16:16,729 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:16:16,729 [DEBUG] Initial domain set to: example.com
2024-12-03 18:16:16,730 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:16:16,730 [INFO] Successfully crawled https://example.com/page0
2024-12-03 18:16:16,730 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-12-03 18:16:16,730 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-12-03 18:16:16,730 [INFO] Starting crawl of URL: https://example.com/page1
2024-12-03 18:16:16,862 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:16:16,863 [DEBUG] Base URL: https://example.com/page1
2024-12-03 18:16:16,863 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:16:16,863 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:16:16,863 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:16:16,863 [INFO] Successfully crawled https://example.com/page1
2024-12-03 18:16:16,863 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-12-03 18:16:16,863 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-12-03 18:16:16,863 [INFO] Starting crawl of URL: https://example.com/page2
2024-12-03 18:16:17,363 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:16:17,363 [DEBUG] Base URL: https://example.com/page2
2024-12-03 18:16:17,364 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:16:17,364 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:16:17,364 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:16:17,364 [INFO] Successfully crawled https://example.com/page2
2024-12-03 18:16:17,364 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-12-03 18:16:17,364 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-12-03 18:16:17,364 [INFO] Starting crawl of URL: https://example.com/page3
2024-12-03 18:16:17,866 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:16:17,866 [DEBUG] Base URL: https://example.com/page3
2024-12-03 18:16:17,866 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:16:17,866 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:16:17,866 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:16:17,866 [INFO] Successfully crawled https://example.com/page3
2024-12-03 18:16:17,866 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-12-03 18:16:17,866 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-12-03 18:16:17,866 [INFO] Starting crawl of URL: https://example.com/page4
2024-12-03 18:16:18,384 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:16:18,385 [DEBUG] Base URL: https://example.com/page4
2024-12-03 18:16:18,385 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:16:18,385 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:16:18,385 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:16:18,385 [INFO] Successfully crawled https://example.com/page4
2024-12-03 18:16:18,385 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:16:18,385 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-12-03 18:16:18,385 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-12-03 18:16:18,385 [INFO] Starting crawl of URL: https://example.com/page0
2024-12-03 18:16:18,913 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:16:18,913 [DEBUG] Base URL: https://example.com/page0
2024-12-03 18:16:18,913 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:16:18,913 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:16:18,913 [DEBUG] Initial domain set to: example.com
2024-12-03 18:16:18,913 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:16:18,914 [INFO] Successfully crawled https://example.com/page0
2024-12-03 18:16:18,914 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-12-03 18:16:18,914 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-12-03 18:16:18,914 [INFO] Starting crawl of URL: https://example.com/page1
2024-12-03 18:16:19,052 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:16:19,052 [DEBUG] Base URL: https://example.com/page1
2024-12-03 18:16:19,053 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:16:19,053 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:16:19,053 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:16:19,053 [INFO] Successfully crawled https://example.com/page1
2024-12-03 18:16:19,053 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-12-03 18:16:19,053 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-12-03 18:16:19,053 [INFO] Starting crawl of URL: https://example.com/page2
2024-12-03 18:16:19,540 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:16:19,541 [DEBUG] Base URL: https://example.com/page2
2024-12-03 18:16:19,541 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:16:19,541 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:16:19,541 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:16:19,541 [INFO] Successfully crawled https://example.com/page2
2024-12-03 18:16:19,541 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-12-03 18:16:19,541 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-12-03 18:16:19,542 [INFO] Starting crawl of URL: https://example.com/page3
2024-12-03 18:16:20,055 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:16:20,055 [DEBUG] Base URL: https://example.com/page3
2024-12-03 18:16:20,055 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:16:20,055 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:16:20,055 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:16:20,055 [INFO] Successfully crawled https://example.com/page3
2024-12-03 18:16:20,056 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-12-03 18:16:20,056 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-12-03 18:16:20,056 [INFO] Starting crawl of URL: https://example.com/page4
2024-12-03 18:16:20,559 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:16:20,559 [DEBUG] Base URL: https://example.com/page4
2024-12-03 18:16:20,559 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:16:20,559 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:16:20,559 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:16:20,560 [INFO] Successfully crawled https://example.com/page4
2024-12-03 18:16:20,562 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:20,563 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:20,565 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x10435d450>
2024-12-03 18:16:20,565 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1042e9550>, 520602.919832708)])']
connector: <aiohttp.connector.TCPConnector object at 0x104271d90>
2024-12-03 18:16:20,565 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x104410190>
2024-12-03 18:16:20,565 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1042eaf20>, 520605.095760958)])']
connector: <aiohttp.connector.TCPConnector object at 0x103be6d10>
2024-12-03 18:16:20,568 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:20,568 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:20,571 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:20,571 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:20,573 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:20,574 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:20,576 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:20,577 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:20,579 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:20,580 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:20,582 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:20,583 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:20,585 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:20,586 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:20,588 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:20,589 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:20,592 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:20,605 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:16:20,608 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:16:20,610 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:16:20,613 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:16:20,615 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:16:20,618 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:16:20,620 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:16:20,623 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:16:20,626 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:16:20,629 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:16:20,631 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:16:20,643 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:20,646 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:20,646 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:20,647 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:16:20,649 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:20,650 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:20,650 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:16:20,653 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:20,656 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:16:20,662 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:16:20,678 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1043a8d10>
2024-12-03 18:16:20,678 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1042e9710>, 520597.50713675)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1042e9a90>, 520598.540862166)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1042e9be0>, 520599.433781333)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1042e9c50>, 520600.729581166)])']
connector: <aiohttp.connector.TCPConnector object at 0x1045399d0>
2024-12-03 18:16:20,684 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:16:20,689 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:16:20,695 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:16:20,701 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:16:20,707 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:16:20,712 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:16:20,717 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:16:20,723 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-03 18:16:20,776 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:20,908 [DEBUG] Using selector: KqueueSelector
2024-12-03 18:16:20,909 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-03 18:16:20,909 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-03 18:16:20,909 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-03 18:16:20,909 [INFO] Starting crawl of URL: https://example.com
2024-12-03 18:16:21,446 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-03 18:16:21,446 [DEBUG] Base URL: https://example.com
2024-12-03 18:16:21,447 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-03 18:16:21,447 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-03 18:16:21,447 [DEBUG] Initial domain set to: example.com
2024-12-03 18:16:21,447 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-03 18:16:21,447 [INFO] Successfully crawled https://example.com
2024-12-03 18:16:21,447 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x11007aa10>
2024-12-03 18:16:21,447 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x11005bd90>, 520605.980697125)])']
connector: <aiohttp.connector.TCPConnector object at 0x1100795d0>
2024-12-03 18:16:21,449 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2024-12-03 18:16:22,619 [INFO] Operation took 0.11 seconds
2024-12-03 18:20:20,764 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-05 01:49:37,856 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-05 01:49:40,521 [INFO] Operation took 0.03 seconds
2024-12-05 01:49:40,785 [INFO] test_operation took 0.10 seconds
2024-12-05 01:49:40,995 [INFO] inner took 0.10 seconds
2024-12-05 01:49:40,995 [INFO] outer took 0.21 seconds
2024-12-05 01:49:40,995 [INFO] error_operation took 0.00 seconds
2024-12-05 01:49:41,043 [INFO] test_operation took 0.00 seconds
2024-12-05 01:50:23,995 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-05 01:50:26,202 [INFO] Operation took 0.03 seconds
2024-12-05 01:50:26,512 [INFO] test_operation took 0.10 seconds
2024-12-05 01:50:26,713 [INFO] inner took 0.10 seconds
2024-12-05 01:50:26,714 [INFO] outer took 0.20 seconds
2024-12-05 01:50:26,715 [INFO] error_operation took 0.00 seconds
2024-12-05 01:50:26,776 [INFO] test_operation took 0.00 seconds
2024-12-05 01:51:46,607 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-05 01:51:48,869 [INFO] Operation took 0.03 seconds
2024-12-05 01:51:49,097 [INFO] test_operation took 0.10 seconds
2024-12-05 01:51:49,299 [INFO] inner took 0.10 seconds
2024-12-05 01:51:49,299 [INFO] outer took 0.20 seconds
2024-12-05 01:51:49,299 [INFO] error_operation took 0.00 seconds
2024-12-05 01:51:49,325 [INFO] test_operation took 0.00 seconds
2024-12-05 02:17:19,300 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-05 02:17:21,429 [INFO] Operation took 0.03 seconds
2024-12-05 02:17:21,681 [INFO] test_operation took 0.10 seconds
2024-12-05 02:17:21,887 [INFO] inner took 0.11 seconds
2024-12-05 02:17:21,887 [INFO] outer took 0.21 seconds
2024-12-05 02:17:21,887 [INFO] error_operation took 0.00 seconds
2024-12-05 02:17:21,918 [INFO] test_operation took 0.00 seconds
2024-12-05 10:55:22,603 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-05 10:56:53,059 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-05 10:56:54,750 [INFO] Operation took 0.02 seconds
2024-12-05 10:56:54,965 [INFO] test_operation took 0.10 seconds
2024-12-05 10:56:55,167 [INFO] inner took 0.10 seconds
2024-12-05 10:56:55,169 [INFO] outer took 0.20 seconds
2024-12-05 10:56:55,169 [INFO] error_operation took 0.00 seconds
2024-12-05 10:56:55,227 [INFO] test_operation took 0.00 seconds
2024-12-05 17:01:58,948 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-05 17:01:59,264 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:01:59,265 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:01:59,266 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:01:59,267 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:01:59,268 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:01:59,268 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:01:59,269 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:01:59,289 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:01:59,290 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:01:59,290 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:01:59,293 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:01:59,293 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:01:59,296 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:01:59,296 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:01:59,299 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:01:59,299 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:01:59,300 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:01:59,300 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:01:59,302 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:01:59,303 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:01:59,303 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:01:59,306 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:00,964 [INFO] Operation took 0.04 seconds
2024-12-05 17:02:01,167 [INFO] test_operation took 0.10 seconds
2024-12-05 17:02:01,370 [INFO] inner took 0.10 seconds
2024-12-05 17:02:01,371 [INFO] outer took 0.20 seconds
2024-12-05 17:02:01,371 [INFO] error_operation took 0.00 seconds
2024-12-05 17:02:01,392 [INFO] test_operation took 0.00 seconds
2024-12-05 17:02:01,491 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:02:01,491 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:01,492 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 17:02:01,492 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 17:02:01,492 [INFO] Starting crawl of URL: https://example.com
2024-12-05 17:02:02,053 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:02:02,053 [DEBUG] Base URL: https://example.com
2024-12-05 17:02:02,053 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:02:02,053 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:02:02,053 [DEBUG] Initial domain set to: example.com
2024-12-05 17:02:02,053 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:02:02,053 [INFO] Successfully crawled https://example.com
2024-12-05 17:02:02,054 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:02,054 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1106e7650>
2024-12-05 17:02:02,054 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x11076de10>, 688946.464388)])']
connector: <aiohttp.connector.TCPConnector object at 0x110547dd0>
2024-12-05 17:02:02,054 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:02:02,055 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:02,055 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 17:02:02,055 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 17:02:02,055 [INFO] Starting crawl of URL: https://example.com
2024-12-05 17:02:02,071 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-12-05 17:02:02,071 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-12-05 17:02:02,071 [INFO] Starting crawl of URL: https://example.org
2024-12-05 17:02:02,610 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:02:02,610 [DEBUG] Base URL: https://example.com
2024-12-05 17:02:02,610 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:02:02,610 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:02:02,610 [DEBUG] Initial domain set to: example.com
2024-12-05 17:02:02,610 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:02:02,610 [INFO] Successfully crawled https://example.com
2024-12-05 17:02:03,126 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:02:03,127 [DEBUG] Base URL: https://example.org
2024-12-05 17:02:03,127 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:02:03,127 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:02:03,127 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:02:03,127 [INFO] Successfully crawled https://example.org
2024-12-05 17:02:03,129 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:03,129 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1106e6b50>
2024-12-05 17:02:03,130 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x11076da90>, 688947.019936625)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x11076de10>, 688947.537011583)])']
connector: <aiohttp.connector.TCPConnector object at 0x1106e43d0>
2024-12-05 17:02:03,131 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:02:03,132 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:03,133 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 17:02:03,133 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 17:02:03,134 [INFO] Starting crawl of URL: https://example.com
2024-12-05 17:02:03,681 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:02:03,682 [DEBUG] Base URL: https://example.com
2024-12-05 17:02:03,682 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:02:03,682 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:02:03,683 [DEBUG] Initial domain set to: example.com
2024-12-05 17:02:03,683 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:02:03,683 [INFO] Successfully crawled https://example.com
2024-12-05 17:02:03,684 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:03,685 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x110559890>
2024-12-05 17:02:03,686 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x11076da20>, 688948.090989)])']
connector: <aiohttp.connector.TCPConnector object at 0x11055bd10>
2024-12-05 17:02:03,687 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:02:03,688 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:03,689 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 17:02:03,689 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 17:02:03,689 [INFO] Starting crawl of URL: https://example.com
2024-12-05 17:02:04,272 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:02:04,272 [DEBUG] Base URL: https://example.com
2024-12-05 17:02:04,272 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:02:04,272 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:02:04,272 [DEBUG] Initial domain set to: example.com
2024-12-05 17:02:04,272 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:02:04,272 [INFO] Successfully crawled https://example.com
2024-12-05 17:02:04,272 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:02:04,272 [DEBUG] Base URL: https://example.com
2024-12-05 17:02:04,272 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:02:04,272 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:02:04,272 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:02:04,277 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:04,277 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:04,277 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:04,278 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:04,278 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:02:04,278 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:04,279 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 17:02:04,279 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 17:02:04,279 [INFO] Starting crawl of URL: https://example.com
2024-12-05 17:02:04,834 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:02:04,834 [DEBUG] Base URL: https://example.com
2024-12-05 17:02:04,834 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:02:04,834 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:02:04,834 [DEBUG] Initial domain set to: example.com
2024-12-05 17:02:04,834 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:02:04,834 [INFO] Successfully crawled https://example.com
2024-12-05 17:02:04,834 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:04,835 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x11075e910>
2024-12-05 17:02:04,835 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x11076dc50>, 688949.244998166)])']
connector: <aiohttp.connector.TCPConnector object at 0x11075c2d0>
2024-12-05 17:02:04,835 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:02:04,835 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:04,835 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2024-12-05 17:02:04,835 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2024-12-05 17:02:04,836 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2024-12-05 17:02:06,896 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-05 17:02:06,901 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-05 17:02:06,901 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-05 17:02:06,904 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1105d1990>
2024-12-05 17:02:06,904 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x11076dcc0>, 688948.683105041)])']
connector: <aiohttp.connector.TCPConnector object at 0x1106d4cd0>
2024-12-05 17:02:06,907 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:06,907 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:02:06,907 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:06,908 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 17:02:06,908 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 17:02:06,908 [INFO] Starting crawl of URL: https://example.com
2024-12-05 17:02:06,923 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-12-05 17:02:06,923 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-12-05 17:02:06,923 [INFO] Starting crawl of URL: https://example.org
2024-12-05 17:02:06,923 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2024-12-05 17:02:06,923 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2024-12-05 17:02:06,923 [INFO] Starting crawl of URL: https://example.net
2024-12-05 17:02:07,458 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:02:07,458 [DEBUG] Base URL: https://example.com
2024-12-05 17:02:07,458 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:02:07,458 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:02:07,458 [DEBUG] Initial domain set to: example.com
2024-12-05 17:02:07,458 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:02:07,458 [INFO] Successfully crawled https://example.com
2024-12-05 17:02:07,967 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:02:07,967 [DEBUG] Base URL: https://example.org
2024-12-05 17:02:07,967 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:02:07,967 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:02:07,967 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:02:07,967 [INFO] Successfully crawled https://example.org
2024-12-05 17:02:08,587 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:02:08,587 [DEBUG] Base URL: https://example.net
2024-12-05 17:02:08,587 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:02:08,587 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:02:08,587 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:02:08,587 [INFO] Successfully crawled https://example.net
2024-12-05 17:02:08,588 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:08,588 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x11059a350>
2024-12-05 17:02:08,588 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x11076f4d0>, 688951.869068458)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x11076d710>, 688952.378044958)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x11076ef20>, 688952.9983305)])']
connector: <aiohttp.connector.TCPConnector object at 0x110598590>
2024-12-05 17:02:08,588 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:02:08,588 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:08,589 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 17:02:08,589 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 17:02:08,589 [INFO] Starting crawl of URL: https://example.com
2024-12-05 17:02:09,146 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:02:09,146 [DEBUG] Base URL: https://example.com
2024-12-05 17:02:09,146 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:02:09,146 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:02:09,146 [DEBUG] Initial domain set to: example.com
2024-12-05 17:02:09,146 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:02:09,146 [INFO] Successfully crawled https://example.com
2024-12-05 17:02:09,147 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:09,149 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:02:09,149 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:09,150 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:09,166 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:02:09,181 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:09,182 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:02:09,182 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:09,182 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:02:09,198 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:09,198 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:02:09,198 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:09,199 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:02:09,199 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2024-12-05 17:02:09,199 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2024-12-05 17:02:09,199 [INFO] Starting crawl of URL: https://example.com/page
2024-12-05 17:02:09,760 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:02:09,760 [DEBUG] Base URL: https://example.com/page
2024-12-05 17:02:09,760 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:02:09,761 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:02:09,761 [DEBUG] Initial domain set to: example.com
2024-12-05 17:02:09,761 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:02:09,761 [INFO] Successfully crawled https://example.com/page
2024-12-05 17:02:09,761 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2024-12-05 17:02:09,761 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2024-12-05 17:02:09,761 [INFO] Starting crawl of URL: https://other-domain.com/page
2024-12-05 17:02:15,958 [DEBUG] 
Evaluating link: ./
2024-12-05 17:02:15,958 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,958 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-05 17:02:15,958 [DEBUG] Absolute link: https://other-domain.com/
2024-12-05 17:02:15,958 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,958 [DEBUG] 
Evaluating link: ./
2024-12-05 17:02:15,958 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,959 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-05 17:02:15,959 [DEBUG] Absolute link: https://other-domain.com/
2024-12-05 17:02:15,959 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,959 [DEBUG] 
Evaluating link: ./hiyou.php
2024-12-05 17:02:15,959 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,959 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-12-05 17:02:15,959 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-12-05 17:02:15,959 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,959 [DEBUG] 
Evaluating link: ./date.php
2024-12-05 17:02:15,959 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,959 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-12-05 17:02:15,959 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-12-05 17:02:15,959 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,959 [DEBUG] 
Evaluating link: ./contact.php
2024-12-05 17:02:15,959 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,959 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2024-12-05 17:02:15,959 [DEBUG] Absolute link: https://other-domain.com/contact.php
2024-12-05 17:02:15,959 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,959 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 17:02:15,959 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,959 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 17:02:15,959 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 17:02:15,959 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-05 17:02:15,959 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:02:15,959 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,959 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:02:15,959 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:02:15,959 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-05 17:02:15,959 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:02:15,959 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,959 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:02:15,959 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:02:15,959 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-05 17:02:15,959 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:02:15,959 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,959 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:02:15,959 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:02:15,959 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-05 17:02:15,959 [DEBUG] 
Evaluating link: ./contact
2024-12-05 17:02:15,959 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,959 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-12-05 17:02:15,959 [DEBUG] Absolute link: https://other-domain.com/contact
2024-12-05 17:02:15,960 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,960 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:02:15,960 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,960 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:02:15,960 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:02:15,960 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-05 17:02:15,960 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:02:15,960 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,960 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:02:15,960 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:02:15,960 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-05 17:02:15,960 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:02:15,960 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,960 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:02:15,960 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:02:15,960 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-05 17:02:15,960 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-12-05 17:02:15,960 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,960 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:02:15,960 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:02:15,960 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-12-05 17:02:15,960 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 17:02:15,960 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,960 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 17:02:15,960 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 17:02:15,960 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-05 17:02:15,960 [DEBUG] 
Evaluating link: #indexes
2024-12-05 17:02:15,960 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,960 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2024-12-05 17:02:15,960 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,960 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,960 [DEBUG] 
Evaluating link: #a
2024-12-05 17:02:15,960 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,960 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2024-12-05 17:02:15,960 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,960 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,960 [DEBUG] 
Evaluating link: #b
2024-12-05 17:02:15,960 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,960 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-12-05 17:02:15,960 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,960 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,960 [DEBUG] 
Evaluating link: #c
2024-12-05 17:02:15,960 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,961 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-12-05 17:02:15,961 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,961 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,961 [DEBUG] 
Evaluating link: #d
2024-12-05 17:02:15,961 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,961 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-12-05 17:02:15,961 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,961 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,961 [DEBUG] 
Evaluating link: #e
2024-12-05 17:02:15,961 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,961 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-12-05 17:02:15,961 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,961 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,961 [DEBUG] 
Evaluating link: #f
2024-12-05 17:02:15,961 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,961 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-12-05 17:02:15,961 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,961 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,961 [DEBUG] 
Evaluating link: #g
2024-12-05 17:02:15,961 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,961 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-12-05 17:02:15,961 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,961 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,961 [DEBUG] 
Evaluating link: #h
2024-12-05 17:02:15,961 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,961 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-12-05 17:02:15,961 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,961 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,961 [DEBUG] 
Evaluating link: #i
2024-12-05 17:02:15,961 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,961 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-12-05 17:02:15,961 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,961 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,961 [DEBUG] 
Evaluating link: #j
2024-12-05 17:02:15,961 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,961 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-12-05 17:02:15,961 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,961 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,961 [DEBUG] 
Evaluating link: #k
2024-12-05 17:02:15,961 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,961 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-12-05 17:02:15,961 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,961 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,962 [DEBUG] 
Evaluating link: #l
2024-12-05 17:02:15,962 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,962 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-12-05 17:02:15,962 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,962 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,962 [DEBUG] 
Evaluating link: #m
2024-12-05 17:02:15,962 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,962 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-12-05 17:02:15,962 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,962 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,962 [DEBUG] 
Evaluating link: #n
2024-12-05 17:02:15,962 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,962 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-12-05 17:02:15,962 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,962 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,962 [DEBUG] 
Evaluating link: #o
2024-12-05 17:02:15,962 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,962 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-12-05 17:02:15,962 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,962 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,962 [DEBUG] 
Evaluating link: #p
2024-12-05 17:02:15,962 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,962 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-12-05 17:02:15,962 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,962 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,962 [DEBUG] 
Evaluating link: #q
2024-12-05 17:02:15,962 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,962 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-12-05 17:02:15,962 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,962 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,962 [DEBUG] 
Evaluating link: #r
2024-12-05 17:02:15,962 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,962 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-12-05 17:02:15,962 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,962 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,962 [DEBUG] 
Evaluating link: #s
2024-12-05 17:02:15,962 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,962 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-12-05 17:02:15,962 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,962 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,962 [DEBUG] 
Evaluating link: #t
2024-12-05 17:02:15,962 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,962 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-12-05 17:02:15,963 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,963 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,963 [DEBUG] 
Evaluating link: #u
2024-12-05 17:02:15,963 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,963 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-12-05 17:02:15,963 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,963 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,963 [DEBUG] 
Evaluating link: #v
2024-12-05 17:02:15,963 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,963 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-12-05 17:02:15,963 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,963 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,963 [DEBUG] 
Evaluating link: #w
2024-12-05 17:02:15,963 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,963 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-12-05 17:02:15,963 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,963 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,963 [DEBUG] 
Evaluating link: #x
2024-12-05 17:02:15,963 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,963 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-12-05 17:02:15,963 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,963 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,963 [DEBUG] 
Evaluating link: #y
2024-12-05 17:02:15,963 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,963 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-12-05 17:02:15,963 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,963 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,963 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:02:15,963 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,963 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:02:15,963 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:02:15,963 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-05 17:02:15,963 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:02:15,963 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,963 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:02:15,963 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:02:15,963 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-05 17:02:15,963 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:02:15,963 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,963 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:02:15,963 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:02:15,963 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-05 17:02:15,963 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-12-05 17:02:15,964 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,964 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:02:15,964 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:02:15,964 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-12-05 17:02:15,964 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 17:02:15,964 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,964 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 17:02:15,964 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 17:02:15,964 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-05 17:02:15,964 [DEBUG] 
Evaluating link: https://spriq.jp/
2024-12-05 17:02:15,964 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,964 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2024-12-05 17:02:15,964 [DEBUG] Absolute link: https://spriq.jp/
2024-12-05 17:02:15,964 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2024-12-05 17:02:15,964 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:02:15,964 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,964 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:02:15,964 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:02:15,964 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-05 17:02:15,964 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:02:15,964 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,964 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:02:15,964 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:02:15,964 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-05 17:02:15,964 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:02:15,964 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,964 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:02:15,964 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:02:15,964 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-05 17:02:15,964 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:02:15,964 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,964 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:02:15,965 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:02:15,965 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-05 17:02:15,965 [DEBUG] 
Evaluating link: ./contact
2024-12-05 17:02:15,965 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,965 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-12-05 17:02:15,965 [DEBUG] Absolute link: https://other-domain.com/contact
2024-12-05 17:02:15,965 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,965 [DEBUG] 
Evaluating link: ./
2024-12-05 17:02:15,965 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,965 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-05 17:02:15,965 [DEBUG] Absolute link: https://other-domain.com/
2024-12-05 17:02:15,965 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,965 [DEBUG] 
Evaluating link: ./hiyou.php
2024-12-05 17:02:15,965 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,965 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-12-05 17:02:15,965 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-12-05 17:02:15,965 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,965 [DEBUG] 
Evaluating link: ./date.php
2024-12-05 17:02:15,965 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,965 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-12-05 17:02:15,965 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-12-05 17:02:15,965 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,965 [DEBUG] 
Evaluating link: #u
2024-12-05 17:02:15,965 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,965 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-12-05 17:02:15,965 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,965 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,965 [DEBUG] 
Evaluating link: #v
2024-12-05 17:02:15,965 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,965 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-12-05 17:02:15,965 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,965 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,965 [DEBUG] 
Evaluating link: #w
2024-12-05 17:02:15,965 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,965 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-12-05 17:02:15,965 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,965 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,965 [DEBUG] 
Evaluating link: #x
2024-12-05 17:02:15,965 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,966 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-12-05 17:02:15,966 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,966 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,966 [DEBUG] 
Evaluating link: #y
2024-12-05 17:02:15,966 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,966 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-12-05 17:02:15,966 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,966 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,966 [DEBUG] 
Evaluating link: #p
2024-12-05 17:02:15,966 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,966 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-12-05 17:02:15,966 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,966 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,966 [DEBUG] 
Evaluating link: #q
2024-12-05 17:02:15,966 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,966 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-12-05 17:02:15,966 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,966 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,966 [DEBUG] 
Evaluating link: #r
2024-12-05 17:02:15,966 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,966 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-12-05 17:02:15,966 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,966 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,966 [DEBUG] 
Evaluating link: #s
2024-12-05 17:02:15,966 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,966 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-12-05 17:02:15,966 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,966 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,966 [DEBUG] 
Evaluating link: #t
2024-12-05 17:02:15,966 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,966 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-12-05 17:02:15,966 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,966 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,966 [DEBUG] 
Evaluating link: #k
2024-12-05 17:02:15,966 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,966 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-12-05 17:02:15,966 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,966 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,966 [DEBUG] 
Evaluating link: #l
2024-12-05 17:02:15,966 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,966 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-12-05 17:02:15,966 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,966 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,966 [DEBUG] 
Evaluating link: #m
2024-12-05 17:02:15,966 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,966 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-12-05 17:02:15,966 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,967 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,967 [DEBUG] 
Evaluating link: #n
2024-12-05 17:02:15,967 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,967 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-12-05 17:02:15,967 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,967 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,967 [DEBUG] 
Evaluating link: #o
2024-12-05 17:02:15,967 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,967 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-12-05 17:02:15,967 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,967 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,967 [DEBUG] 
Evaluating link: #f
2024-12-05 17:02:15,967 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,967 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-12-05 17:02:15,967 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,967 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,967 [DEBUG] 
Evaluating link: #g
2024-12-05 17:02:15,967 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,967 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-12-05 17:02:15,967 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,967 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,967 [DEBUG] 
Evaluating link: #h
2024-12-05 17:02:15,967 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,967 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-12-05 17:02:15,967 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,967 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,967 [DEBUG] 
Evaluating link: #i
2024-12-05 17:02:15,967 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,967 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-12-05 17:02:15,967 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,967 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,967 [DEBUG] 
Evaluating link: #j
2024-12-05 17:02:15,967 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,967 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-12-05 17:02:15,967 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,967 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,967 [DEBUG] 
Evaluating link: #b
2024-12-05 17:02:15,967 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,967 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-12-05 17:02:15,967 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,967 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,967 [DEBUG] 
Evaluating link: #c
2024-12-05 17:02:15,967 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,967 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-12-05 17:02:15,967 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,967 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,967 [DEBUG] 
Evaluating link: #d
2024-12-05 17:02:15,967 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,967 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-12-05 17:02:15,968 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,968 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,968 [DEBUG] 
Evaluating link: #e
2024-12-05 17:02:15,968 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,968 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-12-05 17:02:15,968 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,968 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,968 [DEBUG] 
Evaluating link: #site
2024-12-05 17:02:15,968 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:02:15,968 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2024-12-05 17:02:15,968 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:02:15,968 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:02:15,968 [INFO] Successfully crawled https://other-domain.com/page
2024-12-05 17:02:15,972 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:15,973 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:02:15,973 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:15,973 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-12-05 17:02:15,973 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-12-05 17:02:15,973 [INFO] Starting crawl of URL: https://example.com/page0
2024-12-05 17:02:16,614 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:02:16,615 [DEBUG] Base URL: https://example.com/page0
2024-12-05 17:02:16,615 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:02:16,615 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:02:16,615 [DEBUG] Initial domain set to: example.com
2024-12-05 17:02:16,615 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:02:16,615 [INFO] Successfully crawled https://example.com/page0
2024-12-05 17:02:16,615 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-12-05 17:02:16,615 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-12-05 17:02:16,615 [INFO] Starting crawl of URL: https://example.com/page1
2024-12-05 17:02:16,816 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:02:16,816 [DEBUG] Base URL: https://example.com/page1
2024-12-05 17:02:16,816 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:02:16,816 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:02:16,816 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:02:16,816 [INFO] Successfully crawled https://example.com/page1
2024-12-05 17:02:16,816 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-12-05 17:02:16,816 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-12-05 17:02:16,816 [INFO] Starting crawl of URL: https://example.com/page2
2024-12-05 17:02:17,256 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:02:17,256 [DEBUG] Base URL: https://example.com/page2
2024-12-05 17:02:17,256 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:02:17,256 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:02:17,256 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:02:17,256 [INFO] Successfully crawled https://example.com/page2
2024-12-05 17:02:17,256 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-12-05 17:02:17,257 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-12-05 17:02:17,257 [INFO] Starting crawl of URL: https://example.com/page3
2024-12-05 17:02:17,757 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:02:17,757 [DEBUG] Base URL: https://example.com/page3
2024-12-05 17:02:17,758 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:02:17,758 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:02:17,758 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:02:17,758 [INFO] Successfully crawled https://example.com/page3
2024-12-05 17:02:17,758 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-12-05 17:02:17,758 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-12-05 17:02:17,758 [INFO] Starting crawl of URL: https://example.com/page4
2024-12-05 17:02:18,256 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:02:18,257 [DEBUG] Base URL: https://example.com/page4
2024-12-05 17:02:18,257 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:02:18,257 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:02:18,257 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:02:18,257 [INFO] Successfully crawled https://example.com/page4
2024-12-05 17:02:18,257 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:02:18,257 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-12-05 17:02:18,257 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-12-05 17:02:18,257 [INFO] Starting crawl of URL: https://example.com/page0
2024-12-05 17:02:18,873 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:02:18,873 [DEBUG] Base URL: https://example.com/page0
2024-12-05 17:02:18,873 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:02:18,873 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:02:18,873 [DEBUG] Initial domain set to: example.com
2024-12-05 17:02:18,873 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:02:18,873 [INFO] Successfully crawled https://example.com/page0
2024-12-05 17:02:18,873 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-12-05 17:02:18,873 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-12-05 17:02:18,873 [INFO] Starting crawl of URL: https://example.com/page1
2024-12-05 17:02:19,012 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:02:19,012 [DEBUG] Base URL: https://example.com/page1
2024-12-05 17:02:19,012 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:02:19,012 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:02:19,012 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:02:19,012 [INFO] Successfully crawled https://example.com/page1
2024-12-05 17:02:19,012 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-12-05 17:02:19,012 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-12-05 17:02:19,013 [INFO] Starting crawl of URL: https://example.com/page2
2024-12-05 17:02:19,512 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:02:19,512 [DEBUG] Base URL: https://example.com/page2
2024-12-05 17:02:19,512 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:02:19,512 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:02:19,512 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:02:19,513 [INFO] Successfully crawled https://example.com/page2
2024-12-05 17:02:19,513 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-12-05 17:02:19,513 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-12-05 17:02:19,513 [INFO] Starting crawl of URL: https://example.com/page3
2024-12-05 17:02:20,021 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:02:20,021 [DEBUG] Base URL: https://example.com/page3
2024-12-05 17:02:20,021 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:02:20,021 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:02:20,021 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:02:20,021 [INFO] Successfully crawled https://example.com/page3
2024-12-05 17:02:20,021 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-12-05 17:02:20,021 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-12-05 17:02:20,021 [INFO] Starting crawl of URL: https://example.com/page4
2024-12-05 17:02:20,513 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:02:20,513 [DEBUG] Base URL: https://example.com/page4
2024-12-05 17:02:20,513 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:02:20,513 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:02:20,513 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:02:20,513 [INFO] Successfully crawled https://example.com/page4
2024-12-05 17:02:20,516 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:20,516 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:20,520 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:20,520 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:20,523 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:20,524 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:20,525 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1105bae10>
2024-12-05 17:02:20,525 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x11076d6a0>, 688962.667610791)])']
connector: <aiohttp.connector.TCPConnector object at 0x1105bba10>
2024-12-05 17:02:20,525 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1105cca50>
2024-12-05 17:02:20,525 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x110771400>, 688964.923911833)])']
connector: <aiohttp.connector.TCPConnector object at 0x1107fe3d0>
2024-12-05 17:02:20,528 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:20,529 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:20,531 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:20,531 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:20,534 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:20,534 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:20,537 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:20,537 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:20,540 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:20,540 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:20,543 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:20,543 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:20,546 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:20,557 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:20,560 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:20,563 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:20,565 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:20,568 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:20,570 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:20,573 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:20,576 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:20,578 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:20,580 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:20,583 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:20,596 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:20,599 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:20,599 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:20,599 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:20,603 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:20,603 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:20,603 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:20,607 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:20,609 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:20,625 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1105240d0>
2024-12-05 17:02:20,626 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x110599550>
2024-12-05 17:02:20,626 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x11076e9e0>, 688954.17159175)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x110770130>, 688956.645825041)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1107702f0>, 688959.116883083)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1107703d0>, 688960.362358458)])']
connector: <aiohttp.connector.TCPConnector object at 0x110598ed0>
2024-12-05 17:02:20,638 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:20,644 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:20,650 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:20,656 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:20,662 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:20,668 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:20,673 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:20,680 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:20,686 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:20,740 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:20,878 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:20,880 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:02:20,880 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 17:02:20,880 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 17:02:20,880 [INFO] Starting crawl of URL: https://example.com
2024-12-05 17:02:21,440 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:02:21,441 [DEBUG] Base URL: https://example.com
2024-12-05 17:02:21,441 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:02:21,441 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:02:21,441 [DEBUG] Initial domain set to: example.com
2024-12-05 17:02:21,441 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:02:21,441 [INFO] Successfully crawled https://example.com
2024-12-05 17:02:21,441 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x114141210>
2024-12-05 17:02:21,441 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1140d4590>, 688965.851452291)])']
connector: <aiohttp.connector.TCPConnector object at 0x114142010>
2024-12-05 17:02:21,442 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2024-12-05 17:02:21,606 [INFO] Operation took 0.10 seconds
2024-12-05 17:02:21,652 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:02:21,652 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:21,653 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:22,252 [ERROR] Could not find documentation URL for package: file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-167/test_full_site_crawl0/test_docs/index.html
2024-12-05 17:02:22,252 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2024-12-05 17:02:22,261 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:22,263 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:02:22,263 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:22,263 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:22,871 [ERROR] Could not find documentation URL for package: file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-167/test_content_processing_pipeli0/test_docs/guide.html
2024-12-05 17:02:22,871 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2024-12-05 17:02:22,878 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:22,880 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:02:22,880 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:22,880 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:23,352 [ERROR] Could not find documentation URL for package: file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-167/test_quality_checks0/test_docs/api.html
2024-12-05 17:02:23,352 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2024-12-05 17:02:23,360 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:23,362 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:02:23,362 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:23,362 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:23,931 [ERROR] Could not find documentation URL for package: file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-167/test_document_organization0/test_docs/index.html
2024-12-05 17:02:23,931 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2024-12-05 17:02:23,939 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:23,940 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:02:23,940 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:23,941 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:24,429 [ERROR] Could not find documentation URL for package: file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-167/test_search_functionality0/test_docs/index.html
2024-12-05 17:02:24,429 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2024-12-05 17:02:24,456 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:24,458 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:02:24,458 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:24,458 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:24,960 [ERROR] Could not find documentation URL for package: file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-167/test_error_handling_and_recove0/test_docs/index.html
2024-12-05 17:02:24,961 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2024-12-05 17:02:24,968 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:24,968 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:24,971 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:24,971 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:24,972 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:24,972 [ERROR] Error during crawl: 'str' object has no attribute 'url'
2024-12-05 17:02:24,974 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:24,974 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:24,975 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:24,976 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:24,976 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:02:24,978 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:25,287 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:25,291 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:25,291 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:25,294 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:25,295 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:25,298 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:25,298 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:25,301 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:25,301 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:25,304 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:25,304 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:25,307 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:25,307 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:02:25,327 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:16:24,598 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-05 17:38:55,669 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-05 17:43:00,605 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-05 17:43:00,936 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:00,937 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:00,938 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:00,940 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:00,941 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:00,941 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:00,942 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:00,963 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:00,964 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:00,964 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:00,967 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:00,967 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:00,970 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:00,971 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:00,973 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:00,973 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:00,974 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:00,975 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:00,977 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:00,978 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:00,978 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:00,981 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:02,565 [INFO] Operation took 0.02 seconds
2024-12-05 17:43:02,770 [INFO] test_operation took 0.10 seconds
2024-12-05 17:43:02,976 [INFO] inner took 0.10 seconds
2024-12-05 17:43:02,976 [INFO] outer took 0.21 seconds
2024-12-05 17:43:02,977 [INFO] error_operation took 0.00 seconds
2024-12-05 17:43:03,004 [INFO] test_operation took 0.00 seconds
2024-12-05 17:43:03,028 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:43:03,028 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:03,029 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 17:43:03,029 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 17:43:03,029 [INFO] Starting crawl of URL: https://example.com
2024-12-05 17:43:03,593 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:43:03,593 [DEBUG] Base URL: https://example.com
2024-12-05 17:43:03,593 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:43:03,594 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:43:03,594 [DEBUG] Initial domain set to: example.com
2024-12-05 17:43:03,594 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:43:03,594 [INFO] Successfully crawled https://example.com
2024-12-05 17:43:03,595 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:03,596 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x11947b790>
2024-12-05 17:43:03,596 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1191a6cf0>, 691408.008654666)])']
connector: <aiohttp.connector.TCPConnector object at 0x11873fd50>
2024-12-05 17:43:03,597 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:43:03,597 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:03,598 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 17:43:03,598 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 17:43:03,598 [INFO] Starting crawl of URL: https://example.com
2024-12-05 17:43:03,643 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-12-05 17:43:03,643 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-12-05 17:43:03,643 [INFO] Starting crawl of URL: https://example.org
2024-12-05 17:43:04,169 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:43:04,169 [DEBUG] Base URL: https://example.com
2024-12-05 17:43:04,169 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:43:04,170 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:43:04,170 [DEBUG] Initial domain set to: example.com
2024-12-05 17:43:04,170 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:43:04,170 [INFO] Successfully crawled https://example.com
2024-12-05 17:43:04,684 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:43:04,685 [DEBUG] Base URL: https://example.org
2024-12-05 17:43:04,685 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:43:04,685 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:43:04,685 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:43:04,685 [INFO] Successfully crawled https://example.org
2024-12-05 17:43:04,686 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:04,687 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x11931df10>
2024-12-05 17:43:04,687 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1191da430>, 691408.58305125)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1193f81a0>, 691409.100439375)])']
connector: <aiohttp.connector.TCPConnector object at 0x11931d750>
2024-12-05 17:43:04,687 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:43:04,687 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:04,689 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 17:43:04,689 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 17:43:04,689 [INFO] Starting crawl of URL: https://example.com
2024-12-05 17:43:05,234 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:43:05,234 [DEBUG] Base URL: https://example.com
2024-12-05 17:43:05,234 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:43:05,234 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:43:05,234 [DEBUG] Initial domain set to: example.com
2024-12-05 17:43:05,234 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:43:05,234 [INFO] Successfully crawled https://example.com
2024-12-05 17:43:05,235 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:05,236 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x119348610>
2024-12-05 17:43:05,236 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1191a7230>, 691409.649655125)])']
connector: <aiohttp.connector.TCPConnector object at 0x119349010>
2024-12-05 17:43:05,236 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:43:05,237 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:05,237 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 17:43:05,237 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 17:43:05,237 [INFO] Starting crawl of URL: https://example.com
2024-12-05 17:43:05,806 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:43:05,806 [DEBUG] Base URL: https://example.com
2024-12-05 17:43:05,806 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:43:05,806 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:43:05,806 [DEBUG] Initial domain set to: example.com
2024-12-05 17:43:05,806 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:43:05,806 [INFO] Successfully crawled https://example.com
2024-12-05 17:43:05,807 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:43:05,807 [DEBUG] Base URL: https://example.com
2024-12-05 17:43:05,807 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:43:05,807 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:43:05,807 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:43:05,813 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:05,814 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:05,814 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:05,815 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:05,815 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:43:05,815 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:05,816 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 17:43:05,816 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 17:43:05,816 [INFO] Starting crawl of URL: https://example.com
2024-12-05 17:43:06,367 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:43:06,368 [DEBUG] Base URL: https://example.com
2024-12-05 17:43:06,368 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:43:06,368 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:43:06,368 [DEBUG] Initial domain set to: example.com
2024-12-05 17:43:06,368 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:43:06,368 [INFO] Successfully crawled https://example.com
2024-12-05 17:43:06,369 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:06,369 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x107c8ff90>
2024-12-05 17:43:06,369 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1193f82f0>, 691410.783372166)])']
connector: <aiohttp.connector.TCPConnector object at 0x1194e4710>
2024-12-05 17:43:06,370 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:43:06,370 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:06,371 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2024-12-05 17:43:06,371 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2024-12-05 17:43:06,371 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2024-12-05 17:43:08,432 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-05 17:43:08,437 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-05 17:43:08,437 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-05 17:43:08,449 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:08,449 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:43:08,449 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:08,450 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 17:43:08,450 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 17:43:08,450 [INFO] Starting crawl of URL: https://example.com
2024-12-05 17:43:08,470 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-12-05 17:43:08,470 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-12-05 17:43:08,470 [INFO] Starting crawl of URL: https://example.org
2024-12-05 17:43:08,470 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2024-12-05 17:43:08,470 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2024-12-05 17:43:08,470 [INFO] Starting crawl of URL: https://example.net
2024-12-05 17:43:09,019 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:43:09,019 [DEBUG] Base URL: https://example.com
2024-12-05 17:43:09,019 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:43:09,019 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:43:09,019 [DEBUG] Initial domain set to: example.com
2024-12-05 17:43:09,019 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:43:09,020 [INFO] Successfully crawled https://example.com
2024-12-05 17:43:09,494 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:43:09,495 [DEBUG] Base URL: https://example.org
2024-12-05 17:43:09,495 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:43:09,495 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:43:09,495 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:43:09,495 [INFO] Successfully crawled https://example.org
2024-12-05 17:43:10,148 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:43:10,148 [DEBUG] Base URL: https://example.net
2024-12-05 17:43:10,148 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:43:10,148 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:43:10,149 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:43:10,149 [INFO] Successfully crawled https://example.net
2024-12-05 17:43:10,149 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:10,150 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1194e5a90>
2024-12-05 17:43:10,150 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1193f9240>, 691413.434846666)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1193f9860>, 691413.910201333)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1193f9a20>, 691414.564447833)])']
connector: <aiohttp.connector.TCPConnector object at 0x119200510>
2024-12-05 17:43:10,150 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:43:10,150 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:10,151 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 17:43:10,151 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 17:43:10,151 [INFO] Starting crawl of URL: https://example.com
2024-12-05 17:43:10,718 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:43:10,719 [DEBUG] Base URL: https://example.com
2024-12-05 17:43:10,719 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:43:10,719 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:43:10,719 [DEBUG] Initial domain set to: example.com
2024-12-05 17:43:10,719 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:43:10,719 [INFO] Successfully crawled https://example.com
2024-12-05 17:43:10,720 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:10,727 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:43:10,727 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:10,728 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:10,749 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:43:10,766 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:10,766 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:43:10,766 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:10,767 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:43:10,783 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:10,783 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:43:10,783 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:10,784 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:43:10,784 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2024-12-05 17:43:10,784 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2024-12-05 17:43:10,784 [INFO] Starting crawl of URL: https://example.com/page
2024-12-05 17:43:11,342 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:43:11,342 [DEBUG] Base URL: https://example.com/page
2024-12-05 17:43:11,342 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:43:11,342 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:43:11,342 [DEBUG] Initial domain set to: example.com
2024-12-05 17:43:11,342 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:43:11,342 [INFO] Successfully crawled https://example.com/page
2024-12-05 17:43:11,342 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2024-12-05 17:43:11,343 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2024-12-05 17:43:11,343 [INFO] Starting crawl of URL: https://other-domain.com/page
2024-12-05 17:43:17,461 [DEBUG] 
Evaluating link: ./
2024-12-05 17:43:17,462 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,462 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-05 17:43:17,462 [DEBUG] Absolute link: https://other-domain.com/
2024-12-05 17:43:17,462 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,462 [DEBUG] 
Evaluating link: ./
2024-12-05 17:43:17,462 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,462 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-05 17:43:17,463 [DEBUG] Absolute link: https://other-domain.com/
2024-12-05 17:43:17,463 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,463 [DEBUG] 
Evaluating link: ./hiyou.php
2024-12-05 17:43:17,463 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,463 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-12-05 17:43:17,463 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-12-05 17:43:17,463 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,463 [DEBUG] 
Evaluating link: ./date.php
2024-12-05 17:43:17,463 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,463 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-12-05 17:43:17,463 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-12-05 17:43:17,463 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,463 [DEBUG] 
Evaluating link: ./contact.php
2024-12-05 17:43:17,463 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,463 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2024-12-05 17:43:17,463 [DEBUG] Absolute link: https://other-domain.com/contact.php
2024-12-05 17:43:17,463 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,463 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 17:43:17,463 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,463 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 17:43:17,463 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 17:43:17,463 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-05 17:43:17,463 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:43:17,463 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,463 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:43:17,463 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:43:17,463 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-05 17:43:17,463 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:43:17,463 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,463 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:43:17,463 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:43:17,463 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-05 17:43:17,463 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:43:17,463 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,464 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:43:17,464 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:43:17,464 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-05 17:43:17,464 [DEBUG] 
Evaluating link: ./contact
2024-12-05 17:43:17,464 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,464 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-12-05 17:43:17,464 [DEBUG] Absolute link: https://other-domain.com/contact
2024-12-05 17:43:17,464 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,464 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:43:17,464 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,464 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:43:17,464 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:43:17,464 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-05 17:43:17,464 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:43:17,464 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,464 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:43:17,464 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:43:17,464 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-05 17:43:17,464 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:43:17,464 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,464 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:43:17,464 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:43:17,464 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-05 17:43:17,464 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-12-05 17:43:17,464 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,464 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:43:17,464 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:43:17,464 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-12-05 17:43:17,464 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 17:43:17,464 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,464 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 17:43:17,464 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 17:43:17,464 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-05 17:43:17,464 [DEBUG] 
Evaluating link: #indexes
2024-12-05 17:43:17,464 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,464 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2024-12-05 17:43:17,465 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,465 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,465 [DEBUG] 
Evaluating link: #a
2024-12-05 17:43:17,465 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,465 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2024-12-05 17:43:17,465 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,465 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,465 [DEBUG] 
Evaluating link: #b
2024-12-05 17:43:17,465 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,465 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-12-05 17:43:17,465 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,465 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,465 [DEBUG] 
Evaluating link: #c
2024-12-05 17:43:17,465 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,480 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-12-05 17:43:17,481 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,481 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,481 [DEBUG] 
Evaluating link: #d
2024-12-05 17:43:17,481 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,481 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-12-05 17:43:17,481 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,481 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,481 [DEBUG] 
Evaluating link: #e
2024-12-05 17:43:17,481 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,481 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-12-05 17:43:17,481 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,481 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,481 [DEBUG] 
Evaluating link: #f
2024-12-05 17:43:17,481 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,481 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-12-05 17:43:17,481 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,481 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,481 [DEBUG] 
Evaluating link: #g
2024-12-05 17:43:17,481 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,481 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-12-05 17:43:17,481 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,481 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,481 [DEBUG] 
Evaluating link: #h
2024-12-05 17:43:17,481 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,482 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-12-05 17:43:17,482 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,482 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,482 [DEBUG] 
Evaluating link: #i
2024-12-05 17:43:17,482 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,482 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-12-05 17:43:17,482 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,482 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,482 [DEBUG] 
Evaluating link: #j
2024-12-05 17:43:17,482 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,482 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-12-05 17:43:17,482 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,482 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,482 [DEBUG] 
Evaluating link: #k
2024-12-05 17:43:17,482 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,482 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-12-05 17:43:17,482 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,482 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,482 [DEBUG] 
Evaluating link: #l
2024-12-05 17:43:17,482 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,482 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-12-05 17:43:17,482 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,482 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,482 [DEBUG] 
Evaluating link: #m
2024-12-05 17:43:17,482 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,482 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-12-05 17:43:17,482 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,483 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,483 [DEBUG] 
Evaluating link: #n
2024-12-05 17:43:17,483 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,483 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-12-05 17:43:17,483 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,483 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,483 [DEBUG] 
Evaluating link: #o
2024-12-05 17:43:17,483 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,483 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-12-05 17:43:17,483 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,483 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,483 [DEBUG] 
Evaluating link: #p
2024-12-05 17:43:17,483 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,483 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-12-05 17:43:17,483 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,483 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,483 [DEBUG] 
Evaluating link: #q
2024-12-05 17:43:17,483 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,483 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-12-05 17:43:17,483 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,483 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,483 [DEBUG] 
Evaluating link: #r
2024-12-05 17:43:17,483 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,483 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-12-05 17:43:17,483 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,483 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,483 [DEBUG] 
Evaluating link: #s
2024-12-05 17:43:17,484 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,484 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-12-05 17:43:17,484 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,484 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,484 [DEBUG] 
Evaluating link: #t
2024-12-05 17:43:17,484 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,484 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-12-05 17:43:17,484 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,484 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,484 [DEBUG] 
Evaluating link: #u
2024-12-05 17:43:17,484 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,484 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-12-05 17:43:17,484 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,484 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,484 [DEBUG] 
Evaluating link: #v
2024-12-05 17:43:17,484 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,484 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-12-05 17:43:17,484 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,484 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,484 [DEBUG] 
Evaluating link: #w
2024-12-05 17:43:17,484 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,484 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-12-05 17:43:17,484 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,484 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,484 [DEBUG] 
Evaluating link: #x
2024-12-05 17:43:17,484 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,484 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-12-05 17:43:17,484 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,484 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,485 [DEBUG] 
Evaluating link: #y
2024-12-05 17:43:17,485 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,485 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-12-05 17:43:17,485 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,485 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,485 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:43:17,485 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,485 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:43:17,485 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:43:17,485 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-05 17:43:17,485 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:43:17,485 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,485 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:43:17,485 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:43:17,485 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-05 17:43:17,485 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:43:17,485 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,485 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:43:17,485 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:43:17,485 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-05 17:43:17,485 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-12-05 17:43:17,485 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,485 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:43:17,485 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:43:17,485 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-12-05 17:43:17,485 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 17:43:17,485 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,485 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 17:43:17,485 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 17:43:17,486 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-05 17:43:17,486 [DEBUG] 
Evaluating link: https://spriq.jp/
2024-12-05 17:43:17,486 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,486 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2024-12-05 17:43:17,486 [DEBUG] Absolute link: https://spriq.jp/
2024-12-05 17:43:17,486 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2024-12-05 17:43:17,486 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:43:17,486 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,486 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:43:17,486 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:43:17,486 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-05 17:43:17,486 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:43:17,486 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,486 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:43:17,486 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:43:17,486 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-05 17:43:17,486 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:43:17,486 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,486 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:43:17,486 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:43:17,486 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-05 17:43:17,486 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:43:17,486 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,486 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:43:17,486 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:43:17,486 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-05 17:43:17,486 [DEBUG] 
Evaluating link: ./contact
2024-12-05 17:43:17,486 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,486 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-12-05 17:43:17,486 [DEBUG] Absolute link: https://other-domain.com/contact
2024-12-05 17:43:17,487 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,487 [DEBUG] 
Evaluating link: ./
2024-12-05 17:43:17,487 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,487 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-05 17:43:17,487 [DEBUG] Absolute link: https://other-domain.com/
2024-12-05 17:43:17,487 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,487 [DEBUG] 
Evaluating link: ./hiyou.php
2024-12-05 17:43:17,487 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,487 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-12-05 17:43:17,487 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-12-05 17:43:17,487 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,487 [DEBUG] 
Evaluating link: ./date.php
2024-12-05 17:43:17,487 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,487 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-12-05 17:43:17,487 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-12-05 17:43:17,487 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,487 [DEBUG] 
Evaluating link: #u
2024-12-05 17:43:17,487 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,487 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-12-05 17:43:17,487 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,487 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,487 [DEBUG] 
Evaluating link: #v
2024-12-05 17:43:17,487 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,487 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-12-05 17:43:17,487 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,487 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,487 [DEBUG] 
Evaluating link: #w
2024-12-05 17:43:17,487 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,487 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-12-05 17:43:17,487 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,487 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,488 [DEBUG] 
Evaluating link: #x
2024-12-05 17:43:17,488 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,488 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-12-05 17:43:17,488 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,488 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,488 [DEBUG] 
Evaluating link: #y
2024-12-05 17:43:17,488 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,488 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-12-05 17:43:17,488 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,488 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,488 [DEBUG] 
Evaluating link: #p
2024-12-05 17:43:17,488 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,488 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-12-05 17:43:17,488 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,488 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,488 [DEBUG] 
Evaluating link: #q
2024-12-05 17:43:17,488 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,488 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-12-05 17:43:17,488 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,488 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,488 [DEBUG] 
Evaluating link: #r
2024-12-05 17:43:17,488 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,488 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-12-05 17:43:17,488 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,488 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,488 [DEBUG] 
Evaluating link: #s
2024-12-05 17:43:17,488 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,488 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-12-05 17:43:17,488 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,488 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,488 [DEBUG] 
Evaluating link: #t
2024-12-05 17:43:17,488 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,489 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-12-05 17:43:17,489 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,489 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,489 [DEBUG] 
Evaluating link: #k
2024-12-05 17:43:17,489 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,489 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-12-05 17:43:17,489 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,489 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,489 [DEBUG] 
Evaluating link: #l
2024-12-05 17:43:17,489 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,489 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-12-05 17:43:17,489 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,489 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,489 [DEBUG] 
Evaluating link: #m
2024-12-05 17:43:17,489 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,489 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-12-05 17:43:17,489 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,489 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,489 [DEBUG] 
Evaluating link: #n
2024-12-05 17:43:17,489 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,489 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-12-05 17:43:17,489 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,489 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,489 [DEBUG] 
Evaluating link: #o
2024-12-05 17:43:17,489 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,489 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-12-05 17:43:17,489 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,489 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,490 [DEBUG] 
Evaluating link: #f
2024-12-05 17:43:17,490 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,490 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-12-05 17:43:17,490 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,490 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,490 [DEBUG] 
Evaluating link: #g
2024-12-05 17:43:17,490 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,490 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-12-05 17:43:17,490 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,490 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,490 [DEBUG] 
Evaluating link: #h
2024-12-05 17:43:17,490 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,490 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-12-05 17:43:17,490 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,490 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,490 [DEBUG] 
Evaluating link: #i
2024-12-05 17:43:17,490 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,490 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-12-05 17:43:17,490 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,490 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,490 [DEBUG] 
Evaluating link: #j
2024-12-05 17:43:17,490 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,490 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-12-05 17:43:17,490 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,490 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,490 [DEBUG] 
Evaluating link: #b
2024-12-05 17:43:17,490 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,490 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-12-05 17:43:17,490 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,490 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,490 [DEBUG] 
Evaluating link: #c
2024-12-05 17:43:17,490 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,490 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-12-05 17:43:17,490 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,490 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,490 [DEBUG] 
Evaluating link: #d
2024-12-05 17:43:17,490 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,490 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-12-05 17:43:17,490 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,490 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,491 [DEBUG] 
Evaluating link: #e
2024-12-05 17:43:17,491 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,491 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-12-05 17:43:17,491 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,491 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,491 [DEBUG] 
Evaluating link: #site
2024-12-05 17:43:17,491 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:43:17,491 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2024-12-05 17:43:17,491 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:43:17,491 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:43:17,491 [INFO] Successfully crawled https://other-domain.com/page
2024-12-05 17:43:17,494 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:17,495 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:43:17,495 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:17,495 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-12-05 17:43:17,496 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-12-05 17:43:17,496 [INFO] Starting crawl of URL: https://example.com/page0
2024-12-05 17:43:18,045 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:43:18,045 [DEBUG] Base URL: https://example.com/page0
2024-12-05 17:43:18,045 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:43:18,045 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:43:18,045 [DEBUG] Initial domain set to: example.com
2024-12-05 17:43:18,045 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:43:18,045 [INFO] Successfully crawled https://example.com/page0
2024-12-05 17:43:18,046 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-12-05 17:43:18,046 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-12-05 17:43:18,046 [INFO] Starting crawl of URL: https://example.com/page1
2024-12-05 17:43:18,186 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:43:18,186 [DEBUG] Base URL: https://example.com/page1
2024-12-05 17:43:18,186 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:43:18,187 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:43:18,187 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:43:18,187 [INFO] Successfully crawled https://example.com/page1
2024-12-05 17:43:18,187 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-12-05 17:43:18,187 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-12-05 17:43:18,187 [INFO] Starting crawl of URL: https://example.com/page2
2024-12-05 17:43:18,686 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:43:18,686 [DEBUG] Base URL: https://example.com/page2
2024-12-05 17:43:18,686 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:43:18,686 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:43:18,686 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:43:18,686 [INFO] Successfully crawled https://example.com/page2
2024-12-05 17:43:18,687 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-12-05 17:43:18,687 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-12-05 17:43:18,687 [INFO] Starting crawl of URL: https://example.com/page3
2024-12-05 17:43:19,185 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:43:19,185 [DEBUG] Base URL: https://example.com/page3
2024-12-05 17:43:19,185 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:43:19,185 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:43:19,185 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:43:19,186 [INFO] Successfully crawled https://example.com/page3
2024-12-05 17:43:19,186 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-12-05 17:43:19,186 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-12-05 17:43:19,186 [INFO] Starting crawl of URL: https://example.com/page4
2024-12-05 17:43:19,688 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:43:19,689 [DEBUG] Base URL: https://example.com/page4
2024-12-05 17:43:19,689 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:43:19,689 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:43:19,689 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:43:19,689 [INFO] Successfully crawled https://example.com/page4
2024-12-05 17:43:19,689 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:43:19,689 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-12-05 17:43:19,689 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-12-05 17:43:19,689 [INFO] Starting crawl of URL: https://example.com/page0
2024-12-05 17:43:20,236 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:43:20,236 [DEBUG] Base URL: https://example.com/page0
2024-12-05 17:43:20,236 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:43:20,236 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:43:20,236 [DEBUG] Initial domain set to: example.com
2024-12-05 17:43:20,236 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:43:20,236 [INFO] Successfully crawled https://example.com/page0
2024-12-05 17:43:20,236 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-12-05 17:43:20,236 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-12-05 17:43:20,236 [INFO] Starting crawl of URL: https://example.com/page1
2024-12-05 17:43:20,369 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:43:20,369 [DEBUG] Base URL: https://example.com/page1
2024-12-05 17:43:20,369 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:43:20,369 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:43:20,369 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:43:20,369 [INFO] Successfully crawled https://example.com/page1
2024-12-05 17:43:20,369 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-12-05 17:43:20,369 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-12-05 17:43:20,369 [INFO] Starting crawl of URL: https://example.com/page2
2024-12-05 17:43:20,880 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:43:20,880 [DEBUG] Base URL: https://example.com/page2
2024-12-05 17:43:20,880 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:43:20,881 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:43:20,881 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:43:20,881 [INFO] Successfully crawled https://example.com/page2
2024-12-05 17:43:20,881 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-12-05 17:43:20,881 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-12-05 17:43:20,881 [INFO] Starting crawl of URL: https://example.com/page3
2024-12-05 17:43:21,373 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:43:21,374 [DEBUG] Base URL: https://example.com/page3
2024-12-05 17:43:21,374 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:43:21,374 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:43:21,374 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:43:21,374 [INFO] Successfully crawled https://example.com/page3
2024-12-05 17:43:21,374 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-12-05 17:43:21,374 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-12-05 17:43:21,374 [INFO] Starting crawl of URL: https://example.com/page4
2024-12-05 17:43:21,873 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:43:21,873 [DEBUG] Base URL: https://example.com/page4
2024-12-05 17:43:21,874 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:43:21,874 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:43:21,874 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:43:21,874 [INFO] Successfully crawled https://example.com/page4
2024-12-05 17:43:21,880 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:21,881 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:21,885 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:21,885 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:21,887 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x119384e10>
2024-12-05 17:43:21,887 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1193fa040>, 691424.104243)])']
connector: <aiohttp.connector.TCPConnector object at 0x1193db910>
2024-12-05 17:43:21,887 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1192ae5d0>
2024-12-05 17:43:21,888 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x119655710>, 691426.289025166)])']
connector: <aiohttp.connector.TCPConnector object at 0x1192ae6d0>
2024-12-05 17:43:21,892 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:21,893 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:21,897 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:21,897 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:21,901 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:21,901 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:21,904 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:21,905 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:21,908 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:21,908 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:21,911 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:21,912 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:21,915 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:21,916 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:21,920 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:21,936 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:21,938 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:21,941 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:21,944 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:21,947 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:21,950 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:21,969 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x11927f010>
2024-12-05 17:43:21,970 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1191a7310>, 691410.222618083)])']
connector: <aiohttp.connector.TCPConnector object at 0x11931c4d0>
2024-12-05 17:43:21,970 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1193cbb90>
2024-12-05 17:43:21,970 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1193f97f0>, 691415.757720666)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1193f9b70>, 691418.320578375)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1193f8210>, 691420.635795291)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1193f9cc0>, 691421.865179583)])']
connector: <aiohttp.connector.TCPConnector object at 0x1193cbf50>
2024-12-05 17:43:21,973 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:21,977 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:21,980 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:21,982 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:21,985 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:21,998 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:22,001 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:22,001 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:22,002 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:22,005 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:22,005 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:22,006 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:22,009 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:22,011 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:22,019 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:22,029 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:22,035 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:22,041 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:22,048 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:22,054 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:22,060 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:22,067 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:22,072 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:22,131 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:22,272 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:22,273 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:43:22,273 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 17:43:22,274 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 17:43:22,274 [INFO] Starting crawl of URL: https://example.com
2024-12-05 17:43:22,837 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:43:22,838 [DEBUG] Base URL: https://example.com
2024-12-05 17:43:22,839 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:43:22,839 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:43:22,839 [DEBUG] Initial domain set to: example.com
2024-12-05 17:43:22,839 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:43:22,839 [INFO] Successfully crawled https://example.com
2024-12-05 17:43:22,839 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x11bba1450>
2024-12-05 17:43:22,840 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x11bb92510>, 691427.253346875)])']
connector: <aiohttp.connector.TCPConnector object at 0x11bba14d0>
2024-12-05 17:43:22,841 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2024-12-05 17:43:23,004 [INFO] Operation took 0.10 seconds
2024-12-05 17:43:23,058 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:43:23,058 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:23,058 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:23,602 [ERROR] Could not find documentation URL for package: file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-168/test_full_site_crawl0/test_docs/index.html
2024-12-05 17:43:23,603 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2024-12-05 17:43:23,616 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:23,619 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:43:23,619 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:23,619 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:24,125 [ERROR] Could not find documentation URL for package: file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-168/test_content_processing_pipeli0/test_docs/guide.html
2024-12-05 17:43:24,125 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2024-12-05 17:43:24,138 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:24,140 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:43:24,140 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:24,140 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:24,614 [ERROR] Could not find documentation URL for package: file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-168/test_quality_checks0/test_docs/api.html
2024-12-05 17:43:24,614 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2024-12-05 17:43:24,626 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:24,628 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:43:24,628 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:24,628 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:25,110 [ERROR] Could not find documentation URL for package: file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-168/test_document_organization0/test_docs/index.html
2024-12-05 17:43:25,110 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2024-12-05 17:43:25,139 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:25,140 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:43:25,140 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:25,141 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:25,635 [ERROR] Could not find documentation URL for package: file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-168/test_search_functionality0/test_docs/index.html
2024-12-05 17:43:25,636 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2024-12-05 17:43:25,647 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:25,649 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:43:25,649 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:25,649 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:26,148 [ERROR] Could not find documentation URL for package: file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-168/test_error_handling_and_recove0/test_docs/index.html
2024-12-05 17:43:26,149 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2024-12-05 17:43:26,162 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:26,162 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:26,165 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:26,165 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:26,165 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:26,166 [ERROR] Error during crawl: 'str' object has no attribute 'url'
2024-12-05 17:43:26,167 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:26,168 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:26,169 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:26,170 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:26,170 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:43:26,172 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:26,522 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:26,525 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:26,526 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:26,529 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:26,529 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:26,532 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:26,532 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:26,541 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:26,544 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:26,554 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:26,556 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:26,572 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:26,574 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:43:26,586 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:40,425 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-05 17:44:40,744 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:40,745 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:40,746 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:40,747 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:40,748 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:40,748 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:40,748 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:44:40,771 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:40,772 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:40,772 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:44:40,775 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:40,775 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:40,778 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:40,778 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:40,781 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:40,781 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:40,782 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:40,783 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:40,785 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:40,786 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:40,787 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:44:40,790 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:42,391 [INFO] Operation took 0.02 seconds
2024-12-05 17:44:42,643 [INFO] test_operation took 0.10 seconds
2024-12-05 17:44:42,847 [INFO] inner took 0.10 seconds
2024-12-05 17:44:42,848 [INFO] outer took 0.20 seconds
2024-12-05 17:44:42,848 [INFO] error_operation took 0.00 seconds
2024-12-05 17:44:42,873 [INFO] test_operation took 0.00 seconds
2024-12-05 17:44:42,901 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:44:42,901 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:42,901 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 17:44:42,901 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 17:44:42,902 [INFO] Starting crawl of URL: https://example.com
2024-12-05 17:44:43,470 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:44:43,470 [DEBUG] Base URL: https://example.com
2024-12-05 17:44:43,470 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:44:43,470 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:44:43,470 [DEBUG] Initial domain set to: example.com
2024-12-05 17:44:43,470 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:44:43,470 [INFO] Successfully crawled https://example.com
2024-12-05 17:44:43,470 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:43,472 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x106329e50>
2024-12-05 17:44:43,472 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x106fd67b0>, 691507.885946333)])']
connector: <aiohttp.connector.TCPConnector object at 0x106e1b3d0>
2024-12-05 17:44:43,472 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:44:43,472 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:43,472 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 17:44:43,473 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 17:44:43,473 [INFO] Starting crawl of URL: https://example.com
2024-12-05 17:44:43,489 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-12-05 17:44:43,489 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-12-05 17:44:43,489 [INFO] Starting crawl of URL: https://example.org
2024-12-05 17:44:44,027 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:44:44,027 [DEBUG] Base URL: https://example.com
2024-12-05 17:44:44,027 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:44:44,027 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:44:44,027 [DEBUG] Initial domain set to: example.com
2024-12-05 17:44:44,027 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:44:44,027 [INFO] Successfully crawled https://example.com
2024-12-05 17:44:44,530 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:44:44,530 [DEBUG] Base URL: https://example.org
2024-12-05 17:44:44,530 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:44:44,530 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:44:44,530 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:44:44,530 [INFO] Successfully crawled https://example.org
2024-12-05 17:44:44,530 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:44,531 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x107011810>
2024-12-05 17:44:44,531 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x106fd6e40>, 691508.440015833)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x106f940c0>, 691508.946035541)])']
connector: <aiohttp.connector.TCPConnector object at 0x106f7edd0>
2024-12-05 17:44:44,531 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:44:44,531 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:44,531 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 17:44:44,531 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 17:44:44,531 [INFO] Starting crawl of URL: https://example.com
2024-12-05 17:44:45,088 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:44:45,089 [DEBUG] Base URL: https://example.com
2024-12-05 17:44:45,089 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:44:45,089 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:44:45,089 [DEBUG] Initial domain set to: example.com
2024-12-05 17:44:45,089 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:44:45,089 [INFO] Successfully crawled https://example.com
2024-12-05 17:44:45,090 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:45,090 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x106e1aad0>
2024-12-05 17:44:45,090 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10659ed60>, 691509.504316083)])']
connector: <aiohttp.connector.TCPConnector object at 0x10704a8d0>
2024-12-05 17:44:45,091 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:44:45,091 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:45,092 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 17:44:45,092 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 17:44:45,092 [INFO] Starting crawl of URL: https://example.com
2024-12-05 17:44:45,658 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:44:45,658 [DEBUG] Base URL: https://example.com
2024-12-05 17:44:45,658 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:44:45,659 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:44:45,659 [DEBUG] Initial domain set to: example.com
2024-12-05 17:44:45,659 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:44:45,659 [INFO] Successfully crawled https://example.com
2024-12-05 17:44:45,660 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:44:45,660 [DEBUG] Base URL: https://example.com
2024-12-05 17:44:45,660 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:44:45,660 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:44:45,660 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:44:45,668 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:45,669 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:44:45,669 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:45,670 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:45,670 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:44:45,670 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:45,670 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 17:44:45,670 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 17:44:45,670 [INFO] Starting crawl of URL: https://example.com
2024-12-05 17:44:46,221 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:44:46,221 [DEBUG] Base URL: https://example.com
2024-12-05 17:44:46,222 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:44:46,222 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:44:46,222 [DEBUG] Initial domain set to: example.com
2024-12-05 17:44:46,222 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:44:46,222 [INFO] Successfully crawled https://example.com
2024-12-05 17:44:46,223 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:46,223 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1070eb910>
2024-12-05 17:44:46,223 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x106f941a0>, 691510.6368455)])']
connector: <aiohttp.connector.TCPConnector object at 0x1070e9850>
2024-12-05 17:44:46,224 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:44:46,224 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:46,225 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2024-12-05 17:44:46,225 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2024-12-05 17:44:46,225 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2024-12-05 17:44:48,243 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-05 17:44:48,245 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-05 17:44:48,245 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-05 17:44:48,253 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:48,254 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:44:48,254 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:48,254 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 17:44:48,255 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 17:44:48,255 [INFO] Starting crawl of URL: https://example.com
2024-12-05 17:44:48,271 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-12-05 17:44:48,271 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-12-05 17:44:48,271 [INFO] Starting crawl of URL: https://example.org
2024-12-05 17:44:48,271 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2024-12-05 17:44:48,272 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2024-12-05 17:44:48,272 [INFO] Starting crawl of URL: https://example.net
2024-12-05 17:44:48,819 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:44:48,819 [DEBUG] Base URL: https://example.com
2024-12-05 17:44:48,819 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:44:48,819 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:44:48,819 [DEBUG] Initial domain set to: example.com
2024-12-05 17:44:48,819 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:44:48,820 [INFO] Successfully crawled https://example.com
2024-12-05 17:44:49,318 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:44:49,318 [DEBUG] Base URL: https://example.org
2024-12-05 17:44:49,319 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:44:49,319 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:44:49,319 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:44:49,319 [INFO] Successfully crawled https://example.org
2024-12-05 17:44:49,819 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:44:49,819 [DEBUG] Base URL: https://example.net
2024-12-05 17:44:49,819 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:44:49,819 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:44:49,819 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:44:49,820 [INFO] Successfully crawled https://example.net
2024-12-05 17:44:49,821 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:49,821 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1070e92d0>
2024-12-05 17:44:49,821 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x106f95160>, 691513.234632458)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x106f95780>, 691513.734133541)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x106f959b0>, 691514.234887833)])']
connector: <aiohttp.connector.TCPConnector object at 0x10707f750>
2024-12-05 17:44:49,822 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:44:49,822 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:49,823 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 17:44:49,823 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 17:44:49,823 [INFO] Starting crawl of URL: https://example.com
2024-12-05 17:44:50,392 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:44:50,393 [DEBUG] Base URL: https://example.com
2024-12-05 17:44:50,393 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:44:50,393 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:44:50,393 [DEBUG] Initial domain set to: example.com
2024-12-05 17:44:50,393 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:44:50,393 [INFO] Successfully crawled https://example.com
2024-12-05 17:44:50,395 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:50,400 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:44:50,400 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:50,401 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:44:50,421 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:44:50,437 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:50,437 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:44:50,438 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:50,438 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:44:50,455 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:50,456 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:44:50,456 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:50,456 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:44:50,456 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2024-12-05 17:44:50,456 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2024-12-05 17:44:50,457 [INFO] Starting crawl of URL: https://example.com/page
2024-12-05 17:44:51,029 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:44:51,029 [DEBUG] Base URL: https://example.com/page
2024-12-05 17:44:51,029 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:44:51,029 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:44:51,029 [DEBUG] Initial domain set to: example.com
2024-12-05 17:44:51,029 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:44:51,030 [INFO] Successfully crawled https://example.com/page
2024-12-05 17:44:51,030 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2024-12-05 17:44:51,030 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2024-12-05 17:44:51,030 [INFO] Starting crawl of URL: https://other-domain.com/page
2024-12-05 17:44:54,542 [DEBUG] 
Evaluating link: ./
2024-12-05 17:44:54,542 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,543 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-05 17:44:54,543 [DEBUG] Absolute link: https://other-domain.com/
2024-12-05 17:44:54,543 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,543 [DEBUG] 
Evaluating link: ./
2024-12-05 17:44:54,543 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,543 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-05 17:44:54,543 [DEBUG] Absolute link: https://other-domain.com/
2024-12-05 17:44:54,543 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,543 [DEBUG] 
Evaluating link: ./hiyou.php
2024-12-05 17:44:54,543 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,543 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-12-05 17:44:54,543 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-12-05 17:44:54,543 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,543 [DEBUG] 
Evaluating link: ./date.php
2024-12-05 17:44:54,543 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,543 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-12-05 17:44:54,543 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-12-05 17:44:54,543 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,543 [DEBUG] 
Evaluating link: ./contact.php
2024-12-05 17:44:54,543 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,543 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2024-12-05 17:44:54,543 [DEBUG] Absolute link: https://other-domain.com/contact.php
2024-12-05 17:44:54,543 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,543 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 17:44:54,543 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,543 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 17:44:54,543 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 17:44:54,543 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-05 17:44:54,543 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:44:54,543 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,543 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:44:54,543 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:44:54,543 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-05 17:44:54,543 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:44:54,544 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,544 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:44:54,544 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:44:54,544 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-05 17:44:54,544 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:44:54,544 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,544 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:44:54,544 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:44:54,544 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-05 17:44:54,544 [DEBUG] 
Evaluating link: ./contact
2024-12-05 17:44:54,544 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,544 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-12-05 17:44:54,544 [DEBUG] Absolute link: https://other-domain.com/contact
2024-12-05 17:44:54,544 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,544 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:44:54,544 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,544 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:44:54,544 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:44:54,544 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-05 17:44:54,544 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:44:54,544 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,544 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:44:54,544 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:44:54,544 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-05 17:44:54,544 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:44:54,544 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,544 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:44:54,544 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:44:54,544 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-05 17:44:54,544 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-12-05 17:44:54,544 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,544 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:44:54,544 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:44:54,544 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-12-05 17:44:54,544 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 17:44:54,544 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,545 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 17:44:54,545 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 17:44:54,545 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-05 17:44:54,545 [DEBUG] 
Evaluating link: #indexes
2024-12-05 17:44:54,545 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,545 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2024-12-05 17:44:54,545 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,545 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,545 [DEBUG] 
Evaluating link: #a
2024-12-05 17:44:54,545 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,545 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2024-12-05 17:44:54,545 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,545 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,545 [DEBUG] 
Evaluating link: #b
2024-12-05 17:44:54,545 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,545 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-12-05 17:44:54,545 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,545 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,545 [DEBUG] 
Evaluating link: #c
2024-12-05 17:44:54,545 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,559 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-12-05 17:44:54,559 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,559 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,559 [DEBUG] 
Evaluating link: #d
2024-12-05 17:44:54,559 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,559 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-12-05 17:44:54,559 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,559 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,559 [DEBUG] 
Evaluating link: #e
2024-12-05 17:44:54,559 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,559 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-12-05 17:44:54,559 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,559 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,559 [DEBUG] 
Evaluating link: #f
2024-12-05 17:44:54,559 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,559 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-12-05 17:44:54,559 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,560 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,560 [DEBUG] 
Evaluating link: #g
2024-12-05 17:44:54,560 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,560 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-12-05 17:44:54,560 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,560 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,560 [DEBUG] 
Evaluating link: #h
2024-12-05 17:44:54,560 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,560 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-12-05 17:44:54,560 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,560 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,560 [DEBUG] 
Evaluating link: #i
2024-12-05 17:44:54,560 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,560 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-12-05 17:44:54,560 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,560 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,560 [DEBUG] 
Evaluating link: #j
2024-12-05 17:44:54,560 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,560 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-12-05 17:44:54,560 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,560 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,560 [DEBUG] 
Evaluating link: #k
2024-12-05 17:44:54,560 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,560 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-12-05 17:44:54,560 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,560 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,560 [DEBUG] 
Evaluating link: #l
2024-12-05 17:44:54,560 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,560 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-12-05 17:44:54,560 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,560 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,560 [DEBUG] 
Evaluating link: #m
2024-12-05 17:44:54,560 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,560 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-12-05 17:44:54,560 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,560 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,560 [DEBUG] 
Evaluating link: #n
2024-12-05 17:44:54,560 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,560 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-12-05 17:44:54,560 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,561 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,561 [DEBUG] 
Evaluating link: #o
2024-12-05 17:44:54,561 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,561 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-12-05 17:44:54,561 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,561 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,561 [DEBUG] 
Evaluating link: #p
2024-12-05 17:44:54,561 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,561 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-12-05 17:44:54,561 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,561 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,561 [DEBUG] 
Evaluating link: #q
2024-12-05 17:44:54,561 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,561 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-12-05 17:44:54,561 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,561 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,561 [DEBUG] 
Evaluating link: #r
2024-12-05 17:44:54,561 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,561 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-12-05 17:44:54,561 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,561 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,561 [DEBUG] 
Evaluating link: #s
2024-12-05 17:44:54,561 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,561 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-12-05 17:44:54,561 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,561 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,561 [DEBUG] 
Evaluating link: #t
2024-12-05 17:44:54,561 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,561 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-12-05 17:44:54,561 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,561 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,561 [DEBUG] 
Evaluating link: #u
2024-12-05 17:44:54,561 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,561 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-12-05 17:44:54,561 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,561 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,561 [DEBUG] 
Evaluating link: #v
2024-12-05 17:44:54,561 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,561 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-12-05 17:44:54,561 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,561 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,561 [DEBUG] 
Evaluating link: #w
2024-12-05 17:44:54,561 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,561 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-12-05 17:44:54,562 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,562 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,562 [DEBUG] 
Evaluating link: #x
2024-12-05 17:44:54,562 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,562 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-12-05 17:44:54,562 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,562 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,562 [DEBUG] 
Evaluating link: #y
2024-12-05 17:44:54,562 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,562 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-12-05 17:44:54,562 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,562 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,562 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:44:54,562 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,562 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:44:54,562 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:44:54,562 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-05 17:44:54,562 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:44:54,562 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,562 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:44:54,562 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:44:54,562 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-05 17:44:54,562 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:44:54,562 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,562 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:44:54,562 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:44:54,562 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-05 17:44:54,562 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-12-05 17:44:54,562 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,562 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:44:54,562 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:44:54,562 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-12-05 17:44:54,562 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 17:44:54,562 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,562 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 17:44:54,562 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 17:44:54,562 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-05 17:44:54,562 [DEBUG] 
Evaluating link: https://spriq.jp/
2024-12-05 17:44:54,562 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,563 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2024-12-05 17:44:54,563 [DEBUG] Absolute link: https://spriq.jp/
2024-12-05 17:44:54,563 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2024-12-05 17:44:54,563 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:44:54,563 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,563 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:44:54,563 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:44:54,563 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-05 17:44:54,563 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:44:54,563 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,563 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:44:54,563 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 17:44:54,563 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-05 17:44:54,563 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:44:54,563 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,563 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:44:54,563 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 17:44:54,563 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-05 17:44:54,563 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:44:54,563 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,563 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:44:54,563 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 17:44:54,563 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-05 17:44:54,563 [DEBUG] 
Evaluating link: ./contact
2024-12-05 17:44:54,563 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,563 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-12-05 17:44:54,563 [DEBUG] Absolute link: https://other-domain.com/contact
2024-12-05 17:44:54,563 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,563 [DEBUG] 
Evaluating link: ./
2024-12-05 17:44:54,563 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,563 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-05 17:44:54,563 [DEBUG] Absolute link: https://other-domain.com/
2024-12-05 17:44:54,563 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,563 [DEBUG] 
Evaluating link: ./hiyou.php
2024-12-05 17:44:54,563 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,563 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-12-05 17:44:54,563 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-12-05 17:44:54,563 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,563 [DEBUG] 
Evaluating link: ./date.php
2024-12-05 17:44:54,563 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,563 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-12-05 17:44:54,564 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-12-05 17:44:54,564 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,564 [DEBUG] 
Evaluating link: #u
2024-12-05 17:44:54,564 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,564 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-12-05 17:44:54,564 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,564 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,564 [DEBUG] 
Evaluating link: #v
2024-12-05 17:44:54,564 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,564 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-12-05 17:44:54,564 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,564 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,564 [DEBUG] 
Evaluating link: #w
2024-12-05 17:44:54,564 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,564 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-12-05 17:44:54,564 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,564 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,564 [DEBUG] 
Evaluating link: #x
2024-12-05 17:44:54,564 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,564 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-12-05 17:44:54,564 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,564 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,564 [DEBUG] 
Evaluating link: #y
2024-12-05 17:44:54,564 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,564 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-12-05 17:44:54,564 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,564 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,564 [DEBUG] 
Evaluating link: #p
2024-12-05 17:44:54,564 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,564 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-12-05 17:44:54,564 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,564 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,564 [DEBUG] 
Evaluating link: #q
2024-12-05 17:44:54,564 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,564 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-12-05 17:44:54,564 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,564 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,564 [DEBUG] 
Evaluating link: #r
2024-12-05 17:44:54,564 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,564 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-12-05 17:44:54,564 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,564 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,564 [DEBUG] 
Evaluating link: #s
2024-12-05 17:44:54,564 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,564 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-12-05 17:44:54,564 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,565 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,565 [DEBUG] 
Evaluating link: #t
2024-12-05 17:44:54,565 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,565 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-12-05 17:44:54,565 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,565 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,565 [DEBUG] 
Evaluating link: #k
2024-12-05 17:44:54,565 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,565 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-12-05 17:44:54,565 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,565 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,565 [DEBUG] 
Evaluating link: #l
2024-12-05 17:44:54,565 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,565 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-12-05 17:44:54,565 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,565 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,565 [DEBUG] 
Evaluating link: #m
2024-12-05 17:44:54,565 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,565 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-12-05 17:44:54,565 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,565 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,565 [DEBUG] 
Evaluating link: #n
2024-12-05 17:44:54,565 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,565 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-12-05 17:44:54,565 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,565 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,565 [DEBUG] 
Evaluating link: #o
2024-12-05 17:44:54,565 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,565 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-12-05 17:44:54,565 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,565 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,565 [DEBUG] 
Evaluating link: #f
2024-12-05 17:44:54,565 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,565 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-12-05 17:44:54,565 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,565 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,565 [DEBUG] 
Evaluating link: #g
2024-12-05 17:44:54,565 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,565 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-12-05 17:44:54,565 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,565 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,565 [DEBUG] 
Evaluating link: #h
2024-12-05 17:44:54,565 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,565 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-12-05 17:44:54,565 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,565 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,566 [DEBUG] 
Evaluating link: #i
2024-12-05 17:44:54,566 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,566 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-12-05 17:44:54,566 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,566 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,566 [DEBUG] 
Evaluating link: #j
2024-12-05 17:44:54,566 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,566 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-12-05 17:44:54,566 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,566 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,566 [DEBUG] 
Evaluating link: #b
2024-12-05 17:44:54,566 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,566 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-12-05 17:44:54,566 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,566 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,566 [DEBUG] 
Evaluating link: #c
2024-12-05 17:44:54,566 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,566 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-12-05 17:44:54,566 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,566 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,566 [DEBUG] 
Evaluating link: #d
2024-12-05 17:44:54,566 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,566 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-12-05 17:44:54,566 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,566 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,566 [DEBUG] 
Evaluating link: #e
2024-12-05 17:44:54,566 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,566 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-12-05 17:44:54,566 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,566 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,566 [DEBUG] 
Evaluating link: #site
2024-12-05 17:44:54,566 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 17:44:54,566 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2024-12-05 17:44:54,566 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 17:44:54,566 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 17:44:54,566 [INFO] Successfully crawled https://other-domain.com/page
2024-12-05 17:44:54,569 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:54,570 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:44:54,570 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:54,570 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-12-05 17:44:54,570 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-12-05 17:44:54,570 [INFO] Starting crawl of URL: https://example.com/page0
2024-12-05 17:44:55,135 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:44:55,135 [DEBUG] Base URL: https://example.com/page0
2024-12-05 17:44:55,136 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:44:55,136 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:44:55,136 [DEBUG] Initial domain set to: example.com
2024-12-05 17:44:55,136 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:44:55,136 [INFO] Successfully crawled https://example.com/page0
2024-12-05 17:44:55,136 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-12-05 17:44:55,136 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-12-05 17:44:55,136 [INFO] Starting crawl of URL: https://example.com/page1
2024-12-05 17:44:55,276 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:44:55,276 [DEBUG] Base URL: https://example.com/page1
2024-12-05 17:44:55,276 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:44:55,276 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:44:55,277 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:44:55,277 [INFO] Successfully crawled https://example.com/page1
2024-12-05 17:44:55,277 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-12-05 17:44:55,277 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-12-05 17:44:55,277 [INFO] Starting crawl of URL: https://example.com/page2
2024-12-05 17:44:55,782 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:44:55,782 [DEBUG] Base URL: https://example.com/page2
2024-12-05 17:44:55,783 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:44:55,783 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:44:55,783 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:44:55,783 [INFO] Successfully crawled https://example.com/page2
2024-12-05 17:44:55,783 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-12-05 17:44:55,783 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-12-05 17:44:55,784 [INFO] Starting crawl of URL: https://example.com/page3
2024-12-05 17:44:56,284 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:44:56,284 [DEBUG] Base URL: https://example.com/page3
2024-12-05 17:44:56,284 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:44:56,284 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:44:56,284 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:44:56,284 [INFO] Successfully crawled https://example.com/page3
2024-12-05 17:44:56,285 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-12-05 17:44:56,285 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-12-05 17:44:56,285 [INFO] Starting crawl of URL: https://example.com/page4
2024-12-05 17:44:56,780 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:44:56,780 [DEBUG] Base URL: https://example.com/page4
2024-12-05 17:44:56,780 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:44:56,780 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:44:56,780 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:44:56,780 [INFO] Successfully crawled https://example.com/page4
2024-12-05 17:44:56,781 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:44:56,781 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-12-05 17:44:56,781 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-12-05 17:44:56,781 [INFO] Starting crawl of URL: https://example.com/page0
2024-12-05 17:44:57,362 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:44:57,362 [DEBUG] Base URL: https://example.com/page0
2024-12-05 17:44:57,362 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:44:57,362 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:44:57,362 [DEBUG] Initial domain set to: example.com
2024-12-05 17:44:57,363 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:44:57,363 [INFO] Successfully crawled https://example.com/page0
2024-12-05 17:44:57,363 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-12-05 17:44:57,363 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-12-05 17:44:57,363 [INFO] Starting crawl of URL: https://example.com/page1
2024-12-05 17:44:57,504 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:44:57,505 [DEBUG] Base URL: https://example.com/page1
2024-12-05 17:44:57,505 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:44:57,505 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:44:57,505 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:44:57,505 [INFO] Successfully crawled https://example.com/page1
2024-12-05 17:44:57,505 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-12-05 17:44:57,505 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-12-05 17:44:57,505 [INFO] Starting crawl of URL: https://example.com/page2
2024-12-05 17:44:58,007 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:44:58,007 [DEBUG] Base URL: https://example.com/page2
2024-12-05 17:44:58,007 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:44:58,007 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:44:58,007 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:44:58,007 [INFO] Successfully crawled https://example.com/page2
2024-12-05 17:44:58,007 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-12-05 17:44:58,007 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-12-05 17:44:58,007 [INFO] Starting crawl of URL: https://example.com/page3
2024-12-05 17:44:58,505 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:44:58,505 [DEBUG] Base URL: https://example.com/page3
2024-12-05 17:44:58,505 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:44:58,506 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:44:58,506 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:44:58,506 [INFO] Successfully crawled https://example.com/page3
2024-12-05 17:44:58,506 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-12-05 17:44:58,506 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-12-05 17:44:58,506 [INFO] Starting crawl of URL: https://example.com/page4
2024-12-05 17:44:59,014 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:44:59,014 [DEBUG] Base URL: https://example.com/page4
2024-12-05 17:44:59,015 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:44:59,015 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:44:59,015 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:44:59,015 [INFO] Successfully crawled https://example.com/page4
2024-12-05 17:44:59,023 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:59,024 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:59,028 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:59,028 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:59,030 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x106f90610>
2024-12-05 17:44:59,030 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x106f95f60>, 691521.195464041)])']
connector: <aiohttp.connector.TCPConnector object at 0x106f90310>
2024-12-05 17:44:59,031 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x106f93410>
2024-12-05 17:44:59,031 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x107155630>, 691523.430024458)])']
connector: <aiohttp.connector.TCPConnector object at 0x106f931d0>
2024-12-05 17:44:59,034 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:59,035 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:59,038 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:59,038 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:59,041 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:59,041 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:59,044 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:59,045 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:59,048 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:59,048 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:59,051 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:59,052 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:59,054 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:59,055 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:59,057 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:59,070 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:44:59,073 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:44:59,076 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:44:59,079 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:44:59,082 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:44:59,084 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:44:59,103 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1070b2e10>
2024-12-05 17:44:59,103 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x106fd6820>, 691510.073947375)])']
connector: <aiohttp.connector.TCPConnector object at 0x107013d90>
2024-12-05 17:44:59,103 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x106e1af10>
2024-12-05 17:44:59,103 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x106f95710>, 691515.443532708)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x106f95a90>, 691516.769119666)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x106f94130>, 691517.778190583)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x106f95be0>, 691518.945047375)])']
connector: <aiohttp.connector.TCPConnector object at 0x106e18fd0>
2024-12-05 17:44:59,107 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:44:59,110 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:44:59,114 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:44:59,119 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:44:59,122 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:44:59,135 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:59,139 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:59,139 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:59,139 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:44:59,142 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:59,143 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:59,143 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:44:59,147 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:59,149 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:44:59,157 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:44:59,163 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:44:59,169 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:44:59,174 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:44:59,181 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:44:59,188 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:44:59,194 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:44:59,200 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:44:59,206 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:44:59,267 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:59,418 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:44:59,421 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:44:59,421 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 17:44:59,421 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 17:44:59,421 [INFO] Starting crawl of URL: https://example.com
2024-12-05 17:44:59,980 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 17:44:59,980 [DEBUG] Base URL: https://example.com
2024-12-05 17:44:59,980 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 17:44:59,980 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 17:44:59,980 [DEBUG] Initial domain set to: example.com
2024-12-05 17:44:59,980 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 17:44:59,980 [INFO] Successfully crawled https://example.com
2024-12-05 17:44:59,980 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x107fa5950>
2024-12-05 17:44:59,981 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x107e96430>, 691524.39612875)])']
connector: <aiohttp.connector.TCPConnector object at 0x107fa4910>
2024-12-05 17:44:59,981 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2024-12-05 17:45:00,144 [INFO] Operation took 0.10 seconds
2024-12-05 17:45:00,194 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:45:00,194 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:45:00,194 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:00,702 [ERROR] Could not find documentation URL for package: file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-169/test_full_site_crawl0/test_docs/index.html
2024-12-05 17:45:00,702 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2024-12-05 17:45:00,712 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:00,714 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:45:00,714 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:45:00,714 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:01,195 [ERROR] Could not find documentation URL for package: file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-169/test_content_processing_pipeli0/test_docs/guide.html
2024-12-05 17:45:01,196 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2024-12-05 17:45:01,208 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:01,210 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:45:01,211 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:45:01,211 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:01,687 [ERROR] Could not find documentation URL for package: file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-169/test_quality_checks0/test_docs/api.html
2024-12-05 17:45:01,687 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2024-12-05 17:45:01,701 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:01,703 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:45:01,703 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:45:01,703 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:02,192 [ERROR] Could not find documentation URL for package: file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-169/test_document_organization0/test_docs/index.html
2024-12-05 17:45:02,193 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2024-12-05 17:45:02,230 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:02,232 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:45:02,232 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:45:02,233 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:02,784 [ERROR] Could not find documentation URL for package: file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-169/test_search_functionality0/test_docs/index.html
2024-12-05 17:45:02,785 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2024-12-05 17:45:02,798 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:02,800 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 17:45:02,800 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:45:02,800 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:03,538 [ERROR] Could not find documentation URL for package: file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-169/test_error_handling_and_recove0/test_docs/index.html
2024-12-05 17:45:03,539 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2024-12-05 17:45:03,551 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:03,551 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:03,554 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:03,555 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:03,555 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:45:03,555 [ERROR] Error during crawl: 'str' object has no attribute 'url'
2024-12-05 17:45:03,557 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:03,557 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:03,559 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:03,559 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:03,559 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 17:45:03,562 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:03,921 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:03,926 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:03,926 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:03,929 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:03,929 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:03,933 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:03,933 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:03,936 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:03,937 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:03,940 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:03,941 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:03,944 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:03,944 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:45:03,946 [DEBUG] Using selector: KqueueSelector
2024-12-05 17:49:31,294 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-05 18:01:13,970 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-05 18:02:09,237 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-05 18:22:16,275 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-05 18:23:10,821 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-05 22:26:54,560 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-05 22:37:34,719 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-05 22:45:32,842 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-05 22:49:37,112 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-05 22:51:44,708 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-05 22:51:58,507 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-05 22:52:22,078 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-05 23:14:57,957 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-05 23:14:58,261 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:14:58,262 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:14:58,263 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:14:58,264 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:14:58,265 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:14:58,265 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:14:58,265 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:14:58,285 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:14:58,286 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:14:58,286 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:14:58,289 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:14:58,289 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:14:58,292 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:14:58,292 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:14:58,295 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:14:58,295 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:14:58,296 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:14:58,296 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:14:58,299 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:14:58,299 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:14:58,299 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:14:58,301 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:14:59,822 [INFO] Operation took 0.02 seconds
2024-12-05 23:15:00,031 [INFO] test_operation took 0.11 seconds
2024-12-05 23:15:00,237 [INFO] inner took 0.10 seconds
2024-12-05 23:15:00,237 [INFO] outer took 0.21 seconds
2024-12-05 23:15:00,237 [INFO] error_operation took 0.00 seconds
2024-12-05 23:15:00,255 [INFO] test_operation took 0.00 seconds
2024-12-05 23:15:00,280 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 23:15:00,280 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:00,281 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 23:15:00,281 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 23:15:00,281 [INFO] Starting crawl of URL: https://example.com
2024-12-05 23:15:00,847 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 23:15:00,847 [DEBUG] Base URL: https://example.com
2024-12-05 23:15:00,847 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 23:15:00,847 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 23:15:00,847 [DEBUG] Initial domain set to: example.com
2024-12-05 23:15:00,847 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 23:15:00,847 [INFO] Successfully crawled https://example.com
2024-12-05 23:15:00,848 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:00,848 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1098a0450>
2024-12-05 23:15:00,848 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10973e7b0>, 711325.380105666)])']
connector: <aiohttp.connector.TCPConnector object at 0x108bbf990>
2024-12-05 23:15:00,848 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 23:15:00,848 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:00,849 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 23:15:00,849 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 23:15:00,849 [INFO] Starting crawl of URL: https://example.com
2024-12-05 23:15:00,868 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-12-05 23:15:00,869 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-12-05 23:15:00,869 [INFO] Starting crawl of URL: https://example.org
2024-12-05 23:15:01,401 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 23:15:01,401 [DEBUG] Base URL: https://example.com
2024-12-05 23:15:01,401 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 23:15:01,401 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 23:15:01,401 [DEBUG] Initial domain set to: example.com
2024-12-05 23:15:01,401 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 23:15:01,401 [INFO] Successfully crawled https://example.com
2024-12-05 23:15:01,904 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 23:15:01,904 [DEBUG] Base URL: https://example.org
2024-12-05 23:15:01,904 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 23:15:01,904 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 23:15:01,904 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 23:15:01,904 [INFO] Successfully crawled https://example.org
2024-12-05 23:15:01,905 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:01,905 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x109806250>
2024-12-05 23:15:01,905 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10973ee40>, 711325.933708416)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1097e80c0>, 711326.438116083)])']
connector: <aiohttp.connector.TCPConnector object at 0x1097ad210>
2024-12-05 23:15:01,905 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 23:15:01,905 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:01,906 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 23:15:01,906 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 23:15:01,906 [INFO] Starting crawl of URL: https://example.com
2024-12-05 23:15:02,460 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 23:15:02,460 [DEBUG] Base URL: https://example.com
2024-12-05 23:15:02,460 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 23:15:02,460 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 23:15:02,460 [DEBUG] Initial domain set to: example.com
2024-12-05 23:15:02,460 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 23:15:02,460 [INFO] Successfully crawled https://example.com
2024-12-05 23:15:02,461 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:02,461 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x10964c290>
2024-12-05 23:15:02,461 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10973ecf0>, 711326.993726375)])']
connector: <aiohttp.connector.TCPConnector object at 0x10964f090>
2024-12-05 23:15:02,461 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 23:15:02,461 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:02,462 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 23:15:02,462 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 23:15:02,462 [INFO] Starting crawl of URL: https://example.com
2024-12-05 23:15:03,010 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 23:15:03,010 [DEBUG] Base URL: https://example.com
2024-12-05 23:15:03,010 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 23:15:03,010 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 23:15:03,010 [DEBUG] Initial domain set to: example.com
2024-12-05 23:15:03,010 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 23:15:03,010 [INFO] Successfully crawled https://example.com
2024-12-05 23:15:03,011 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 23:15:03,011 [DEBUG] Base URL: https://example.com
2024-12-05 23:15:03,011 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 23:15:03,011 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 23:15:03,011 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 23:15:03,016 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:03,017 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:03,017 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:03,017 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:03,018 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 23:15:03,018 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:03,018 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 23:15:03,018 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 23:15:03,018 [INFO] Starting crawl of URL: https://example.com
2024-12-05 23:15:03,581 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 23:15:03,581 [DEBUG] Base URL: https://example.com
2024-12-05 23:15:03,581 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 23:15:03,581 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 23:15:03,581 [DEBUG] Initial domain set to: example.com
2024-12-05 23:15:03,581 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 23:15:03,582 [INFO] Successfully crawled https://example.com
2024-12-05 23:15:03,582 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:03,582 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1097df590>
2024-12-05 23:15:03,582 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1097e81a0>, 711328.115350625)])']
connector: <aiohttp.connector.TCPConnector object at 0x10963e950>
2024-12-05 23:15:03,582 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 23:15:03,582 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:03,583 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2024-12-05 23:15:03,583 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2024-12-05 23:15:03,583 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2024-12-05 23:15:05,646 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-05 23:15:05,648 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-05 23:15:05,648 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-05 23:15:05,652 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:05,652 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 23:15:05,652 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:05,653 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 23:15:05,653 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 23:15:05,653 [INFO] Starting crawl of URL: https://example.com
2024-12-05 23:15:05,676 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-12-05 23:15:05,676 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-12-05 23:15:05,676 [INFO] Starting crawl of URL: https://example.org
2024-12-05 23:15:05,676 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2024-12-05 23:15:05,676 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2024-12-05 23:15:05,676 [INFO] Starting crawl of URL: https://example.net
2024-12-05 23:15:06,199 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 23:15:06,200 [DEBUG] Base URL: https://example.com
2024-12-05 23:15:06,200 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 23:15:06,200 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 23:15:06,200 [DEBUG] Initial domain set to: example.com
2024-12-05 23:15:06,200 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 23:15:06,200 [INFO] Successfully crawled https://example.com
2024-12-05 23:15:06,742 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 23:15:06,742 [DEBUG] Base URL: https://example.org
2024-12-05 23:15:06,742 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 23:15:06,743 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 23:15:06,743 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 23:15:06,743 [INFO] Successfully crawled https://example.org
2024-12-05 23:15:07,353 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 23:15:07,353 [DEBUG] Base URL: https://example.net
2024-12-05 23:15:07,353 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 23:15:07,353 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 23:15:07,353 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 23:15:07,353 [INFO] Successfully crawled https://example.net
2024-12-05 23:15:07,354 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:07,354 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x10963cdd0>
2024-12-05 23:15:07,354 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1097e9160>, 711330.733648291)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1097e9780>, 711331.275536125)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1097e9c50>, 711331.886968375)])']
connector: <aiohttp.connector.TCPConnector object at 0x10963d0d0>
2024-12-05 23:15:07,354 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 23:15:07,354 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:07,355 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 23:15:07,355 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 23:15:07,355 [INFO] Starting crawl of URL: https://example.com
2024-12-05 23:15:07,912 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 23:15:07,912 [DEBUG] Base URL: https://example.com
2024-12-05 23:15:07,913 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 23:15:07,913 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 23:15:07,913 [DEBUG] Initial domain set to: example.com
2024-12-05 23:15:07,913 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 23:15:07,913 [INFO] Successfully crawled https://example.com
2024-12-05 23:15:07,914 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:07,918 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 23:15:07,918 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:07,918 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:07,938 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 23:15:07,957 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:07,957 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 23:15:07,957 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:07,958 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 23:15:07,974 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:07,974 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 23:15:07,975 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:07,975 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 23:15:07,975 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2024-12-05 23:15:07,975 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2024-12-05 23:15:07,975 [INFO] Starting crawl of URL: https://example.com/page
2024-12-05 23:15:08,531 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 23:15:08,531 [DEBUG] Base URL: https://example.com/page
2024-12-05 23:15:08,531 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 23:15:08,531 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 23:15:08,531 [DEBUG] Initial domain set to: example.com
2024-12-05 23:15:08,531 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 23:15:08,531 [INFO] Successfully crawled https://example.com/page
2024-12-05 23:15:08,531 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2024-12-05 23:15:08,531 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2024-12-05 23:15:08,531 [INFO] Starting crawl of URL: https://other-domain.com/page
2024-12-05 23:15:13,126 [DEBUG] 
Evaluating link: ./
2024-12-05 23:15:13,126 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,126 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-05 23:15:13,126 [DEBUG] Absolute link: https://other-domain.com/
2024-12-05 23:15:13,126 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,126 [DEBUG] 
Evaluating link: ./
2024-12-05 23:15:13,126 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,126 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-05 23:15:13,126 [DEBUG] Absolute link: https://other-domain.com/
2024-12-05 23:15:13,126 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,126 [DEBUG] 
Evaluating link: ./hiyou.php
2024-12-05 23:15:13,126 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,126 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-12-05 23:15:13,126 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-12-05 23:15:13,126 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,126 [DEBUG] 
Evaluating link: ./date.php
2024-12-05 23:15:13,127 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,127 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-12-05 23:15:13,127 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-12-05 23:15:13,127 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,127 [DEBUG] 
Evaluating link: ./contact.php
2024-12-05 23:15:13,127 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,127 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2024-12-05 23:15:13,127 [DEBUG] Absolute link: https://other-domain.com/contact.php
2024-12-05 23:15:13,127 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,127 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 23:15:13,127 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,127 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 23:15:13,127 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 23:15:13,127 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-05 23:15:13,127 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 23:15:13,127 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,127 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 23:15:13,127 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 23:15:13,127 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-05 23:15:13,127 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 23:15:13,127 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,127 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 23:15:13,127 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 23:15:13,127 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-05 23:15:13,127 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-05 23:15:13,127 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,127 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 23:15:13,127 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 23:15:13,127 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-05 23:15:13,127 [DEBUG] 
Evaluating link: ./contact
2024-12-05 23:15:13,127 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,127 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-12-05 23:15:13,127 [DEBUG] Absolute link: https://other-domain.com/contact
2024-12-05 23:15:13,128 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,128 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 23:15:13,128 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,128 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 23:15:13,128 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 23:15:13,128 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-05 23:15:13,128 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 23:15:13,128 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,128 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 23:15:13,128 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 23:15:13,128 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-05 23:15:13,128 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-05 23:15:13,128 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,128 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 23:15:13,128 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 23:15:13,128 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-05 23:15:13,128 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-12-05 23:15:13,128 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,128 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 23:15:13,128 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 23:15:13,128 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-12-05 23:15:13,128 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 23:15:13,128 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,128 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 23:15:13,128 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 23:15:13,128 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-05 23:15:13,128 [DEBUG] 
Evaluating link: #indexes
2024-12-05 23:15:13,128 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,128 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2024-12-05 23:15:13,128 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,128 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,128 [DEBUG] 
Evaluating link: #a
2024-12-05 23:15:13,128 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,128 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2024-12-05 23:15:13,128 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,128 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,128 [DEBUG] 
Evaluating link: #b
2024-12-05 23:15:13,128 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,128 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-12-05 23:15:13,128 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,128 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,128 [DEBUG] 
Evaluating link: #c
2024-12-05 23:15:13,128 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,142 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-12-05 23:15:13,142 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,142 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,142 [DEBUG] 
Evaluating link: #d
2024-12-05 23:15:13,142 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,142 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-12-05 23:15:13,142 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,142 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,142 [DEBUG] 
Evaluating link: #e
2024-12-05 23:15:13,142 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,142 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-12-05 23:15:13,142 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,142 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,142 [DEBUG] 
Evaluating link: #f
2024-12-05 23:15:13,142 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,142 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-12-05 23:15:13,142 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,142 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,142 [DEBUG] 
Evaluating link: #g
2024-12-05 23:15:13,142 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,142 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-12-05 23:15:13,142 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,142 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,142 [DEBUG] 
Evaluating link: #h
2024-12-05 23:15:13,142 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,142 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-12-05 23:15:13,142 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,142 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,142 [DEBUG] 
Evaluating link: #i
2024-12-05 23:15:13,142 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,142 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-12-05 23:15:13,142 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,142 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,142 [DEBUG] 
Evaluating link: #j
2024-12-05 23:15:13,142 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,142 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-12-05 23:15:13,142 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,142 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,142 [DEBUG] 
Evaluating link: #k
2024-12-05 23:15:13,143 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,143 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-12-05 23:15:13,143 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,143 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,143 [DEBUG] 
Evaluating link: #l
2024-12-05 23:15:13,143 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,143 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-12-05 23:15:13,143 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,143 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,143 [DEBUG] 
Evaluating link: #m
2024-12-05 23:15:13,143 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,143 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-12-05 23:15:13,143 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,143 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,143 [DEBUG] 
Evaluating link: #n
2024-12-05 23:15:13,143 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,143 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-12-05 23:15:13,143 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,143 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,143 [DEBUG] 
Evaluating link: #o
2024-12-05 23:15:13,143 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,143 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-12-05 23:15:13,143 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,143 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,143 [DEBUG] 
Evaluating link: #p
2024-12-05 23:15:13,143 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,143 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-12-05 23:15:13,143 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,143 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,143 [DEBUG] 
Evaluating link: #q
2024-12-05 23:15:13,143 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,143 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-12-05 23:15:13,143 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,143 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,143 [DEBUG] 
Evaluating link: #r
2024-12-05 23:15:13,143 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,143 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-12-05 23:15:13,143 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,143 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,143 [DEBUG] 
Evaluating link: #s
2024-12-05 23:15:13,143 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,143 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-12-05 23:15:13,143 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,143 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,143 [DEBUG] 
Evaluating link: #t
2024-12-05 23:15:13,144 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,144 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-12-05 23:15:13,144 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,144 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,144 [DEBUG] 
Evaluating link: #u
2024-12-05 23:15:13,144 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,144 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-12-05 23:15:13,144 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,144 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,144 [DEBUG] 
Evaluating link: #v
2024-12-05 23:15:13,144 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,144 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-12-05 23:15:13,144 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,144 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,144 [DEBUG] 
Evaluating link: #w
2024-12-05 23:15:13,144 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,144 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-12-05 23:15:13,144 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,144 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,144 [DEBUG] 
Evaluating link: #x
2024-12-05 23:15:13,144 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,144 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-12-05 23:15:13,144 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,144 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,144 [DEBUG] 
Evaluating link: #y
2024-12-05 23:15:13,144 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,144 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-12-05 23:15:13,144 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,144 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,144 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 23:15:13,144 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,144 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 23:15:13,144 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 23:15:13,144 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-05 23:15:13,144 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 23:15:13,144 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,144 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 23:15:13,144 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 23:15:13,144 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-05 23:15:13,144 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-05 23:15:13,144 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,144 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 23:15:13,144 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 23:15:13,144 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-05 23:15:13,144 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-12-05 23:15:13,145 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,145 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 23:15:13,145 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 23:15:13,145 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-12-05 23:15:13,145 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 23:15:13,145 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,145 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 23:15:13,145 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-05 23:15:13,145 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-05 23:15:13,145 [DEBUG] 
Evaluating link: https://spriq.jp/
2024-12-05 23:15:13,145 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,145 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2024-12-05 23:15:13,145 [DEBUG] Absolute link: https://spriq.jp/
2024-12-05 23:15:13,145 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2024-12-05 23:15:13,145 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 23:15:13,145 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,145 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 23:15:13,145 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 23:15:13,145 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-05 23:15:13,145 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 23:15:13,145 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,145 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 23:15:13,145 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-05 23:15:13,145 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-05 23:15:13,145 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 23:15:13,145 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,145 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 23:15:13,145 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-05 23:15:13,145 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-05 23:15:13,145 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-05 23:15:13,145 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,145 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 23:15:13,145 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-05 23:15:13,145 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-05 23:15:13,145 [DEBUG] 
Evaluating link: ./contact
2024-12-05 23:15:13,145 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,145 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-12-05 23:15:13,145 [DEBUG] Absolute link: https://other-domain.com/contact
2024-12-05 23:15:13,145 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,145 [DEBUG] 
Evaluating link: ./
2024-12-05 23:15:13,145 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,145 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-05 23:15:13,145 [DEBUG] Absolute link: https://other-domain.com/
2024-12-05 23:15:13,145 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,145 [DEBUG] 
Evaluating link: ./hiyou.php
2024-12-05 23:15:13,145 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,146 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-12-05 23:15:13,146 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-12-05 23:15:13,146 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,146 [DEBUG] 
Evaluating link: ./date.php
2024-12-05 23:15:13,146 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,146 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-12-05 23:15:13,146 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-12-05 23:15:13,146 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,146 [DEBUG] 
Evaluating link: #u
2024-12-05 23:15:13,146 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,146 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-12-05 23:15:13,146 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,146 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,146 [DEBUG] 
Evaluating link: #v
2024-12-05 23:15:13,146 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,146 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-12-05 23:15:13,146 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,146 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,146 [DEBUG] 
Evaluating link: #w
2024-12-05 23:15:13,146 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,146 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-12-05 23:15:13,146 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,146 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,146 [DEBUG] 
Evaluating link: #x
2024-12-05 23:15:13,146 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,146 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-12-05 23:15:13,146 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,146 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,146 [DEBUG] 
Evaluating link: #y
2024-12-05 23:15:13,146 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,146 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-12-05 23:15:13,146 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,146 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,146 [DEBUG] 
Evaluating link: #p
2024-12-05 23:15:13,146 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,146 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-12-05 23:15:13,146 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,146 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,146 [DEBUG] 
Evaluating link: #q
2024-12-05 23:15:13,146 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,146 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-12-05 23:15:13,146 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,146 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,146 [DEBUG] 
Evaluating link: #r
2024-12-05 23:15:13,146 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,146 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-12-05 23:15:13,146 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,146 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,146 [DEBUG] 
Evaluating link: #s
2024-12-05 23:15:13,147 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,147 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-12-05 23:15:13,147 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,147 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,147 [DEBUG] 
Evaluating link: #t
2024-12-05 23:15:13,147 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,147 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-12-05 23:15:13,147 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,147 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,147 [DEBUG] 
Evaluating link: #k
2024-12-05 23:15:13,147 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,147 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-12-05 23:15:13,147 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,147 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,147 [DEBUG] 
Evaluating link: #l
2024-12-05 23:15:13,147 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,147 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-12-05 23:15:13,147 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,147 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,147 [DEBUG] 
Evaluating link: #m
2024-12-05 23:15:13,147 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,147 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-12-05 23:15:13,147 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,147 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,147 [DEBUG] 
Evaluating link: #n
2024-12-05 23:15:13,147 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,147 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-12-05 23:15:13,147 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,147 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,147 [DEBUG] 
Evaluating link: #o
2024-12-05 23:15:13,147 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,147 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-12-05 23:15:13,147 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,147 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,147 [DEBUG] 
Evaluating link: #f
2024-12-05 23:15:13,147 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,147 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-12-05 23:15:13,147 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,147 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,147 [DEBUG] 
Evaluating link: #g
2024-12-05 23:15:13,147 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,147 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-12-05 23:15:13,147 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,147 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,147 [DEBUG] 
Evaluating link: #h
2024-12-05 23:15:13,147 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,147 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-12-05 23:15:13,147 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,147 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,148 [DEBUG] 
Evaluating link: #i
2024-12-05 23:15:13,148 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,148 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-12-05 23:15:13,148 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,148 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,148 [DEBUG] 
Evaluating link: #j
2024-12-05 23:15:13,148 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,148 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-12-05 23:15:13,148 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,148 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,148 [DEBUG] 
Evaluating link: #b
2024-12-05 23:15:13,148 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,148 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-12-05 23:15:13,148 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,148 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,148 [DEBUG] 
Evaluating link: #c
2024-12-05 23:15:13,148 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,148 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-12-05 23:15:13,148 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,148 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,148 [DEBUG] 
Evaluating link: #d
2024-12-05 23:15:13,148 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,148 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-12-05 23:15:13,148 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,148 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,148 [DEBUG] 
Evaluating link: #e
2024-12-05 23:15:13,148 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,148 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-12-05 23:15:13,148 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,148 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,148 [DEBUG] 
Evaluating link: #site
2024-12-05 23:15:13,148 [DEBUG] Base URL: https://other-domain.com/page
2024-12-05 23:15:13,148 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2024-12-05 23:15:13,148 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-05 23:15:13,148 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-05 23:15:13,148 [INFO] Successfully crawled https://other-domain.com/page
2024-12-05 23:15:13,151 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:13,151 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 23:15:13,152 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:13,152 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-12-05 23:15:13,152 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-12-05 23:15:13,152 [INFO] Starting crawl of URL: https://example.com/page0
2024-12-05 23:15:13,706 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 23:15:13,706 [DEBUG] Base URL: https://example.com/page0
2024-12-05 23:15:13,707 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 23:15:13,707 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 23:15:13,707 [DEBUG] Initial domain set to: example.com
2024-12-05 23:15:13,707 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 23:15:13,707 [INFO] Successfully crawled https://example.com/page0
2024-12-05 23:15:13,707 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-12-05 23:15:13,707 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-12-05 23:15:13,707 [INFO] Starting crawl of URL: https://example.com/page1
2024-12-05 23:15:13,907 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 23:15:13,907 [DEBUG] Base URL: https://example.com/page1
2024-12-05 23:15:13,907 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 23:15:13,907 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 23:15:13,907 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 23:15:13,907 [INFO] Successfully crawled https://example.com/page1
2024-12-05 23:15:13,907 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-12-05 23:15:13,907 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-12-05 23:15:13,907 [INFO] Starting crawl of URL: https://example.com/page2
2024-12-05 23:15:14,347 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 23:15:14,347 [DEBUG] Base URL: https://example.com/page2
2024-12-05 23:15:14,347 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 23:15:14,347 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 23:15:14,347 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 23:15:14,347 [INFO] Successfully crawled https://example.com/page2
2024-12-05 23:15:14,347 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-12-05 23:15:14,347 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-12-05 23:15:14,347 [INFO] Starting crawl of URL: https://example.com/page3
2024-12-05 23:15:14,848 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 23:15:14,848 [DEBUG] Base URL: https://example.com/page3
2024-12-05 23:15:14,848 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 23:15:14,848 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 23:15:14,848 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 23:15:14,848 [INFO] Successfully crawled https://example.com/page3
2024-12-05 23:15:14,848 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-12-05 23:15:14,848 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-12-05 23:15:14,848 [INFO] Starting crawl of URL: https://example.com/page4
2024-12-05 23:15:15,347 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 23:15:15,347 [DEBUG] Base URL: https://example.com/page4
2024-12-05 23:15:15,347 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 23:15:15,347 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 23:15:15,347 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 23:15:15,347 [INFO] Successfully crawled https://example.com/page4
2024-12-05 23:15:15,347 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 23:15:15,347 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-12-05 23:15:15,347 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-12-05 23:15:15,347 [INFO] Starting crawl of URL: https://example.com/page0
2024-12-05 23:15:15,902 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 23:15:15,902 [DEBUG] Base URL: https://example.com/page0
2024-12-05 23:15:15,902 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 23:15:15,902 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 23:15:15,902 [DEBUG] Initial domain set to: example.com
2024-12-05 23:15:15,902 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 23:15:15,902 [INFO] Successfully crawled https://example.com/page0
2024-12-05 23:15:15,902 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-12-05 23:15:15,902 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-12-05 23:15:15,902 [INFO] Starting crawl of URL: https://example.com/page1
2024-12-05 23:15:16,264 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 23:15:16,264 [DEBUG] Base URL: https://example.com/page1
2024-12-05 23:15:16,264 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 23:15:16,264 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 23:15:16,264 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 23:15:16,264 [INFO] Successfully crawled https://example.com/page1
2024-12-05 23:15:16,264 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-12-05 23:15:16,264 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-12-05 23:15:16,264 [INFO] Starting crawl of URL: https://example.com/page2
2024-12-05 23:15:16,548 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 23:15:16,548 [DEBUG] Base URL: https://example.com/page2
2024-12-05 23:15:16,548 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 23:15:16,548 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 23:15:16,548 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 23:15:16,548 [INFO] Successfully crawled https://example.com/page2
2024-12-05 23:15:16,548 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-12-05 23:15:16,548 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-12-05 23:15:16,548 [INFO] Starting crawl of URL: https://example.com/page3
2024-12-05 23:15:17,042 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 23:15:17,042 [DEBUG] Base URL: https://example.com/page3
2024-12-05 23:15:17,042 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 23:15:17,043 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 23:15:17,043 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 23:15:17,043 [INFO] Successfully crawled https://example.com/page3
2024-12-05 23:15:17,043 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-12-05 23:15:17,043 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-12-05 23:15:17,043 [INFO] Starting crawl of URL: https://example.com/page4
2024-12-05 23:15:17,541 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 23:15:17,542 [DEBUG] Base URL: https://example.com/page4
2024-12-05 23:15:17,542 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 23:15:17,542 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 23:15:17,542 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 23:15:17,542 [INFO] Successfully crawled https://example.com/page4
2024-12-05 23:15:17,544 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:17,545 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:17,548 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:17,549 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:17,550 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x10963bb10>
2024-12-05 23:15:17,550 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1097e9f60>, 711339.881183416)])']
connector: <aiohttp.connector.TCPConnector object at 0x109639a50>
2024-12-05 23:15:17,550 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1098c2c50>
2024-12-05 23:15:17,550 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x109a55630>, 711342.0757415)])']
connector: <aiohttp.connector.TCPConnector object at 0x1098c21d0>
2024-12-05 23:15:17,553 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:17,554 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:17,556 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:17,557 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:17,559 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:17,560 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:17,563 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:17,563 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:17,566 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:17,566 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:17,569 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:17,570 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:17,572 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:17,572 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:17,575 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:17,586 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:17,589 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:17,591 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:17,594 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:17,596 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:17,599 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:17,617 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1096efb10>
2024-12-05 23:15:17,617 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10973e7b0>, 711327.544075166)])']
connector: <aiohttp.connector.TCPConnector object at 0x109785dd0>
2024-12-05 23:15:17,617 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x1098ad690>
2024-12-05 23:15:17,617 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x1097e9710>, 711333.064929708)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1097e9a90>, 711334.669226666)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1097e8130>, 711336.290063916)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x1097e9be0>, 711337.653472583)])']
connector: <aiohttp.connector.TCPConnector object at 0x1097de090>
2024-12-05 23:15:17,621 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:17,623 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:17,626 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:17,629 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:17,631 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:17,645 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:17,648 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:17,649 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:17,649 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:17,652 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:17,652 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:17,653 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:17,656 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:17,658 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:17,665 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:17,671 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:17,677 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:17,684 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:17,690 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:17,695 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:17,701 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:17,707 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:17,712 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:17,768 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:17,902 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:17,904 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 23:15:17,904 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-05 23:15:17,904 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-05 23:15:17,904 [INFO] Starting crawl of URL: https://example.com
2024-12-05 23:15:18,465 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-05 23:15:18,465 [DEBUG] Base URL: https://example.com
2024-12-05 23:15:18,465 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-05 23:15:18,465 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-05 23:15:18,465 [DEBUG] Initial domain set to: example.com
2024-12-05 23:15:18,465 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-05 23:15:18,465 [INFO] Successfully crawled https://example.com
2024-12-05 23:15:18,465 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x10cea5950>
2024-12-05 23:15:18,465 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10cd96430>, 711342.999090333)])']
connector: <aiohttp.connector.TCPConnector object at 0x10cea4950>
2024-12-05 23:15:18,466 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2024-12-05 23:15:18,626 [INFO] Operation took 0.11 seconds
2024-12-05 23:15:18,674 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 23:15:18,675 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:18,675 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:19,284 [ERROR] Could not find documentation URL for package: file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-170/test_full_site_crawl0/test_docs/index.html
2024-12-05 23:15:19,285 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2024-12-05 23:15:19,294 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:19,296 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 23:15:19,296 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:19,296 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:19,897 [ERROR] Could not find documentation URL for package: file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-170/test_content_processing_pipeli0/test_docs/guide.html
2024-12-05 23:15:19,897 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2024-12-05 23:15:19,905 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:19,907 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 23:15:19,907 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:19,908 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:20,529 [ERROR] Could not find documentation URL for package: file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-170/test_quality_checks0/test_docs/api.html
2024-12-05 23:15:20,529 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2024-12-05 23:15:20,537 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:20,539 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 23:15:20,539 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:20,539 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:21,162 [ERROR] Could not find documentation URL for package: file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-170/test_document_organization0/test_docs/index.html
2024-12-05 23:15:21,162 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2024-12-05 23:15:21,187 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:21,188 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 23:15:21,188 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:21,189 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:21,811 [ERROR] Could not find documentation URL for package: file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-170/test_search_functionality0/test_docs/index.html
2024-12-05 23:15:21,811 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2024-12-05 23:15:21,818 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:21,819 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-05 23:15:21,820 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:21,820 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:22,453 [ERROR] Could not find documentation URL for package: file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-170/test_error_handling_and_recove0/test_docs/index.html
2024-12-05 23:15:22,453 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2024-12-05 23:15:22,462 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:22,463 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:22,465 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:22,465 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:22,466 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:22,466 [ERROR] Error during crawl: 'str' object has no attribute 'url'
2024-12-05 23:15:22,467 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:22,468 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:22,469 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:22,470 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:22,470 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-05 23:15:22,472 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:22,799 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:22,802 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:22,802 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:22,805 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:22,805 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:22,808 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:22,808 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:22,811 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:22,812 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:22,814 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:22,814 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:22,817 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:22,817 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:15:22,820 [DEBUG] Using selector: KqueueSelector
2024-12-05 23:20:43,601 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-05 23:27:28,045 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-05 23:31:13,245 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-05 23:58:13,838 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-05 23:59:27,699 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-06 00:01:01,783 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-06 00:13:15,044 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-06 00:18:58,049 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-06 00:29:49,758 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-06 00:34:14,887 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-06 00:47:37,128 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-06 11:07:14,158 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-06 11:07:14,591 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:14,592 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:14,593 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:14,594 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:14,595 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:14,595 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:14,596 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:14,623 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:14,624 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:14,624 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:14,627 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:14,627 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:14,630 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:14,630 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:14,632 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:14,633 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:14,633 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:14,634 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:14,636 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:14,636 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:14,637 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:14,639 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:16,207 [INFO] Operation took 0.02 seconds
2024-12-06 11:07:16,411 [INFO] test_operation took 0.11 seconds
2024-12-06 11:07:16,617 [INFO] inner took 0.10 seconds
2024-12-06 11:07:16,617 [INFO] outer took 0.21 seconds
2024-12-06 11:07:16,617 [INFO] error_operation took 0.00 seconds
2024-12-06 11:07:16,634 [INFO] test_operation took 0.00 seconds
2024-12-06 11:07:16,657 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-06 11:07:16,657 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:16,658 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-06 11:07:16,658 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-06 11:07:16,658 [INFO] Starting crawl of URL: https://example.com
2024-12-06 11:07:17,152 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-06 11:07:17,152 [DEBUG] Base URL: https://example.com
2024-12-06 11:07:17,152 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-06 11:07:17,152 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-06 11:07:17,152 [DEBUG] Initial domain set to: example.com
2024-12-06 11:07:17,153 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-06 11:07:17,153 [INFO] Successfully crawled https://example.com
2024-12-06 11:07:17,153 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:17,153 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x10cd980d0>
2024-12-06 11:07:17,153 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10ccce120>, 754061.640852916)])']
connector: <aiohttp.connector.TCPConnector object at 0x10cd9a510>
2024-12-06 11:07:17,153 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-06 11:07:17,154 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:17,154 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-06 11:07:17,154 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-06 11:07:17,154 [INFO] Starting crawl of URL: https://example.com
2024-12-06 11:07:17,168 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-12-06 11:07:17,168 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-12-06 11:07:17,168 [INFO] Starting crawl of URL: https://example.org
2024-12-06 11:07:17,638 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-06 11:07:17,638 [DEBUG] Base URL: https://example.com
2024-12-06 11:07:17,638 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-06 11:07:17,638 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-06 11:07:17,638 [DEBUG] Initial domain set to: example.com
2024-12-06 11:07:17,638 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-06 11:07:17,638 [INFO] Successfully crawled https://example.com
2024-12-06 11:07:18,139 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-06 11:07:18,139 [DEBUG] Base URL: https://example.org
2024-12-06 11:07:18,139 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-06 11:07:18,139 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-06 11:07:18,139 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-06 11:07:18,139 [INFO] Successfully crawled https://example.org
2024-12-06 11:07:18,140 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:18,140 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x10cf61990>
2024-12-06 11:07:18,140 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10cccdda0>, 754062.126470875)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x10cccdfd0>, 754062.627644666)])']
connector: <aiohttp.connector.TCPConnector object at 0x10cd1f390>
2024-12-06 11:07:18,140 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-06 11:07:18,140 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:18,141 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-06 11:07:18,141 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-06 11:07:18,141 [INFO] Starting crawl of URL: https://example.com
2024-12-06 11:07:18,634 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-06 11:07:18,634 [DEBUG] Base URL: https://example.com
2024-12-06 11:07:18,634 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-06 11:07:18,634 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-06 11:07:18,634 [DEBUG] Initial domain set to: example.com
2024-12-06 11:07:18,634 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-06 11:07:18,634 [INFO] Successfully crawled https://example.com
2024-12-06 11:07:18,634 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:18,634 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x10cd88c90>
2024-12-06 11:07:18,635 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10cccdd30>, 754063.12228975)])']
connector: <aiohttp.connector.TCPConnector object at 0x10cdec650>
2024-12-06 11:07:18,635 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-06 11:07:18,635 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:18,635 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-06 11:07:18,635 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-06 11:07:18,635 [INFO] Starting crawl of URL: https://example.com
2024-12-06 11:07:19,135 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-06 11:07:19,135 [DEBUG] Base URL: https://example.com
2024-12-06 11:07:19,135 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-06 11:07:19,135 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-06 11:07:19,135 [DEBUG] Initial domain set to: example.com
2024-12-06 11:07:19,135 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-06 11:07:19,135 [INFO] Successfully crawled https://example.com
2024-12-06 11:07:19,136 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-06 11:07:19,136 [DEBUG] Base URL: https://example.com
2024-12-06 11:07:19,136 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-06 11:07:19,136 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-06 11:07:19,136 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-06 11:07:19,140 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:19,140 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:19,140 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:19,141 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:19,141 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-06 11:07:19,141 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:19,142 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-06 11:07:19,142 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-06 11:07:19,142 [INFO] Starting crawl of URL: https://example.com
2024-12-06 11:07:19,635 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-06 11:07:19,635 [DEBUG] Base URL: https://example.com
2024-12-06 11:07:19,635 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-06 11:07:19,635 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-06 11:07:19,635 [DEBUG] Initial domain set to: example.com
2024-12-06 11:07:19,635 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-06 11:07:19,635 [INFO] Successfully crawled https://example.com
2024-12-06 11:07:19,635 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:19,636 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x10b82b690>
2024-12-06 11:07:19,636 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10cccedd0>, 754064.123561041)])']
connector: <aiohttp.connector.TCPConnector object at 0x10cf73bd0>
2024-12-06 11:07:19,636 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-06 11:07:19,636 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:19,636 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2024-12-06 11:07:19,636 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2024-12-06 11:07:19,637 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2024-12-06 11:07:21,697 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-06 11:07:21,698 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-06 11:07:21,699 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [nodename nor servname provided, or not known]
2024-12-06 11:07:21,702 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:21,702 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-06 11:07:21,702 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:21,703 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-06 11:07:21,703 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-06 11:07:21,703 [INFO] Starting crawl of URL: https://example.com
2024-12-06 11:07:21,718 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2024-12-06 11:07:21,718 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2024-12-06 11:07:21,718 [INFO] Starting crawl of URL: https://example.org
2024-12-06 11:07:21,718 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2024-12-06 11:07:21,718 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2024-12-06 11:07:21,718 [INFO] Starting crawl of URL: https://example.net
2024-12-06 11:07:22,194 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-06 11:07:22,194 [DEBUG] Base URL: https://example.com
2024-12-06 11:07:22,194 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-06 11:07:22,194 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-06 11:07:22,194 [DEBUG] Initial domain set to: example.com
2024-12-06 11:07:22,194 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-06 11:07:22,194 [INFO] Successfully crawled https://example.com
2024-12-06 11:07:22,695 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-06 11:07:22,695 [DEBUG] Base URL: https://example.org
2024-12-06 11:07:22,695 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-06 11:07:22,695 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-06 11:07:22,695 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-06 11:07:22,695 [INFO] Successfully crawled https://example.org
2024-12-06 11:07:23,316 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-06 11:07:23,316 [DEBUG] Base URL: https://example.net
2024-12-06 11:07:23,316 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-06 11:07:23,316 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-06 11:07:23,316 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-06 11:07:23,316 [INFO] Successfully crawled https://example.net
2024-12-06 11:07:23,316 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:23,317 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x10cd49410>
2024-12-06 11:07:23,317 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10cccf690>, 754066.682656)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x10cccfaf0>, 754067.183769625)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x10cccdd30>, 754067.804290541)])']
connector: <aiohttp.connector.TCPConnector object at 0x10ce94710>
2024-12-06 11:07:23,317 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-06 11:07:23,317 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:23,317 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-06 11:07:23,317 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-06 11:07:23,317 [INFO] Starting crawl of URL: https://example.com
2024-12-06 11:07:23,334 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x10cdf5f50>
2024-12-06 11:07:23,334 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10cccdf60>, 754063.623902666)])']
connector: <aiohttp.connector.TCPConnector object at 0x10cedb210>
2024-12-06 11:07:23,334 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x10cded110>
2024-12-06 11:07:23,818 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-06 11:07:23,818 [DEBUG] Base URL: https://example.com
2024-12-06 11:07:23,818 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-06 11:07:23,818 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-06 11:07:23,818 [DEBUG] Initial domain set to: example.com
2024-12-06 11:07:23,818 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-06 11:07:23,818 [INFO] Successfully crawled https://example.com
2024-12-06 11:07:23,819 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:23,822 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-06 11:07:23,822 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:23,823 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:23,839 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-06 11:07:23,853 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:23,854 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-06 11:07:23,854 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:23,854 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-06 11:07:23,869 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:23,870 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-06 11:07:23,870 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:23,870 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-06 11:07:23,870 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2024-12-06 11:07:23,870 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2024-12-06 11:07:23,870 [INFO] Starting crawl of URL: https://example.com/page
2024-12-06 11:07:24,362 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-06 11:07:24,362 [DEBUG] Base URL: https://example.com/page
2024-12-06 11:07:24,362 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-06 11:07:24,362 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-06 11:07:24,362 [DEBUG] Initial domain set to: example.com
2024-12-06 11:07:24,362 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-06 11:07:24,362 [INFO] Successfully crawled https://example.com/page
2024-12-06 11:07:24,362 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2024-12-06 11:07:24,362 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2024-12-06 11:07:24,362 [INFO] Starting crawl of URL: https://other-domain.com/page
2024-12-06 11:07:28,897 [DEBUG] 
Evaluating link: ./
2024-12-06 11:07:28,898 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,898 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-06 11:07:28,898 [DEBUG] Absolute link: https://other-domain.com/
2024-12-06 11:07:28,898 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,898 [DEBUG] 
Evaluating link: ./
2024-12-06 11:07:28,898 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,898 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-06 11:07:28,898 [DEBUG] Absolute link: https://other-domain.com/
2024-12-06 11:07:28,898 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,898 [DEBUG] 
Evaluating link: ./hiyou.php
2024-12-06 11:07:28,898 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,898 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-12-06 11:07:28,898 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-12-06 11:07:28,898 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,898 [DEBUG] 
Evaluating link: ./date.php
2024-12-06 11:07:28,898 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,898 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-12-06 11:07:28,898 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-12-06 11:07:28,898 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,898 [DEBUG] 
Evaluating link: ./contact.php
2024-12-06 11:07:28,898 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,898 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2024-12-06 11:07:28,898 [DEBUG] Absolute link: https://other-domain.com/contact.php
2024-12-06 11:07:28,898 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,899 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-06 11:07:28,899 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,899 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-06 11:07:28,899 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-06 11:07:28,899 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-06 11:07:28,899 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-06 11:07:28,899 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,899 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-06 11:07:28,899 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-06 11:07:28,899 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-06 11:07:28,899 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-06 11:07:28,899 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,899 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-06 11:07:28,899 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-06 11:07:28,899 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-06 11:07:28,899 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-06 11:07:28,899 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,899 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-06 11:07:28,899 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-06 11:07:28,899 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-06 11:07:28,899 [DEBUG] 
Evaluating link: ./contact
2024-12-06 11:07:28,899 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,899 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-12-06 11:07:28,899 [DEBUG] Absolute link: https://other-domain.com/contact
2024-12-06 11:07:28,899 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,899 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-06 11:07:28,899 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,899 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-06 11:07:28,899 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-06 11:07:28,899 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-06 11:07:28,899 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-06 11:07:28,899 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,899 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-06 11:07:28,899 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-06 11:07:28,899 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-06 11:07:28,899 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-06 11:07:28,899 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,899 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-06 11:07:28,899 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-06 11:07:28,900 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-06 11:07:28,900 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-12-06 11:07:28,900 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,900 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-06 11:07:28,900 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-06 11:07:28,900 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-12-06 11:07:28,900 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-06 11:07:28,900 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,900 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-06 11:07:28,900 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-06 11:07:28,900 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-06 11:07:28,900 [DEBUG] 
Evaluating link: #indexes
2024-12-06 11:07:28,900 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,900 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2024-12-06 11:07:28,900 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,900 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,900 [DEBUG] 
Evaluating link: #a
2024-12-06 11:07:28,900 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,900 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2024-12-06 11:07:28,900 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,900 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,900 [DEBUG] 
Evaluating link: #b
2024-12-06 11:07:28,900 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,900 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-12-06 11:07:28,900 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,900 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,900 [DEBUG] 
Evaluating link: #c
2024-12-06 11:07:28,900 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,900 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-12-06 11:07:28,900 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,900 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,900 [DEBUG] 
Evaluating link: #d
2024-12-06 11:07:28,900 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,900 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-12-06 11:07:28,900 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,900 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,900 [DEBUG] 
Evaluating link: #e
2024-12-06 11:07:28,900 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,900 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-12-06 11:07:28,900 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,900 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,900 [DEBUG] 
Evaluating link: #f
2024-12-06 11:07:28,901 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,901 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-12-06 11:07:28,901 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,901 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,901 [DEBUG] 
Evaluating link: #g
2024-12-06 11:07:28,901 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,901 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-12-06 11:07:28,901 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,901 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,901 [DEBUG] 
Evaluating link: #h
2024-12-06 11:07:28,901 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,901 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-12-06 11:07:28,901 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,901 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,901 [DEBUG] 
Evaluating link: #i
2024-12-06 11:07:28,901 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,901 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-12-06 11:07:28,901 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,901 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,901 [DEBUG] 
Evaluating link: #j
2024-12-06 11:07:28,901 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,901 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-12-06 11:07:28,901 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,901 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,901 [DEBUG] 
Evaluating link: #k
2024-12-06 11:07:28,901 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,901 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-12-06 11:07:28,901 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,901 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,901 [DEBUG] 
Evaluating link: #l
2024-12-06 11:07:28,901 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,901 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-12-06 11:07:28,901 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,901 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,901 [DEBUG] 
Evaluating link: #m
2024-12-06 11:07:28,901 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,901 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-12-06 11:07:28,901 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,901 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,901 [DEBUG] 
Evaluating link: #n
2024-12-06 11:07:28,901 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,901 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-12-06 11:07:28,902 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,902 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,902 [DEBUG] 
Evaluating link: #o
2024-12-06 11:07:28,902 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,902 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-12-06 11:07:28,902 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,902 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,902 [DEBUG] 
Evaluating link: #p
2024-12-06 11:07:28,902 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,902 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-12-06 11:07:28,902 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,902 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,902 [DEBUG] 
Evaluating link: #q
2024-12-06 11:07:28,902 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,902 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-12-06 11:07:28,902 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,902 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,902 [DEBUG] 
Evaluating link: #r
2024-12-06 11:07:28,902 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,902 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-12-06 11:07:28,902 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,902 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,902 [DEBUG] 
Evaluating link: #s
2024-12-06 11:07:28,902 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,902 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-12-06 11:07:28,902 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,902 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,902 [DEBUG] 
Evaluating link: #t
2024-12-06 11:07:28,902 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,902 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-12-06 11:07:28,902 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,902 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,902 [DEBUG] 
Evaluating link: #u
2024-12-06 11:07:28,902 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,902 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-12-06 11:07:28,902 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,902 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,902 [DEBUG] 
Evaluating link: #v
2024-12-06 11:07:28,902 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,902 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-12-06 11:07:28,902 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,902 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,902 [DEBUG] 
Evaluating link: #w
2024-12-06 11:07:28,902 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,903 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-12-06 11:07:28,903 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,903 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,903 [DEBUG] 
Evaluating link: #x
2024-12-06 11:07:28,903 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,903 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-12-06 11:07:28,903 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,903 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,903 [DEBUG] 
Evaluating link: #y
2024-12-06 11:07:28,903 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,903 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-12-06 11:07:28,903 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,903 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,903 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-06 11:07:28,903 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,903 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-06 11:07:28,903 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-06 11:07:28,903 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-06 11:07:28,903 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-06 11:07:28,903 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,903 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-06 11:07:28,903 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-06 11:07:28,903 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-06 11:07:28,903 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-06 11:07:28,903 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,903 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-06 11:07:28,903 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-06 11:07:28,903 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-06 11:07:28,903 [DEBUG] 
Evaluating link: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問
2024-12-06 11:07:28,903 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,903 [DEBUG] Normalized URL: https://getpocket.com/edit?url=https://www.other-domain.com/&title=パッケージ印刷に関するよくある疑問 -> https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-06 11:07:28,903 [DEBUG] Absolute link: https://getpocket.com/edit?title=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-06 11:07:28,903 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2024-12-06 11:07:28,903 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-06 11:07:28,903 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,903 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-06 11:07:28,903 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2024-12-06 11:07:28,903 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2024-12-06 11:07:28,903 [DEBUG] 
Evaluating link: https://spriq.jp/
2024-12-06 11:07:28,904 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,904 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2024-12-06 11:07:28,904 [DEBUG] Absolute link: https://spriq.jp/
2024-12-06 11:07:28,904 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2024-12-06 11:07:28,904 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-06 11:07:28,904 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,904 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-06 11:07:28,904 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-06 11:07:28,904 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-06 11:07:28,904 [DEBUG] 
Evaluating link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-06 11:07:28,904 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,904 [DEBUG] Normalized URL: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/ -> https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-06 11:07:28,904 [DEBUG] Absolute link: https://twitter.com/intent/tweet?text=パッケージ印刷に関するよくある疑問&url=https://www.other-domain.com/
2024-12-06 11:07:28,904 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2024-12-06 11:07:28,904 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-06 11:07:28,904 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,904 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-06 11:07:28,904 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2024-12-06 11:07:28,904 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2024-12-06 11:07:28,904 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2024-12-06 11:07:28,904 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,904 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-06 11:07:28,904 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2024-12-06 11:07:28,904 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2024-12-06 11:07:28,904 [DEBUG] 
Evaluating link: ./contact
2024-12-06 11:07:28,904 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,904 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2024-12-06 11:07:28,904 [DEBUG] Absolute link: https://other-domain.com/contact
2024-12-06 11:07:28,904 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,904 [DEBUG] 
Evaluating link: ./
2024-12-06 11:07:28,904 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,904 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2024-12-06 11:07:28,904 [DEBUG] Absolute link: https://other-domain.com/
2024-12-06 11:07:28,904 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,904 [DEBUG] 
Evaluating link: ./hiyou.php
2024-12-06 11:07:28,904 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,904 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2024-12-06 11:07:28,904 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2024-12-06 11:07:28,904 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,904 [DEBUG] 
Evaluating link: ./date.php
2024-12-06 11:07:28,904 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,904 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2024-12-06 11:07:28,904 [DEBUG] Absolute link: https://other-domain.com/date.php
2024-12-06 11:07:28,905 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,905 [DEBUG] 
Evaluating link: #u
2024-12-06 11:07:28,905 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,905 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2024-12-06 11:07:28,905 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,905 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,905 [DEBUG] 
Evaluating link: #v
2024-12-06 11:07:28,905 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,905 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2024-12-06 11:07:28,905 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,905 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,905 [DEBUG] 
Evaluating link: #w
2024-12-06 11:07:28,905 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,905 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2024-12-06 11:07:28,905 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,905 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,905 [DEBUG] 
Evaluating link: #x
2024-12-06 11:07:28,905 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,905 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2024-12-06 11:07:28,905 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,905 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,905 [DEBUG] 
Evaluating link: #y
2024-12-06 11:07:28,905 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,905 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2024-12-06 11:07:28,905 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,905 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,905 [DEBUG] 
Evaluating link: #p
2024-12-06 11:07:28,905 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,905 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2024-12-06 11:07:28,905 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,905 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,905 [DEBUG] 
Evaluating link: #q
2024-12-06 11:07:28,905 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,905 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2024-12-06 11:07:28,905 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,905 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,905 [DEBUG] 
Evaluating link: #r
2024-12-06 11:07:28,905 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,905 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2024-12-06 11:07:28,905 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,905 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,905 [DEBUG] 
Evaluating link: #s
2024-12-06 11:07:28,905 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,906 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2024-12-06 11:07:28,906 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,906 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,906 [DEBUG] 
Evaluating link: #t
2024-12-06 11:07:28,906 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,906 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2024-12-06 11:07:28,906 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,906 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,906 [DEBUG] 
Evaluating link: #k
2024-12-06 11:07:28,906 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,906 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2024-12-06 11:07:28,906 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,906 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,906 [DEBUG] 
Evaluating link: #l
2024-12-06 11:07:28,906 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,906 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2024-12-06 11:07:28,906 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,906 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,906 [DEBUG] 
Evaluating link: #m
2024-12-06 11:07:28,906 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,906 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2024-12-06 11:07:28,906 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,906 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,906 [DEBUG] 
Evaluating link: #n
2024-12-06 11:07:28,906 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,906 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2024-12-06 11:07:28,906 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,906 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,906 [DEBUG] 
Evaluating link: #o
2024-12-06 11:07:28,906 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,906 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2024-12-06 11:07:28,906 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,906 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,906 [DEBUG] 
Evaluating link: #f
2024-12-06 11:07:28,906 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,906 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2024-12-06 11:07:28,906 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,906 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,907 [DEBUG] 
Evaluating link: #g
2024-12-06 11:07:28,907 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,907 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2024-12-06 11:07:28,907 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,907 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,907 [DEBUG] 
Evaluating link: #h
2024-12-06 11:07:28,907 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,907 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2024-12-06 11:07:28,907 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,907 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,907 [DEBUG] 
Evaluating link: #i
2024-12-06 11:07:28,907 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,907 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2024-12-06 11:07:28,907 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,907 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,907 [DEBUG] 
Evaluating link: #j
2024-12-06 11:07:28,907 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,907 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2024-12-06 11:07:28,907 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,907 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,907 [DEBUG] 
Evaluating link: #b
2024-12-06 11:07:28,907 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,907 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2024-12-06 11:07:28,907 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,907 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,907 [DEBUG] 
Evaluating link: #c
2024-12-06 11:07:28,907 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,907 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2024-12-06 11:07:28,907 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,907 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,907 [DEBUG] 
Evaluating link: #d
2024-12-06 11:07:28,907 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,907 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2024-12-06 11:07:28,907 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,907 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,907 [DEBUG] 
Evaluating link: #e
2024-12-06 11:07:28,907 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,907 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2024-12-06 11:07:28,907 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,907 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,907 [DEBUG] 
Evaluating link: #site
2024-12-06 11:07:28,907 [DEBUG] Base URL: https://other-domain.com/page
2024-12-06 11:07:28,907 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2024-12-06 11:07:28,907 [DEBUG] Absolute link: https://other-domain.com/page
2024-12-06 11:07:28,908 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2024-12-06 11:07:28,908 [INFO] Successfully crawled https://other-domain.com/page
2024-12-06 11:07:28,910 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:28,911 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-06 11:07:28,911 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:28,911 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-12-06 11:07:28,911 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-12-06 11:07:28,911 [INFO] Starting crawl of URL: https://example.com/page0
2024-12-06 11:07:29,416 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-06 11:07:29,416 [DEBUG] Base URL: https://example.com/page0
2024-12-06 11:07:29,416 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-06 11:07:29,416 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-06 11:07:29,416 [DEBUG] Initial domain set to: example.com
2024-12-06 11:07:29,416 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-06 11:07:29,416 [INFO] Successfully crawled https://example.com/page0
2024-12-06 11:07:29,416 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-12-06 11:07:29,416 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-12-06 11:07:29,416 [INFO] Starting crawl of URL: https://example.com/page1
2024-12-06 11:07:29,553 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-06 11:07:29,553 [DEBUG] Base URL: https://example.com/page1
2024-12-06 11:07:29,553 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-06 11:07:29,553 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-06 11:07:29,553 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-06 11:07:29,553 [INFO] Successfully crawled https://example.com/page1
2024-12-06 11:07:29,553 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-12-06 11:07:29,553 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-12-06 11:07:29,553 [INFO] Starting crawl of URL: https://example.com/page2
2024-12-06 11:07:30,050 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-06 11:07:30,050 [DEBUG] Base URL: https://example.com/page2
2024-12-06 11:07:30,050 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-06 11:07:30,050 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-06 11:07:30,050 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-06 11:07:30,050 [INFO] Successfully crawled https://example.com/page2
2024-12-06 11:07:30,050 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-12-06 11:07:30,050 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-12-06 11:07:30,050 [INFO] Starting crawl of URL: https://example.com/page3
2024-12-06 11:07:30,553 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-06 11:07:30,553 [DEBUG] Base URL: https://example.com/page3
2024-12-06 11:07:30,553 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-06 11:07:30,553 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-06 11:07:30,553 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-06 11:07:30,553 [INFO] Successfully crawled https://example.com/page3
2024-12-06 11:07:30,553 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-12-06 11:07:30,553 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-12-06 11:07:30,553 [INFO] Starting crawl of URL: https://example.com/page4
2024-12-06 11:07:31,054 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x10cf71e50>
2024-12-06 11:07:31,054 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10cccdef0>, 754068.850457333)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x10cccfc40>, 754070.457291875)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x10cccf9a0>, 754072.060812625)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x10cccedd0>, 754073.379070291)])']
connector: <aiohttp.connector.TCPConnector object at 0x10cc280d0>
2024-12-06 11:07:31,056 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-06 11:07:31,056 [DEBUG] Base URL: https://example.com/page4
2024-12-06 11:07:31,056 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-06 11:07:31,056 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-06 11:07:31,056 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-06 11:07:31,056 [INFO] Successfully crawled https://example.com/page4
2024-12-06 11:07:31,056 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-06 11:07:31,056 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2024-12-06 11:07:31,056 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2024-12-06 11:07:31,056 [INFO] Starting crawl of URL: https://example.com/page0
2024-12-06 11:07:31,564 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-06 11:07:31,564 [DEBUG] Base URL: https://example.com/page0
2024-12-06 11:07:31,564 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-06 11:07:31,564 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-06 11:07:31,564 [DEBUG] Initial domain set to: example.com
2024-12-06 11:07:31,564 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-06 11:07:31,564 [INFO] Successfully crawled https://example.com/page0
2024-12-06 11:07:31,564 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2024-12-06 11:07:31,564 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2024-12-06 11:07:31,564 [INFO] Starting crawl of URL: https://example.com/page1
2024-12-06 11:07:31,705 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-06 11:07:31,705 [DEBUG] Base URL: https://example.com/page1
2024-12-06 11:07:31,705 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-06 11:07:31,705 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-06 11:07:31,705 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-06 11:07:31,705 [INFO] Successfully crawled https://example.com/page1
2024-12-06 11:07:31,705 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2024-12-06 11:07:31,705 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2024-12-06 11:07:31,705 [INFO] Starting crawl of URL: https://example.com/page2
2024-12-06 11:07:32,204 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-06 11:07:32,204 [DEBUG] Base URL: https://example.com/page2
2024-12-06 11:07:32,204 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-06 11:07:32,204 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-06 11:07:32,204 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-06 11:07:32,204 [INFO] Successfully crawled https://example.com/page2
2024-12-06 11:07:32,204 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2024-12-06 11:07:32,204 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2024-12-06 11:07:32,204 [INFO] Starting crawl of URL: https://example.com/page3
2024-12-06 11:07:32,706 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-06 11:07:32,706 [DEBUG] Base URL: https://example.com/page3
2024-12-06 11:07:32,706 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-06 11:07:32,706 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-06 11:07:32,706 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-06 11:07:32,706 [INFO] Successfully crawled https://example.com/page3
2024-12-06 11:07:32,706 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2024-12-06 11:07:32,706 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2024-12-06 11:07:32,706 [INFO] Starting crawl of URL: https://example.com/page4
2024-12-06 11:07:33,211 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-06 11:07:33,211 [DEBUG] Base URL: https://example.com/page4
2024-12-06 11:07:33,211 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-06 11:07:33,211 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-06 11:07:33,211 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-06 11:07:33,211 [INFO] Successfully crawled https://example.com/page4
2024-12-06 11:07:33,214 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:33,215 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:33,219 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:33,219 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:33,222 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:33,222 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:33,226 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:33,227 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:33,229 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:33,229 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:33,232 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:33,233 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:33,235 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:33,236 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:33,239 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:33,239 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:33,242 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:33,242 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:33,245 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:33,256 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:33,259 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:33,262 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:33,264 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:33,267 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:33,269 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:33,272 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:33,274 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:33,277 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:33,279 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:33,281 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:33,306 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x10cf04f50>
2024-12-06 11:07:33,306 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10cccfd90>, 754075.542391791)])']
connector: <aiohttp.connector.TCPConnector object at 0x10cf059d0>
2024-12-06 11:07:33,306 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x10cdba650>
2024-12-06 11:07:33,306 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10cccdfd0>, 754077.699335583)])']
connector: <aiohttp.connector.TCPConnector object at 0x10cf06c50>
2024-12-06 11:07:33,314 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:33,317 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:33,318 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:33,318 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:33,321 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:33,322 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:33,322 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:33,325 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:33,328 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:33,336 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:33,342 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:33,349 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:33,355 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:33,361 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:33,367 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:33,373 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:33,378 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:33,384 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:33,443 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:33,608 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:33,610 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-06 11:07:33,610 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2024-12-06 11:07:33,610 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2024-12-06 11:07:33,610 [INFO] Starting crawl of URL: https://example.com
2024-12-06 11:07:34,101 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2024-12-06 11:07:34,101 [DEBUG] Base URL: https://example.com
2024-12-06 11:07:34,101 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2024-12-06 11:07:34,101 [DEBUG] Absolute link: https://iana.org/domains/example
2024-12-06 11:07:34,101 [DEBUG] Initial domain set to: example.com
2024-12-06 11:07:34,101 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2024-12-06 11:07:34,101 [INFO] Successfully crawled https://example.com
2024-12-06 11:07:34,101 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x10d92ed90>
2024-12-06 11:07:34,101 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x10d9220b0>, 754078.589241833)])']
connector: <aiohttp.connector.TCPConnector object at 0x10d92c310>
2024-12-06 11:07:34,102 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2024-12-06 11:07:34,263 [INFO] Operation took 0.10 seconds
2024-12-06 11:07:34,314 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-06 11:07:34,314 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:34,314 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:34,816 [ERROR] Could not find documentation URL for package: file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-171/test_full_site_crawl0/test_docs/index.html
2024-12-06 11:07:34,816 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2024-12-06 11:07:34,825 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:34,827 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-06 11:07:34,827 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:34,828 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:35,298 [ERROR] Could not find documentation URL for package: file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-171/test_content_processing_pipeli0/test_docs/guide.html
2024-12-06 11:07:35,298 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2024-12-06 11:07:35,311 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:35,312 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-06 11:07:35,312 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:35,313 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:35,784 [ERROR] Could not find documentation URL for package: file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-171/test_quality_checks0/test_docs/api.html
2024-12-06 11:07:35,784 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2024-12-06 11:07:35,794 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:35,796 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-06 11:07:35,797 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:35,797 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:36,273 [ERROR] Could not find documentation URL for package: file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-171/test_document_organization0/test_docs/index.html
2024-12-06 11:07:36,273 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2024-12-06 11:07:36,281 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:36,283 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-06 11:07:36,283 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:36,283 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:36,882 [ERROR] Could not find documentation URL for package: file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-171/test_search_functionality0/test_docs/index.html
2024-12-06 11:07:36,883 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2024-12-06 11:07:36,916 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:36,918 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2024-12-06 11:07:36,918 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:36,918 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:37,485 [ERROR] Could not find documentation URL for package: file:///private/var/folders/c6/0glsj2qj54s2pr7y8ncq92l40000gn/T/pytest-of-alex/pytest-171/test_error_handling_and_recove0/test_docs/index.html
2024-12-06 11:07:37,485 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2024-12-06 11:07:37,493 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:37,494 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:37,496 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:37,497 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:37,497 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:37,497 [ERROR] Error during crawl: 'str' object has no attribute 'url'
2024-12-06 11:07:37,499 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:37,499 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:37,501 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:37,501 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:37,502 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2024-12-06 11:07:37,503 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:37,839 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:37,842 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:37,843 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:37,845 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:37,846 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:37,848 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:37,849 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:37,851 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:37,851 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:37,854 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:37,855 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:37,857 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:37,857 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:07:37,860 [DEBUG] Using selector: KqueueSelector
2024-12-06 11:08:42,558 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-06 11:11:02,615 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-06 11:12:00,246 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2024-12-06 11:13:15,782 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-05 13:22:11,112 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-05 13:31:29,273 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-05 13:32:21,582 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-05 13:32:23,870 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:23,874 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:23,879 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:23,884 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:23,890 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:23,893 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:23,895 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:32:23,913 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:23,915 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:23,921 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:32:23,942 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:23,945 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:23,965 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:23,968 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:23,986 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:23,990 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:23,994 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:23,996 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:24,024 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:24,027 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:24,029 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:32:24,049 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:28,858 [INFO] Operation took 0.20 seconds
2025-02-05 13:32:29,466 [INFO] test_operation took 0.10 seconds
2025-02-05 13:32:29,667 [INFO] inner took 0.10 seconds
2025-02-05 13:32:29,667 [INFO] outer took 0.20 seconds
2025-02-05 13:32:29,667 [INFO] error_operation took 0.00 seconds
2025-02-05 13:32:29,809 [INFO] test_operation took 0.00 seconds
2025-02-05 13:32:30,011 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-05 13:32:30,012 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:30,017 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-05 13:32:30,017 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-05 13:32:30,018 [INFO] Starting crawl of URL: https://example.com
2025-02-05 13:32:30,285 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-05 13:32:30,285 [DEBUG] Base URL: https://example.com
2025-02-05 13:32:30,285 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-05 13:32:30,285 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-05 13:32:30,286 [DEBUG] Initial domain set to: example.com
2025-02-05 13:32:30,286 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-05 13:32:30,286 [INFO] Successfully crawled https://example.com
2025-02-05 13:32:30,287 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:30,289 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000002DA8EE75FD0>
2025-02-05 13:32:30,289 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000002DA8F0F5190>, 87786.2136339)])']
connector: <aiohttp.connector.TCPConnector object at 0x000002DA8EE75010>
2025-02-05 13:32:30,290 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-05 13:32:30,290 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:30,292 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-05 13:32:30,292 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-05 13:32:30,292 [INFO] Starting crawl of URL: https://example.com
2025-02-05 13:32:30,340 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2025-02-05 13:32:30,341 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2025-02-05 13:32:30,341 [INFO] Starting crawl of URL: https://example.org
2025-02-05 13:32:30,476 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-05 13:32:30,476 [DEBUG] Base URL: https://example.com
2025-02-05 13:32:30,476 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-05 13:32:30,476 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-05 13:32:30,477 [DEBUG] Initial domain set to: example.com
2025-02-05 13:32:30,477 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-05 13:32:30,477 [INFO] Successfully crawled https://example.com
2025-02-05 13:32:31,223 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-05 13:32:31,224 [DEBUG] Base URL: https://example.org
2025-02-05 13:32:31,224 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-05 13:32:31,225 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-05 13:32:31,225 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-05 13:32:31,226 [INFO] Successfully crawled https://example.org
2025-02-05 13:32:31,234 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:31,240 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000002DA8EED96D0>
2025-02-05 13:32:31,242 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000002DA8F0F49B0>, 87786.4049954)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x000002DA8EFBBD10>, 87787.1473734)])']
connector: <aiohttp.connector.TCPConnector object at 0x000002DA8EEDA490>
2025-02-05 13:32:31,246 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-05 13:32:31,248 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:31,260 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-05 13:32:31,261 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-05 13:32:31,263 [INFO] Starting crawl of URL: https://example.com
2025-02-05 13:32:31,505 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-05 13:32:31,506 [DEBUG] Base URL: https://example.com
2025-02-05 13:32:31,506 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-05 13:32:31,506 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-05 13:32:31,507 [DEBUG] Initial domain set to: example.com
2025-02-05 13:32:31,507 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-05 13:32:31,507 [INFO] Successfully crawled https://example.com
2025-02-05 13:32:31,510 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:31,516 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000002DA8EEDA210>
2025-02-05 13:32:31,517 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000002DA8F0F7050>, 87787.4328537)])']
connector: <aiohttp.connector.TCPConnector object at 0x000002DA8EEDB9D0>
2025-02-05 13:32:31,519 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-05 13:32:31,521 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:31,525 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-05 13:32:31,526 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-05 13:32:31,526 [INFO] Starting crawl of URL: https://example.com
2025-02-05 13:32:31,774 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-05 13:32:31,775 [DEBUG] Base URL: https://example.com
2025-02-05 13:32:31,775 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-05 13:32:31,775 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-05 13:32:31,776 [DEBUG] Initial domain set to: example.com
2025-02-05 13:32:31,776 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-05 13:32:31,777 [INFO] Successfully crawled https://example.com
2025-02-05 13:32:31,783 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-05 13:32:31,784 [DEBUG] Base URL: https://example.com
2025-02-05 13:32:31,784 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-05 13:32:31,785 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-05 13:32:31,785 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-05 13:32:31,851 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:31,856 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:32:31,857 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:31,870 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:31,880 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-05 13:32:31,881 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:31,885 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-05 13:32:31,885 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-05 13:32:31,885 [INFO] Starting crawl of URL: https://example.com
2025-02-05 13:32:32,089 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-05 13:32:32,089 [DEBUG] Base URL: https://example.com
2025-02-05 13:32:32,090 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-05 13:32:32,090 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-05 13:32:32,090 [DEBUG] Initial domain set to: example.com
2025-02-05 13:32:32,090 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-05 13:32:32,091 [INFO] Successfully crawled https://example.com
2025-02-05 13:32:32,093 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:32,096 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000002DA8ED73250>
2025-02-05 13:32:32,096 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000002DA8F0F7C50>, 87788.0170538)])']
connector: <aiohttp.connector.TCPConnector object at 0x000002DA8EE63CE0>
2025-02-05 13:32:32,097 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-05 13:32:32,098 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:32,101 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2025-02-05 13:32:32,101 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2025-02-05 13:32:32,102 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2025-02-05 13:32:34,472 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [getaddrinfo failed]
2025-02-05 13:32:34,474 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [getaddrinfo failed]
2025-02-05 13:32:34,475 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [getaddrinfo failed]
2025-02-05 13:32:34,505 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:34,507 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-05 13:32:34,509 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:34,513 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-05 13:32:34,514 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-05 13:32:34,514 [INFO] Starting crawl of URL: https://example.com
2025-02-05 13:32:34,584 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2025-02-05 13:32:34,584 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2025-02-05 13:32:34,585 [INFO] Starting crawl of URL: https://example.org
2025-02-05 13:32:34,585 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2025-02-05 13:32:34,585 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2025-02-05 13:32:34,586 [INFO] Starting crawl of URL: https://example.net
2025-02-05 13:32:34,722 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-05 13:32:34,722 [DEBUG] Base URL: https://example.com
2025-02-05 13:32:34,723 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-05 13:32:34,723 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-05 13:32:34,723 [DEBUG] Initial domain set to: example.com
2025-02-05 13:32:34,724 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-05 13:32:34,724 [INFO] Successfully crawled https://example.com
2025-02-05 13:32:35,398 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-05 13:32:35,399 [DEBUG] Base URL: https://example.org
2025-02-05 13:32:35,400 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-05 13:32:35,400 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-05 13:32:35,401 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-05 13:32:35,402 [INFO] Successfully crawled https://example.org
2025-02-05 13:32:36,038 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-05 13:32:36,038 [DEBUG] Base URL: https://example.net
2025-02-05 13:32:36,039 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-05 13:32:36,039 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-05 13:32:36,040 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-05 13:32:36,042 [INFO] Successfully crawled https://example.net
2025-02-05 13:32:36,048 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:36,053 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000002DA8EEDAD50>
2025-02-05 13:32:36,054 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000002DA8F0F79B0>, 87790.6506852)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x000002DA8F0F7E90>, 87791.3225046)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x000002DA8F0F73B0>, 87791.9648288)])']
connector: <aiohttp.connector.TCPConnector object at 0x000002DA8F6F82B0>
2025-02-05 13:32:36,056 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-05 13:32:36,059 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:36,068 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-05 13:32:36,069 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-05 13:32:36,070 [INFO] Starting crawl of URL: https://example.com
2025-02-05 13:32:36,320 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-05 13:32:36,320 [DEBUG] Base URL: https://example.com
2025-02-05 13:32:36,321 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-05 13:32:36,321 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-05 13:32:36,322 [DEBUG] Initial domain set to: example.com
2025-02-05 13:32:36,322 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-05 13:32:36,323 [INFO] Successfully crawled https://example.com
2025-02-05 13:32:36,330 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:36,413 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-05 13:32:36,414 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:36,418 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:32:37,238 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-05 13:32:37,290 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:37,293 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-05 13:32:37,294 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:37,296 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-05 13:32:37,346 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:37,348 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-05 13:32:37,348 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:37,350 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-05 13:32:37,350 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2025-02-05 13:32:37,350 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2025-02-05 13:32:37,350 [INFO] Starting crawl of URL: https://example.com/page
2025-02-05 13:32:37,786 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-05 13:32:37,786 [DEBUG] Base URL: https://example.com/page
2025-02-05 13:32:37,787 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-05 13:32:37,787 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-05 13:32:37,788 [DEBUG] Initial domain set to: example.com
2025-02-05 13:32:37,789 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-05 13:32:37,792 [INFO] Successfully crawled https://example.com/page
2025-02-05 13:32:37,793 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2025-02-05 13:32:37,794 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2025-02-05 13:32:37,795 [INFO] Starting crawl of URL: https://other-domain.com/page
2025-02-05 13:32:40,799 [DEBUG] 
Evaluating link: ./
2025-02-05 13:32:40,799 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,799 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2025-02-05 13:32:40,799 [DEBUG] Absolute link: https://other-domain.com/
2025-02-05 13:32:40,799 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,800 [DEBUG] 
Evaluating link: ./
2025-02-05 13:32:40,800 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,800 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2025-02-05 13:32:40,800 [DEBUG] Absolute link: https://other-domain.com/
2025-02-05 13:32:40,800 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,800 [DEBUG] 
Evaluating link: ./hiyou.php
2025-02-05 13:32:40,800 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,801 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2025-02-05 13:32:40,801 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2025-02-05 13:32:40,801 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,801 [DEBUG] 
Evaluating link: ./date.php
2025-02-05 13:32:40,801 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,801 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2025-02-05 13:32:40,801 [DEBUG] Absolute link: https://other-domain.com/date.php
2025-02-05 13:32:40,801 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,802 [DEBUG] 
Evaluating link: ./contact.php
2025-02-05 13:32:40,802 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,803 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2025-02-05 13:32:40,804 [DEBUG] Absolute link: https://other-domain.com/contact.php
2025-02-05 13:32:40,805 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,805 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-05 13:32:40,805 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,806 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-05 13:32:40,806 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-05 13:32:40,806 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2025-02-05 13:32:40,824 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,830 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-05 13:32:40,831 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-05 13:32:40,831 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,831 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-05 13:32:40,831 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-05 13:32:40,831 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-05 13:32:40,831 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-05 13:32:40,832 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,832 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-05 13:32:40,832 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-05 13:32:40,832 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-05 13:32:40,832 [DEBUG] 
Evaluating link: ./contact
2025-02-05 13:32:40,832 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,832 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2025-02-05 13:32:40,832 [DEBUG] Absolute link: https://other-domain.com/contact
2025-02-05 13:32:40,833 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,839 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,846 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-05 13:32:40,846 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-05 13:32:40,846 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,846 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-05 13:32:40,846 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-05 13:32:40,846 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-05 13:32:40,847 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-05 13:32:40,847 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,847 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-05 13:32:40,847 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-05 13:32:40,847 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-05 13:32:40,850 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,860 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2025-02-05 13:32:40,860 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-05 13:32:40,860 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,860 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-05 13:32:40,860 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-05 13:32:40,860 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2025-02-05 13:32:40,860 [DEBUG] 
Evaluating link: #indexes
2025-02-05 13:32:40,860 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,861 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2025-02-05 13:32:40,861 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,861 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,861 [DEBUG] 
Evaluating link: #a
2025-02-05 13:32:40,861 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,861 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2025-02-05 13:32:40,861 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,861 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,861 [DEBUG] 
Evaluating link: #b
2025-02-05 13:32:40,862 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,862 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2025-02-05 13:32:40,862 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,862 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,862 [DEBUG] 
Evaluating link: #c
2025-02-05 13:32:40,862 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,862 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2025-02-05 13:32:40,862 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,862 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,862 [DEBUG] 
Evaluating link: #d
2025-02-05 13:32:40,863 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,863 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2025-02-05 13:32:40,863 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,863 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,863 [DEBUG] 
Evaluating link: #e
2025-02-05 13:32:40,863 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,863 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2025-02-05 13:32:40,863 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,863 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,864 [DEBUG] 
Evaluating link: #f
2025-02-05 13:32:40,864 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,864 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2025-02-05 13:32:40,864 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,864 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,864 [DEBUG] 
Evaluating link: #g
2025-02-05 13:32:40,864 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,864 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2025-02-05 13:32:40,865 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,865 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,865 [DEBUG] 
Evaluating link: #h
2025-02-05 13:32:40,865 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,865 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2025-02-05 13:32:40,865 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,865 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,865 [DEBUG] 
Evaluating link: #i
2025-02-05 13:32:40,865 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,866 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2025-02-05 13:32:40,866 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,866 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,866 [DEBUG] 
Evaluating link: #j
2025-02-05 13:32:40,866 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,866 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2025-02-05 13:32:40,866 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,866 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,866 [DEBUG] 
Evaluating link: #k
2025-02-05 13:32:40,867 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,867 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2025-02-05 13:32:40,867 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,867 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,867 [DEBUG] 
Evaluating link: #l
2025-02-05 13:32:40,867 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,867 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2025-02-05 13:32:40,867 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,867 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,867 [DEBUG] 
Evaluating link: #m
2025-02-05 13:32:40,868 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,868 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2025-02-05 13:32:40,868 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,868 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,868 [DEBUG] 
Evaluating link: #n
2025-02-05 13:32:40,868 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,868 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2025-02-05 13:32:40,868 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,870 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,871 [DEBUG] 
Evaluating link: #o
2025-02-05 13:32:40,872 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,872 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2025-02-05 13:32:40,872 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,873 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,873 [DEBUG] 
Evaluating link: #p
2025-02-05 13:32:40,873 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,873 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2025-02-05 13:32:40,873 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,873 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,873 [DEBUG] 
Evaluating link: #q
2025-02-05 13:32:40,873 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,874 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2025-02-05 13:32:40,874 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,874 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,874 [DEBUG] 
Evaluating link: #r
2025-02-05 13:32:40,874 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,874 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2025-02-05 13:32:40,874 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,874 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,874 [DEBUG] 
Evaluating link: #s
2025-02-05 13:32:40,874 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,875 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2025-02-05 13:32:40,875 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,875 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,875 [DEBUG] 
Evaluating link: #t
2025-02-05 13:32:40,875 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,875 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2025-02-05 13:32:40,875 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,875 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,875 [DEBUG] 
Evaluating link: #u
2025-02-05 13:32:40,876 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,876 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2025-02-05 13:32:40,876 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,876 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,876 [DEBUG] 
Evaluating link: #v
2025-02-05 13:32:40,876 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,876 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2025-02-05 13:32:40,876 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,876 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,877 [DEBUG] 
Evaluating link: #w
2025-02-05 13:32:40,877 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,877 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2025-02-05 13:32:40,877 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,877 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,877 [DEBUG] 
Evaluating link: #x
2025-02-05 13:32:40,877 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,877 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2025-02-05 13:32:40,877 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,878 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,878 [DEBUG] 
Evaluating link: #y
2025-02-05 13:32:40,878 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,878 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2025-02-05 13:32:40,878 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,878 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,878 [DEBUG] 
Evaluating link: #z
2025-02-05 13:32:40,878 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,878 [DEBUG] Normalized URL: https://other-domain.com/page#z -> https://other-domain.com/page
2025-02-05 13:32:40,879 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,879 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,879 [DEBUG] 
Evaluating link: #A
2025-02-05 13:32:40,879 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,879 [DEBUG] Normalized URL: https://other-domain.com/page#A -> https://other-domain.com/page
2025-02-05 13:32:40,879 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,879 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,879 [DEBUG] 
Evaluating link: #B
2025-02-05 13:32:40,879 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,880 [DEBUG] Normalized URL: https://other-domain.com/page#B -> https://other-domain.com/page
2025-02-05 13:32:40,880 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,880 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,880 [DEBUG] 
Evaluating link: #C
2025-02-05 13:32:40,880 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,880 [DEBUG] Normalized URL: https://other-domain.com/page#C -> https://other-domain.com/page
2025-02-05 13:32:40,880 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,880 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,880 [DEBUG] 
Evaluating link: #D
2025-02-05 13:32:40,880 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,881 [DEBUG] Normalized URL: https://other-domain.com/page#D -> https://other-domain.com/page
2025-02-05 13:32:40,881 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,881 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,884 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,892 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-05 13:32:40,892 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-05 13:32:40,892 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,893 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-05 13:32:40,893 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-05 13:32:40,893 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-05 13:32:40,893 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-05 13:32:40,893 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,893 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-05 13:32:40,893 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-05 13:32:40,893 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-05 13:32:40,896 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,902 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2025-02-05 13:32:40,902 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-05 13:32:40,903 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,904 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-05 13:32:40,905 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-05 13:32:40,905 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2025-02-05 13:32:40,905 [DEBUG] 
Evaluating link: https://spriq.jp/
2025-02-05 13:32:40,905 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,906 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2025-02-05 13:32:40,906 [DEBUG] Absolute link: https://spriq.jp/
2025-02-05 13:32:40,906 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2025-02-05 13:32:40,909 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,916 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-05 13:32:40,922 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,929 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-05 13:32:40,929 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-05 13:32:40,930 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,930 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-05 13:32:40,930 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-05 13:32:40,930 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-05 13:32:40,930 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-05 13:32:40,930 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,930 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-05 13:32:40,930 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-05 13:32:40,931 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-05 13:32:40,931 [DEBUG] 
Evaluating link: ./contact
2025-02-05 13:32:40,931 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,931 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2025-02-05 13:32:40,931 [DEBUG] Absolute link: https://other-domain.com/contact
2025-02-05 13:32:40,931 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,931 [DEBUG] 
Evaluating link: ./
2025-02-05 13:32:40,931 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,932 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2025-02-05 13:32:40,932 [DEBUG] Absolute link: https://other-domain.com/
2025-02-05 13:32:40,932 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,932 [DEBUG] 
Evaluating link: ./hiyou.php
2025-02-05 13:32:40,932 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,932 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2025-02-05 13:32:40,932 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2025-02-05 13:32:40,932 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,933 [DEBUG] 
Evaluating link: ./date.php
2025-02-05 13:32:40,933 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,933 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2025-02-05 13:32:40,933 [DEBUG] Absolute link: https://other-domain.com/date.php
2025-02-05 13:32:40,933 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,933 [DEBUG] 
Evaluating link: #z
2025-02-05 13:32:40,933 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,933 [DEBUG] Normalized URL: https://other-domain.com/page#z -> https://other-domain.com/page
2025-02-05 13:32:40,934 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,934 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,934 [DEBUG] 
Evaluating link: #A
2025-02-05 13:32:40,934 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,934 [DEBUG] Normalized URL: https://other-domain.com/page#A -> https://other-domain.com/page
2025-02-05 13:32:40,934 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,934 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,934 [DEBUG] 
Evaluating link: #B
2025-02-05 13:32:40,935 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,935 [DEBUG] Normalized URL: https://other-domain.com/page#B -> https://other-domain.com/page
2025-02-05 13:32:40,935 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,935 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,935 [DEBUG] 
Evaluating link: #C
2025-02-05 13:32:40,937 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,938 [DEBUG] Normalized URL: https://other-domain.com/page#C -> https://other-domain.com/page
2025-02-05 13:32:40,938 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,938 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,939 [DEBUG] 
Evaluating link: #D
2025-02-05 13:32:40,939 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,939 [DEBUG] Normalized URL: https://other-domain.com/page#D -> https://other-domain.com/page
2025-02-05 13:32:40,939 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,939 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,939 [DEBUG] 
Evaluating link: #u
2025-02-05 13:32:40,940 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,940 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2025-02-05 13:32:40,940 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,940 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,940 [DEBUG] 
Evaluating link: #v
2025-02-05 13:32:40,940 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,940 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2025-02-05 13:32:40,940 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,941 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,941 [DEBUG] 
Evaluating link: #w
2025-02-05 13:32:40,941 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,941 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2025-02-05 13:32:40,941 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,941 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,941 [DEBUG] 
Evaluating link: #x
2025-02-05 13:32:40,941 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,942 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2025-02-05 13:32:40,942 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,942 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,942 [DEBUG] 
Evaluating link: #y
2025-02-05 13:32:40,942 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,942 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2025-02-05 13:32:40,942 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,942 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,942 [DEBUG] 
Evaluating link: #p
2025-02-05 13:32:40,943 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,943 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2025-02-05 13:32:40,943 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,943 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,943 [DEBUG] 
Evaluating link: #q
2025-02-05 13:32:40,943 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,943 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2025-02-05 13:32:40,943 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,944 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,944 [DEBUG] 
Evaluating link: #r
2025-02-05 13:32:40,944 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,944 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2025-02-05 13:32:40,944 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,944 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,944 [DEBUG] 
Evaluating link: #s
2025-02-05 13:32:40,944 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,945 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2025-02-05 13:32:40,945 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,945 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,945 [DEBUG] 
Evaluating link: #t
2025-02-05 13:32:40,945 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,945 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2025-02-05 13:32:40,945 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,945 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,946 [DEBUG] 
Evaluating link: #k
2025-02-05 13:32:40,946 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,946 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2025-02-05 13:32:40,946 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,946 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,946 [DEBUG] 
Evaluating link: #l
2025-02-05 13:32:40,946 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,946 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2025-02-05 13:32:40,946 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,947 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,947 [DEBUG] 
Evaluating link: #m
2025-02-05 13:32:40,947 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,947 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2025-02-05 13:32:40,947 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,947 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,947 [DEBUG] 
Evaluating link: #n
2025-02-05 13:32:40,947 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,948 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2025-02-05 13:32:40,948 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,948 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,948 [DEBUG] 
Evaluating link: #o
2025-02-05 13:32:40,948 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,948 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2025-02-05 13:32:40,948 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,948 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,948 [DEBUG] 
Evaluating link: #f
2025-02-05 13:32:40,949 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,949 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2025-02-05 13:32:40,949 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,949 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,949 [DEBUG] 
Evaluating link: #g
2025-02-05 13:32:40,949 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,949 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2025-02-05 13:32:40,950 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,950 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,950 [DEBUG] 
Evaluating link: #h
2025-02-05 13:32:40,950 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,950 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2025-02-05 13:32:40,950 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,950 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,950 [DEBUG] 
Evaluating link: #i
2025-02-05 13:32:40,950 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,951 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2025-02-05 13:32:40,951 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,951 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,951 [DEBUG] 
Evaluating link: #j
2025-02-05 13:32:40,951 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,951 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2025-02-05 13:32:40,951 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,951 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,952 [DEBUG] 
Evaluating link: #b
2025-02-05 13:32:40,952 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,953 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2025-02-05 13:32:40,954 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,955 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,955 [DEBUG] 
Evaluating link: #c
2025-02-05 13:32:40,955 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,956 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2025-02-05 13:32:40,956 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,956 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,956 [DEBUG] 
Evaluating link: #d
2025-02-05 13:32:40,957 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,957 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2025-02-05 13:32:40,957 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,957 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,957 [DEBUG] 
Evaluating link: #e
2025-02-05 13:32:40,957 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,957 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2025-02-05 13:32:40,958 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,958 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,958 [DEBUG] 
Evaluating link: #site
2025-02-05 13:32:40,958 [DEBUG] Base URL: https://other-domain.com/page
2025-02-05 13:32:40,958 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2025-02-05 13:32:40,958 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-05 13:32:40,958 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-05 13:32:40,959 [INFO] Successfully crawled https://other-domain.com/page
2025-02-05 13:32:40,982 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:40,984 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-05 13:32:40,984 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:40,988 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2025-02-05 13:32:40,989 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2025-02-05 13:32:40,989 [INFO] Starting crawl of URL: https://example.com/page0
2025-02-05 13:32:41,984 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-05 13:32:41,984 [DEBUG] Base URL: https://example.com/page0
2025-02-05 13:32:41,985 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-05 13:32:41,986 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-05 13:32:41,987 [DEBUG] Initial domain set to: example.com
2025-02-05 13:32:41,988 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-05 13:32:41,989 [INFO] Successfully crawled https://example.com/page0
2025-02-05 13:32:41,990 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2025-02-05 13:32:41,990 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2025-02-05 13:32:41,991 [INFO] Starting crawl of URL: https://example.com/page1
2025-02-05 13:32:42,289 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-05 13:32:42,289 [DEBUG] Base URL: https://example.com/page1
2025-02-05 13:32:42,290 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-05 13:32:42,290 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-05 13:32:42,290 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-05 13:32:42,290 [INFO] Successfully crawled https://example.com/page1
2025-02-05 13:32:42,290 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2025-02-05 13:32:42,291 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2025-02-05 13:32:42,291 [INFO] Starting crawl of URL: https://example.com/page2
2025-02-05 13:32:42,803 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-05 13:32:42,805 [DEBUG] Base URL: https://example.com/page2
2025-02-05 13:32:42,806 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-05 13:32:42,807 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-05 13:32:42,808 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-05 13:32:42,809 [INFO] Successfully crawled https://example.com/page2
2025-02-05 13:32:42,810 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2025-02-05 13:32:42,811 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2025-02-05 13:32:42,811 [INFO] Starting crawl of URL: https://example.com/page3
2025-02-05 13:32:43,312 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-05 13:32:43,312 [DEBUG] Base URL: https://example.com/page3
2025-02-05 13:32:43,313 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-05 13:32:43,313 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-05 13:32:43,313 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-05 13:32:43,314 [INFO] Successfully crawled https://example.com/page3
2025-02-05 13:32:43,315 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2025-02-05 13:32:43,315 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2025-02-05 13:32:43,315 [INFO] Starting crawl of URL: https://example.com/page4
2025-02-05 13:32:44,339 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-05 13:32:44,340 [DEBUG] Base URL: https://example.com/page4
2025-02-05 13:32:44,340 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-05 13:32:44,340 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-05 13:32:44,340 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-05 13:32:44,341 [INFO] Successfully crawled https://example.com/page4
2025-02-05 13:32:44,341 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-05 13:32:44,341 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2025-02-05 13:32:44,342 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2025-02-05 13:32:44,342 [INFO] Starting crawl of URL: https://example.com/page0
2025-02-05 13:32:44,586 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-05 13:32:44,586 [DEBUG] Base URL: https://example.com/page0
2025-02-05 13:32:44,586 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-05 13:32:44,586 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-05 13:32:44,586 [DEBUG] Initial domain set to: example.com
2025-02-05 13:32:44,587 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-05 13:32:44,587 [INFO] Successfully crawled https://example.com/page0
2025-02-05 13:32:44,587 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2025-02-05 13:32:44,587 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2025-02-05 13:32:44,587 [INFO] Starting crawl of URL: https://example.com/page1
2025-02-05 13:32:44,990 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-05 13:32:44,991 [DEBUG] Base URL: https://example.com/page1
2025-02-05 13:32:44,992 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-05 13:32:44,993 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-05 13:32:44,993 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-05 13:32:44,994 [INFO] Successfully crawled https://example.com/page1
2025-02-05 13:32:44,994 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2025-02-05 13:32:44,995 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2025-02-05 13:32:44,995 [INFO] Starting crawl of URL: https://example.com/page2
2025-02-05 13:32:45,495 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-05 13:32:45,495 [DEBUG] Base URL: https://example.com/page2
2025-02-05 13:32:45,496 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-05 13:32:45,496 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-05 13:32:45,497 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-05 13:32:45,497 [INFO] Successfully crawled https://example.com/page2
2025-02-05 13:32:45,498 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2025-02-05 13:32:45,500 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2025-02-05 13:32:45,502 [INFO] Starting crawl of URL: https://example.com/page3
2025-02-05 13:32:46,022 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-05 13:32:46,022 [DEBUG] Base URL: https://example.com/page3
2025-02-05 13:32:46,023 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-05 13:32:46,023 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-05 13:32:46,023 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-05 13:32:46,024 [INFO] Successfully crawled https://example.com/page3
2025-02-05 13:32:46,024 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2025-02-05 13:32:46,024 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2025-02-05 13:32:46,024 [INFO] Starting crawl of URL: https://example.com/page4
2025-02-05 13:32:46,540 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-05 13:32:46,541 [DEBUG] Base URL: https://example.com/page4
2025-02-05 13:32:46,541 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-05 13:32:46,542 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-05 13:32:46,542 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-05 13:32:46,543 [INFO] Successfully crawled https://example.com/page4
2025-02-05 13:32:46,579 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:46,590 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:46,629 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:46,637 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:46,659 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:46,662 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:46,689 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:46,694 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:46,712 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:46,720 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:46,739 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000002DA8EED87D0>
2025-02-05 13:32:46,739 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000002DA8F0F5FD0>, 87787.7014985)])']
connector: <aiohttp.connector.TCPConnector object at 0x000002DA8EE63E10>
2025-02-05 13:32:46,740 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000002DA8ED739D0>
2025-02-05 13:32:46,741 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000002DA8EEDAD50>
2025-02-05 13:32:46,742 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000002DA8F081490>, 87793.7128824)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x000002DA8F0F67B0>, 87794.8394988)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x000002DA8F0F4BF0>, 87795.9651511)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x000002DA8F0F7C50>, 87796.6868267)])']
connector: <aiohttp.connector.TCPConnector object at 0x000002DA8F6F8050>
2025-02-05 13:32:46,743 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000002DA8EEDAC10>
2025-02-05 13:32:46,744 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000002DA8F1DCB90>, 87800.2662349)])']
connector: <aiohttp.connector.TCPConnector object at 0x000002DA8F6F8FC0>
2025-02-05 13:32:46,744 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000002DA8EEDAE90>
2025-02-05 13:32:46,745 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000002DA8F0A64B0>, 87802.4614242)])']
connector: <aiohttp.connector.TCPConnector object at 0x000002DA8F6F90F0>
2025-02-05 13:32:46,761 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:46,769 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:46,806 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:46,809 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:46,846 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:46,853 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:46,878 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:46,886 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:46,905 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:47,025 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:32:47,069 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:32:47,101 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:32:47,144 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:32:47,187 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:32:47,219 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:32:47,245 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:32:47,278 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:32:47,303 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:32:47,335 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:32:47,368 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:32:47,457 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:47,484 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:47,486 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:47,488 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:32:47,524 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:47,526 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:47,528 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:32:47,554 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:47,564 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:32:47,600 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:32:47,640 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:32:47,668 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:32:47,707 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:32:47,734 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:32:47,768 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:32:47,790 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:32:47,821 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:32:47,857 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:32:48,636 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:49,387 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:49,390 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-05 13:32:49,391 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-05 13:32:49,391 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-05 13:32:49,391 [INFO] Starting crawl of URL: https://example.com
2025-02-05 13:32:49,595 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-05 13:32:49,595 [DEBUG] Base URL: https://example.com
2025-02-05 13:32:49,596 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-05 13:32:49,596 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-05 13:32:49,596 [DEBUG] Initial domain set to: example.com
2025-02-05 13:32:49,597 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-05 13:32:49,597 [INFO] Successfully crawled https://example.com
2025-02-05 13:32:49,598 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000002DA8F079450>
2025-02-05 13:32:49,598 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000002DA8F6BA330>, 87805.5223779)])']
connector: <aiohttp.connector.TCPConnector object at 0x000002DA8F6FAEA0>
2025-02-05 13:32:49,603 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2025-02-05 13:32:49,925 [INFO] Operation took 0.10 seconds
2025-02-05 13:32:50,446 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-05 13:32:50,446 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:32:50,448 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:58,562 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-0\test_full_site_crawl0\test_docs/index.html
2025-02-05 13:32:58,563 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-05 13:32:58,672 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:32:58,689 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-05 13:32:58,690 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:32:58,691 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:09,368 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-0\test_content_processing_pipeli0\test_docs/guide.html
2025-02-05 13:33:09,370 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-05 13:33:09,466 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:09,487 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-05 13:33:09,488 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:33:09,488 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:20,506 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-0\test_quality_checks0\test_docs/api.html
2025-02-05 13:33:20,507 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-05 13:33:20,617 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:20,630 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-05 13:33:20,631 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:33:20,634 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:32,079 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-0\test_document_organization0\test_docs/index.html
2025-02-05 13:33:32,081 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-05 13:33:32,166 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:32,185 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-05 13:33:32,186 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:33:32,187 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:40,072 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-0\test_search_functionality0\test_docs/index.html
2025-02-05 13:33:40,073 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-05 13:33:40,172 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:40,194 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-05 13:33:40,195 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:33:40,196 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:48,147 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-0\test_error_handling_and_recove0\test_docs/index.html
2025-02-05 13:33:48,149 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-05 13:33:48,303 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:48,319 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:48,365 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:48,368 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:48,370 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:33:48,370 [ERROR] Error during crawl: 'str' object has no attribute 'url'
2025-02-05 13:33:48,396 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:48,399 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:48,415 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:48,418 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:48,421 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-05 13:33:48,443 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:51,025 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:51,044 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:51,049 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:51,074 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:51,077 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:51,098 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:51,100 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:51,116 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:51,119 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:51,139 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:51,142 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:51,166 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:51,168 [DEBUG] Using proactor: IocpProactor
2025-02-05 13:33:51,183 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:22:19,096 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 09:24:13,897 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 09:25:24,036 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 09:25:25,895 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:25,902 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:26,450 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:26,573 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:26,741 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:26,750 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:26,757 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:25:26,821 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:26,823 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:26,824 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:25:26,841 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:26,846 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:26,867 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:26,869 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:26,889 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:26,892 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:26,917 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:26,919 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:26,947 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:26,949 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:26,951 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:25:26,969 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:32,482 [INFO] Operation took 0.20 seconds
2025-02-06 09:25:32,764 [DEBUG] IDNA encoding failed for example.com@evil.com: Codepoint U+0040 at position 4 of 'com@evil' not allowed
2025-02-06 09:25:32,902 [DEBUG] IDNA encoding failed for user@example.com: Codepoint U+0040 at position 5 of 'user@example' not allowed
2025-02-06 09:25:32,902 [DEBUG] IDNA encoding failed for :pass@example.com: Empty domain
2025-02-06 09:25:32,998 [DEBUG] IDNA encoding failed for .example.com: Empty Label
2025-02-06 09:25:32,999 [DEBUG] IDNA encoding failed for example..com: Empty Label
2025-02-06 09:25:32,999 [DEBUG] IDNA encoding failed for example.com..: Empty Label
2025-02-06 09:25:33,349 [INFO] test_operation took 0.10 seconds
2025-02-06 09:25:33,550 [INFO] inner took 0.10 seconds
2025-02-06 09:25:33,550 [INFO] outer took 0.20 seconds
2025-02-06 09:25:33,550 [INFO] error_operation took 0.00 seconds
2025-02-06 09:25:33,661 [INFO] test_operation took 0.00 seconds
2025-02-06 09:25:34,009 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 09:25:34,010 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:34,015 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-06 09:25:34,015 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-06 09:25:34,016 [INFO] Starting crawl of URL: https://example.com
2025-02-06 09:25:34,578 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 09:25:34,578 [DEBUG] Base URL: https://example.com
2025-02-06 09:25:34,578 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 09:25:34,578 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 09:25:34,580 [DEBUG] Initial domain set to: example.com
2025-02-06 09:25:34,580 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 09:25:34,580 [INFO] Successfully crawled https://example.com
2025-02-06 09:25:34,582 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:34,584 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001DBF1721D30>
2025-02-06 09:25:34,584 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001DBF1C21370>, 159376.010422)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001DBF1721A90>
2025-02-06 09:25:34,586 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 09:25:34,587 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:34,590 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-06 09:25:34,590 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-06 09:25:34,591 [INFO] Starting crawl of URL: https://example.com
2025-02-06 09:25:34,729 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2025-02-06 09:25:34,730 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2025-02-06 09:25:34,730 [INFO] Starting crawl of URL: https://example.org
2025-02-06 09:25:35,009 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 09:25:35,009 [DEBUG] Base URL: https://example.com
2025-02-06 09:25:35,010 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 09:25:35,010 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 09:25:35,011 [DEBUG] Initial domain set to: example.com
2025-02-06 09:25:35,011 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 09:25:35,011 [INFO] Successfully crawled https://example.com
2025-02-06 09:25:35,494 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 09:25:35,495 [DEBUG] Base URL: https://example.org
2025-02-06 09:25:35,495 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 09:25:35,495 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 09:25:35,495 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 09:25:35,495 [INFO] Successfully crawled https://example.org
2025-02-06 09:25:35,497 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:35,498 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001DBF17FA990>
2025-02-06 09:25:35,498 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001DBF1C205F0>, 159376.4376584)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001DBF1C226F0>, 159376.9282781)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001DBF17F87D0>
2025-02-06 09:25:35,499 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 09:25:35,499 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:35,502 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-06 09:25:35,502 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-06 09:25:35,503 [INFO] Starting crawl of URL: https://example.com
2025-02-06 09:25:35,832 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 09:25:35,832 [DEBUG] Base URL: https://example.com
2025-02-06 09:25:35,833 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 09:25:35,833 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 09:25:35,833 [DEBUG] Initial domain set to: example.com
2025-02-06 09:25:35,834 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 09:25:35,834 [INFO] Successfully crawled https://example.com
2025-02-06 09:25:35,840 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:35,844 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001DBF17F8A50>
2025-02-06 09:25:35,845 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001DBF1C22450>, 159377.2643527)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001DBF17F9590>
2025-02-06 09:25:35,846 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 09:25:35,848 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:35,853 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-06 09:25:35,854 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-06 09:25:35,855 [INFO] Starting crawl of URL: https://example.com
2025-02-06 09:25:36,206 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 09:25:36,206 [DEBUG] Base URL: https://example.com
2025-02-06 09:25:36,206 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 09:25:36,207 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 09:25:36,207 [DEBUG] Initial domain set to: example.com
2025-02-06 09:25:36,207 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 09:25:36,207 [INFO] Successfully crawled https://example.com
2025-02-06 09:25:36,209 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 09:25:36,209 [DEBUG] Base URL: https://example.com
2025-02-06 09:25:36,209 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 09:25:36,209 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 09:25:36,209 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 09:25:36,250 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:36,254 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:25:36,255 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:36,259 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:36,261 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 09:25:36,262 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:36,264 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-06 09:25:36,264 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-06 09:25:36,264 [INFO] Starting crawl of URL: https://example.com
2025-02-06 09:25:36,592 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 09:25:36,592 [DEBUG] Base URL: https://example.com
2025-02-06 09:25:36,593 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 09:25:36,593 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 09:25:36,593 [DEBUG] Initial domain set to: example.com
2025-02-06 09:25:36,593 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 09:25:36,594 [INFO] Successfully crawled https://example.com
2025-02-06 09:25:36,596 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:36,599 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001DBF166F110>
2025-02-06 09:25:36,599 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001DBF1C0F5F0>, 159378.0225985)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001DBF174BE10>
2025-02-06 09:25:36,602 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 09:25:36,604 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:36,609 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2025-02-06 09:25:36,609 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2025-02-06 09:25:36,610 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2025-02-06 09:25:38,708 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [getaddrinfo failed]
2025-02-06 09:25:38,710 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [getaddrinfo failed]
2025-02-06 09:25:38,711 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [getaddrinfo failed]
2025-02-06 09:25:38,749 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:38,754 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 09:25:38,755 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:38,757 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-06 09:25:38,757 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-06 09:25:38,757 [INFO] Starting crawl of URL: https://example.com
2025-02-06 09:25:38,807 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2025-02-06 09:25:38,807 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2025-02-06 09:25:38,807 [INFO] Starting crawl of URL: https://example.org
2025-02-06 09:25:38,808 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2025-02-06 09:25:38,808 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2025-02-06 09:25:38,808 [INFO] Starting crawl of URL: https://example.net
2025-02-06 09:25:39,092 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 09:25:39,093 [DEBUG] Base URL: https://example.com
2025-02-06 09:25:39,093 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 09:25:39,093 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 09:25:39,093 [DEBUG] Initial domain set to: example.com
2025-02-06 09:25:39,094 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 09:25:39,094 [INFO] Successfully crawled https://example.com
2025-02-06 09:25:39,603 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 09:25:39,604 [DEBUG] Base URL: https://example.org
2025-02-06 09:25:39,604 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 09:25:39,604 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 09:25:39,604 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 09:25:39,604 [INFO] Successfully crawled https://example.org
2025-02-06 09:25:40,056 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 09:25:40,056 [DEBUG] Base URL: https://example.net
2025-02-06 09:25:40,056 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 09:25:40,057 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 09:25:40,057 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 09:25:40,057 [INFO] Successfully crawled https://example.net
2025-02-06 09:25:40,059 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:40,061 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001DBF17F91D0>
2025-02-06 09:25:40,061 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001DBF1C0C890>, 159380.522344)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001DBF1C0C470>, 159381.0361237)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001DBF1C0F4D0>, 159381.4886394)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001DBF1830770>
2025-02-06 09:25:40,062 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 09:25:40,063 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:40,067 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-06 09:25:40,067 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-06 09:25:40,067 [INFO] Starting crawl of URL: https://example.com
2025-02-06 09:25:40,408 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 09:25:40,408 [DEBUG] Base URL: https://example.com
2025-02-06 09:25:40,408 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 09:25:40,408 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 09:25:40,408 [DEBUG] Initial domain set to: example.com
2025-02-06 09:25:40,408 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 09:25:40,409 [INFO] Successfully crawled https://example.com
2025-02-06 09:25:40,410 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:40,427 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 09:25:40,428 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:40,429 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:25:41,139 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 09:25:41,186 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:41,188 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 09:25:41,189 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:41,191 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 09:25:41,243 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:41,250 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 09:25:41,251 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:41,253 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 09:25:41,254 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2025-02-06 09:25:41,254 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2025-02-06 09:25:41,254 [INFO] Starting crawl of URL: https://example.com/page
2025-02-06 09:25:41,924 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 09:25:41,924 [DEBUG] Base URL: https://example.com/page
2025-02-06 09:25:41,925 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 09:25:41,925 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 09:25:41,925 [DEBUG] Initial domain set to: example.com
2025-02-06 09:25:41,925 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 09:25:41,926 [INFO] Successfully crawled https://example.com/page
2025-02-06 09:25:41,926 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2025-02-06 09:25:41,926 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2025-02-06 09:25:41,926 [INFO] Starting crawl of URL: https://other-domain.com/page
2025-02-06 09:25:43,985 [DEBUG] 
Evaluating link: ./
2025-02-06 09:25:43,985 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:43,986 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2025-02-06 09:25:43,986 [DEBUG] Absolute link: https://other-domain.com/
2025-02-06 09:25:43,986 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:43,986 [DEBUG] 
Evaluating link: ./
2025-02-06 09:25:43,986 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:43,986 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2025-02-06 09:25:43,986 [DEBUG] Absolute link: https://other-domain.com/
2025-02-06 09:25:43,986 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:43,986 [DEBUG] 
Evaluating link: ./hiyou.php
2025-02-06 09:25:43,987 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:43,987 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2025-02-06 09:25:43,987 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2025-02-06 09:25:43,987 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:43,987 [DEBUG] 
Evaluating link: ./date.php
2025-02-06 09:25:43,987 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:43,987 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2025-02-06 09:25:43,987 [DEBUG] Absolute link: https://other-domain.com/date.php
2025-02-06 09:25:43,987 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:43,988 [DEBUG] 
Evaluating link: ./contact.php
2025-02-06 09:25:43,988 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:43,988 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2025-02-06 09:25:43,988 [DEBUG] Absolute link: https://other-domain.com/contact.php
2025-02-06 09:25:43,988 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:43,988 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-06 09:25:43,988 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:43,988 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-06 09:25:43,988 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-06 09:25:43,989 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2025-02-06 09:25:44,088 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,096 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-06 09:25:44,096 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 09:25:44,096 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,096 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 09:25:44,097 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 09:25:44,097 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-06 09:25:44,097 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-06 09:25:44,097 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,097 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-06 09:25:44,097 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-06 09:25:44,097 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-06 09:25:44,097 [DEBUG] 
Evaluating link: ./contact
2025-02-06 09:25:44,098 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,098 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2025-02-06 09:25:44,098 [DEBUG] Absolute link: https://other-domain.com/contact
2025-02-06 09:25:44,098 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,101 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,106 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-06 09:25:44,107 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 09:25:44,107 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,107 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 09:25:44,108 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 09:25:44,108 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-06 09:25:44,108 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-06 09:25:44,108 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,108 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-06 09:25:44,108 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-06 09:25:44,108 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-06 09:25:44,115 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,120 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2025-02-06 09:25:44,120 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-06 09:25:44,121 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,121 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-06 09:25:44,121 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-06 09:25:44,121 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2025-02-06 09:25:44,121 [DEBUG] 
Evaluating link: #indexes
2025-02-06 09:25:44,121 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,121 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2025-02-06 09:25:44,122 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,122 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,122 [DEBUG] 
Evaluating link: #a
2025-02-06 09:25:44,122 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,122 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2025-02-06 09:25:44,122 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,122 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,122 [DEBUG] 
Evaluating link: #b
2025-02-06 09:25:44,122 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,122 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2025-02-06 09:25:44,123 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,128 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,129 [DEBUG] 
Evaluating link: #c
2025-02-06 09:25:44,129 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,129 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2025-02-06 09:25:44,130 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,130 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,130 [DEBUG] 
Evaluating link: #d
2025-02-06 09:25:44,130 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,130 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2025-02-06 09:25:44,131 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,131 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,131 [DEBUG] 
Evaluating link: #e
2025-02-06 09:25:44,131 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,132 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2025-02-06 09:25:44,132 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,132 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,132 [DEBUG] 
Evaluating link: #f
2025-02-06 09:25:44,132 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,132 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2025-02-06 09:25:44,132 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,132 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,133 [DEBUG] 
Evaluating link: #g
2025-02-06 09:25:44,133 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,133 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2025-02-06 09:25:44,133 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,133 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,133 [DEBUG] 
Evaluating link: #h
2025-02-06 09:25:44,133 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,133 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2025-02-06 09:25:44,133 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,134 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,134 [DEBUG] 
Evaluating link: #i
2025-02-06 09:25:44,134 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,134 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2025-02-06 09:25:44,134 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,134 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,134 [DEBUG] 
Evaluating link: #j
2025-02-06 09:25:44,134 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,134 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2025-02-06 09:25:44,135 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,135 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,135 [DEBUG] 
Evaluating link: #k
2025-02-06 09:25:44,135 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,135 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2025-02-06 09:25:44,135 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,135 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,135 [DEBUG] 
Evaluating link: #l
2025-02-06 09:25:44,135 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,135 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2025-02-06 09:25:44,136 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,136 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,136 [DEBUG] 
Evaluating link: #m
2025-02-06 09:25:44,136 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,136 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2025-02-06 09:25:44,136 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,136 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,136 [DEBUG] 
Evaluating link: #n
2025-02-06 09:25:44,136 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,137 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2025-02-06 09:25:44,137 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,137 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,137 [DEBUG] 
Evaluating link: #o
2025-02-06 09:25:44,137 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,137 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2025-02-06 09:25:44,137 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,137 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,137 [DEBUG] 
Evaluating link: #p
2025-02-06 09:25:44,137 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,138 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2025-02-06 09:25:44,138 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,138 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,138 [DEBUG] 
Evaluating link: #q
2025-02-06 09:25:44,138 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,138 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2025-02-06 09:25:44,138 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,138 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,138 [DEBUG] 
Evaluating link: #r
2025-02-06 09:25:44,139 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,139 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2025-02-06 09:25:44,139 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,139 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,139 [DEBUG] 
Evaluating link: #s
2025-02-06 09:25:44,139 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,139 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2025-02-06 09:25:44,139 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,140 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,140 [DEBUG] 
Evaluating link: #t
2025-02-06 09:25:44,140 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,140 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2025-02-06 09:25:44,140 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,140 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,140 [DEBUG] 
Evaluating link: #u
2025-02-06 09:25:44,140 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,140 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2025-02-06 09:25:44,141 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,141 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,141 [DEBUG] 
Evaluating link: #v
2025-02-06 09:25:44,141 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,141 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2025-02-06 09:25:44,141 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,141 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,141 [DEBUG] 
Evaluating link: #w
2025-02-06 09:25:44,141 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,142 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2025-02-06 09:25:44,142 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,142 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,142 [DEBUG] 
Evaluating link: #x
2025-02-06 09:25:44,142 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,142 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2025-02-06 09:25:44,142 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,142 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,142 [DEBUG] 
Evaluating link: #y
2025-02-06 09:25:44,143 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,143 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2025-02-06 09:25:44,143 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,144 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,145 [DEBUG] 
Evaluating link: #z
2025-02-06 09:25:44,145 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,145 [DEBUG] Normalized URL: https://other-domain.com/page#z -> https://other-domain.com/page
2025-02-06 09:25:44,146 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,146 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,146 [DEBUG] 
Evaluating link: #A
2025-02-06 09:25:44,146 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,146 [DEBUG] Normalized URL: https://other-domain.com/page#A -> https://other-domain.com/page
2025-02-06 09:25:44,146 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,146 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,146 [DEBUG] 
Evaluating link: #B
2025-02-06 09:25:44,146 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,147 [DEBUG] Normalized URL: https://other-domain.com/page#B -> https://other-domain.com/page
2025-02-06 09:25:44,147 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,147 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,147 [DEBUG] 
Evaluating link: #C
2025-02-06 09:25:44,147 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,147 [DEBUG] Normalized URL: https://other-domain.com/page#C -> https://other-domain.com/page
2025-02-06 09:25:44,147 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,147 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,148 [DEBUG] 
Evaluating link: #D
2025-02-06 09:25:44,148 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,148 [DEBUG] Normalized URL: https://other-domain.com/page#D -> https://other-domain.com/page
2025-02-06 09:25:44,148 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,148 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,151 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,156 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-06 09:25:44,157 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 09:25:44,157 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,157 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 09:25:44,157 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 09:25:44,157 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-06 09:25:44,157 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-06 09:25:44,157 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,157 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-06 09:25:44,158 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-06 09:25:44,158 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-06 09:25:44,162 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,168 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2025-02-06 09:25:44,168 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-06 09:25:44,169 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,169 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-06 09:25:44,169 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-06 09:25:44,169 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2025-02-06 09:25:44,169 [DEBUG] 
Evaluating link: https://spriq.jp/
2025-02-06 09:25:44,169 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,169 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2025-02-06 09:25:44,169 [DEBUG] Absolute link: https://spriq.jp/
2025-02-06 09:25:44,169 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2025-02-06 09:25:44,173 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,180 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-06 09:25:44,183 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,189 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-06 09:25:44,189 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 09:25:44,189 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,189 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 09:25:44,189 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 09:25:44,189 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-06 09:25:44,190 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-06 09:25:44,190 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,190 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-06 09:25:44,190 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-06 09:25:44,190 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-06 09:25:44,190 [DEBUG] 
Evaluating link: ./contact
2025-02-06 09:25:44,190 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,190 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2025-02-06 09:25:44,190 [DEBUG] Absolute link: https://other-domain.com/contact
2025-02-06 09:25:44,191 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,191 [DEBUG] 
Evaluating link: ./
2025-02-06 09:25:44,191 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,191 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2025-02-06 09:25:44,191 [DEBUG] Absolute link: https://other-domain.com/
2025-02-06 09:25:44,191 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,191 [DEBUG] 
Evaluating link: ./hiyou.php
2025-02-06 09:25:44,191 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,191 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2025-02-06 09:25:44,192 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2025-02-06 09:25:44,192 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,192 [DEBUG] 
Evaluating link: ./date.php
2025-02-06 09:25:44,192 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,192 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2025-02-06 09:25:44,192 [DEBUG] Absolute link: https://other-domain.com/date.php
2025-02-06 09:25:44,192 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,192 [DEBUG] 
Evaluating link: #z
2025-02-06 09:25:44,192 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,192 [DEBUG] Normalized URL: https://other-domain.com/page#z -> https://other-domain.com/page
2025-02-06 09:25:44,193 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,193 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,193 [DEBUG] 
Evaluating link: #A
2025-02-06 09:25:44,194 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,195 [DEBUG] Normalized URL: https://other-domain.com/page#A -> https://other-domain.com/page
2025-02-06 09:25:44,195 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,196 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,196 [DEBUG] 
Evaluating link: #B
2025-02-06 09:25:44,196 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,196 [DEBUG] Normalized URL: https://other-domain.com/page#B -> https://other-domain.com/page
2025-02-06 09:25:44,196 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,197 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,197 [DEBUG] 
Evaluating link: #C
2025-02-06 09:25:44,197 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,197 [DEBUG] Normalized URL: https://other-domain.com/page#C -> https://other-domain.com/page
2025-02-06 09:25:44,197 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,198 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,198 [DEBUG] 
Evaluating link: #D
2025-02-06 09:25:44,198 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,198 [DEBUG] Normalized URL: https://other-domain.com/page#D -> https://other-domain.com/page
2025-02-06 09:25:44,198 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,198 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,198 [DEBUG] 
Evaluating link: #u
2025-02-06 09:25:44,199 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,199 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2025-02-06 09:25:44,199 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,199 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,199 [DEBUG] 
Evaluating link: #v
2025-02-06 09:25:44,199 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,199 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2025-02-06 09:25:44,199 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,199 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,200 [DEBUG] 
Evaluating link: #w
2025-02-06 09:25:44,200 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,200 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2025-02-06 09:25:44,200 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,200 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,200 [DEBUG] 
Evaluating link: #x
2025-02-06 09:25:44,200 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,200 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2025-02-06 09:25:44,200 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,200 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,201 [DEBUG] 
Evaluating link: #y
2025-02-06 09:25:44,201 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,201 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2025-02-06 09:25:44,201 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,201 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,201 [DEBUG] 
Evaluating link: #p
2025-02-06 09:25:44,201 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,201 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2025-02-06 09:25:44,201 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,201 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,202 [DEBUG] 
Evaluating link: #q
2025-02-06 09:25:44,202 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,202 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2025-02-06 09:25:44,202 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,202 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,202 [DEBUG] 
Evaluating link: #r
2025-02-06 09:25:44,202 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,202 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2025-02-06 09:25:44,202 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,203 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,203 [DEBUG] 
Evaluating link: #s
2025-02-06 09:25:44,203 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,203 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2025-02-06 09:25:44,203 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,203 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,203 [DEBUG] 
Evaluating link: #t
2025-02-06 09:25:44,203 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,204 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2025-02-06 09:25:44,204 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,204 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,204 [DEBUG] 
Evaluating link: #k
2025-02-06 09:25:44,204 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,204 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2025-02-06 09:25:44,204 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,204 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,204 [DEBUG] 
Evaluating link: #l
2025-02-06 09:25:44,205 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,205 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2025-02-06 09:25:44,205 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,205 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,205 [DEBUG] 
Evaluating link: #m
2025-02-06 09:25:44,205 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,205 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2025-02-06 09:25:44,205 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,205 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,205 [DEBUG] 
Evaluating link: #n
2025-02-06 09:25:44,206 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,206 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2025-02-06 09:25:44,206 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,206 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,206 [DEBUG] 
Evaluating link: #o
2025-02-06 09:25:44,206 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,206 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2025-02-06 09:25:44,206 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,206 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,206 [DEBUG] 
Evaluating link: #f
2025-02-06 09:25:44,207 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,207 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2025-02-06 09:25:44,207 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,207 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,207 [DEBUG] 
Evaluating link: #g
2025-02-06 09:25:44,207 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,207 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2025-02-06 09:25:44,207 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,207 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,208 [DEBUG] 
Evaluating link: #h
2025-02-06 09:25:44,208 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,208 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2025-02-06 09:25:44,208 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,208 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,208 [DEBUG] 
Evaluating link: #i
2025-02-06 09:25:44,208 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,208 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2025-02-06 09:25:44,208 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,208 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,209 [DEBUG] 
Evaluating link: #j
2025-02-06 09:25:44,209 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,209 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2025-02-06 09:25:44,209 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,209 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,209 [DEBUG] 
Evaluating link: #b
2025-02-06 09:25:44,209 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,209 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2025-02-06 09:25:44,209 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,211 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,211 [DEBUG] 
Evaluating link: #c
2025-02-06 09:25:44,212 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,212 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2025-02-06 09:25:44,212 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,213 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,213 [DEBUG] 
Evaluating link: #d
2025-02-06 09:25:44,213 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,213 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2025-02-06 09:25:44,213 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,213 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,214 [DEBUG] 
Evaluating link: #e
2025-02-06 09:25:44,214 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,214 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2025-02-06 09:25:44,214 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,214 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,214 [DEBUG] 
Evaluating link: #site
2025-02-06 09:25:44,214 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 09:25:44,215 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2025-02-06 09:25:44,215 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 09:25:44,215 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 09:25:44,215 [INFO] Successfully crawled https://other-domain.com/page
2025-02-06 09:25:44,234 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:44,236 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 09:25:44,237 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:44,239 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2025-02-06 09:25:44,239 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2025-02-06 09:25:44,239 [INFO] Starting crawl of URL: https://example.com/page0
2025-02-06 09:25:44,981 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 09:25:44,981 [DEBUG] Base URL: https://example.com/page0
2025-02-06 09:25:44,981 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 09:25:44,981 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 09:25:44,982 [DEBUG] Initial domain set to: example.com
2025-02-06 09:25:44,982 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 09:25:44,982 [INFO] Successfully crawled https://example.com/page0
2025-02-06 09:25:44,983 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2025-02-06 09:25:44,983 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2025-02-06 09:25:44,983 [INFO] Starting crawl of URL: https://example.com/page1
2025-02-06 09:25:45,071 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 09:25:45,071 [DEBUG] Base URL: https://example.com/page1
2025-02-06 09:25:45,072 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 09:25:45,072 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 09:25:45,072 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 09:25:45,072 [INFO] Successfully crawled https://example.com/page1
2025-02-06 09:25:45,072 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2025-02-06 09:25:45,072 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2025-02-06 09:25:45,072 [INFO] Starting crawl of URL: https://example.com/page2
2025-02-06 09:25:46,241 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 09:25:46,242 [DEBUG] Base URL: https://example.com/page2
2025-02-06 09:25:46,243 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 09:25:46,244 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 09:25:46,245 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 09:25:46,246 [INFO] Successfully crawled https://example.com/page2
2025-02-06 09:25:46,247 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2025-02-06 09:25:46,247 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2025-02-06 09:25:46,248 [INFO] Starting crawl of URL: https://example.com/page3
2025-02-06 09:25:46,942 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 09:25:46,943 [DEBUG] Base URL: https://example.com/page3
2025-02-06 09:25:46,944 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 09:25:46,944 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 09:25:46,945 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 09:25:46,945 [INFO] Successfully crawled https://example.com/page3
2025-02-06 09:25:46,945 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2025-02-06 09:25:46,946 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2025-02-06 09:25:46,946 [INFO] Starting crawl of URL: https://example.com/page4
2025-02-06 09:25:47,643 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 09:25:47,644 [DEBUG] Base URL: https://example.com/page4
2025-02-06 09:25:47,645 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 09:25:47,645 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 09:25:47,646 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 09:25:47,646 [INFO] Successfully crawled https://example.com/page4
2025-02-06 09:25:47,647 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 09:25:47,647 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2025-02-06 09:25:47,647 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2025-02-06 09:25:47,648 [INFO] Starting crawl of URL: https://example.com/page0
2025-02-06 09:25:47,999 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 09:25:48,000 [DEBUG] Base URL: https://example.com/page0
2025-02-06 09:25:48,001 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 09:25:48,001 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 09:25:48,002 [DEBUG] Initial domain set to: example.com
2025-02-06 09:25:48,002 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 09:25:48,002 [INFO] Successfully crawled https://example.com/page0
2025-02-06 09:25:48,003 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2025-02-06 09:25:48,003 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2025-02-06 09:25:48,003 [INFO] Starting crawl of URL: https://example.com/page1
2025-02-06 09:25:48,315 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 09:25:48,316 [DEBUG] Base URL: https://example.com/page1
2025-02-06 09:25:48,316 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 09:25:48,316 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 09:25:48,317 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 09:25:48,317 [INFO] Successfully crawled https://example.com/page1
2025-02-06 09:25:48,318 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2025-02-06 09:25:48,318 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2025-02-06 09:25:48,318 [INFO] Starting crawl of URL: https://example.com/page2
2025-02-06 09:25:48,813 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 09:25:48,814 [DEBUG] Base URL: https://example.com/page2
2025-02-06 09:25:48,814 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 09:25:48,814 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 09:25:48,814 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 09:25:48,815 [INFO] Successfully crawled https://example.com/page2
2025-02-06 09:25:48,815 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2025-02-06 09:25:48,815 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2025-02-06 09:25:48,816 [INFO] Starting crawl of URL: https://example.com/page3
2025-02-06 09:25:49,311 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 09:25:49,312 [DEBUG] Base URL: https://example.com/page3
2025-02-06 09:25:49,312 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 09:25:49,312 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 09:25:49,312 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 09:25:49,312 [INFO] Successfully crawled https://example.com/page3
2025-02-06 09:25:49,313 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2025-02-06 09:25:49,313 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2025-02-06 09:25:49,313 [INFO] Starting crawl of URL: https://example.com/page4
2025-02-06 09:25:49,826 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 09:25:49,826 [DEBUG] Base URL: https://example.com/page4
2025-02-06 09:25:49,826 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 09:25:49,826 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 09:25:49,826 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 09:25:49,826 [INFO] Successfully crawled https://example.com/page4
2025-02-06 09:25:49,836 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:49,841 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:49,881 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:49,884 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:49,901 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:49,904 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:49,928 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:49,930 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:49,949 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:49,953 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:49,967 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001DBF17FB890>
2025-02-06 09:25:49,967 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001DBF1C0C6B0>, 159377.6386415)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001DBF17496E0>
2025-02-06 09:25:49,968 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001DBF166F610>
2025-02-06 09:25:49,968 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001DBF17F91D0>
2025-02-06 09:25:49,968 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001DBF1CB8530>, 159383.3566521)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001DBF1C21550>, 159384.1659821)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001DBF1C23830>, 159384.6280605)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001DBF1C21B50>, 159385.3673894)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001DBF1831810>
2025-02-06 09:25:49,969 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001DBF17F9A90>
2025-02-06 09:25:49,969 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001DBF1C6EF30>, 159389.0734809)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001DBF1831A70>
2025-02-06 09:25:49,970 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001DBF17F8190>
2025-02-06 09:25:49,970 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001DBF20CFCB0>, 159391.2586505)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001DBF1830510>
2025-02-06 09:25:49,986 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:49,991 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:50,004 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:50,009 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:50,035 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:50,037 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:50,052 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:50,058 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:50,077 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:50,166 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:25:50,167 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:50,170 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:50,177 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:25:50,179 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:50,182 [DEBUG] 
Evaluating link: https://example.com/doc1
2025-02-06 09:25:50,182 [DEBUG] Target domain: example.com
2025-02-06 09:25:50,182 [DEBUG] URL domain: example.com
2025-02-06 09:25:50,182 [DEBUG] URL approved for crawling: https://example.com/doc1
2025-02-06 09:25:50,183 [DEBUG] 
Evaluating link: https://example.com/excluded/page
2025-02-06 09:25:50,184 [DEBUG] Target domain: example.com
2025-02-06 09:25:50,185 [DEBUG] URL domain: example.com
2025-02-06 09:25:50,185 [DEBUG] URL approved for crawling: https://example.com/excluded/page
2025-02-06 09:25:50,211 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:50,213 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:25:50,214 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:50,230 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:50,264 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:25:50,265 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:50,267 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-06 09:25:50,330 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:50,332 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:25:50,333 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:50,351 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:50,356 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:25:50,359 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:50,362 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-06 09:25:50,414 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:50,417 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:25:50,418 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:50,423 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-06 09:25:50,483 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:50,486 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:25:50,487 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:50,492 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-06 09:25:50,536 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:50,543 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:25:50,545 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:50,548 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-06 09:25:50,727 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:50,731 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:25:50,733 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:50,736 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-06 09:25:50,829 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:50,831 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:25:50,832 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:50,835 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-06 09:25:50,908 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:50,915 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:51,418 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:51,422 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:51,425 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:25:51,461 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:51,463 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:51,465 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:25:51,486 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:51,498 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:25:51,551 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:25:51,579 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:25:51,612 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:25:51,644 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:25:51,667 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:25:51,684 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001DBF17FAFD0>
2025-02-06 09:25:51,684 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001DBF1DC9490>, 159392.5406447)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001DBF1DC9C70>, 159392.7812376)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001DBF1749940>
2025-02-06 09:25:51,710 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:25:51,742 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:25:51,770 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:25:51,800 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:25:52,717 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:53,577 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:25:53,580 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 09:25:53,580 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-06 09:25:53,581 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-06 09:25:53,581 [INFO] Starting crawl of URL: https://example.com
2025-02-06 09:25:53,906 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 09:25:53,906 [DEBUG] Base URL: https://example.com
2025-02-06 09:25:53,906 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 09:25:53,906 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 09:25:53,906 [DEBUG] Initial domain set to: example.com
2025-02-06 09:25:53,906 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 09:25:53,907 [INFO] Successfully crawled https://example.com
2025-02-06 09:25:53,907 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001DBF2134F50>
2025-02-06 09:25:53,907 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001DBF2150230>, 159395.3387428)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001DBF1833950>
2025-02-06 09:25:53,910 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2025-02-06 09:25:54,241 [INFO] Operation took 0.10 seconds
2025-02-06 09:25:55,108 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 09:25:55,108 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:25:55,109 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:02,819 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-1\test_full_site_crawl0\test_docs/index.html
2025-02-06 09:26:02,819 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-06 09:26:02,917 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:02,933 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 09:26:02,933 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:26:02,934 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:10,999 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-1\test_content_processing_pipeli0\test_docs/guide.html
2025-02-06 09:26:11,000 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-06 09:26:11,031 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:11,044 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 09:26:11,044 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:26:11,045 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:18,473 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-1\test_quality_checks0\test_docs/api.html
2025-02-06 09:26:18,474 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-06 09:26:18,530 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:18,543 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 09:26:18,544 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:26:18,546 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:25,980 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-1\test_document_organization0\test_docs/index.html
2025-02-06 09:26:25,980 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-06 09:26:26,017 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:26,031 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 09:26:26,032 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:26:26,033 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:33,606 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-1\test_search_functionality0\test_docs/index.html
2025-02-06 09:26:33,607 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-06 09:26:33,648 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:33,661 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 09:26:33,661 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:26:33,662 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:41,213 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-1\test_error_handling_and_recove0\test_docs/index.html
2025-02-06 09:26:41,214 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-06 09:26:41,251 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:41,254 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:41,297 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:41,300 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:41,302 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:26:41,302 [ERROR] Error during crawl: 'str' object has no attribute 'url'
2025-02-06 09:26:41,319 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:41,322 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:41,348 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:41,351 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:41,354 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 09:26:41,382 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:44,409 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:44,475 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:44,479 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:44,501 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:44,510 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:44,529 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:44,531 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:44,552 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:44,559 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:44,583 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:44,586 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:44,609 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:44,612 [DEBUG] Using proactor: IocpProactor
2025-02-06 09:26:44,630 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:21:12,589 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 10:21:32,906 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 10:21:36,436 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:21:36,438 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:21:36,815 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:22:38,598 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 10:22:40,665 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:22:40,669 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:22:41,016 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:24:26,149 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 10:24:41,223 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 10:24:43,012 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:24:43,014 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:24:43,018 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:24:43,023 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:24:43,393 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:25:17,883 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 10:25:19,485 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:25:19,489 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:25:19,492 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:25:19,496 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:25:19,499 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:25:19,503 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:25:19,506 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 10:25:19,991 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:25:35,797 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 10:25:37,497 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:25:37,499 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:25:37,504 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:25:37,508 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:25:37,511 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:25:37,514 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:25:37,516 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 10:25:37,915 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:26:15,145 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 10:26:16,797 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:26:16,800 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:26:16,804 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:26:16,807 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:26:16,811 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:26:16,815 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:26:16,819 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 10:26:17,254 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:26:32,927 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 10:26:34,504 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:26:34,506 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:26:34,509 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:26:34,513 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:26:34,518 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:26:34,521 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:26:34,524 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 10:26:34,526 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:26:34,530 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:26:34,534 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 10:26:34,977 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:33:14,705 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 10:33:16,393 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:33:16,396 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:33:16,399 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:33:16,404 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:33:16,408 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:33:16,411 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:33:16,413 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 10:33:16,415 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:33:16,418 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:33:16,423 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 10:33:16,966 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:33:45,030 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 10:33:46,779 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:33:46,781 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:33:46,784 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:33:46,788 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:33:46,794 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:33:46,797 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:33:46,799 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 10:33:46,801 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:33:46,805 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:33:46,809 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 10:33:47,303 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:34:35,220 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 10:34:36,981 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:34:36,983 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:34:36,986 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:34:36,990 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:34:36,996 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:34:36,999 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:34:37,002 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 10:34:37,004 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:34:37,008 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:34:37,012 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 10:34:37,580 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:35:10,019 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 10:35:11,833 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:35:11,836 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:35:11,839 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:35:11,842 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:35:11,848 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:35:11,851 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:35:11,853 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 10:35:11,855 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:35:11,859 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:35:11,862 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 10:35:12,379 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:38:15,764 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 10:38:17,675 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:38:17,677 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:38:17,681 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:38:17,687 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:38:17,692 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:38:17,696 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:38:17,700 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 10:38:17,705 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:38:17,708 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:38:17,710 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 10:38:18,257 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:53:02,642 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 10:53:03,844 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:53:03,846 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:53:03,850 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:53:03,853 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:53:03,857 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:53:03,859 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:53:03,861 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 10:53:03,864 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:53:03,867 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:53:03,870 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 10:53:04,561 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:53:52,954 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 10:53:54,451 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:53:54,453 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:53:54,457 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:53:54,461 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:53:54,464 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:53:54,466 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:53:54,468 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 10:53:54,469 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:53:54,473 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:53:54,478 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 10:53:54,481 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:53:54,484 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:53:54,818 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:55:15,567 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 10:55:17,174 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:55:17,176 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:55:17,182 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:55:17,186 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:55:17,189 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:55:17,194 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:55:17,197 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 10:55:17,200 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:55:17,203 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:55:17,206 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 10:55:17,209 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:55:17,213 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:55:17,556 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:56:34,808 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 10:56:36,263 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:56:36,265 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:56:36,268 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:56:36,272 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:56:36,310 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:56:36,312 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:56:36,317 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 10:56:36,319 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:56:36,327 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:56:36,330 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 10:56:36,335 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:56:36,344 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:56:36,755 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:56:58,639 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 10:57:00,169 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:57:00,171 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:57:00,174 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:57:00,177 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:57:00,180 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:57:00,185 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:57:00,187 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 10:57:00,189 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:57:00,191 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:57:00,192 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 10:57:00,194 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:57:00,196 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:57:00,603 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:57:31,915 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 10:57:33,480 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:57:33,482 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:57:33,488 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:57:33,491 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:57:33,494 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:57:33,496 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:57:33,498 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 10:57:33,503 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:57:33,505 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:57:33,507 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 10:57:33,511 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:57:33,513 [DEBUG] Using proactor: IocpProactor
2025-02-06 10:57:33,851 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:03:37,430 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 11:03:39,005 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:03:39,009 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:03:39,012 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:03:39,015 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:03:39,018 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:03:39,020 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:03:39,023 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:03:39,026 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:03:39,029 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:03:39,030 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:03:39,033 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:03:39,036 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:03:39,380 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:04:08,722 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 11:04:10,084 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:04:10,086 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:04:10,089 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:04:10,092 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:04:10,097 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:04:10,100 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:04:10,102 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:04:10,103 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:04:10,105 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:04:10,107 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:04:10,113 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:04:10,116 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:04:10,570 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:07:24,799 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 11:07:27,070 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:07:27,072 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:07:27,075 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:07:27,081 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:07:27,084 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:07:27,086 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:07:27,088 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:07:27,090 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:07:27,092 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:07:27,096 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:07:27,099 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:07:27,102 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:07:27,537 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:08:17,990 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 11:08:20,486 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:08:20,489 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:08:20,497 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:08:20,500 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:08:20,504 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:08:20,507 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:08:20,514 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:08:20,516 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:08:20,519 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:08:20,522 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:08:20,532 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:08:20,535 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:08:20,990 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:13:56,025 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 11:14:18,847 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 11:14:22,279 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:14:22,282 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:14:22,285 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:14:22,289 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:14:22,292 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:14:22,294 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:14:22,296 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:14:22,297 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:14:22,299 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:14:22,301 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:14:22,308 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:14:22,311 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:14:22,817 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:15:21,015 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 11:15:23,021 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:15:23,023 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:15:23,025 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:15:23,032 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:15:23,034 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:15:23,036 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:15:23,038 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:15:23,040 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:15:23,042 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:15:23,043 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:15:23,050 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:15:23,052 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:15:23,057 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:15:23,059 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:15:23,443 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:16:08,104 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 11:16:10,060 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:16:10,062 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:16:10,065 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:16:10,072 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:16:10,075 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:16:10,077 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:16:10,079 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:16:10,081 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:16:10,083 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:16:10,086 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:16:10,090 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:16:10,093 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:16:10,098 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:16:10,105 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:16:10,482 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:10,708 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 11:22:12,781 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:12,784 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:12,789 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:12,797 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:12,802 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:12,805 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:12,814 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:22:12,816 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:12,820 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:12,822 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:22:12,829 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:12,833 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:12,846 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:12,852 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:12,856 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:12,866 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:13,434 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:37,153 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 11:22:38,947 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:38,950 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:38,954 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:38,958 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:38,963 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:38,965 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:38,969 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:22:38,971 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:38,975 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:38,977 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:22:38,979 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:38,981 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:38,985 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:38,989 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:38,994 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:38,997 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:39,002 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:39,009 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:39,474 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:22:58,307 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 11:23:00,319 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:00,321 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:00,324 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:00,327 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:00,330 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:00,335 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:00,338 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:23:00,339 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:00,341 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:00,345 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:23:00,347 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:00,354 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:00,357 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:00,359 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:00,362 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:00,369 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:00,372 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:00,374 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:00,753 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:37,621 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 11:23:39,483 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:39,485 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:39,488 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:39,492 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:39,498 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:39,500 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:39,502 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:23:39,504 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:39,509 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:39,512 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:23:39,516 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:39,518 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:39,523 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:39,526 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:39,534 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:39,536 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:39,541 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:39,543 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:23:39,925 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:30:53,376 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 11:31:12,884 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 11:31:15,944 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:15,946 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:15,951 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:15,955 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:15,958 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:15,960 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:15,962 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:31:15,963 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:15,968 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:15,971 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:31:15,973 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:15,977 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:15,981 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:15,986 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:15,990 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:15,995 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:16,000 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:16,003 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:16,528 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:42,437 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 11:31:44,694 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:44,696 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:44,699 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:44,702 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:44,708 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:44,711 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:44,713 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:31:44,714 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:44,719 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:44,726 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:31:44,728 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:44,730 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:44,736 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:44,742 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:44,745 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:44,750 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:44,753 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:44,755 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:44,762 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:44,764 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:31:44,767 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:31:45,161 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:07,131 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 11:32:09,045 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:09,047 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:09,054 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:09,057 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:09,060 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:09,062 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:09,064 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:32:09,069 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:09,071 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:09,073 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:32:09,077 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:09,080 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:09,086 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:09,089 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:09,091 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:09,094 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:09,098 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:09,105 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:09,109 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:09,113 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:09,120 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:32:09,597 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:41,727 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 11:32:44,172 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:44,174 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:44,177 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:44,179 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:44,182 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:44,186 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:44,189 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:32:44,190 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:44,192 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:44,194 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:32:44,196 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:44,198 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:44,205 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:44,207 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:44,210 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:44,214 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:44,221 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:44,224 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:44,229 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:44,231 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:32:44,234 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:32:44,622 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:33:13,948 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 11:33:39,200 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 11:33:57,884 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 11:33:59,696 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:33:59,699 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:33:59,703 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:33:59,706 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:33:59,709 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:33:59,711 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:33:59,714 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:33:59,718 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:33:59,720 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:33:59,722 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:33:59,727 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:33:59,730 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:33:59,737 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:33:59,740 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:33:59,744 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:33:59,747 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:33:59,755 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:33:59,758 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:33:59,764 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:33:59,769 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:33:59,772 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:34:00,160 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:34:56,237 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 11:35:22,563 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 11:35:24,842 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:24,844 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:24,847 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:24,853 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:24,856 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:24,858 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:24,860 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:35:24,861 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:24,863 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:24,870 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:35:24,873 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:24,877 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:24,881 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:24,887 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:24,891 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:24,893 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:24,988 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:24,991 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:25,037 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:25,040 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:25,047 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:35:25,441 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:55,264 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 11:35:57,340 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:57,342 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:57,345 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:57,347 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:57,353 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:57,356 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:57,358 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:35:57,360 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:57,365 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:57,367 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:35:57,372 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:57,375 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:57,381 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:57,383 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:57,389 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:57,391 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:57,396 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:57,399 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:57,404 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:57,407 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:35:57,409 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:35:57,797 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:36:26,379 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 11:36:28,498 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:36:28,500 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:36:28,504 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:36:28,509 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:36:28,512 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:36:28,514 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:36:28,516 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:36:28,517 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:36:28,519 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:36:28,521 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:36:28,527 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:36:28,530 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:36:28,535 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:36:28,541 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:36:28,546 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:36:28,548 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:36:28,554 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:36:28,559 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:36:28,563 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:36:28,568 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:36:28,569 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:36:28,571 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:16,712 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 11:37:19,032 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:19,034 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:19,037 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:19,040 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:19,044 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:19,047 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:19,049 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:37:19,050 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:19,052 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:19,054 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:37:19,056 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:19,059 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:19,064 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:19,068 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:19,071 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:19,077 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:19,085 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:19,088 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:19,092 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:19,100 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:19,102 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:37:19,104 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:56,195 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 11:37:58,564 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:58,566 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:58,570 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:58,577 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:58,582 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:58,585 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:58,587 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:37:58,588 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:58,591 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:58,596 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:37:58,599 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:58,602 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:58,609 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:58,616 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:58,623 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:58,625 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:58,635 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:58,637 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:58,642 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:58,644 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:37:58,650 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:37:58,653 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:38:33,221 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 11:38:35,195 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:38:35,197 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:38:35,200 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:38:35,203 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:38:35,206 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:38:35,211 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:38:35,214 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:38:35,216 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:38:35,218 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:38:35,221 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:38:35,223 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:38:35,229 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:38:35,233 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:38:35,235 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:38:35,238 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:38:35,240 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:38:35,243 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:38:35,249 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:38:35,252 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:38:35,254 [DEBUG] Using proactor: IocpProactor
2025-02-06 11:38:35,258 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 11:38:35,259 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:01:32,138 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 12:01:36,466 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:01:36,501 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:01:36,504 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:01:36,553 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:01:36,561 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:01:36,563 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:01:36,565 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 12:01:36,567 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:01:36,569 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:01:36,571 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 12:01:36,577 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:01:36,580 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:01:36,585 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:01:36,587 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:01:36,594 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:01:36,597 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:01:36,600 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:01:36,602 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:01:36,609 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:01:36,611 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:01:36,613 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 12:01:36,617 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:03:42,201 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 12:03:44,781 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:03:44,786 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:03:44,798 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:03:44,805 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:03:44,817 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:03:44,824 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:03:44,828 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 12:03:44,843 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:03:44,845 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:03:44,850 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 12:03:44,859 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:03:44,864 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:03:44,883 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:03:44,898 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:03:44,926 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:03:44,929 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:03:44,934 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:03:44,940 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:03:44,946 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:03:44,949 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:03:44,955 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 12:03:44,958 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:00,350 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 12:05:02,975 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:02,978 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:02,985 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:02,989 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:02,992 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:02,996 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:03,001 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 12:05:03,003 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:03,005 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:03,010 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 12:05:03,017 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:03,021 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:03,027 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:03,034 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:03,038 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:03,043 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:03,053 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:03,056 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:03,065 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:03,069 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:03,071 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 12:05:03,075 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:39,232 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 12:05:41,953 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:41,957 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:41,982 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:41,989 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:42,063 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:42,066 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:42,069 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 12:05:42,071 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:42,074 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:42,077 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 12:05:42,081 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:42,084 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:42,090 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:42,094 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:42,100 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:42,107 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:42,117 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:42,119 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:42,127 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:42,130 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:05:42,133 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 12:05:42,136 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:06:39,993 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 12:06:41,827 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:06:41,832 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:06:41,839 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:06:41,843 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:06:41,851 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:06:41,860 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:06:41,864 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 12:06:41,868 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:06:41,872 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:06:41,875 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 12:06:41,878 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:06:41,881 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:06:41,890 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:06:41,893 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:06:41,896 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:06:41,905 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:06:41,964 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:06:42,005 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:06:42,060 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:06:42,092 [DEBUG] Using proactor: IocpProactor
2025-02-06 12:06:42,094 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 12:06:42,115 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:44:03,536 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 14:44:06,153 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:44:06,165 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:44:06,202 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:44:06,213 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:44:06,229 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:44:06,232 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:44:06,236 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 14:44:06,245 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:44:06,247 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:44:06,249 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 14:44:06,251 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:44:06,258 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:44:06,264 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:44:06,267 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:44:06,276 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:44:06,279 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:44:06,284 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:44:06,291 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:44:06,300 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:44:06,307 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:44:06,311 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 14:44:06,314 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:45:40,020 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 14:45:42,941 [INFO] Starting _process_content
2025-02-06 14:45:42,945 [ERROR] Exception in _process_content: 'NoneType' object is not callable
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 778, in _process_content
    self._process_links(body, base_url)
    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 467, in _process_links
    new_tag = soup.new_tag('span')
TypeError: 'NoneType' object is not callable
2025-02-06 14:46:47,551 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 14:46:49,716 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:46:49,726 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:46:49,730 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:46:49,737 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:46:49,742 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:46:49,745 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:46:49,747 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 14:46:49,749 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:46:49,757 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:46:49,760 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 14:46:49,763 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:46:49,766 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:46:49,774 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:46:49,777 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:46:49,780 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:46:49,785 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:46:49,792 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:46:49,795 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:46:49,799 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:46:49,808 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:46:49,811 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 14:46:49,823 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:46:49,830 [INFO] Starting _process_content
2025-02-06 14:46:49,842 [INFO] Starting _process_content
2025-02-06 14:46:49,858 [INFO] Starting _process_content
2025-02-06 14:46:49,893 [INFO] Starting _process_content
2025-02-06 14:46:49,928 [INFO] Starting _process_content
2025-02-06 14:46:49,941 [INFO] Starting _process_content
2025-02-06 14:46:50,011 [INFO] Starting _process_content
2025-02-06 14:47:47,405 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 14:47:49,835 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:47:49,838 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:47:49,873 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:47:49,890 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:47:49,937 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:47:49,942 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:47:49,954 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 14:47:49,984 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:47:49,987 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:47:49,990 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 14:47:50,024 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:47:50,032 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:47:50,126 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:47:50,130 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:47:50,135 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:47:50,141 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:47:50,152 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:47:50,155 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:47:50,159 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:47:50,166 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:47:50,168 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 14:47:50,170 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:47:50,185 [INFO] Starting _process_content
2025-02-06 14:47:50,196 [INFO] Starting _process_content
2025-02-06 14:47:50,201 [INFO] Starting _process_content
2025-02-06 14:47:50,213 [INFO] Starting _process_content
2025-02-06 14:47:50,222 [INFO] Starting _process_content
2025-02-06 14:47:50,241 [INFO] Starting _process_content
2025-02-06 14:47:50,253 [INFO] Starting _process_content
2025-02-06 14:49:00,559 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 14:49:08,609 [INFO] Starting _process_content
2025-02-06 14:50:59,645 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 14:51:06,971 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:51:06,982 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:51:06,998 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:51:07,002 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:51:07,017 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:51:07,021 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:51:07,034 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 14:51:07,039 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:51:07,049 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:51:07,053 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 14:51:07,067 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:51:07,070 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:51:07,101 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:51:07,238 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:51:07,390 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:51:07,395 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:51:07,432 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:51:07,434 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:51:07,503 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:51:07,507 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:51:07,520 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 14:51:07,538 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:51:07,554 [INFO] Starting _process_content
2025-02-06 14:51:07,617 [INFO] Starting _process_content
2025-02-06 14:51:07,649 [INFO] Starting _process_content
2025-02-06 14:51:07,674 [INFO] Starting _process_content
2025-02-06 14:51:07,700 [INFO] Starting _process_content
2025-02-06 14:51:07,733 [INFO] Starting _process_content
2025-02-06 14:51:07,813 [INFO] Starting _process_content
2025-02-06 14:52:58,899 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 14:53:45,567 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 14:53:50,142 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:53:50,145 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:53:50,150 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:53:50,157 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:53:50,164 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:53:50,167 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:53:50,177 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 14:53:50,180 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:53:50,190 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:53:50,194 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 14:53:50,199 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:53:50,208 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:53:50,237 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:53:50,247 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:53:50,263 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:53:50,267 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:53:50,281 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:53:50,285 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:53:50,297 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:53:50,302 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:53:50,311 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 14:53:50,314 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:55:27,843 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 14:55:33,732 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:55:33,743 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:55:33,751 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:55:33,781 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:55:33,806 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:55:33,812 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:55:33,817 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 14:55:33,823 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:55:33,829 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:55:33,833 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 14:55:33,851 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:55:33,864 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:55:33,886 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:55:33,890 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:55:33,904 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:55:33,916 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:55:33,922 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:55:33,935 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:55:33,948 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:55:33,951 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:55:33,963 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 14:55:33,968 [DEBUG] Using proactor: IocpProactor
2025-02-06 14:55:34,164 [INFO] Starting _process_content
2025-02-06 15:11:00,367 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 15:12:52,526 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 15:16:42,418 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 15:16:54,417 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:16:54,420 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:16:54,429 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:16:54,434 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:16:54,446 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:16:54,451 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:16:54,458 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 15:16:54,463 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:16:54,468 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:16:54,479 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 15:16:54,482 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:16:54,495 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:16:54,513 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:16:54,519 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:16:54,533 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:16:54,545 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:16:54,553 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:16:54,566 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:16:54,580 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:16:54,586 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:16:54,599 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 15:16:54,603 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:16:54,620 [INFO] Starting _process_content
2025-02-06 15:19:40,537 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 15:19:47,354 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:19:47,357 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:19:47,366 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:19:47,372 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:19:47,385 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:19:47,394 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:19:47,401 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 15:19:47,403 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:19:47,407 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:19:47,416 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 15:19:47,418 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:19:47,421 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:19:47,428 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:19:47,434 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:19:47,447 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:19:47,451 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:19:47,456 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:19:47,463 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:19:47,468 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:19:47,471 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:19:47,480 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 15:19:47,482 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:19:47,490 [INFO] Starting _process_content
2025-02-06 15:19:47,501 [INFO] Starting _process_content
2025-02-06 15:21:47,257 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 15:21:51,280 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:21:51,283 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:21:51,306 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:21:51,335 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:21:51,351 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:21:51,363 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:21:51,367 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 15:21:51,373 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:21:51,386 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:21:51,397 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 15:21:51,401 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:21:51,406 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:21:51,419 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:21:51,423 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:21:51,428 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:21:51,432 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:21:51,437 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:21:51,441 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:21:51,449 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:21:51,453 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:21:51,458 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 15:21:51,466 [DEBUG] Using proactor: IocpProactor
2025-02-06 15:21:51,480 [INFO] Starting _process_content
2025-02-06 15:21:51,497 [INFO] Starting _process_content
2025-02-06 15:21:51,514 [INFO] Starting _process_content
2025-02-06 15:24:21,314 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 15:24:21,655 [INFO] Starting _process_content
2025-02-06 15:24:21,664 [INFO] Starting _process_content
2025-02-06 15:24:21,677 [INFO] Starting _process_content
2025-02-06 15:25:34,454 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 15:25:34,892 [INFO] Starting _process_content
2025-02-06 15:25:34,903 [INFO] Starting _process_content
2025-02-06 15:25:34,919 [INFO] Starting _process_content
2025-02-06 15:27:00,344 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 15:27:00,639 [INFO] Starting _process_content
2025-02-06 15:27:00,647 [INFO] Starting _process_content
2025-02-06 15:27:00,660 [INFO] Starting _process_content
2025-02-06 15:29:15,685 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 15:29:15,970 [INFO] Starting _process_content
2025-02-06 15:29:15,980 [INFO] Starting _process_content
2025-02-06 15:29:15,995 [INFO] Starting _process_content
2025-02-06 15:30:40,901 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 15:30:41,207 [INFO] Starting _process_content
2025-02-06 15:30:41,216 [INFO] Starting _process_content
2025-02-06 15:30:41,224 [INFO] Starting _process_content
2025-02-06 15:33:38,756 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 15:33:39,019 [INFO] Starting _process_content
2025-02-06 15:33:39,028 [INFO] Starting _process_content
2025-02-06 15:33:39,051 [INFO] Starting _process_content
2025-02-06 15:35:25,417 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 15:35:26,141 [INFO] Starting _process_content
2025-02-06 15:35:26,148 [INFO] Starting _process_content
2025-02-06 15:35:26,157 [INFO] Starting _process_content
2025-02-06 15:35:26,170 [INFO] Starting _process_content
2025-02-06 15:36:56,695 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 15:36:57,498 [INFO] Starting _process_content
2025-02-06 15:36:57,505 [INFO] Starting _process_content
2025-02-06 15:36:57,520 [INFO] Starting _process_content
2025-02-06 15:36:57,546 [INFO] Starting _process_content
2025-02-06 15:36:57,556 [INFO] Starting _process_content
2025-02-06 15:38:22,080 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 15:38:22,813 [INFO] Starting _process_content
2025-02-06 15:38:22,820 [INFO] Starting _process_content
2025-02-06 15:38:22,838 [INFO] Starting _process_content
2025-02-06 15:38:22,857 [INFO] Starting _process_content
2025-02-06 15:38:22,868 [INFO] Starting _process_content
2025-02-06 15:38:22,878 [INFO] Starting _process_content
2025-02-06 15:41:08,489 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 15:41:09,169 [INFO] Starting _process_content
2025-02-06 15:41:09,176 [INFO] Starting _process_content
2025-02-06 15:41:09,183 [INFO] Starting _process_content
2025-02-06 15:41:09,195 [INFO] Starting _process_content
2025-02-06 15:41:09,205 [INFO] Starting _process_content
2025-02-06 15:41:09,212 [INFO] Starting _process_content
2025-02-06 15:41:09,222 [INFO] Starting _process_content
2025-02-06 15:41:09,228 [INFO] Starting _process_content
2025-02-06 15:41:09,238 [INFO] Starting _process_content
2025-02-06 15:41:09,244 [INFO] Starting _process_content
2025-02-06 15:41:09,254 [INFO] Starting _process_content
2025-02-06 15:41:09,260 [INFO] Starting _process_content
2025-02-06 15:41:09,271 [INFO] Starting _process_content
2025-02-06 15:41:09,277 [INFO] Starting _process_content
2025-02-06 15:41:09,280 [INFO] Starting _process_content
2025-02-06 15:41:09,292 [INFO] Starting _process_content
2025-02-06 15:41:09,296 [INFO] Starting _process_content
2025-02-06 15:43:07,669 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 15:43:08,316 [INFO] Starting _process_content
2025-02-06 15:43:08,328 [INFO] Starting _process_content
2025-02-06 15:43:08,338 [INFO] Starting _process_content
2025-02-06 15:43:08,350 [INFO] Starting _process_content
2025-02-06 15:43:08,361 [INFO] Starting _process_content
2025-02-06 15:43:08,367 [INFO] Starting _process_content
2025-02-06 15:43:08,380 [INFO] Starting _process_content
2025-02-06 15:43:08,398 [INFO] Starting _process_content
2025-02-06 15:43:08,412 [INFO] Starting _process_content
2025-02-06 15:43:08,430 [INFO] Starting _process_content
2025-02-06 15:43:08,441 [INFO] Starting _process_content
2025-02-06 15:43:08,450 [INFO] Starting _process_content
2025-02-06 15:43:08,468 [INFO] Starting _process_content
2025-02-06 15:43:08,480 [INFO] Starting _process_content
2025-02-06 15:43:08,492 [INFO] Starting _process_content
2025-02-06 15:43:08,500 [INFO] Starting _process_content
2025-02-06 15:43:08,513 [INFO] Starting _process_content
2025-02-06 15:43:08,522 [INFO] Starting _process_content
2025-02-06 15:43:09,062 [INFO] Starting _process_content
2025-02-06 15:43:09,093 [INFO] Starting _process_content
2025-02-06 15:43:09,258 [INFO] Starting _process_content
2025-02-06 15:43:09,391 [INFO] Starting _process_content
2025-02-06 15:43:09,398 [INFO] Starting _process_content
2025-02-06 15:43:09,410 [INFO] Starting _process_content
2025-02-06 15:43:09,510 [INFO] Starting _process_content
2025-02-06 15:43:09,517 [INFO] Starting _process_content
2025-02-06 15:43:09,637 [INFO] Starting _process_content
2025-02-06 15:43:09,746 [INFO] Starting _process_content
2025-02-06 15:43:09,757 [INFO] Starting _process_content
2025-02-06 15:43:09,758 [ERROR] Exception in _process_content: 'NoneType' object is not callable
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 802, in _process_content
    self._process_images(body, base_url)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 578, in _process_images
    new_tag = soup.new_tag('span')
TypeError: 'NoneType' object is not callable
2025-02-06 15:43:09,859 [INFO] Starting _process_content
2025-02-06 15:43:09,860 [ERROR] Exception in _process_content: 'NoneType' object is not callable
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 802, in _process_content
    self._process_images(body, base_url)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 563, in _process_images
    new_tag = soup.new_tag('span')
TypeError: 'NoneType' object is not callable
2025-02-06 15:43:09,867 [INFO] Starting _process_content
2025-02-06 15:43:09,869 [ERROR] Exception in _process_content: 'NoneType' object is not callable
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 802, in _process_content
    self._process_images(body, base_url)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 578, in _process_images
    new_tag = soup.new_tag('span')
TypeError: 'NoneType' object is not callable
2025-02-06 15:43:09,965 [INFO] Starting _process_content
2025-02-06 15:43:09,975 [INFO] Starting _process_content
2025-02-06 15:43:10,046 [INFO] Starting _process_content
2025-02-06 15:43:10,055 [INFO] Starting _process_content
2025-02-06 15:43:10,132 [INFO] Starting _process_content
2025-02-06 15:43:10,199 [INFO] Starting _process_content
2025-02-06 15:43:10,213 [INFO] Starting _process_content
2025-02-06 15:43:10,226 [INFO] Starting _process_content
2025-02-06 15:43:10,236 [INFO] Starting _process_content
2025-02-06 15:43:10,238 [ERROR] Exception in _process_content: 'NoneType' object is not callable
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 802, in _process_content
    self._process_images(body, base_url)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 578, in _process_images
    new_tag = soup.new_tag('span')
TypeError: 'NoneType' object is not callable
2025-02-06 15:43:10,314 [INFO] Starting _process_content
2025-02-06 15:43:10,315 [ERROR] Exception in _process_content: 'NoneType' object is not callable
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 802, in _process_content
    self._process_images(body, base_url)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 578, in _process_images
    new_tag = soup.new_tag('span')
TypeError: 'NoneType' object is not callable
2025-02-06 15:43:10,409 [INFO] Starting _process_content
2025-02-06 15:43:10,410 [ERROR] Exception in _process_content: 'NoneType' object is not callable
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 802, in _process_content
    self._process_images(body, base_url)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 563, in _process_images
    new_tag = soup.new_tag('span')
TypeError: 'NoneType' object is not callable
2025-02-06 15:43:10,415 [INFO] Starting _process_content
2025-02-06 15:43:10,416 [ERROR] Exception in _process_content: 'NoneType' object is not callable
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 802, in _process_content
    self._process_images(body, base_url)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 578, in _process_images
    new_tag = soup.new_tag('span')
TypeError: 'NoneType' object is not callable
2025-02-06 15:43:10,491 [INFO] Starting _process_content
2025-02-06 15:43:10,498 [INFO] Starting _process_content
2025-02-06 15:43:10,508 [INFO] Starting _process_content
2025-02-06 15:43:10,682 [INFO] Starting _process_content
2025-02-06 15:43:10,844 [INFO] Starting _process_content
2025-02-06 15:43:10,914 [INFO] Starting _process_content
2025-02-06 15:43:11,062 [INFO] Starting _process_content
2025-02-06 15:43:11,074 [INFO] Starting _process_content
2025-02-06 15:43:11,085 [INFO] Starting _process_content
2025-02-06 15:43:11,097 [INFO] Starting _process_content
2025-02-06 15:43:11,113 [INFO] Starting _process_content
2025-02-06 15:43:11,130 [INFO] Starting _process_content
2025-02-06 15:43:11,149 [INFO] Starting _process_content
2025-02-06 15:43:11,164 [INFO] Starting _process_content
2025-02-06 15:43:11,245 [INFO] Starting _process_content
2025-02-06 15:43:11,254 [INFO] Starting _process_content
2025-02-06 15:43:11,328 [INFO] Starting _process_content
2025-02-06 15:43:11,391 [INFO] Starting _process_content
2025-02-06 15:43:11,397 [INFO] Starting _process_content
2025-02-06 15:43:11,477 [INFO] Starting _process_content
2025-02-06 15:43:11,556 [INFO] Starting _process_content
2025-02-06 15:43:11,557 [ERROR] Exception in _process_content: 'NoneType' object is not callable
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 802, in _process_content
    self._process_images(body, base_url)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 578, in _process_images
    new_tag = soup.new_tag('span')
TypeError: 'NoneType' object is not callable
2025-02-06 15:43:11,710 [INFO] Starting _process_content
2025-02-06 15:43:11,779 [INFO] Starting _process_content
2025-02-06 15:43:11,787 [INFO] Starting _process_content
2025-02-06 15:43:11,793 [INFO] Starting _process_content
2025-02-06 15:43:11,808 [INFO] Starting _process_content
2025-02-06 15:43:11,927 [INFO] Starting _process_content
2025-02-06 15:43:11,978 [INFO] Starting _process_content
2025-02-06 15:43:11,998 [INFO] Starting _process_content
2025-02-06 15:43:12,043 [INFO] Starting _process_content
2025-02-06 15:43:12,044 [ERROR] Exception in _process_content: 'NoneType' object is not callable
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 802, in _process_content
    self._process_images(body, base_url)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 578, in _process_images
    new_tag = soup.new_tag('span')
TypeError: 'NoneType' object is not callable
2025-02-06 15:43:12,240 [INFO] Starting _process_content
2025-02-06 15:43:12,362 [INFO] Starting _process_content
2025-02-06 15:43:12,489 [INFO] Starting _process_content
2025-02-06 15:43:12,497 [INFO] Starting _process_content
2025-02-06 15:43:12,499 [ERROR] Exception in _process_content: 'NoneType' object is not callable
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 802, in _process_content
    self._process_images(body, base_url)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 578, in _process_images
    new_tag = soup.new_tag('span')
TypeError: 'NoneType' object is not callable
2025-02-06 15:43:12,573 [INFO] Starting _process_content
2025-02-06 15:43:12,647 [INFO] Starting _process_content
2025-02-06 15:43:12,726 [INFO] Starting _process_content
2025-02-06 15:43:12,738 [INFO] Starting _process_content
2025-02-06 15:43:12,830 [INFO] Starting _process_content
2025-02-06 15:43:12,841 [INFO] Starting _process_content
2025-02-06 15:43:12,949 [INFO] Starting _process_content
2025-02-06 15:43:12,957 [INFO] Starting _process_content
2025-02-06 15:43:12,963 [INFO] Starting _process_content
2025-02-06 15:43:12,974 [INFO] Starting _process_content
2025-02-06 15:43:13,055 [INFO] Starting _process_content
2025-02-06 15:43:13,056 [ERROR] Exception in _process_content: 'NoneType' object is not callable
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 802, in _process_content
    self._process_images(body, base_url)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 578, in _process_images
    new_tag = soup.new_tag('span')
TypeError: 'NoneType' object is not callable
2025-02-06 15:43:13,127 [INFO] Starting _process_content
2025-02-06 15:43:13,137 [INFO] Starting _process_content
2025-02-06 15:43:13,209 [INFO] Starting _process_content
2025-02-06 15:43:13,216 [INFO] Starting _process_content
2025-02-06 15:43:13,227 [INFO] Starting _process_content
2025-02-06 15:43:13,238 [INFO] Starting _process_content
2025-02-06 15:43:13,355 [INFO] Starting _process_content
2025-02-06 15:43:13,361 [INFO] Starting _process_content
2025-02-06 15:43:13,372 [INFO] Starting _process_content
2025-02-06 15:43:13,380 [INFO] Starting _process_content
2025-02-06 15:43:13,392 [INFO] Starting _process_content
2025-02-06 15:43:13,397 [INFO] Starting _process_content
2025-02-06 15:43:13,410 [INFO] Starting _process_content
2025-02-06 15:43:13,430 [INFO] Starting _process_content
2025-02-06 15:43:13,443 [INFO] Starting _process_content
2025-02-06 15:43:13,452 [INFO] Starting _process_content
2025-02-06 15:43:13,460 [INFO] Starting _process_content
2025-02-06 15:43:13,472 [INFO] Starting _process_content
2025-02-06 15:43:13,479 [INFO] Starting _process_content
2025-02-06 15:43:13,492 [INFO] Starting _process_content
2025-02-06 15:43:13,496 [INFO] Starting _process_content
2025-02-06 15:43:13,611 [INFO] Starting _process_content
2025-02-06 15:43:13,663 [INFO] Starting _process_content
2025-02-06 15:43:13,739 [INFO] Starting _process_content
2025-02-06 15:43:13,803 [INFO] Starting _process_content
2025-02-06 15:43:13,874 [INFO] Starting _process_content
2025-02-06 15:43:13,971 [INFO] Starting _process_content
2025-02-06 15:43:13,983 [INFO] Starting _process_content
2025-02-06 15:43:14,112 [INFO] Starting _process_content
2025-02-06 15:43:14,188 [INFO] Starting _process_content
2025-02-06 15:43:14,259 [INFO] Starting _process_content
2025-02-06 15:43:14,337 [INFO] Starting _process_content
2025-02-06 15:43:14,409 [INFO] Starting _process_content
2025-02-06 15:43:14,418 [INFO] Starting _process_content
2025-02-06 15:43:14,421 [ERROR] Exception in _process_content: 'NoneType' object is not callable
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 802, in _process_content
    self._process_images(body, base_url)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 578, in _process_images
    new_tag = soup.new_tag('span')
TypeError: 'NoneType' object is not callable
2025-02-06 15:43:14,492 [INFO] Starting _process_content
2025-02-06 15:43:14,569 [INFO] Starting _process_content
2025-02-06 15:43:14,575 [INFO] Starting _process_content
2025-02-06 15:43:14,576 [ERROR] Exception in _process_content: 'NoneType' object is not callable
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 802, in _process_content
    self._process_images(body, base_url)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 554, in _process_images
    new_tag = soup.new_tag('span')
TypeError: 'NoneType' object is not callable
2025-02-06 15:43:14,657 [INFO] Starting _process_content
2025-02-06 15:43:14,658 [ERROR] Exception in _process_content: 'NoneType' object is not callable
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 802, in _process_content
    self._process_images(body, base_url)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 578, in _process_images
    new_tag = soup.new_tag('span')
TypeError: 'NoneType' object is not callable
2025-02-06 15:43:14,727 [INFO] Starting _process_content
2025-02-06 15:43:14,806 [INFO] Starting _process_content
2025-02-06 15:43:14,812 [INFO] Starting _process_content
2025-02-06 15:51:37,614 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 15:51:37,768 [INFO] Starting _process_content
2025-02-06 15:51:37,779 [INFO] Starting _process_content
2025-02-06 15:51:37,788 [INFO] Starting _process_content
2025-02-06 15:51:37,800 [INFO] Starting _process_content
2025-02-06 15:52:06,604 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 15:52:06,712 [INFO] Starting _process_content
2025-02-06 15:52:06,720 [INFO] Starting _process_content
2025-02-06 15:52:06,725 [INFO] Starting _process_content
2025-02-06 15:52:06,733 [INFO] Starting _process_content
2025-02-06 15:52:06,738 [INFO] Starting _process_content
2025-02-06 15:52:06,746 [INFO] Starting _process_content
2025-02-06 15:52:07,058 [INFO] Starting _process_content
2025-02-06 15:53:15,152 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 15:53:15,339 [INFO] Starting _process_content
2025-02-06 15:53:46,916 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 15:53:47,088 [INFO] Starting _process_content
2025-02-06 15:53:47,098 [INFO] Starting _process_content
2025-02-06 15:53:47,115 [INFO] Starting _process_content
2025-02-06 15:53:47,664 [INFO] Starting _process_content
2025-02-06 15:53:47,690 [INFO] Starting _process_content
2025-02-06 15:53:47,837 [INFO] Starting _process_content
2025-02-06 15:53:47,982 [INFO] Starting _process_content
2025-02-06 15:55:27,198 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 15:55:27,414 [INFO] Starting _process_content
2025-02-06 15:56:29,276 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 15:56:29,504 [INFO] Starting _process_content
2025-02-06 16:04:23,281 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 16:04:23,413 [INFO] Starting _process_content
2025-02-06 16:04:23,419 [INFO] Starting _process_content
2025-02-06 16:04:23,429 [INFO] Starting _process_content
2025-02-06 16:04:23,441 [INFO] Starting _process_content
2025-02-06 16:04:23,451 [INFO] Starting _process_content
2025-02-06 16:04:23,840 [INFO] Starting _process_content
2025-02-06 16:04:23,850 [INFO] Starting _process_content
2025-02-06 16:05:45,883 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 16:05:46,048 [INFO] Starting _process_content
2025-02-06 16:06:50,906 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 16:06:51,052 [INFO] Starting _process_content
2025-02-06 16:08:28,559 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 16:08:28,705 [INFO] Starting _process_content
2025-02-06 16:09:46,504 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 16:09:46,655 [INFO] Starting _process_content
2025-02-06 16:11:06,265 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 16:11:06,413 [INFO] Starting _process_content
2025-02-06 16:12:25,934 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 16:12:26,095 [INFO] Starting _process_content
2025-02-06 16:14:08,519 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 16:14:08,667 [INFO] Starting _process_content
2025-02-06 16:15:36,637 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 16:15:36,793 [INFO] Starting _process_content
2025-02-06 16:18:26,657 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 16:18:28,231 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:28,233 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:28,239 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:28,247 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:28,253 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:28,259 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:28,268 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:18:28,270 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:28,275 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:28,285 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:18:28,288 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:28,291 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:28,302 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:28,305 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:28,308 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:28,315 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:28,320 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:28,323 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:28,327 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:28,334 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:28,336 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:18:28,338 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:28,342 [INFO] Starting _process_content
2025-02-06 16:18:28,353 [INFO] Starting _process_content
2025-02-06 16:18:28,365 [INFO] Starting _process_content
2025-02-06 16:18:28,382 [INFO] Starting _process_content
2025-02-06 16:18:28,390 [INFO] Starting _process_content
2025-02-06 16:18:28,402 [INFO] Starting _process_content
2025-02-06 16:18:28,409 [INFO] Starting _process_content
2025-02-06 16:18:28,421 [INFO] Starting _process_content
2025-02-06 16:18:28,433 [INFO] Starting _process_content
2025-02-06 16:18:28,440 [INFO] Starting _process_content
2025-02-06 16:18:28,454 [INFO] Starting _process_content
2025-02-06 16:18:28,464 [INFO] Starting _process_content
2025-02-06 16:18:28,472 [INFO] Starting _process_content
2025-02-06 16:18:28,488 [INFO] Starting _process_content
2025-02-06 16:18:28,501 [INFO] Starting _process_content
2025-02-06 16:18:28,506 [INFO] Starting _process_content
2025-02-06 16:18:28,516 [INFO] Starting _process_content
2025-02-06 16:18:28,532 [INFO] Starting _process_content
2025-02-06 16:18:28,542 [INFO] Starting _process_content
2025-02-06 16:18:28,554 [INFO] Starting _process_content
2025-02-06 16:18:29,090 [INFO] Starting _process_content
2025-02-06 16:18:29,167 [INFO] Starting _process_content
2025-02-06 16:18:29,185 [INFO] Starting _process_content
2025-02-06 16:18:29,284 [INFO] Starting _process_content
2025-02-06 16:18:29,701 [INFO] Starting _process_content
2025-02-06 16:18:29,786 [INFO] Starting _process_content
2025-02-06 16:18:29,795 [INFO] Starting _process_content
2025-02-06 16:18:29,810 [INFO] Starting _process_content
2025-02-06 16:18:29,821 [INFO] Starting _process_content
2025-02-06 16:18:29,969 [INFO] Starting _process_content
2025-02-06 16:18:29,978 [INFO] Starting _process_content
2025-02-06 16:18:30,057 [INFO] Starting _process_content
2025-02-06 16:18:30,068 [INFO] Starting _process_content
2025-02-06 16:18:30,146 [INFO] Starting _process_content
2025-02-06 16:18:30,153 [INFO] Starting _process_content
2025-02-06 16:18:30,241 [INFO] Starting _process_content
2025-02-06 16:18:30,317 [INFO] Starting _process_content
2025-02-06 16:18:30,323 [INFO] Starting _process_content
2025-02-06 16:18:30,335 [INFO] Starting _process_content
2025-02-06 16:18:30,345 [INFO] Starting _process_content
2025-02-06 16:18:30,355 [INFO] Starting _process_content
2025-02-06 16:18:30,367 [INFO] Starting _process_content
2025-02-06 16:18:30,377 [INFO] Starting _process_content
2025-02-06 16:18:30,396 [INFO] Starting _process_content
2025-02-06 16:18:30,405 [INFO] Starting _process_content
2025-02-06 16:18:30,418 [INFO] Starting _process_content
2025-02-06 16:18:30,665 [INFO] Starting _process_content
2025-02-06 16:18:30,956 [INFO] Starting _process_content
2025-02-06 16:18:30,967 [INFO] Starting _process_content
2025-02-06 16:18:31,061 [INFO] Starting _process_content
2025-02-06 16:18:31,076 [INFO] Starting _process_content
2025-02-06 16:18:31,088 [INFO] Starting _process_content
2025-02-06 16:18:31,103 [INFO] Starting _process_content
2025-02-06 16:18:31,116 [INFO] Starting _process_content
2025-02-06 16:18:31,130 [INFO] Starting _process_content
2025-02-06 16:18:31,145 [INFO] Starting _process_content
2025-02-06 16:18:31,151 [INFO] Starting _process_content
2025-02-06 16:18:31,245 [INFO] Starting _process_content
2025-02-06 16:18:31,254 [INFO] Starting _process_content
2025-02-06 16:18:31,322 [INFO] Starting _process_content
2025-02-06 16:18:31,401 [INFO] Starting _process_content
2025-02-06 16:18:31,410 [INFO] Starting _process_content
2025-02-06 16:18:31,474 [INFO] Starting _process_content
2025-02-06 16:18:31,538 [INFO] Starting _process_content
2025-02-06 16:18:31,670 [INFO] Starting _process_content
2025-02-06 16:18:31,734 [INFO] Starting _process_content
2025-02-06 16:18:31,739 [INFO] Starting _process_content
2025-02-06 16:18:31,749 [INFO] Starting _process_content
2025-02-06 16:18:31,760 [INFO] Starting _process_content
2025-02-06 16:18:31,771 [INFO] Starting _process_content
2025-02-06 16:18:31,782 [INFO] Starting _process_content
2025-02-06 16:18:31,788 [INFO] Starting _process_content
2025-02-06 16:18:31,796 [INFO] Starting _process_content
2025-02-06 16:18:31,867 [INFO] Starting _process_content
2025-02-06 16:18:31,949 [INFO] Starting _process_content
2025-02-06 16:18:32,011 [INFO] Starting _process_content
2025-02-06 16:18:32,018 [INFO] Starting _process_content
2025-02-06 16:18:32,209 [INFO] Starting _process_content
2025-02-06 16:18:32,370 [INFO] Starting _process_content
2025-02-06 16:18:32,493 [INFO] Starting _process_content
2025-02-06 16:18:32,503 [INFO] Starting _process_content
2025-02-06 16:18:32,514 [INFO] Starting _process_content
2025-02-06 16:18:32,520 [INFO] Starting _process_content
2025-02-06 16:18:32,622 [INFO] Starting _process_content
2025-02-06 16:18:32,635 [INFO] Starting _process_content
2025-02-06 16:18:32,646 [INFO] Starting _process_content
2025-02-06 16:18:32,653 [INFO] Starting _process_content
2025-02-06 16:18:32,731 [INFO] Starting _process_content
2025-02-06 16:18:32,806 [INFO] Starting _process_content
2025-02-06 16:18:32,817 [INFO] Starting _process_content
2025-02-06 16:18:32,896 [INFO] Starting _process_content
2025-02-06 16:18:32,902 [INFO] Starting _process_content
2025-02-06 16:18:32,914 [INFO] Starting _process_content
2025-02-06 16:18:32,926 [INFO] Starting _process_content
2025-02-06 16:18:33,005 [INFO] Starting _process_content
2025-02-06 16:18:33,014 [INFO] Starting _process_content
2025-02-06 16:18:33,019 [INFO] Starting _process_content
2025-02-06 16:18:33,030 [INFO] Starting _process_content
2025-02-06 16:18:33,038 [INFO] Starting _process_content
2025-02-06 16:18:33,050 [INFO] Starting _process_content
2025-02-06 16:18:33,055 [INFO] Starting _process_content
2025-02-06 16:18:33,078 [INFO] Starting _process_content
2025-02-06 16:18:33,084 [INFO] Starting _process_content
2025-02-06 16:18:33,091 [INFO] Starting _process_content
2025-02-06 16:18:33,100 [INFO] Starting _process_content
2025-02-06 16:18:33,104 [INFO] Starting _process_content
2025-02-06 16:18:33,116 [INFO] Starting _process_content
2025-02-06 16:18:33,171 [INFO] Starting _process_content
2025-02-06 16:18:33,219 [INFO] Starting _process_content
2025-02-06 16:18:33,435 [INFO] Starting _process_content
2025-02-06 16:18:33,535 [INFO] Starting _process_content
2025-02-06 16:18:33,548 [INFO] Starting _process_content
2025-02-06 16:18:33,561 [INFO] Starting _process_content
2025-02-06 16:18:33,570 [INFO] Starting _process_content
2025-02-06 16:18:33,582 [INFO] Starting _process_content
2025-02-06 16:18:33,593 [INFO] Starting _process_content
2025-02-06 16:18:33,719 [INFO] Starting _process_content
2025-02-06 16:18:33,793 [INFO] Starting _process_content
2025-02-06 16:18:33,863 [INFO] Starting _process_content
2025-02-06 16:18:33,935 [INFO] Starting _process_content
2025-02-06 16:18:34,012 [INFO] Starting _process_content
2025-02-06 16:18:34,021 [INFO] Starting _process_content
2025-02-06 16:18:34,035 [INFO] Starting _process_content
2025-02-06 16:18:34,099 [INFO] Starting _process_content
2025-02-06 16:18:34,231 [INFO] Starting _process_content
2025-02-06 16:18:34,236 [INFO] Starting _process_content
2025-02-06 16:18:34,245 [INFO] Starting _process_content
2025-02-06 16:18:34,313 [INFO] Starting _process_content
2025-02-06 16:18:34,318 [INFO] Starting _process_content
2025-02-06 16:18:34,328 [INFO] Starting _process_content
2025-02-06 16:18:34,342 [INFO] Starting _process_content
2025-02-06 16:18:34,461 [INFO] Starting _process_content
2025-02-06 16:18:34,467 [INFO] Starting _process_content
2025-02-06 16:18:34,478 [INFO] Starting _process_content
2025-02-06 16:18:34,485 [INFO] Starting _process_content
2025-02-06 16:18:34,495 [INFO] Starting _process_content
2025-02-06 16:18:34,500 [INFO] Starting _process_content
2025-02-06 16:18:34,511 [INFO] Starting _process_content
2025-02-06 16:18:34,626 [INFO] Starting _process_content
2025-02-06 16:18:34,773 [INFO] Operation took 0.23 seconds
2025-02-06 16:18:34,778 [INFO] Starting _process_content
2025-02-06 16:18:34,808 [INFO] Starting _process_content
2025-02-06 16:18:34,990 [DEBUG] IDNA encoding failed for example.com@evil.com: Codepoint U+0040 at position 4 of 'com@evil' not allowed
2025-02-06 16:18:35,119 [DEBUG] IDNA encoding failed for user@example.com: Codepoint U+0040 at position 5 of 'user@example' not allowed
2025-02-06 16:18:35,119 [DEBUG] IDNA encoding failed for :pass@example.com: Empty domain
2025-02-06 16:18:35,198 [DEBUG] IDNA encoding failed for .example.com: Empty Label
2025-02-06 16:18:35,199 [DEBUG] IDNA encoding failed for example..com: Empty Label
2025-02-06 16:18:35,199 [DEBUG] IDNA encoding failed for example.com..: Empty Label
2025-02-06 16:18:35,561 [INFO] test_operation took 0.10 seconds
2025-02-06 16:18:35,763 [INFO] inner took 0.10 seconds
2025-02-06 16:18:35,763 [INFO] outer took 0.20 seconds
2025-02-06 16:18:35,763 [INFO] error_operation took 0.00 seconds
2025-02-06 16:18:35,867 [INFO] test_operation took 0.00 seconds
2025-02-06 16:18:36,046 [INFO] Starting _process_content
2025-02-06 16:18:36,060 [INFO] Starting _process_content
2025-02-06 16:18:36,084 [INFO] Starting _process_content
2025-02-06 16:18:36,279 [INFO] Starting _process_content
2025-02-06 16:18:36,394 [INFO] Starting _process_content
2025-02-06 16:18:36,508 [INFO] Starting _process_content
2025-02-06 16:18:36,749 [INFO] Starting _process_content
2025-02-06 16:18:36,812 [INFO] Starting _process_content
2025-02-06 16:18:36,832 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 16:18:36,832 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:36,850 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-06 16:18:36,850 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-06 16:18:36,851 [INFO] Starting crawl of URL: https://example.com
2025-02-06 16:18:37,182 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 16:18:37,182 [DEBUG] Base URL: https://example.com
2025-02-06 16:18:37,183 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 16:18:37,183 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 16:18:37,183 [DEBUG] Initial domain set to: example.com
2025-02-06 16:18:37,183 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 16:18:37,183 [INFO] Successfully crawled https://example.com
2025-02-06 16:18:37,185 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:37,189 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001BAE1B8A3C0>
2025-02-06 16:18:37,189 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001BAE1C75310>, 184158.784401)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001BAE1B89FD0>
2025-02-06 16:18:37,191 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 16:18:37,191 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:37,194 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-06 16:18:37,194 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-06 16:18:37,194 [INFO] Starting crawl of URL: https://example.com
2025-02-06 16:18:37,263 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2025-02-06 16:18:37,263 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2025-02-06 16:18:37,264 [INFO] Starting crawl of URL: https://example.org
2025-02-06 16:18:37,397 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 16:18:37,398 [DEBUG] Base URL: https://example.com
2025-02-06 16:18:37,398 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 16:18:37,398 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 16:18:37,398 [DEBUG] Initial domain set to: example.com
2025-02-06 16:18:37,398 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 16:18:37,398 [INFO] Successfully crawled https://example.com
2025-02-06 16:18:38,073 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 16:18:38,074 [DEBUG] Base URL: https://example.org
2025-02-06 16:18:38,074 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 16:18:38,074 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 16:18:38,074 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 16:18:38,075 [INFO] Successfully crawled https://example.org
2025-02-06 16:18:38,088 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:38,090 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001BAE1C4C690>
2025-02-06 16:18:38,090 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001BAE1C75F10>, 184159.0001268)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001BAE1C754F0>, 184159.6752919)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001BAE1C4EFD0>
2025-02-06 16:18:38,092 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 16:18:38,092 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:38,094 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-06 16:18:38,095 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-06 16:18:38,095 [INFO] Starting crawl of URL: https://example.com
2025-02-06 16:18:38,294 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 16:18:38,294 [DEBUG] Base URL: https://example.com
2025-02-06 16:18:38,294 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 16:18:38,294 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 16:18:38,295 [DEBUG] Initial domain set to: example.com
2025-02-06 16:18:38,295 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 16:18:38,295 [INFO] Successfully crawled https://example.com
2025-02-06 16:18:38,323 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:38,325 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001BAE1C4DE50>
2025-02-06 16:18:38,326 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001BAE1C762D0>, 184159.8958477)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001BAE1C4DBD0>
2025-02-06 16:18:38,327 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 16:18:38,327 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:38,329 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-06 16:18:38,329 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-06 16:18:38,329 [INFO] Starting crawl of URL: https://example.com
2025-02-06 16:18:38,536 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 16:18:38,536 [DEBUG] Base URL: https://example.com
2025-02-06 16:18:38,537 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 16:18:38,537 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 16:18:38,537 [DEBUG] Initial domain set to: example.com
2025-02-06 16:18:38,537 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 16:18:38,538 [INFO] Successfully crawled https://example.com
2025-02-06 16:18:38,540 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 16:18:38,540 [DEBUG] Base URL: https://example.com
2025-02-06 16:18:38,540 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 16:18:38,541 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 16:18:38,541 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 16:18:38,732 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:38,740 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:18:38,741 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:38,748 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:38,757 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 16:18:38,757 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:38,760 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-06 16:18:38,761 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-06 16:18:38,764 [INFO] Starting crawl of URL: https://example.com
2025-02-06 16:18:38,985 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 16:18:38,985 [DEBUG] Base URL: https://example.com
2025-02-06 16:18:38,986 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 16:18:38,986 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 16:18:38,986 [DEBUG] Initial domain set to: example.com
2025-02-06 16:18:38,986 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 16:18:38,986 [INFO] Successfully crawled https://example.com
2025-02-06 16:18:38,988 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:38,990 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001BAE1C4E490>
2025-02-06 16:18:38,990 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001BAE1C756D0>, 184160.5860543)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001BAE1B3BBB0>
2025-02-06 16:18:38,992 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 16:18:38,992 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:38,994 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2025-02-06 16:18:38,995 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2025-02-06 16:18:38,995 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2025-02-06 16:18:41,203 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [getaddrinfo failed]
2025-02-06 16:18:41,208 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [getaddrinfo failed]
2025-02-06 16:18:41,209 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [getaddrinfo failed]
2025-02-06 16:18:41,228 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:41,238 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 16:18:41,239 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:41,242 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-06 16:18:41,242 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-06 16:18:41,242 [INFO] Starting crawl of URL: https://example.com
2025-02-06 16:18:41,374 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2025-02-06 16:18:41,376 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2025-02-06 16:18:41,376 [INFO] Starting crawl of URL: https://example.org
2025-02-06 16:18:41,376 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2025-02-06 16:18:41,376 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2025-02-06 16:18:41,377 [INFO] Starting crawl of URL: https://example.net
2025-02-06 16:18:41,511 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 16:18:41,511 [DEBUG] Base URL: https://example.com
2025-02-06 16:18:41,511 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 16:18:41,512 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 16:18:41,512 [DEBUG] Initial domain set to: example.com
2025-02-06 16:18:41,512 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 16:18:41,512 [INFO] Successfully crawled https://example.com
2025-02-06 16:18:42,157 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 16:18:42,158 [DEBUG] Base URL: https://example.org
2025-02-06 16:18:42,158 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 16:18:42,158 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 16:18:42,158 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 16:18:42,158 [INFO] Successfully crawled https://example.org
2025-02-06 16:18:42,609 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 16:18:42,609 [DEBUG] Base URL: https://example.net
2025-02-06 16:18:42,609 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 16:18:42,609 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 16:18:42,610 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 16:18:42,610 [INFO] Successfully crawled https://example.net
2025-02-06 16:18:42,611 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:42,613 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001BAE1C4F4D0>
2025-02-06 16:18:42,613 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001BAE1C76B70>, 184163.1128593)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001BAE1C76D50>, 184163.7601466)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001BAE1C76CF0>, 184164.2117335)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001BAE1CE8180>
2025-02-06 16:18:42,616 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 16:18:42,617 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:42,620 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-06 16:18:42,620 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-06 16:18:42,620 [INFO] Starting crawl of URL: https://example.com
2025-02-06 16:18:42,811 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 16:18:42,812 [DEBUG] Base URL: https://example.com
2025-02-06 16:18:42,812 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 16:18:42,812 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 16:18:42,812 [DEBUG] Initial domain set to: example.com
2025-02-06 16:18:42,812 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 16:18:42,812 [INFO] Successfully crawled https://example.com
2025-02-06 16:18:42,819 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:42,838 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 16:18:42,838 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:42,842 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:18:44,111 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 16:18:44,187 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:44,189 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 16:18:44,190 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:44,192 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 16:18:44,269 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:44,271 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 16:18:44,272 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:44,274 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 16:18:44,274 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2025-02-06 16:18:44,274 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2025-02-06 16:18:44,275 [INFO] Starting crawl of URL: https://example.com/page
2025-02-06 16:18:46,599 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 16:18:46,599 [DEBUG] Base URL: https://example.com/page
2025-02-06 16:18:46,599 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 16:18:46,600 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 16:18:46,600 [DEBUG] Initial domain set to: example.com
2025-02-06 16:18:46,600 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 16:18:46,600 [INFO] Successfully crawled https://example.com/page
2025-02-06 16:18:46,600 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2025-02-06 16:18:46,601 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2025-02-06 16:18:46,601 [INFO] Starting crawl of URL: https://other-domain.com/page
2025-02-06 16:18:48,728 [DEBUG] 
Evaluating link: ./
2025-02-06 16:18:48,728 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,729 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2025-02-06 16:18:48,729 [DEBUG] Absolute link: https://other-domain.com/
2025-02-06 16:18:48,729 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,730 [DEBUG] 
Evaluating link: ./
2025-02-06 16:18:48,730 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,730 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2025-02-06 16:18:48,730 [DEBUG] Absolute link: https://other-domain.com/
2025-02-06 16:18:48,730 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,731 [DEBUG] 
Evaluating link: ./hiyou.php
2025-02-06 16:18:48,731 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,731 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2025-02-06 16:18:48,731 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2025-02-06 16:18:48,731 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,731 [DEBUG] 
Evaluating link: ./date.php
2025-02-06 16:18:48,731 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,731 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2025-02-06 16:18:48,732 [DEBUG] Absolute link: https://other-domain.com/date.php
2025-02-06 16:18:48,732 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,732 [DEBUG] 
Evaluating link: ./contact.php
2025-02-06 16:18:48,732 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,732 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2025-02-06 16:18:48,732 [DEBUG] Absolute link: https://other-domain.com/contact.php
2025-02-06 16:18:48,732 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,732 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-06 16:18:48,732 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,732 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-06 16:18:48,733 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-06 16:18:48,733 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2025-02-06 16:18:48,774 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,789 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-06 16:18:48,789 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 16:18:48,790 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,790 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 16:18:48,791 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 16:18:48,794 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-06 16:18:48,795 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-06 16:18:48,795 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,795 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-06 16:18:48,796 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-06 16:18:48,796 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-06 16:18:48,797 [DEBUG] 
Evaluating link: ./contact
2025-02-06 16:18:48,797 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,797 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2025-02-06 16:18:48,797 [DEBUG] Absolute link: https://other-domain.com/contact
2025-02-06 16:18:48,797 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,803 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,817 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-06 16:18:48,818 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 16:18:48,818 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,818 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 16:18:48,818 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 16:18:48,818 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-06 16:18:48,819 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-06 16:18:48,819 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,819 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-06 16:18:48,819 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-06 16:18:48,820 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-06 16:18:48,829 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,840 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2025-02-06 16:18:48,840 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-06 16:18:48,840 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,841 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-06 16:18:48,844 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-06 16:18:48,845 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2025-02-06 16:18:48,845 [DEBUG] 
Evaluating link: #indexes
2025-02-06 16:18:48,845 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,845 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2025-02-06 16:18:48,846 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,846 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,846 [DEBUG] 
Evaluating link: #a
2025-02-06 16:18:48,846 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,846 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2025-02-06 16:18:48,846 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,847 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,847 [DEBUG] 
Evaluating link: #b
2025-02-06 16:18:48,847 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,847 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2025-02-06 16:18:48,847 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,847 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,847 [DEBUG] 
Evaluating link: #c
2025-02-06 16:18:48,847 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,847 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2025-02-06 16:18:48,848 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,848 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,848 [DEBUG] 
Evaluating link: #d
2025-02-06 16:18:48,848 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,848 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2025-02-06 16:18:48,848 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,848 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,848 [DEBUG] 
Evaluating link: #e
2025-02-06 16:18:48,848 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,849 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2025-02-06 16:18:48,849 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,849 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,849 [DEBUG] 
Evaluating link: #f
2025-02-06 16:18:48,849 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,849 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2025-02-06 16:18:48,849 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,849 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,850 [DEBUG] 
Evaluating link: #g
2025-02-06 16:18:48,850 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,850 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2025-02-06 16:18:48,850 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,850 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,850 [DEBUG] 
Evaluating link: #h
2025-02-06 16:18:48,850 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,850 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2025-02-06 16:18:48,851 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,851 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,851 [DEBUG] 
Evaluating link: #i
2025-02-06 16:18:48,851 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,851 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2025-02-06 16:18:48,851 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,852 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,852 [DEBUG] 
Evaluating link: #j
2025-02-06 16:18:48,852 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,853 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2025-02-06 16:18:48,853 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,853 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,853 [DEBUG] 
Evaluating link: #k
2025-02-06 16:18:48,853 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,853 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2025-02-06 16:18:48,853 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,853 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,853 [DEBUG] 
Evaluating link: #l
2025-02-06 16:18:48,854 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,854 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2025-02-06 16:18:48,854 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,854 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,854 [DEBUG] 
Evaluating link: #m
2025-02-06 16:18:48,854 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,854 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2025-02-06 16:18:48,854 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,854 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,855 [DEBUG] 
Evaluating link: #n
2025-02-06 16:18:48,855 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,855 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2025-02-06 16:18:48,855 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,855 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,855 [DEBUG] 
Evaluating link: #o
2025-02-06 16:18:48,855 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,855 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2025-02-06 16:18:48,856 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,856 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,856 [DEBUG] 
Evaluating link: #p
2025-02-06 16:18:48,856 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,856 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2025-02-06 16:18:48,856 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,857 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,857 [DEBUG] 
Evaluating link: #q
2025-02-06 16:18:48,857 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,857 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2025-02-06 16:18:48,857 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,861 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,861 [DEBUG] 
Evaluating link: #r
2025-02-06 16:18:48,862 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,862 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2025-02-06 16:18:48,862 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,862 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,863 [DEBUG] 
Evaluating link: #s
2025-02-06 16:18:48,863 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,863 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2025-02-06 16:18:48,863 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,863 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,864 [DEBUG] 
Evaluating link: #t
2025-02-06 16:18:48,864 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,864 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2025-02-06 16:18:48,864 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,864 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,864 [DEBUG] 
Evaluating link: #u
2025-02-06 16:18:48,864 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,864 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2025-02-06 16:18:48,864 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,865 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,865 [DEBUG] 
Evaluating link: #v
2025-02-06 16:18:48,865 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,865 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2025-02-06 16:18:48,865 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,865 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,865 [DEBUG] 
Evaluating link: #w
2025-02-06 16:18:48,865 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,865 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2025-02-06 16:18:48,866 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,866 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,866 [DEBUG] 
Evaluating link: #x
2025-02-06 16:18:48,866 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,866 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2025-02-06 16:18:48,866 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,866 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,866 [DEBUG] 
Evaluating link: #y
2025-02-06 16:18:48,866 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,866 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2025-02-06 16:18:48,867 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,867 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,867 [DEBUG] 
Evaluating link: #z
2025-02-06 16:18:48,867 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,867 [DEBUG] Normalized URL: https://other-domain.com/page#z -> https://other-domain.com/page
2025-02-06 16:18:48,867 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,867 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,867 [DEBUG] 
Evaluating link: #A
2025-02-06 16:18:48,867 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,868 [DEBUG] Normalized URL: https://other-domain.com/page#A -> https://other-domain.com/page
2025-02-06 16:18:48,869 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,869 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,869 [DEBUG] 
Evaluating link: #B
2025-02-06 16:18:48,869 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,869 [DEBUG] Normalized URL: https://other-domain.com/page#B -> https://other-domain.com/page
2025-02-06 16:18:48,869 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,869 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,870 [DEBUG] 
Evaluating link: #C
2025-02-06 16:18:48,870 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,870 [DEBUG] Normalized URL: https://other-domain.com/page#C -> https://other-domain.com/page
2025-02-06 16:18:48,870 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,870 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,870 [DEBUG] 
Evaluating link: #D
2025-02-06 16:18:48,870 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,870 [DEBUG] Normalized URL: https://other-domain.com/page#D -> https://other-domain.com/page
2025-02-06 16:18:48,870 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,871 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,874 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,886 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-06 16:18:48,887 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 16:18:48,887 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,887 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 16:18:48,887 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 16:18:48,887 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-06 16:18:48,887 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-06 16:18:48,888 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,888 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-06 16:18:48,888 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-06 16:18:48,888 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-06 16:18:48,897 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,905 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2025-02-06 16:18:48,905 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-06 16:18:48,906 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,906 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-06 16:18:48,906 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-06 16:18:48,906 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2025-02-06 16:18:48,906 [DEBUG] 
Evaluating link: https://spriq.jp/
2025-02-06 16:18:48,906 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,906 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2025-02-06 16:18:48,906 [DEBUG] Absolute link: https://spriq.jp/
2025-02-06 16:18:48,906 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2025-02-06 16:18:48,913 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,920 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-06 16:18:48,923 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,933 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-06 16:18:48,934 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 16:18:48,934 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,934 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 16:18:48,934 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 16:18:48,934 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-06 16:18:48,934 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-06 16:18:48,934 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,935 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-06 16:18:48,935 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-06 16:18:48,935 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-06 16:18:48,935 [DEBUG] 
Evaluating link: ./contact
2025-02-06 16:18:48,936 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,936 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2025-02-06 16:18:48,936 [DEBUG] Absolute link: https://other-domain.com/contact
2025-02-06 16:18:48,936 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,936 [DEBUG] 
Evaluating link: ./
2025-02-06 16:18:48,936 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,937 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2025-02-06 16:18:48,937 [DEBUG] Absolute link: https://other-domain.com/
2025-02-06 16:18:48,937 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,937 [DEBUG] 
Evaluating link: ./hiyou.php
2025-02-06 16:18:48,937 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,938 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2025-02-06 16:18:48,938 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2025-02-06 16:18:48,938 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,938 [DEBUG] 
Evaluating link: ./date.php
2025-02-06 16:18:48,939 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,939 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2025-02-06 16:18:48,939 [DEBUG] Absolute link: https://other-domain.com/date.php
2025-02-06 16:18:48,940 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,940 [DEBUG] 
Evaluating link: #z
2025-02-06 16:18:48,940 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,940 [DEBUG] Normalized URL: https://other-domain.com/page#z -> https://other-domain.com/page
2025-02-06 16:18:48,940 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,941 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,947 [DEBUG] 
Evaluating link: #A
2025-02-06 16:18:48,947 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,947 [DEBUG] Normalized URL: https://other-domain.com/page#A -> https://other-domain.com/page
2025-02-06 16:18:48,947 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,947 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,947 [DEBUG] 
Evaluating link: #B
2025-02-06 16:18:48,947 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,948 [DEBUG] Normalized URL: https://other-domain.com/page#B -> https://other-domain.com/page
2025-02-06 16:18:48,948 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,948 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,948 [DEBUG] 
Evaluating link: #C
2025-02-06 16:18:48,948 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,948 [DEBUG] Normalized URL: https://other-domain.com/page#C -> https://other-domain.com/page
2025-02-06 16:18:48,948 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,948 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,948 [DEBUG] 
Evaluating link: #D
2025-02-06 16:18:48,949 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,949 [DEBUG] Normalized URL: https://other-domain.com/page#D -> https://other-domain.com/page
2025-02-06 16:18:48,949 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,949 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,949 [DEBUG] 
Evaluating link: #u
2025-02-06 16:18:48,949 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,949 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2025-02-06 16:18:48,949 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,949 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,950 [DEBUG] 
Evaluating link: #v
2025-02-06 16:18:48,950 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,950 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2025-02-06 16:18:48,950 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,950 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,950 [DEBUG] 
Evaluating link: #w
2025-02-06 16:18:48,950 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,950 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2025-02-06 16:18:48,951 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,951 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,951 [DEBUG] 
Evaluating link: #x
2025-02-06 16:18:48,951 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,951 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2025-02-06 16:18:48,951 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,951 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,951 [DEBUG] 
Evaluating link: #y
2025-02-06 16:18:48,951 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,952 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2025-02-06 16:18:48,952 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,952 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,952 [DEBUG] 
Evaluating link: #p
2025-02-06 16:18:48,952 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,952 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2025-02-06 16:18:48,952 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,952 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,952 [DEBUG] 
Evaluating link: #q
2025-02-06 16:18:48,952 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,953 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2025-02-06 16:18:48,953 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,953 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,953 [DEBUG] 
Evaluating link: #r
2025-02-06 16:18:48,953 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,953 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2025-02-06 16:18:48,953 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,953 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,953 [DEBUG] 
Evaluating link: #s
2025-02-06 16:18:48,954 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,954 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2025-02-06 16:18:48,954 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,954 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,954 [DEBUG] 
Evaluating link: #t
2025-02-06 16:18:48,954 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,954 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2025-02-06 16:18:48,954 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,954 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,954 [DEBUG] 
Evaluating link: #k
2025-02-06 16:18:48,955 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,955 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2025-02-06 16:18:48,955 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,955 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,955 [DEBUG] 
Evaluating link: #l
2025-02-06 16:18:48,955 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,955 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2025-02-06 16:18:48,955 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,955 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,955 [DEBUG] 
Evaluating link: #m
2025-02-06 16:18:48,956 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,956 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2025-02-06 16:18:48,956 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,956 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,956 [DEBUG] 
Evaluating link: #n
2025-02-06 16:18:48,956 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,956 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2025-02-06 16:18:48,956 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,956 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,956 [DEBUG] 
Evaluating link: #o
2025-02-06 16:18:48,957 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,957 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2025-02-06 16:18:48,957 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,957 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,957 [DEBUG] 
Evaluating link: #f
2025-02-06 16:18:48,957 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,957 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2025-02-06 16:18:48,961 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,961 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,961 [DEBUG] 
Evaluating link: #g
2025-02-06 16:18:48,961 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,962 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2025-02-06 16:18:48,962 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,962 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,962 [DEBUG] 
Evaluating link: #h
2025-02-06 16:18:48,963 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,963 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2025-02-06 16:18:48,963 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,963 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,963 [DEBUG] 
Evaluating link: #i
2025-02-06 16:18:48,964 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,964 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2025-02-06 16:18:48,964 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,964 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,964 [DEBUG] 
Evaluating link: #j
2025-02-06 16:18:48,964 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,964 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2025-02-06 16:18:48,964 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,965 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,965 [DEBUG] 
Evaluating link: #b
2025-02-06 16:18:48,965 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,965 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2025-02-06 16:18:48,965 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,965 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,965 [DEBUG] 
Evaluating link: #c
2025-02-06 16:18:48,965 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,965 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2025-02-06 16:18:48,965 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,966 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,966 [DEBUG] 
Evaluating link: #d
2025-02-06 16:18:48,966 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,966 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2025-02-06 16:18:48,966 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,966 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,966 [DEBUG] 
Evaluating link: #e
2025-02-06 16:18:48,966 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,967 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2025-02-06 16:18:48,967 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,967 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,967 [DEBUG] 
Evaluating link: #site
2025-02-06 16:18:48,967 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 16:18:48,968 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2025-02-06 16:18:48,968 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 16:18:48,968 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 16:18:48,968 [INFO] Successfully crawled https://other-domain.com/page
2025-02-06 16:18:48,985 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001BAE1C4EAD0>
2025-02-06 16:18:48,985 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001BAE1C77890>, 184160.1369048)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001BAE1B38FC0>
2025-02-06 16:18:48,985 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001BAE1C4CF50>
2025-02-06 16:18:48,998 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:49,002 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 16:18:49,002 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:49,004 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2025-02-06 16:18:49,004 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2025-02-06 16:18:49,005 [INFO] Starting crawl of URL: https://example.com/page0
2025-02-06 16:18:50,860 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 16:18:50,860 [DEBUG] Base URL: https://example.com/page0
2025-02-06 16:18:50,860 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 16:18:50,860 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 16:18:50,861 [DEBUG] Initial domain set to: example.com
2025-02-06 16:18:50,861 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 16:18:50,861 [INFO] Successfully crawled https://example.com/page0
2025-02-06 16:18:50,861 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2025-02-06 16:18:50,861 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2025-02-06 16:18:50,862 [INFO] Starting crawl of URL: https://example.com/page1
2025-02-06 16:18:51,442 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 16:18:51,443 [DEBUG] Base URL: https://example.com/page1
2025-02-06 16:18:51,443 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 16:18:51,443 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 16:18:51,443 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 16:18:51,443 [INFO] Successfully crawled https://example.com/page1
2025-02-06 16:18:51,444 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2025-02-06 16:18:51,444 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2025-02-06 16:18:51,444 [INFO] Starting crawl of URL: https://example.com/page2
2025-02-06 16:18:53,466 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 16:18:53,467 [DEBUG] Base URL: https://example.com/page2
2025-02-06 16:18:53,467 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 16:18:53,467 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 16:18:53,467 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 16:18:53,468 [INFO] Successfully crawled https://example.com/page2
2025-02-06 16:18:53,468 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2025-02-06 16:18:53,468 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2025-02-06 16:18:53,468 [INFO] Starting crawl of URL: https://example.com/page3
2025-02-06 16:18:55,279 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 16:18:55,279 [DEBUG] Base URL: https://example.com/page3
2025-02-06 16:18:55,279 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 16:18:55,280 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 16:18:55,280 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 16:18:55,280 [INFO] Successfully crawled https://example.com/page3
2025-02-06 16:18:55,280 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2025-02-06 16:18:55,280 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2025-02-06 16:18:55,280 [INFO] Starting crawl of URL: https://example.com/page4
2025-02-06 16:18:56,537 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 16:18:56,537 [DEBUG] Base URL: https://example.com/page4
2025-02-06 16:18:56,537 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 16:18:56,537 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 16:18:56,537 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 16:18:56,538 [INFO] Successfully crawled https://example.com/page4
2025-02-06 16:18:56,538 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 16:18:56,538 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2025-02-06 16:18:56,538 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2025-02-06 16:18:56,538 [INFO] Starting crawl of URL: https://example.com/page0
2025-02-06 16:18:56,738 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 16:18:56,738 [DEBUG] Base URL: https://example.com/page0
2025-02-06 16:18:56,738 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 16:18:56,738 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 16:18:56,738 [DEBUG] Initial domain set to: example.com
2025-02-06 16:18:56,739 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 16:18:56,739 [INFO] Successfully crawled https://example.com/page0
2025-02-06 16:18:56,739 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2025-02-06 16:18:56,739 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2025-02-06 16:18:56,739 [INFO] Starting crawl of URL: https://example.com/page1
2025-02-06 16:18:57,249 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 16:18:57,250 [DEBUG] Base URL: https://example.com/page1
2025-02-06 16:18:57,250 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 16:18:57,251 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 16:18:57,251 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 16:18:57,252 [INFO] Successfully crawled https://example.com/page1
2025-02-06 16:18:57,252 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2025-02-06 16:18:57,252 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2025-02-06 16:18:57,253 [INFO] Starting crawl of URL: https://example.com/page2
2025-02-06 16:18:57,682 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 16:18:57,682 [DEBUG] Base URL: https://example.com/page2
2025-02-06 16:18:57,682 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 16:18:57,682 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 16:18:57,683 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 16:18:57,684 [INFO] Successfully crawled https://example.com/page2
2025-02-06 16:18:57,684 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2025-02-06 16:18:57,684 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2025-02-06 16:18:57,685 [INFO] Starting crawl of URL: https://example.com/page3
2025-02-06 16:18:58,157 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 16:18:58,157 [DEBUG] Base URL: https://example.com/page3
2025-02-06 16:18:58,157 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 16:18:58,157 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 16:18:58,157 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 16:18:58,158 [INFO] Successfully crawled https://example.com/page3
2025-02-06 16:18:58,158 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2025-02-06 16:18:58,158 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2025-02-06 16:18:58,158 [INFO] Starting crawl of URL: https://example.com/page4
2025-02-06 16:18:58,657 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 16:18:58,657 [DEBUG] Base URL: https://example.com/page4
2025-02-06 16:18:58,657 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 16:18:58,658 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 16:18:58,658 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 16:18:58,658 [INFO] Successfully crawled https://example.com/page4
2025-02-06 16:18:58,692 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:58,699 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:58,761 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:58,764 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:58,784 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:58,788 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:58,809 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:58,813 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,005 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,060 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,084 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,088 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,125 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,133 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,172 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,176 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,203 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,209 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,246 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,308 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001BAE08E0050>
2025-02-06 16:18:59,308 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001BAE1CAFEF0>, 184178.1380382)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001BAE1B3BBB0>
2025-02-06 16:18:59,309 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001BAE1C4E490>
2025-02-06 16:18:59,309 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001BAE1D861B0>, 184180.2589885)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001BAE1B3BE10>
2025-02-06 16:18:59,422 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:18:59,423 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,426 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,430 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:18:59,431 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,438 [DEBUG] 
Evaluating link: https://example.com/doc1
2025-02-06 16:18:59,438 [DEBUG] Target domain: example.com
2025-02-06 16:18:59,438 [DEBUG] URL domain: example.com
2025-02-06 16:18:59,438 [DEBUG] URL approved for crawling: https://example.com/doc1
2025-02-06 16:18:59,438 [DEBUG] 
Evaluating link: https://example.com/excluded/page
2025-02-06 16:18:59,438 [DEBUG] Target domain: example.com
2025-02-06 16:18:59,438 [DEBUG] URL domain: example.com
2025-02-06 16:18:59,439 [DEBUG] URL approved for crawling: https://example.com/excluded/page
2025-02-06 16:18:59,494 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,497 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:18:59,500 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,531 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,542 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:18:59,544 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,546 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-06 16:18:59,605 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,608 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:18:59,609 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,630 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,637 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:18:59,638 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,640 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-06 16:18:59,691 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,695 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:18:59,697 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,705 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-06 16:18:59,758 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,761 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:18:59,762 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,765 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-06 16:18:59,812 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,820 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:18:59,821 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,824 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-06 16:18:59,874 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,876 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:18:59,877 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,879 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-06 16:18:59,922 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,925 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:18:59,926 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,928 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-06 16:18:59,974 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:18:59,994 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:00,538 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:00,544 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:00,550 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:19:00,644 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:00,652 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:00,658 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:19:00,676 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001BAE1C4CF50>
2025-02-06 16:19:00,677 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001BAE1DDE150>, 184181.7910241)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001BAE1DDEBD0>, 184182.0112971)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001BAE1CE9E00>
2025-02-06 16:19:00,701 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:00,707 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:19:00,811 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:19:00,858 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:19:00,904 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:19:00,938 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:19:00,985 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:19:01,020 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:19:01,059 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:19:01,093 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:19:01,127 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:19:02,218 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:02,722 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:02,725 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 16:19:02,725 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-06 16:19:02,725 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-06 16:19:02,725 [INFO] Starting crawl of URL: https://example.com
2025-02-06 16:19:02,955 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 16:19:02,955 [DEBUG] Base URL: https://example.com
2025-02-06 16:19:02,955 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 16:19:02,955 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 16:19:02,955 [DEBUG] Initial domain set to: example.com
2025-02-06 16:19:02,955 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 16:19:02,956 [INFO] Successfully crawled https://example.com
2025-02-06 16:19:02,956 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001BAE277E0D0>
2025-02-06 16:19:02,956 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001BAE34A1610>, 184184.5574352)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001BAE1CEA060>
2025-02-06 16:19:02,959 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2025-02-06 16:19:03,592 [INFO] Operation took 0.10 seconds
2025-02-06 16:19:04,052 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001BAE1C4F110>
2025-02-06 16:19:04,053 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001BAE1C764B0>, 184168.2008504)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001BAE1C75070>, 184169.0600022)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001BAE1C756D0>, 184169.6083283)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001BAE1C77830>, 184170.2732313)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001BAE1CE8640>
2025-02-06 16:19:04,247 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 16:19:04,248 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:19:04,249 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:11,762 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-2\test_full_site_crawl0\test_docs/index.html
2025-02-06 16:19:11,763 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-06 16:19:11,868 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:11,891 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 16:19:11,891 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:19:11,892 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:19,635 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-2\test_content_processing_pipeli0\test_docs/guide.html
2025-02-06 16:19:19,636 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-06 16:19:19,666 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:19,678 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 16:19:19,678 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:19:19,678 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:27,180 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-2\test_quality_checks0\test_docs/api.html
2025-02-06 16:19:27,181 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-06 16:19:27,236 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:27,260 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 16:19:27,260 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:19:27,261 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:34,747 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-2\test_document_organization0\test_docs/index.html
2025-02-06 16:19:34,747 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-06 16:19:34,777 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:34,790 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 16:19:34,791 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:19:34,791 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:42,331 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-2\test_search_functionality0\test_docs/index.html
2025-02-06 16:19:42,332 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-06 16:19:42,385 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:42,402 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 16:19:42,402 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:19:42,403 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:49,917 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-2\test_error_handling_and_recove0\test_docs/index.html
2025-02-06 16:19:49,918 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-06 16:19:50,024 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:50,081 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:50,308 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:50,324 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:50,344 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:19:50,345 [ERROR] Error during crawl: 'str' object has no attribute 'url'
2025-02-06 16:19:50,392 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:50,452 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:50,483 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:50,487 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:50,514 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 16:19:50,593 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:54,608 [INFO] Starting _process_content
2025-02-06 16:19:54,641 [INFO] Starting _process_content
2025-02-06 16:19:54,667 [INFO] Starting _process_content
2025-02-06 16:19:54,670 [INFO] Starting _process_content
2025-02-06 16:19:54,678 [INFO] Starting _process_content
2025-02-06 16:19:54,685 [INFO] Starting _process_content
2025-02-06 16:19:54,694 [INFO] Starting _process_content
2025-02-06 16:19:54,703 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:54,754 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:54,756 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:54,777 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:54,785 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:54,801 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:54,804 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:54,823 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:54,826 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:54,858 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:54,861 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:54,894 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:54,902 [DEBUG] Using proactor: IocpProactor
2025-02-06 16:19:54,924 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:09:11,927 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 17:09:13,270 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:09:13,273 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:09:13,276 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:09:13,278 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:09:13,281 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:09:13,283 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:09:13,287 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:09:13,289 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:09:13,291 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:09:13,293 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:09:13,295 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:09:13,297 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:09:13,301 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:09:13,304 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:09:13,308 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:09:13,311 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:09:13,315 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:09:13,319 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:09:13,325 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:09:13,327 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:09:13,332 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:09:13,336 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:09:13,347 [INFO] Starting _process_content
2025-02-06 17:09:13,357 [INFO] Starting _process_content
2025-02-06 17:09:13,368 [INFO] Starting _process_content
2025-02-06 17:09:13,381 [INFO] Starting _process_content
2025-02-06 17:09:13,390 [INFO] Starting _process_content
2025-02-06 17:09:13,398 [INFO] Starting _process_content
2025-02-06 17:09:13,406 [INFO] Starting _process_content
2025-02-06 17:09:13,413 [INFO] Starting _process_content
2025-02-06 17:09:13,418 [INFO] Starting _process_content
2025-02-06 17:09:13,425 [INFO] Starting _process_content
2025-02-06 17:09:13,432 [INFO] Starting _process_content
2025-02-06 17:09:13,440 [INFO] Starting _process_content
2025-02-06 17:09:13,447 [INFO] Starting _process_content
2025-02-06 17:09:13,452 [INFO] Starting _process_content
2025-02-06 17:09:13,459 [INFO] Starting _process_content
2025-02-06 17:09:13,465 [INFO] Starting _process_content
2025-02-06 17:09:13,472 [INFO] Starting _process_content
2025-02-06 17:09:13,479 [INFO] Starting _process_content
2025-02-06 17:09:13,484 [INFO] Starting _process_content
2025-02-06 17:09:13,492 [INFO] Starting _process_content
2025-02-06 17:11:22,224 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 17:11:23,475 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:23,477 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:23,483 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:23,486 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:23,489 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:23,491 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:23,493 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:11:23,499 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:23,501 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:23,503 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:11:23,508 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:23,510 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:23,519 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:23,525 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:23,533 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:23,535 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:23,541 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:23,543 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:23,551 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:23,555 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:23,557 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:11:23,558 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:23,568 [INFO] Starting _process_content
2025-02-06 17:11:23,575 [INFO] Starting _process_content
2025-02-06 17:11:23,588 [INFO] Starting _process_content
2025-02-06 17:11:23,599 [INFO] Starting _process_content
2025-02-06 17:11:23,607 [INFO] Starting _process_content
2025-02-06 17:11:23,616 [INFO] Starting _process_content
2025-02-06 17:11:23,623 [INFO] Starting _process_content
2025-02-06 17:11:23,631 [INFO] Starting _process_content
2025-02-06 17:11:23,637 [INFO] Starting _process_content
2025-02-06 17:11:23,641 [INFO] Starting _process_content
2025-02-06 17:11:23,650 [INFO] Starting _process_content
2025-02-06 17:11:23,654 [INFO] Starting _process_content
2025-02-06 17:11:23,658 [INFO] Starting _process_content
2025-02-06 17:11:23,667 [INFO] Starting _process_content
2025-02-06 17:11:23,672 [INFO] Starting _process_content
2025-02-06 17:11:23,676 [INFO] Starting _process_content
2025-02-06 17:11:23,684 [INFO] Starting _process_content
2025-02-06 17:11:23,691 [INFO] Starting _process_content
2025-02-06 17:11:23,700 [INFO] Starting _process_content
2025-02-06 17:11:23,707 [INFO] Starting _process_content
2025-02-06 17:11:23,717 [INFO] Starting _process_content
2025-02-06 17:11:40,636 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 17:11:41,985 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:41,987 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:41,990 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:41,992 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:41,999 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:42,001 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:42,003 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:11:42,004 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:42,006 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:42,008 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:11:42,015 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:42,018 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:42,025 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:42,032 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:42,035 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:42,041 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:42,049 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:42,051 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:42,056 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:42,058 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:42,065 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:11:42,066 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:11:42,072 [INFO] Starting _process_content
2025-02-06 17:11:42,082 [INFO] Starting _process_content
2025-02-06 17:11:42,088 [INFO] Starting _process_content
2025-02-06 17:11:42,099 [INFO] Starting _process_content
2025-02-06 17:11:42,107 [INFO] Starting _process_content
2025-02-06 17:11:42,115 [INFO] Starting _process_content
2025-02-06 17:11:42,122 [INFO] Starting _process_content
2025-02-06 17:11:42,126 [INFO] Starting _process_content
2025-02-06 17:11:42,136 [INFO] Starting _process_content
2025-02-06 17:11:42,140 [INFO] Starting _process_content
2025-02-06 17:11:42,148 [INFO] Starting _process_content
2025-02-06 17:11:42,152 [INFO] Starting _process_content
2025-02-06 17:11:42,156 [INFO] Starting _process_content
2025-02-06 17:11:42,165 [INFO] Starting _process_content
2025-02-06 17:11:42,170 [INFO] Starting _process_content
2025-02-06 17:11:42,174 [INFO] Starting _process_content
2025-02-06 17:11:42,182 [INFO] Starting _process_content
2025-02-06 17:11:42,190 [INFO] Starting _process_content
2025-02-06 17:11:42,198 [INFO] Starting _process_content
2025-02-06 17:11:42,205 [INFO] Starting _process_content
2025-02-06 17:11:42,214 [INFO] Starting _process_content
2025-02-06 17:11:42,220 [INFO] Starting _process_content
2025-02-06 17:11:42,224 [INFO] Starting _process_content
2025-02-06 17:11:42,233 [INFO] Starting _process_content
2025-02-06 17:11:42,240 [INFO] Starting _process_content
2025-02-06 17:11:42,249 [INFO] Starting _process_content
2025-02-06 17:11:42,256 [INFO] Starting _process_content
2025-02-06 17:11:42,267 [INFO] Starting _process_content
2025-02-06 17:11:42,272 [INFO] Starting _process_content
2025-02-06 17:12:11,354 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 17:12:11,506 [INFO] Starting _process_content
2025-02-06 17:12:26,716 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 17:12:26,857 [INFO] Starting _process_content
2025-02-06 17:12:44,227 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 17:12:44,362 [INFO] Starting _process_content
2025-02-06 17:12:44,363 [INFO] Stylesheet links found: []
2025-02-06 17:13:20,706 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 17:13:21,149 [INFO] Starting _process_content
2025-02-06 17:13:21,156 [INFO] HTML content being processed: 
<html>
<head>
<link href="styles.css" rel="stylesheet"/>
<script src="app.js"></script>
</head>
<body>
<img alt="Image" src="image.png"/>
<video src="video.mp4"></video>
<audio src="audio.mp3"></audio>
<source src="media.webm" type="video/webm"/>
</body>
</html>

2025-02-06 17:13:21,163 [INFO] Stylesheet links found: []
2025-02-06 17:14:00,096 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 17:14:00,994 [INFO] Starting _process_content
2025-02-06 17:14:00,994 [INFO] HTML content being processed: 
<html>
<head>
<link href="styles.css" rel="stylesheet"/>
<script src="app.js"></script>
</head>
<body>
<img alt="Image" src="image.png"/>
<video src="video.mp4"></video>
<audio src="audio.mp3"></audio>
<source src="media.webm" type="video/webm"/>
</body>
</html>

2025-02-06 17:14:00,995 [INFO] Stylesheet links found: []
2025-02-06 17:14:40,183 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 17:14:40,323 [INFO] Starting _process_content
2025-02-06 17:14:40,323 [INFO] HTML content being processed: 
<html>
<head>
<link href="styles.css" rel="stylesheet"/>
<script src="app.js"></script>
</head>
<body>
<img alt="Image" src="image.png"/>
<video src="video.mp4"></video>
<audio src="audio.mp3"></audio>
<source src="media.webm" type="video/webm"/>
</body>
</html>

2025-02-06 17:14:40,325 [INFO] Soup Head content: None
2025-02-06 17:14:40,325 [INFO] Stylesheet links found: []
2025-02-06 17:15:03,145 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 17:15:04,552 [INFO] Starting _process_content
2025-02-06 17:15:04,569 [INFO] HTML content being processed: 
<html>
<head>
<link href="styles.css" rel="stylesheet"/>
<script src="app.js"></script>
</head>
<body>
<img alt="Image" src="image.png"/>
<video src="video.mp4"></video>
<audio src="audio.mp3"></audio>
<source src="media.webm" type="video/webm"/>
</body>
</html>

2025-02-06 17:15:04,585 [INFO] Soup Head content: <head>
<link href="styles.css" rel="stylesheet"/>
<script src="app.js"></script>
</head>
2025-02-06 17:15:04,602 [INFO] Stylesheet links found: [<link href="styles.css" rel="stylesheet"/>]
2025-02-06 17:15:04,613 [INFO] Stylesheet href: styles.css, processed URL: styles.css
2025-02-06 17:15:04,613 [INFO] Extracted stylesheet URL: styles.css
2025-02-06 17:15:04,614 [INFO] Stylesheets assets after append: ['styles.css']
2025-02-06 17:15:38,801 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 17:15:45,094 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:15:45,096 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:15:45,102 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:15:45,106 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:15:45,110 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:15:45,113 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:15:45,119 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:15:45,121 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:15:45,126 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:15:45,130 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:15:45,138 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:15:45,141 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:15:45,147 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:15:45,155 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:15:45,160 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:15:45,162 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:15:45,171 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:15:45,175 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:15:45,178 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:15:45,180 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:15:45,187 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:15:45,189 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:15:45,197 [INFO] Starting _process_content
2025-02-06 17:15:45,204 [INFO] Soup Head content: None
2025-02-06 17:15:45,205 [INFO] Stylesheet links found: []
2025-02-06 17:15:45,211 [INFO] Starting _process_content
2025-02-06 17:15:45,212 [INFO] Soup Head content: None
2025-02-06 17:15:45,212 [INFO] Stylesheet links found: []
2025-02-06 17:15:45,222 [INFO] Starting _process_content
2025-02-06 17:15:45,224 [INFO] Soup Head content: None
2025-02-06 17:15:45,224 [INFO] Stylesheet links found: []
2025-02-06 17:15:45,237 [INFO] Starting _process_content
2025-02-06 17:15:45,239 [INFO] Soup Head content: None
2025-02-06 17:15:45,241 [INFO] Stylesheet links found: []
2025-02-06 17:15:45,245 [INFO] Starting _process_content
2025-02-06 17:15:45,246 [INFO] Soup Head content: None
2025-02-06 17:15:45,246 [INFO] Stylesheet links found: []
2025-02-06 17:15:45,255 [INFO] Starting _process_content
2025-02-06 17:15:45,256 [INFO] Soup Head content: None
2025-02-06 17:15:45,256 [INFO] Stylesheet links found: []
2025-02-06 17:15:45,261 [INFO] Starting _process_content
2025-02-06 17:15:45,262 [INFO] Soup Head content: <head><title>Test Document</title></head>
2025-02-06 17:15:45,262 [INFO] Stylesheet links found: []
2025-02-06 17:15:45,271 [INFO] Starting _process_content
2025-02-06 17:15:45,272 [INFO] Soup Head content: <head>
<script type="text/javascript">var a = 1;</script>
</head>
2025-02-06 17:15:45,272 [INFO] Stylesheet links found: []
2025-02-06 17:15:45,278 [INFO] Starting _process_content
2025-02-06 17:15:45,279 [INFO] Soup Head content: <head>
<title>Script Test</title>
<script>alert('Hello');</script>
</head>
2025-02-06 17:15:45,279 [INFO] Stylesheet links found: []
2025-02-06 17:15:45,288 [INFO] Starting _process_content
2025-02-06 17:15:45,293 [INFO] Soup Head content: <head>
<title>Invalid JSON-LD</title>
<script type="application/ld+json">
                { invalid json }
                </script>
</head>
2025-02-06 17:15:45,293 [INFO] Stylesheet links found: []
2025-02-06 17:15:45,347 [INFO] Starting _process_content
2025-02-06 17:15:45,354 [INFO] Soup Head content: <head>
<script type="text/javascript">
                    console.log('JS Script');
                </script>
<script type="application/json">
                    {"key": "value"}
                </script>
</head>
2025-02-06 17:15:45,355 [INFO] Stylesheet links found: []
2025-02-06 17:15:45,407 [INFO] Starting _process_content
2025-02-06 17:15:45,408 [INFO] Soup Head content: None
2025-02-06 17:15:45,409 [INFO] Stylesheet links found: []
2025-02-06 17:15:45,475 [INFO] Starting _process_content
2025-02-06 17:15:45,476 [INFO] Soup Head content: None
2025-02-06 17:15:45,476 [INFO] Stylesheet links found: []
2025-02-06 17:15:45,491 [INFO] Starting _process_content
2025-02-06 17:15:45,492 [INFO] Soup Head content: None
2025-02-06 17:15:45,492 [INFO] Stylesheet links found: []
2025-02-06 17:15:45,504 [INFO] Starting _process_content
2025-02-06 17:15:45,505 [INFO] Soup Head content: None
2025-02-06 17:15:45,505 [INFO] Stylesheet links found: []
2025-02-06 17:15:45,519 [INFO] Starting _process_content
2025-02-06 17:15:45,520 [INFO] Soup Head content: None
2025-02-06 17:15:45,520 [INFO] Stylesheet links found: []
2025-02-06 17:15:45,536 [INFO] Starting _process_content
2025-02-06 17:15:45,561 [INFO] Soup Head content: None
2025-02-06 17:15:45,562 [INFO] Stylesheet links found: []
2025-02-06 17:15:45,586 [INFO] Starting _process_content
2025-02-06 17:15:45,588 [INFO] Soup Head content: None
2025-02-06 17:15:45,589 [INFO] Stylesheet links found: []
2025-02-06 17:15:45,594 [INFO] Starting _process_content
2025-02-06 17:15:45,597 [INFO] Soup Head content: None
2025-02-06 17:15:45,597 [INFO] Stylesheet links found: []
2025-02-06 17:15:45,612 [INFO] Starting _process_content
2025-02-06 17:15:45,619 [INFO] Soup Head content: None
2025-02-06 17:15:45,620 [INFO] Stylesheet links found: []
2025-02-06 17:15:45,629 [INFO] Starting _process_content
2025-02-06 17:15:45,637 [INFO] Soup Head content: None
2025-02-06 17:15:45,637 [INFO] Stylesheet links found: []
2025-02-06 17:15:45,646 [INFO] Starting _process_content
2025-02-06 17:15:45,654 [INFO] Soup Head content: <head>
<title>Meta Test</title>
<meta content="A test description." name="description"/>
<meta content="OG Meta Title" property="og:title"/>
</head>
2025-02-06 17:15:45,654 [INFO] Stylesheet links found: []
2025-02-06 17:15:45,660 [INFO] Starting _process_content
2025-02-06 17:15:45,662 [INFO] Soup Head content: <head>
<title>Complex Meta</title>
<meta content="A complex description." name="description"/>
<meta content="Dublin Core Title" name="dc.title"/>
<meta content="website" property="og:type"/>
<script type="application/ld+json">
                {
                    "@context": "https://schema.org",
                    "@type": "WebSite",
                    "name": "Example Site",
                    "url": "https://example.com"
                }
                </script>
</head>
2025-02-06 17:15:45,663 [INFO] Stylesheet links found: []
2025-02-06 17:15:45,674 [INFO] Starting _process_content
2025-02-06 17:15:45,677 [INFO] Soup Head content: <head>
<title>First Title</title>
<title>Second Title</title>
<meta content="First description" name="description"/>
<meta content="Second description" name="description"/>
<meta content="First OG title" property="og:title"/>
<meta content="Second OG title" property="og:title"/>
</head>
2025-02-06 17:15:45,677 [INFO] Stylesheet links found: []
2025-02-06 17:15:45,701 [INFO] Starting _process_content
2025-02-06 17:15:45,703 [INFO] Soup Head content: <head>
<title>Inside head</title>
<meta content="inside, head" name="keywords"/>
</head>
2025-02-06 17:15:45,704 [INFO] Stylesheet links found: []
2025-02-06 17:15:45,710 [INFO] Starting _process_content
2025-02-06 17:15:45,782 [INFO] Soup Head content: <head>
<title></title>
<meta content="" name="empty-content"/>
<meta name="no-content"/>
<meta content="no-name" name=""/>
<meta content="no-name-or-property"/>
<meta content="empty-property" property=""/>
</head>
2025-02-06 17:15:45,783 [INFO] Stylesheet links found: []
2025-02-06 17:15:45,902 [INFO] Starting _process_content
2025-02-06 17:15:45,904 [INFO] Soup Head content: <head>
<script type="application/ld+json">
                {
                    "@context": "https://schema.org",
                    "@type": "Article",
                    "headline": "Article Title",
                    "author": {
                        "@type": "Person",
                        "name": "Author Name"
                    },
                    "description": "Article description"
                }
                </script>
</head>
2025-02-06 17:15:45,904 [INFO] Stylesheet links found: []
2025-02-06 17:15:45,958 [INFO] Starting _process_content
2025-02-06 17:15:45,962 [INFO] Soup Head content: <head>
<!-- Standard meta tags -->
<title>Page Title</title>
<meta content="Page description" name="description"/>
<meta content="key1, key2, key3" name="keywords"/>
<meta content="Author Name" name="author"/>
<!-- OpenGraph meta tags -->
<meta content="OG Title" property="og:title"/>
<meta content="OG description" property="og:description"/>
<meta content="https://example.com/image.jpg" property="og:image"/>
<meta content="https://example.com/page" property="og:url"/>
<!-- Twitter Card meta tags -->
<meta content="summary" name="twitter:card"/>
<meta content="Twitter Title" name="twitter:title"/>
<meta content="Twitter description" name="twitter:description"/>
<meta content="https://example.com/twitter.jpg" name="twitter:image"/>
<!-- Dublin Core meta tags -->
<meta content="DC Title" name="dc.title"/>
<meta content="DC Creator" name="dc.creator"/>
<meta content="DC Subject" name="dc.subject"/>
<!-- Custom meta tags -->
<meta content="Custom value" name="custom-tag"/>
<meta content="Property value" property="custom:property"/>
</head>
2025-02-06 17:15:45,962 [INFO] Stylesheet links found: []
2025-02-06 17:15:45,972 [INFO] Starting _process_content
2025-02-06 17:15:45,974 [INFO] Soup Head content: <head>
<link href="styles.css" rel="stylesheet"/>
<script src="app.js"></script>
</head>
2025-02-06 17:15:45,974 [INFO] Stylesheet links found: [<link href="styles.css" rel="stylesheet"/>]
2025-02-06 17:15:45,974 [INFO] Stylesheet href: styles.css, processed URL: styles.css
2025-02-06 17:15:45,974 [INFO] Extracted stylesheet URL: styles.css
2025-02-06 17:15:45,974 [INFO] Stylesheets assets after append: ['styles.css']
2025-02-06 17:15:45,978 [INFO] Starting _process_content
2025-02-06 17:15:45,988 [INFO] Soup Head content: None
2025-02-06 17:15:45,989 [INFO] Stylesheet links found: []
2025-02-06 17:15:45,993 [INFO] Starting _process_content
2025-02-06 17:15:45,994 [INFO] Soup Head content: None
2025-02-06 17:15:45,994 [INFO] Stylesheet links found: [<link href="styles.css" rel="stylesheet"/>]
2025-02-06 17:15:45,994 [INFO] Stylesheet href: styles.css, processed URL: https://example.com/styles.css
2025-02-06 17:15:45,994 [INFO] Extracted stylesheet URL: https://example.com/styles.css
2025-02-06 17:15:45,994 [INFO] Stylesheets assets after append: ['https://example.com/styles.css']
2025-02-06 17:18:03,691 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 17:18:07,046 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:18:07,050 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:18:07,056 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:18:07,060 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:18:07,063 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:18:07,066 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:18:07,070 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:18:07,073 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:18:07,076 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:18:07,078 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:18:07,081 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:18:07,089 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:18:07,094 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:18:07,097 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:18:07,102 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:18:07,106 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:18:07,110 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:18:07,114 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:18:07,118 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:18:07,123 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:18:07,128 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:18:07,133 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:18:07,142 [INFO] Starting _process_content
2025-02-06 17:18:07,144 [INFO] Soup Head content: None
2025-02-06 17:18:07,144 [INFO] Stylesheet links found: []
2025-02-06 17:18:07,151 [INFO] Starting _process_content
2025-02-06 17:18:07,156 [INFO] Soup Head content: None
2025-02-06 17:18:07,156 [INFO] Stylesheet links found: []
2025-02-06 17:18:07,166 [INFO] Starting _process_content
2025-02-06 17:18:07,173 [INFO] Soup Head content: None
2025-02-06 17:18:07,173 [INFO] Stylesheet links found: []
2025-02-06 17:18:07,178 [INFO] Starting _process_content
2025-02-06 17:18:07,179 [INFO] Soup Head content: None
2025-02-06 17:18:07,179 [INFO] Stylesheet links found: []
2025-02-06 17:18:07,189 [INFO] Starting _process_content
2025-02-06 17:18:07,191 [INFO] Soup Head content: None
2025-02-06 17:18:07,191 [INFO] Stylesheet links found: []
2025-02-06 17:18:07,200 [INFO] Starting _process_content
2025-02-06 17:18:07,201 [INFO] Soup Head content: None
2025-02-06 17:18:07,202 [INFO] Stylesheet links found: []
2025-02-06 17:18:07,209 [INFO] Starting _process_content
2025-02-06 17:18:07,211 [INFO] Soup Head content: <head><title>Test Document</title></head>
2025-02-06 17:18:07,211 [INFO] Stylesheet links found: []
2025-02-06 17:18:07,223 [INFO] Starting _process_content
2025-02-06 17:18:07,225 [INFO] Soup Head content: <head>
<script type="text/javascript">var a = 1;</script>
</head>
2025-02-06 17:18:07,225 [INFO] Stylesheet links found: []
2025-02-06 17:18:07,234 [INFO] Starting _process_content
2025-02-06 17:18:07,240 [INFO] Soup Head content: <head>
<title>Script Test</title>
<script>alert('Hello');</script>
</head>
2025-02-06 17:18:07,240 [INFO] Stylesheet links found: []
2025-02-06 17:18:07,249 [INFO] Starting _process_content
2025-02-06 17:18:07,250 [INFO] Soup Head content: <head>
<title>Invalid JSON-LD</title>
<script type="application/ld+json">
                { invalid json }
                </script>
</head>
2025-02-06 17:18:07,251 [INFO] Stylesheet links found: []
2025-02-06 17:18:07,262 [INFO] Starting _process_content
2025-02-06 17:18:07,263 [INFO] Soup Head content: <head>
<script type="text/javascript">
                    console.log('JS Script');
                </script>
<script type="application/json">
                    {"key": "value"}
                </script>
</head>
2025-02-06 17:18:07,263 [INFO] Stylesheet links found: []
2025-02-06 17:18:07,269 [INFO] Starting _process_content
2025-02-06 17:18:07,270 [INFO] Soup Head content: None
2025-02-06 17:18:07,270 [INFO] Stylesheet links found: []
2025-02-06 17:18:07,278 [INFO] Starting _process_content
2025-02-06 17:18:07,281 [INFO] Soup Head content: None
2025-02-06 17:18:07,282 [INFO] Stylesheet links found: []
2025-02-06 17:18:07,287 [INFO] Starting _process_content
2025-02-06 17:18:07,287 [INFO] Soup Head content: None
2025-02-06 17:18:07,288 [INFO] Stylesheet links found: []
2025-02-06 17:18:07,295 [INFO] Starting _process_content
2025-02-06 17:18:07,295 [INFO] Soup Head content: None
2025-02-06 17:18:07,296 [INFO] Stylesheet links found: []
2025-02-06 17:18:07,303 [INFO] Starting _process_content
2025-02-06 17:18:07,304 [INFO] Soup Head content: None
2025-02-06 17:18:07,305 [INFO] Stylesheet links found: []
2025-02-06 17:18:07,314 [INFO] Starting _process_content
2025-02-06 17:18:07,317 [INFO] Soup Head content: None
2025-02-06 17:18:07,317 [INFO] Stylesheet links found: []
2025-02-06 17:18:07,324 [INFO] Starting _process_content
2025-02-06 17:18:07,325 [INFO] Soup Head content: None
2025-02-06 17:18:07,325 [INFO] Stylesheet links found: []
2025-02-06 17:18:07,328 [INFO] Starting _process_content
2025-02-06 17:18:07,330 [INFO] Soup Head content: None
2025-02-06 17:18:07,330 [INFO] Stylesheet links found: []
2025-02-06 17:18:07,340 [INFO] Starting _process_content
2025-02-06 17:18:07,344 [INFO] Soup Head content: None
2025-02-06 17:18:07,345 [INFO] Stylesheet links found: []
2025-02-06 17:18:07,353 [INFO] Starting _process_content
2025-02-06 17:18:07,357 [INFO] Soup Head content: None
2025-02-06 17:18:07,358 [INFO] Stylesheet links found: []
2025-02-06 17:18:07,364 [INFO] Starting _process_content
2025-02-06 17:18:07,367 [INFO] Soup Head content: <head>
<title>Meta Test</title>
<meta content="A test description." name="description"/>
<meta content="OG Meta Title" property="og:title"/>
</head>
2025-02-06 17:18:07,367 [INFO] Stylesheet links found: []
2025-02-06 17:18:07,376 [INFO] Starting _process_content
2025-02-06 17:18:07,377 [INFO] Soup Head content: <head>
<title>Complex Meta</title>
<meta content="A complex description." name="description"/>
<meta content="Dublin Core Title" name="dc.title"/>
<meta content="website" property="og:type"/>
<script type="application/ld+json">
                {
                    "@context": "https://schema.org",
                    "@type": "WebSite",
                    "name": "Example Site",
                    "url": "https://example.com"
                }
                </script>
</head>
2025-02-06 17:18:07,377 [INFO] Stylesheet links found: []
2025-02-06 17:18:07,393 [INFO] Starting _process_content
2025-02-06 17:18:07,395 [INFO] Soup Head content: <head>
<title>First Title</title>
<title>Second Title</title>
<meta content="First description" name="description"/>
<meta content="Second description" name="description"/>
<meta content="First OG title" property="og:title"/>
<meta content="Second OG title" property="og:title"/>
</head>
2025-02-06 17:18:07,395 [INFO] Stylesheet links found: []
2025-02-06 17:18:07,403 [INFO] Starting _process_content
2025-02-06 17:18:07,406 [INFO] Soup Head content: <head>
<title>Inside head</title>
<meta content="inside, head" name="keywords"/>
</head>
2025-02-06 17:18:07,407 [INFO] Stylesheet links found: []
2025-02-06 17:18:07,415 [INFO] Starting _process_content
2025-02-06 17:18:07,418 [INFO] Soup Head content: <head>
<title></title>
<meta content="" name="empty-content"/>
<meta name="no-content"/>
<meta content="no-name" name=""/>
<meta content="no-name-or-property"/>
<meta content="empty-property" property=""/>
</head>
2025-02-06 17:18:07,418 [INFO] Stylesheet links found: []
2025-02-06 17:18:07,433 [INFO] Starting _process_content
2025-02-06 17:18:07,438 [INFO] Soup Head content: <head>
<script type="application/ld+json">
                {
                    "@context": "https://schema.org",
                    "@type": "Article",
                    "headline": "Article Title",
                    "author": {
                        "@type": "Person",
                        "name": "Author Name"
                    },
                    "description": "Article description"
                }
                </script>
</head>
2025-02-06 17:18:07,438 [INFO] Stylesheet links found: []
2025-02-06 17:18:07,446 [INFO] Starting _process_content
2025-02-06 17:18:07,453 [INFO] Soup Head content: <head>
<!-- Standard meta tags -->
<title>Page Title</title>
<meta content="Page description" name="description"/>
<meta content="key1, key2, key3" name="keywords"/>
<meta content="Author Name" name="author"/>
<!-- OpenGraph meta tags -->
<meta content="OG Title" property="og:title"/>
<meta content="OG description" property="og:description"/>
<meta content="https://example.com/image.jpg" property="og:image"/>
<meta content="https://example.com/page" property="og:url"/>
<!-- Twitter Card meta tags -->
<meta content="summary" name="twitter:card"/>
<meta content="Twitter Title" name="twitter:title"/>
<meta content="Twitter description" name="twitter:description"/>
<meta content="https://example.com/twitter.jpg" name="twitter:image"/>
<!-- Dublin Core meta tags -->
<meta content="DC Title" name="dc.title"/>
<meta content="DC Creator" name="dc.creator"/>
<meta content="DC Subject" name="dc.subject"/>
<!-- Custom meta tags -->
<meta content="Custom value" name="custom-tag"/>
<meta content="Property value" property="custom:property"/>
</head>
2025-02-06 17:18:07,455 [INFO] Stylesheet links found: []
2025-02-06 17:18:07,462 [INFO] Starting _process_content
2025-02-06 17:18:07,464 [INFO] Soup Head content: <head>
<link href="https://example.com/styles.css" rel="stylesheet"/>
<script src="app.js"></script>
</head>
2025-02-06 17:18:07,464 [INFO] Stylesheet links found: [<link href="https://example.com/styles.css" rel="stylesheet"/>]
2025-02-06 17:18:07,464 [INFO] Stylesheet href: https://example.com/styles.css, processed URL: https://example.com/styles.css
2025-02-06 17:18:07,465 [INFO] Extracted stylesheet URL: https://example.com/styles.css
2025-02-06 17:18:07,465 [INFO] Stylesheets assets after append: ['https://example.com/styles.css']
2025-02-06 17:18:07,475 [INFO] Starting _process_content
2025-02-06 17:18:07,476 [INFO] Soup Head content: None
2025-02-06 17:18:07,477 [INFO] Stylesheet links found: []
2025-02-06 17:18:07,487 [INFO] Starting _process_content
2025-02-06 17:18:07,489 [INFO] Soup Head content: None
2025-02-06 17:18:07,490 [INFO] Stylesheet links found: [<link href="https://example.com/styles.css" rel="stylesheet"/>]
2025-02-06 17:18:07,490 [INFO] Stylesheet href: https://example.com/styles.css, processed URL: https://example.com/styles.css
2025-02-06 17:18:07,490 [INFO] Extracted stylesheet URL: https://example.com/styles.css
2025-02-06 17:18:07,490 [INFO] Stylesheets assets after append: ['https://example.com/styles.css']
2025-02-06 17:18:07,500 [INFO] Starting _process_content
2025-02-06 17:18:07,504 [INFO] Soup Head content: None
2025-02-06 17:18:07,505 [INFO] Stylesheet links found: []
2025-02-06 17:18:07,512 [INFO] Starting _process_content
2025-02-06 17:18:07,513 [INFO] Soup Head content: <head>
<base href="https://example.com/subdir/"/>
</head>
2025-02-06 17:18:07,513 [INFO] Stylesheet links found: []
2025-02-06 17:32:55,304 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 17:32:56,753 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:32:56,755 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:32:56,758 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:32:56,761 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:32:56,768 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:32:56,771 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:32:56,774 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:32:56,776 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:32:56,782 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:32:56,785 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:32:56,788 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:32:56,794 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:32:56,806 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:32:56,808 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:32:56,817 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:32:56,820 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:32:56,826 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:32:56,833 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:32:56,837 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:32:56,841 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:32:56,843 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:32:56,850 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:32:56,857 [INFO] Starting _process_content
2025-02-06 17:32:56,858 [INFO] Soup Head content: None
2025-02-06 17:32:56,859 [INFO] Stylesheet links found: []
2025-02-06 17:32:56,868 [INFO] Starting _process_content
2025-02-06 17:32:56,873 [INFO] Soup Head content: None
2025-02-06 17:32:56,873 [INFO] Stylesheet links found: []
2025-02-06 17:32:56,885 [INFO] Starting _process_content
2025-02-06 17:32:56,889 [INFO] Soup Head content: None
2025-02-06 17:32:56,889 [INFO] Stylesheet links found: []
2025-02-06 17:32:56,903 [INFO] Starting _process_content
2025-02-06 17:32:56,907 [INFO] Soup Head content: None
2025-02-06 17:32:56,907 [INFO] Stylesheet links found: []
2025-02-06 17:32:56,917 [INFO] Starting _process_content
2025-02-06 17:32:56,918 [INFO] Soup Head content: None
2025-02-06 17:32:56,918 [INFO] Stylesheet links found: []
2025-02-06 17:32:56,921 [INFO] Starting _process_content
2025-02-06 17:32:56,923 [INFO] Soup Head content: None
2025-02-06 17:32:56,923 [INFO] Stylesheet links found: []
2025-02-06 17:32:56,928 [INFO] Starting _process_content
2025-02-06 17:32:56,932 [INFO] Soup Head content: <head><title>Test Document</title></head>
2025-02-06 17:32:56,932 [INFO] Stylesheet links found: []
2025-02-06 17:32:56,941 [INFO] Starting _process_content
2025-02-06 17:32:56,942 [INFO] Soup Head content: <head>
<script type="text/javascript">var a = 1;</script>
</head>
2025-02-06 17:32:56,942 [INFO] Stylesheet links found: []
2025-02-06 17:32:56,951 [INFO] Starting _process_content
2025-02-06 17:32:56,956 [INFO] Soup Head content: <head>
<title>Script Test</title>
<script>alert('Hello');</script>
</head>
2025-02-06 17:32:56,956 [INFO] Stylesheet links found: []
2025-02-06 17:32:56,960 [INFO] Starting _process_content
2025-02-06 17:32:56,961 [INFO] Soup Head content: <head>
<title>Invalid JSON-LD</title>
<script type="application/ld+json">
                { invalid json }
                </script>
</head>
2025-02-06 17:32:56,961 [INFO] Stylesheet links found: []
2025-02-06 17:32:56,970 [INFO] Starting _process_content
2025-02-06 17:32:56,970 [INFO] Soup Head content: <head>
<script type="text/javascript">
                    console.log('JS Script');
                </script>
<script type="application/json">
                    {"key": "value"}
                </script>
</head>
2025-02-06 17:32:56,971 [INFO] Stylesheet links found: []
2025-02-06 17:32:56,976 [INFO] Starting _process_content
2025-02-06 17:32:56,978 [INFO] Soup Head content: None
2025-02-06 17:32:56,978 [INFO] Stylesheet links found: []
2025-02-06 17:32:56,988 [INFO] Starting _process_content
2025-02-06 17:32:56,990 [INFO] Soup Head content: None
2025-02-06 17:32:56,991 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,001 [INFO] Starting _process_content
2025-02-06 17:32:57,001 [INFO] Soup Head content: None
2025-02-06 17:32:57,002 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,005 [INFO] Starting _process_content
2025-02-06 17:32:57,005 [INFO] Soup Head content: None
2025-02-06 17:32:57,006 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,018 [INFO] Starting _process_content
2025-02-06 17:32:57,019 [INFO] Soup Head content: None
2025-02-06 17:32:57,019 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,025 [INFO] Starting _process_content
2025-02-06 17:32:57,026 [INFO] Soup Head content: None
2025-02-06 17:32:57,026 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,034 [INFO] Starting _process_content
2025-02-06 17:32:57,035 [INFO] Soup Head content: None
2025-02-06 17:32:57,036 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,039 [INFO] Starting _process_content
2025-02-06 17:32:57,040 [INFO] Soup Head content: None
2025-02-06 17:32:57,041 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,044 [INFO] Starting _process_content
2025-02-06 17:32:57,051 [INFO] Soup Head content: None
2025-02-06 17:32:57,052 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,059 [INFO] Starting _process_content
2025-02-06 17:32:57,061 [INFO] Soup Head content: None
2025-02-06 17:32:57,067 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,076 [INFO] Starting _process_content
2025-02-06 17:32:57,078 [INFO] Soup Head content: <head>
<title>Meta Test</title>
<meta content="A test description." name="description"/>
<meta content="OG Meta Title" property="og:title"/>
</head>
2025-02-06 17:32:57,083 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,092 [INFO] Starting _process_content
2025-02-06 17:32:57,093 [INFO] Soup Head content: <head>
<title>Complex Meta</title>
<meta content="A complex description." name="description"/>
<meta content="Dublin Core Title" name="dc.title"/>
<meta content="website" property="og:type"/>
<script type="application/ld+json">
                {
                    "@context": "https://schema.org",
                    "@type": "WebSite",
                    "name": "Example Site",
                    "url": "https://example.com"
                }
                </script>
</head>
2025-02-06 17:32:57,093 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,105 [INFO] Starting _process_content
2025-02-06 17:32:57,108 [INFO] Soup Head content: <head>
<title>First Title</title>
<title>Second Title</title>
<meta content="First description" name="description"/>
<meta content="Second description" name="description"/>
<meta content="First OG title" property="og:title"/>
<meta content="Second OG title" property="og:title"/>
</head>
2025-02-06 17:32:57,109 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,120 [INFO] Starting _process_content
2025-02-06 17:32:57,121 [INFO] Soup Head content: <head>
<title>Inside head</title>
<meta content="inside, head" name="keywords"/>
</head>
2025-02-06 17:32:57,121 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,127 [INFO] Starting _process_content
2025-02-06 17:32:57,133 [INFO] Soup Head content: <head>
<title></title>
<meta content="" name="empty-content"/>
<meta name="no-content"/>
<meta content="no-name" name=""/>
<meta content="no-name-or-property"/>
<meta content="empty-property" property=""/>
</head>
2025-02-06 17:32:57,134 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,139 [INFO] Starting _process_content
2025-02-06 17:32:57,140 [INFO] Soup Head content: <head>
<script type="application/ld+json">
                {
                    "@context": "https://schema.org",
                    "@type": "Article",
                    "headline": "Article Title",
                    "author": {
                        "@type": "Person",
                        "name": "Author Name"
                    },
                    "description": "Article description"
                }
                </script>
</head>
2025-02-06 17:32:57,140 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,150 [INFO] Starting _process_content
2025-02-06 17:32:57,157 [INFO] Soup Head content: <head>
<!-- Standard meta tags -->
<title>Page Title</title>
<meta content="Page description" name="description"/>
<meta content="key1, key2, key3" name="keywords"/>
<meta content="Author Name" name="author"/>
<!-- OpenGraph meta tags -->
<meta content="OG Title" property="og:title"/>
<meta content="OG description" property="og:description"/>
<meta content="https://example.com/image.jpg" property="og:image"/>
<meta content="https://example.com/page" property="og:url"/>
<!-- Twitter Card meta tags -->
<meta content="summary" name="twitter:card"/>
<meta content="Twitter Title" name="twitter:title"/>
<meta content="Twitter description" name="twitter:description"/>
<meta content="https://example.com/twitter.jpg" name="twitter:image"/>
<!-- Dublin Core meta tags -->
<meta content="DC Title" name="dc.title"/>
<meta content="DC Creator" name="dc.creator"/>
<meta content="DC Subject" name="dc.subject"/>
<!-- Custom meta tags -->
<meta content="Custom value" name="custom-tag"/>
<meta content="Property value" property="custom:property"/>
</head>
2025-02-06 17:32:57,157 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,168 [INFO] Starting _process_content
2025-02-06 17:32:57,169 [INFO] Soup Head content: <head>
<link href="https://example.com/styles.css" rel="stylesheet"/>
<script src="app.js"></script>
</head>
2025-02-06 17:32:57,169 [INFO] Stylesheet links found: [<link href="https://example.com/styles.css" rel="stylesheet"/>]
2025-02-06 17:32:57,170 [INFO] Stylesheet href: https://example.com/styles.css, processed URL: https://example.com/styles.css
2025-02-06 17:32:57,170 [INFO] Extracted stylesheet URL: https://example.com/styles.css
2025-02-06 17:32:57,170 [INFO] Stylesheets assets after append: ['https://example.com/styles.css']
2025-02-06 17:32:57,176 [INFO] Starting _process_content
2025-02-06 17:32:57,187 [INFO] Soup Head content: None
2025-02-06 17:32:57,187 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,199 [INFO] Starting _process_content
2025-02-06 17:32:57,201 [INFO] Soup Head content: None
2025-02-06 17:32:57,201 [INFO] Stylesheet links found: [<link href="https://example.com/styles.css" rel="stylesheet"/>]
2025-02-06 17:32:57,201 [INFO] Stylesheet href: https://example.com/styles.css, processed URL: https://example.com/styles.css
2025-02-06 17:32:57,201 [INFO] Extracted stylesheet URL: https://example.com/styles.css
2025-02-06 17:32:57,201 [INFO] Stylesheets assets after append: ['https://example.com/styles.css']
2025-02-06 17:32:57,205 [INFO] Starting _process_content
2025-02-06 17:32:57,208 [INFO] Soup Head content: None
2025-02-06 17:32:57,209 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,220 [INFO] Starting _process_content
2025-02-06 17:32:57,224 [INFO] Soup Head content: <head>
<base href="https://example.com/subdir/"/>
</head>
2025-02-06 17:32:57,224 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,621 [INFO] Starting _process_content
2025-02-06 17:32:57,622 [INFO] Soup Head content: None
2025-02-06 17:32:57,623 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,632 [INFO] Starting _process_content
2025-02-06 17:32:57,635 [INFO] Soup Head content: None
2025-02-06 17:32:57,635 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,703 [INFO] Starting _process_content
2025-02-06 17:32:57,705 [INFO] Soup Head content: None
2025-02-06 17:32:57,705 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,768 [INFO] Starting _process_content
2025-02-06 17:32:57,769 [INFO] Soup Head content: None
2025-02-06 17:32:57,769 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,773 [INFO] Starting _process_content
2025-02-06 17:32:57,774 [INFO] Soup Head content: None
2025-02-06 17:32:57,774 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,782 [INFO] Starting _process_content
2025-02-06 17:32:57,783 [INFO] Soup Head content: None
2025-02-06 17:32:57,784 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,787 [INFO] Starting _process_content
2025-02-06 17:32:57,788 [INFO] Soup Head content: None
2025-02-06 17:32:57,788 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,792 [INFO] Starting _process_content
2025-02-06 17:32:57,793 [INFO] Soup Head content: None
2025-02-06 17:32:57,793 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,802 [INFO] Starting _process_content
2025-02-06 17:32:57,805 [INFO] Soup Head content: None
2025-02-06 17:32:57,806 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,810 [INFO] Starting _process_content
2025-02-06 17:32:57,811 [INFO] Soup Head content: None
2025-02-06 17:32:57,815 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,832 [INFO] Starting _process_content
2025-02-06 17:32:57,838 [INFO] Soup Head content: None
2025-02-06 17:32:57,839 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,843 [INFO] Starting _process_content
2025-02-06 17:32:57,844 [INFO] Soup Head content: None
2025-02-06 17:32:57,848 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,857 [INFO] Starting _process_content
2025-02-06 17:32:57,867 [INFO] Soup Head content: None
2025-02-06 17:32:57,867 [INFO] Stylesheet links found: []
2025-02-06 17:32:57,935 [INFO] Starting _process_content
2025-02-06 17:32:57,937 [INFO] Soup Head content: None
2025-02-06 17:32:57,937 [INFO] Stylesheet links found: []
2025-02-06 17:32:58,001 [INFO] Starting _process_content
2025-02-06 17:32:58,004 [INFO] Soup Head content: None
2025-02-06 17:32:58,004 [INFO] Stylesheet links found: []
2025-02-06 17:32:58,010 [INFO] Starting _process_content
2025-02-06 17:32:58,016 [INFO] Soup Head content: None
2025-02-06 17:32:58,016 [INFO] Stylesheet links found: []
2025-02-06 17:32:58,085 [INFO] Starting _process_content
2025-02-06 17:32:58,086 [INFO] Soup Head content: None
2025-02-06 17:32:58,086 [INFO] Stylesheet links found: []
2025-02-06 17:32:58,089 [INFO] Starting _process_content
2025-02-06 17:32:58,090 [INFO] Soup Head content: None
2025-02-06 17:32:58,091 [INFO] Stylesheet links found: []
2025-02-06 17:32:58,097 [INFO] Starting _process_content
2025-02-06 17:32:58,099 [INFO] Soup Head content: None
2025-02-06 17:32:58,099 [INFO] Stylesheet links found: []
2025-02-06 17:32:58,102 [INFO] Starting _process_content
2025-02-06 17:32:58,104 [INFO] Soup Head content: None
2025-02-06 17:32:58,104 [INFO] Stylesheet links found: []
2025-02-06 17:32:58,108 [INFO] Starting _process_content
2025-02-06 17:32:58,109 [INFO] Soup Head content: None
2025-02-06 17:32:58,109 [INFO] Stylesheet links found: []
2025-02-06 17:32:58,118 [INFO] Starting _process_content
2025-02-06 17:32:58,120 [INFO] Soup Head content: None
2025-02-06 17:32:58,120 [INFO] Stylesheet links found: []
2025-02-06 17:32:58,124 [INFO] Starting _process_content
2025-02-06 17:32:58,125 [INFO] Soup Head content: None
2025-02-06 17:32:58,125 [INFO] Stylesheet links found: []
2025-02-06 17:32:58,134 [INFO] Starting _process_content
2025-02-06 17:32:58,135 [INFO] Soup Head content: None
2025-02-06 17:32:58,136 [INFO] Stylesheet links found: []
2025-02-06 17:32:58,201 [INFO] Starting _process_content
2025-02-06 17:32:58,202 [INFO] Soup Head content: None
2025-02-06 17:32:58,202 [INFO] Stylesheet links found: []
2025-02-06 17:32:58,206 [INFO] Starting _process_content
2025-02-06 17:32:58,207 [INFO] Soup Head content: None
2025-02-06 17:32:58,207 [INFO] Stylesheet links found: []
2025-02-06 17:32:58,268 [INFO] Starting _process_content
2025-02-06 17:32:58,270 [INFO] Soup Head content: <head>
<title>Custom Config Test</title>
<meta content="Custom Meta Value" name="custom:meta"/>
</head>
2025-02-06 17:32:58,271 [INFO] Stylesheet links found: []
2025-02-06 17:32:58,333 [INFO] Starting _process_content
2025-02-06 17:32:58,334 [INFO] Soup Head content: <head>
<title>Inside head</title>
<meta content="inside, head" name="keywords"/>
</head>
2025-02-06 17:32:58,334 [INFO] Stylesheet links found: []
2025-02-06 17:32:58,338 [INFO] Starting _process_content
2025-02-06 17:32:58,400 [INFO] Starting _process_content
2025-02-06 17:32:58,401 [INFO] Soup Head content: None
2025-02-06 17:32:58,401 [INFO] Stylesheet links found: []
2025-02-06 17:32:58,487 [INFO] Starting _process_content
2025-02-06 17:32:58,488 [INFO] Soup Head content: None
2025-02-06 17:32:58,488 [INFO] Stylesheet links found: []
2025-02-06 17:32:58,635 [INFO] Starting _process_content
2025-02-06 17:32:58,636 [INFO] Soup Head content: <head>
<title>Title Only</title>
</head>
2025-02-06 17:32:58,637 [INFO] Stylesheet links found: []
2025-02-06 17:32:58,715 [INFO] Starting _process_content
2025-02-06 17:32:58,716 [INFO] Soup Head content: None
2025-02-06 17:32:58,717 [INFO] Stylesheet links found: []
2025-02-06 17:32:58,720 [INFO] Starting _process_content
2025-02-06 17:32:58,721 [INFO] Soup Head content: None
2025-02-06 17:32:58,721 [INFO] Stylesheet links found: []
2025-02-06 17:32:58,724 [INFO] Starting _process_content
2025-02-06 17:32:58,725 [INFO] Soup Head content: None
2025-02-06 17:32:58,726 [INFO] Stylesheet links found: []
2025-02-06 17:32:58,733 [INFO] Starting _process_content
2025-02-06 17:32:58,734 [INFO] Soup Head content: None
2025-02-06 17:32:58,734 [INFO] Stylesheet links found: []
2025-02-06 17:32:58,740 [INFO] Starting _process_content
2025-02-06 17:32:58,742 [INFO] Soup Head content: None
2025-02-06 17:32:58,742 [INFO] Stylesheet links found: []
2025-02-06 17:32:58,750 [INFO] Starting _process_content
2025-02-06 17:32:58,751 [INFO] Soup Head content: None
2025-02-06 17:32:58,752 [INFO] Stylesheet links found: []
2025-02-06 17:32:58,758 [INFO] Starting _process_content
2025-02-06 17:32:58,759 [INFO] Soup Head content: None
2025-02-06 17:32:58,759 [INFO] Stylesheet links found: []
2025-02-06 17:32:58,768 [INFO] Starting _process_content
2025-02-06 17:32:58,769 [INFO] Soup Head content: None
2025-02-06 17:32:58,769 [INFO] Stylesheet links found: []
2025-02-06 17:32:58,833 [INFO] Starting _process_content
2025-02-06 17:32:58,834 [INFO] Soup Head content: None
2025-02-06 17:32:58,834 [INFO] Stylesheet links found: []
2025-02-06 17:32:58,920 [INFO] Starting _process_content
2025-02-06 17:32:58,921 [INFO] Soup Head content: None
2025-02-06 17:32:58,921 [INFO] Stylesheet links found: []
2025-02-06 17:32:58,999 [INFO] Starting _process_content
2025-02-06 17:32:58,999 [INFO] Soup Head content: None
2025-02-06 17:32:59,000 [INFO] Stylesheet links found: []
2025-02-06 17:32:59,003 [INFO] Starting _process_content
2025-02-06 17:32:59,004 [INFO] Soup Head content: None
2025-02-06 17:32:59,005 [INFO] Stylesheet links found: []
2025-02-06 17:32:59,070 [INFO] Starting _process_content
2025-02-06 17:32:59,071 [INFO] Soup Head content: None
2025-02-06 17:32:59,071 [INFO] Stylesheet links found: []
2025-02-06 17:32:59,133 [INFO] Starting _process_content
2025-02-06 17:32:59,134 [INFO] Soup Head content: <head>
<title>Empty Content Test</title>
</head>
2025-02-06 17:32:59,134 [INFO] Stylesheet links found: []
2025-02-06 17:32:59,197 [INFO] Starting _process_content
2025-02-06 17:32:59,199 [INFO] Soup Head content: <head>
<meta content="0; url=https://example.com/redirect" http-equiv="refresh"/>
<title>Redirect Page</title>
</head>
2025-02-06 17:32:59,199 [INFO] Stylesheet links found: []
2025-02-06 17:32:59,203 [INFO] Starting _process_content
2025-02-06 17:32:59,204 [INFO] Soup Head content: <head>
<title>First Title</title>
<meta content="First description" name="description"/>
<meta content="Second description" name="description"/>
</head>
2025-02-06 17:32:59,204 [INFO] Stylesheet links found: []
2025-02-06 17:32:59,208 [INFO] Starting _process_content
2025-02-06 17:32:59,209 [INFO] Soup Head content: <head>
<title>Case Sensitivity Test</title>
<meta content="Uppercase Description" name="Description"/>
<meta content="Uppercase OG Title" property="OG:Title"/>
</head>
2025-02-06 17:32:59,209 [INFO] Stylesheet links found: []
2025-02-06 17:32:59,217 [INFO] Starting _process_content
2025-02-06 17:32:59,218 [INFO] Soup Head content: <head>
<title>Multiple JSON-LD</title>
<script type="application/ld+json">
                {
                    "@context": "https://schema.org",
                    "@type": "Person",
                    "name": "John Doe",
                    "jobTitle": "Developer"
                }
                </script>
<script type="application/ld+json">
                {
                    "@context": "https://schema.org",
                    "@type": "Organization",
                    "name": "Example Corp",
                    "url": "https://example.com"
                }
                </script>
</head>
2025-02-06 17:32:59,218 [INFO] Stylesheet links found: []
2025-02-06 17:32:59,315 [INFO] Starting _process_content
2025-02-06 17:32:59,316 [INFO] Soup Head content: <head>
<title>Custom Prefixes Test</title>
<meta content="Jane Doe" name="custom:author"/>
<meta content="1.0.0" name="app:version"/>
</head>
2025-02-06 17:32:59,316 [INFO] Stylesheet links found: []
2025-02-06 17:32:59,320 [INFO] Starting _process_content
2025-02-06 17:32:59,321 [INFO] Soup Head content: <head>
<title>Microdata Test</title>
<meta content="Testing microdata extraction." name="description"/>
</head>
2025-02-06 17:32:59,321 [INFO] Stylesheet links found: []
2025-02-06 17:32:59,325 [INFO] Starting _process_content
2025-02-06 17:32:59,328 [INFO] Soup Head content: <head>
<meta content="5; URL='https://example.com/redirected-page'" http-equiv="refresh"/>
<meta content="10; URL='https://example.com/another-page'" http-equiv="refresh"/>
</head>
2025-02-06 17:32:59,328 [INFO] Stylesheet links found: []
2025-02-06 17:32:59,334 [INFO] Starting _process_content
2025-02-06 17:32:59,335 [INFO] Soup Head content: None
2025-02-06 17:32:59,335 [INFO] Stylesheet links found: []
2025-02-06 17:32:59,404 [INFO] Starting _process_content
2025-02-06 17:32:59,406 [INFO] Soup Head content: None
2025-02-06 17:32:59,406 [INFO] Stylesheet links found: []
2025-02-06 17:32:59,497 [INFO] Starting _process_content
2025-02-06 17:32:59,500 [INFO] Soup Head content: None
2025-02-06 17:32:59,500 [INFO] Stylesheet links found: []
2025-02-06 17:32:59,511 [INFO] Starting _process_content
2025-02-06 17:32:59,515 [INFO] Soup Head content: <head>
<title>Custom Config Test</title>
<meta content="Custom Meta Value" name="custom:meta"/>
</head>
2025-02-06 17:32:59,516 [INFO] Stylesheet links found: []
2025-02-06 17:32:59,607 [INFO] Starting _process_content
2025-02-06 17:32:59,611 [INFO] Soup Head content: <head>
<title>Unicode Test</title>
</head>
2025-02-06 17:32:59,611 [INFO] Stylesheet links found: []
2025-02-06 17:32:59,616 [INFO] Starting _process_content
2025-02-06 17:32:59,617 [INFO] Soup Head content: <head>
<meta charset="utf-8"/>
<title>Mixed Encodings</title>
</head>
2025-02-06 17:32:59,618 [INFO] Stylesheet links found: []
2025-02-06 17:32:59,622 [INFO] Starting _process_content
2025-02-06 17:32:59,624 [INFO] Soup Head content: None
2025-02-06 17:32:59,624 [INFO] Stylesheet links found: []
2025-02-06 17:32:59,633 [INFO] Starting _process_content
2025-02-06 17:32:59,634 [INFO] Soup Head content: <head>
<title>Multiple JSON-LD</title>
<script type="application/ld+json">
                {
                    "@context": "https://schema.org",
                    "@type": "Person",
                    "name": "John Doe",
                    "jobTitle": "Developer"
                }
                </script>
<script type="application/ld+json">
                {
                    "@context": "https://schema.org",
                    "@type": "Organization",
                    "name": "Example Corp",
                    "url": "https://example.com"
                }
                </script>
</head>
2025-02-06 17:32:59,634 [INFO] Stylesheet links found: []
2025-02-06 17:33:01,798 [INFO] Starting _process_content
2025-02-06 17:33:01,822 [INFO] Soup Head content: <head>
<title>Redirect Test</title>
<meta content="0; URL='https://example.com/home'" http-equiv="refresh"/>
</head>
2025-02-06 17:33:01,823 [INFO] Stylesheet links found: []
2025-02-06 17:33:01,875 [INFO] Starting _process_content
2025-02-06 17:33:01,923 [INFO] Soup Head content: None
2025-02-06 17:33:01,935 [INFO] Stylesheet links found: []
2025-02-06 17:33:01,975 [INFO] Starting _process_content
2025-02-06 17:33:01,998 [INFO] Soup Head content: None
2025-02-06 17:33:01,999 [INFO] Stylesheet links found: []
2025-02-06 17:33:02,067 [INFO] Starting _process_content
2025-02-06 17:33:02,093 [INFO] Soup Head content: None
2025-02-06 17:33:02,095 [INFO] Stylesheet links found: []
2025-02-06 17:33:02,181 [INFO] Starting _process_content
2025-02-06 17:33:02,187 [INFO] Soup Head content: None
2025-02-06 17:33:02,202 [INFO] Stylesheet links found: []
2025-02-06 17:33:02,268 [INFO] Starting _process_content
2025-02-06 17:33:02,317 [INFO] Soup Head content: None
2025-02-06 17:33:02,319 [INFO] Stylesheet links found: []
2025-02-06 17:33:02,421 [INFO] Starting _process_content
2025-02-06 17:33:02,453 [INFO] Soup Head content: None
2025-02-06 17:33:02,454 [INFO] Stylesheet links found: []
2025-02-06 17:33:02,632 [INFO] Starting _process_content
2025-02-06 17:33:02,654 [INFO] Soup Head content: None
2025-02-06 17:33:02,656 [INFO] Stylesheet links found: []
2025-02-06 17:33:02,722 [INFO] Starting _process_content
2025-02-06 17:33:02,754 [INFO] Soup Head content: None
2025-02-06 17:33:02,755 [INFO] Stylesheet links found: []
2025-02-06 17:33:02,822 [INFO] Starting _process_content
2025-02-06 17:33:02,835 [INFO] Soup Head content: None
2025-02-06 17:33:02,837 [INFO] Stylesheet links found: []
2025-02-06 17:33:02,904 [INFO] Starting _process_content
2025-02-06 17:33:02,936 [INFO] Soup Head content: None
2025-02-06 17:33:02,937 [INFO] Stylesheet links found: []
2025-02-06 17:33:02,985 [INFO] Starting _process_content
2025-02-06 17:33:03,005 [INFO] Soup Head content: None
2025-02-06 17:33:03,009 [INFO] Stylesheet links found: []
2025-02-06 17:33:03,085 [INFO] Starting _process_content
2025-02-06 17:33:03,097 [INFO] Soup Head content: <head>
<title>First Title</title>
</head>
2025-02-06 17:33:03,101 [INFO] Stylesheet links found: []
2025-02-06 17:33:03,182 [INFO] Starting _process_content
2025-02-06 17:33:03,201 [INFO] Soup Head content: None
2025-02-06 17:33:03,202 [INFO] Stylesheet links found: []
2025-02-06 17:33:03,272 [INFO] Starting _process_content
2025-02-06 17:33:03,288 [INFO] Soup Head content: None
2025-02-06 17:33:03,295 [INFO] Stylesheet links found: []
2025-02-06 17:33:03,711 [INFO] Starting _process_content
2025-02-06 17:33:03,719 [INFO] Soup Head content: None
2025-02-06 17:33:03,721 [INFO] Stylesheet links found: []
2025-02-06 17:33:03,769 [INFO] Starting _process_content
2025-02-06 17:33:03,787 [INFO] Soup Head content: None
2025-02-06 17:33:03,788 [INFO] Stylesheet links found: []
2025-02-06 17:33:03,886 [INFO] Starting _process_content
2025-02-06 17:33:03,914 [INFO] Soup Head content: <head>
<!-- Standard meta tags -->
<title>Page Title</title>
<meta content="Page description" name="description"/>
<meta content="key1, key2, key3" name="keywords"/>
<meta content="Author Name" name="author"/>
<!-- OpenGraph meta tags -->
<meta content="OG Title" property="og:title"/>
<meta content="OG description" property="og:description"/>
<meta content="https://example.com/image.jpg" property="og:image"/>
<meta content="https://example.com/page" property="og:url"/>
<!-- Twitter Card meta tags -->
<meta content="summary" name="twitter:card"/>
<meta content="Twitter Title" name="twitter:title"/>
<meta content="Twitter description" name="twitter:description"/>
<meta content="https://example.com/twitter.jpg" name="twitter:image"/>
<!-- Dublin Core meta tags -->
<meta content="DC Title" name="dc.title"/>
<meta content="DC Creator" name="dc.creator"/>
<meta content="DC Subject" name="dc.subject"/>
<!-- Custom meta tags -->
<meta content="Custom value" name="custom-tag"/>
<meta content="Property value" property="custom:property"/>
</head>
2025-02-06 17:33:03,915 [INFO] Stylesheet links found: []
2025-02-06 17:33:03,961 [INFO] Starting _process_content
2025-02-06 17:33:03,970 [INFO] Soup Head content: <head>
<title>Case Sensitivity Test</title>
<meta content="Uppercase Description" name="Description"/>
<meta content="Uppercase OG Title" property="OG:Title"/>
</head>
2025-02-06 17:33:03,971 [INFO] Stylesheet links found: []
2025-02-06 17:33:04,036 [INFO] Starting _process_content
2025-02-06 17:33:04,049 [INFO] Soup Head content: <head>
<title>Microdata Test</title>
<meta content="Testing microdata extraction." name="description"/>
</head>
2025-02-06 17:33:04,051 [INFO] Stylesheet links found: []
2025-02-06 17:33:04,104 [INFO] Starting _process_content
2025-02-06 17:33:04,121 [INFO] Soup Head content: <head>
<meta content="5; URL='https://example.com/redirected-page'" http-equiv="refresh"/>
<meta content="10; URL='https://example.com/another-page'" http-equiv="refresh"/>
</head>
2025-02-06 17:33:04,129 [INFO] Stylesheet links found: []
2025-02-06 17:33:04,186 [INFO] Starting _process_content
2025-02-06 17:33:04,218 [INFO] Soup Head content: <head>
<title>Custom Config Test</title>
<meta content="Custom Meta Value" name="custom:meta"/>
</head>
2025-02-06 17:33:04,220 [INFO] Stylesheet links found: []
2025-02-06 17:33:04,854 [INFO] Starting _process_content
2025-02-06 17:33:04,895 [INFO] Soup Head content: <head>
<title>Custom Config Test</title>
<meta content="Custom Meta Value" name="custom:meta"/>
</head>
2025-02-06 17:33:04,898 [INFO] Stylesheet links found: []
2025-02-06 17:33:05,430 [INFO] Starting _process_content
2025-02-06 17:33:05,434 [INFO] Soup Head content: None
2025-02-06 17:33:05,435 [INFO] Stylesheet links found: []
2025-02-06 17:33:05,547 [INFO] Starting _process_content
2025-02-06 17:33:05,550 [INFO] Soup Head content: None
2025-02-06 17:33:05,551 [INFO] Stylesheet links found: []
2025-02-06 17:33:05,666 [INFO] Starting _process_content
2025-02-06 17:33:05,668 [INFO] Soup Head content: None
2025-02-06 17:33:05,669 [INFO] Stylesheet links found: []
2025-02-06 17:33:05,785 [INFO] Starting _process_content
2025-02-06 17:33:05,790 [INFO] Soup Head content: None
2025-02-06 17:33:05,790 [INFO] Stylesheet links found: []
2025-02-06 17:33:05,797 [INFO] Starting _process_content
2025-02-06 17:33:05,799 [INFO] Soup Head content: None
2025-02-06 17:33:05,799 [INFO] Stylesheet links found: []
2025-02-06 17:33:05,816 [INFO] Starting _process_content
2025-02-06 17:33:05,819 [INFO] Soup Head content: <head>
<link href="style1.css" rel="stylesheet"/>
<link href="style2.css" rel="stylesheet"/>
<script src="script1.js"></script>
<script src="script2.js"></script>
</head>
2025-02-06 17:33:05,819 [INFO] Stylesheet links found: [<link href="style1.css" rel="stylesheet"/>, <link href="style2.css" rel="stylesheet"/>]
2025-02-06 17:33:05,819 [INFO] Stylesheet href: style1.css, processed URL: https://example.com/style1.css
2025-02-06 17:33:05,820 [INFO] Extracted stylesheet URL: https://example.com/style1.css
2025-02-06 17:33:05,827 [INFO] Stylesheets assets after append: ['https://example.com/style1.css']
2025-02-06 17:33:05,828 [INFO] Stylesheet href: style2.css, processed URL: https://example.com/style2.css
2025-02-06 17:33:05,828 [INFO] Extracted stylesheet URL: https://example.com/style2.css
2025-02-06 17:33:05,829 [INFO] Stylesheets assets after append: ['https://example.com/style1.css', 'https://example.com/style2.css']
2025-02-06 17:33:05,844 [INFO] Starting _process_content
2025-02-06 17:33:05,847 [INFO] Soup Head content: <head>
<title>Meta Test</title>
<meta content="keyword1, keyword2" name="keywords"/>
<meta content="keyword3, keyword4" name="keywords"/>
</head>
2025-02-06 17:33:05,848 [INFO] Stylesheet links found: []
2025-02-06 17:33:05,999 [INFO] Starting _process_content
2025-02-06 17:33:06,000 [INFO] Soup Head content: None
2025-02-06 17:33:06,001 [INFO] Stylesheet links found: []
2025-02-06 17:33:06,010 [INFO] Starting _process_content
2025-02-06 17:33:06,012 [INFO] Soup Head content: None
2025-02-06 17:33:06,013 [INFO] Stylesheet links found: []
2025-02-06 17:33:06,019 [INFO] Starting _process_content
2025-02-06 17:33:06,028 [INFO] Soup Head content: None
2025-02-06 17:33:06,029 [INFO] Stylesheet links found: []
2025-02-06 17:33:06,134 [INFO] Starting _process_content
2025-02-06 17:33:06,136 [INFO] Soup Head content: None
2025-02-06 17:33:06,139 [INFO] Stylesheet links found: []
2025-02-06 17:33:06,147 [INFO] Starting _process_content
2025-02-06 17:33:06,149 [INFO] Soup Head content: None
2025-02-06 17:33:06,149 [INFO] Stylesheet links found: []
2025-02-06 17:33:06,176 [INFO] Starting _process_content
2025-02-06 17:33:06,178 [INFO] Soup Head content: <head><title>Test Page</title></head>
2025-02-06 17:33:06,179 [INFO] Stylesheet links found: []
2025-02-06 17:33:06,195 [INFO] Starting _process_content
2025-02-06 17:33:06,196 [INFO] Soup Head content: None
2025-02-06 17:33:06,197 [INFO] Stylesheet links found: []
2025-02-06 17:33:06,249 [INFO] Starting _process_content
2025-02-06 17:33:06,251 [INFO] Soup Head content: None
2025-02-06 17:33:06,252 [INFO] Stylesheet links found: []
2025-02-06 17:33:06,264 [INFO] Starting _process_content
2025-02-06 17:33:06,268 [INFO] Soup Head content: None
2025-02-06 17:33:06,269 [INFO] Stylesheet links found: []
2025-02-06 17:33:06,280 [INFO] Starting _process_content
2025-02-06 17:33:06,281 [INFO] Soup Head content: None
2025-02-06 17:33:06,281 [INFO] Stylesheet links found: []
2025-02-06 17:33:06,292 [INFO] Starting _process_content
2025-02-06 17:33:06,294 [INFO] Soup Head content: None
2025-02-06 17:33:06,294 [INFO] Stylesheet links found: []
2025-02-06 17:33:06,309 [INFO] Starting _process_content
2025-02-06 17:33:06,310 [INFO] Soup Head content: None
2025-02-06 17:33:06,310 [INFO] Stylesheet links found: []
2025-02-06 17:33:06,319 [INFO] Starting _process_content
2025-02-06 17:33:06,325 [INFO] Soup Head content: None
2025-02-06 17:33:06,325 [INFO] Stylesheet links found: []
2025-02-06 17:33:06,334 [INFO] Starting _process_content
2025-02-06 17:33:06,335 [INFO] Soup Head content: None
2025-02-06 17:33:06,336 [INFO] Stylesheet links found: []
2025-02-06 17:33:06,530 [INFO] Starting _process_content
2025-02-06 17:33:06,567 [INFO] Soup Head content: None
2025-02-06 17:33:06,568 [INFO] Stylesheet links found: []
2025-02-06 17:33:06,745 [INFO] Operation took 0.36 seconds
2025-02-06 17:33:06,747 [INFO] Starting _process_content
2025-02-06 17:33:06,748 [INFO] Soup Head content: None
2025-02-06 17:33:06,748 [INFO] Stylesheet links found: []
2025-02-06 17:33:06,772 [INFO] Starting _process_content
2025-02-06 17:33:06,774 [INFO] Soup Head content: <head>
<meta content="Page description" name="description"/>
<meta content="test, example" name="keywords"/>
<meta content="Open Graph Title" property="og:title"/>
</head>
2025-02-06 17:33:06,775 [INFO] Stylesheet links found: []
2025-02-06 17:33:06,917 [DEBUG] IDNA encoding failed for example.com@evil.com: Codepoint U+0040 at position 4 of 'com@evil' not allowed
2025-02-06 17:33:07,058 [DEBUG] IDNA encoding failed for user@example.com: Codepoint U+0040 at position 5 of 'user@example' not allowed
2025-02-06 17:33:07,058 [DEBUG] IDNA encoding failed for :pass@example.com: Empty domain
2025-02-06 17:33:07,127 [DEBUG] IDNA encoding failed for .example.com: Empty Label
2025-02-06 17:33:07,127 [DEBUG] IDNA encoding failed for example..com: Empty Label
2025-02-06 17:33:07,127 [DEBUG] IDNA encoding failed for example.com..: Empty Label
2025-02-06 17:33:07,710 [INFO] test_operation took 0.12 seconds
2025-02-06 17:33:08,094 [INFO] inner took 0.28 seconds
2025-02-06 17:33:08,097 [INFO] outer took 0.38 seconds
2025-02-06 17:33:08,099 [INFO] error_operation took 0.00 seconds
2025-02-06 17:33:09,859 [INFO] test_operation took 0.00 seconds
2025-02-06 17:33:10,202 [INFO] Starting _process_content
2025-02-06 17:33:10,207 [INFO] Soup Head content: None
2025-02-06 17:33:10,212 [INFO] Stylesheet links found: []
2025-02-06 17:33:10,309 [INFO] Starting _process_content
2025-02-06 17:33:10,313 [INFO] Soup Head content: None
2025-02-06 17:33:10,314 [INFO] Stylesheet links found: []
2025-02-06 17:33:10,464 [INFO] Starting _process_content
2025-02-06 17:33:10,482 [INFO] Soup Head content: None
2025-02-06 17:33:10,490 [INFO] Stylesheet links found: []
2025-02-06 17:33:10,740 [INFO] Starting _process_content
2025-02-06 17:33:10,790 [INFO] Soup Head content: None
2025-02-06 17:33:10,792 [INFO] Stylesheet links found: []
2025-02-06 17:33:10,877 [INFO] Starting _process_content
2025-02-06 17:33:10,881 [INFO] Soup Head content: None
2025-02-06 17:33:10,893 [INFO] Stylesheet links found: []
2025-02-06 17:33:11,012 [INFO] Starting _process_content
2025-02-06 17:33:11,023 [INFO] Soup Head content: None
2025-02-06 17:33:11,025 [INFO] Stylesheet links found: []
2025-02-06 17:33:11,123 [INFO] Starting _process_content
2025-02-06 17:33:11,128 [INFO] Soup Head content: None
2025-02-06 17:33:11,129 [INFO] Stylesheet links found: []
2025-02-06 17:33:11,271 [INFO] Starting _process_content
2025-02-06 17:33:11,278 [INFO] Soup Head content: None
2025-02-06 17:33:11,280 [INFO] Stylesheet links found: []
2025-02-06 17:33:11,560 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 17:33:11,562 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:11,589 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-06 17:33:11,590 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-06 17:33:11,591 [INFO] Starting crawl of URL: https://example.com
2025-02-06 17:33:12,196 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 17:33:12,197 [DEBUG] Base URL: https://example.com
2025-02-06 17:33:12,199 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 17:33:12,201 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 17:33:12,202 [DEBUG] Initial domain set to: example.com
2025-02-06 17:33:12,203 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 17:33:12,204 [INFO] Successfully crawled https://example.com
2025-02-06 17:33:12,211 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:12,218 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x0000019CA726ABA0>
2025-02-06 17:33:12,220 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x0000019CA76DB170>, 188633.7906796)])']
connector: <aiohttp.connector.TCPConnector object at 0x0000019CA726A7B0>
2025-02-06 17:33:12,223 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 17:33:12,225 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:12,238 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-06 17:33:12,240 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-06 17:33:12,240 [INFO] Starting crawl of URL: https://example.com
2025-02-06 17:33:12,592 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2025-02-06 17:33:12,593 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2025-02-06 17:33:12,594 [INFO] Starting crawl of URL: https://example.org
2025-02-06 17:33:12,838 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 17:33:12,839 [DEBUG] Base URL: https://example.com
2025-02-06 17:33:12,840 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 17:33:12,841 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 17:33:12,841 [DEBUG] Initial domain set to: example.com
2025-02-06 17:33:12,842 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 17:33:12,843 [INFO] Successfully crawled https://example.com
2025-02-06 17:33:13,305 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 17:33:13,306 [DEBUG] Base URL: https://example.org
2025-02-06 17:33:13,306 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 17:33:13,307 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 17:33:13,307 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 17:33:13,308 [INFO] Successfully crawled https://example.org
2025-02-06 17:33:13,314 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:13,321 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x0000019CA72BE0D0>
2025-02-06 17:33:13,322 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x0000019CA76DB950>, 188634.4335309)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x0000019CA76D9490>, 188634.9029658)])']
connector: <aiohttp.connector.TCPConnector object at 0x0000019CA72BC690>
2025-02-06 17:33:13,326 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 17:33:13,328 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:13,343 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-06 17:33:13,344 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-06 17:33:13,345 [INFO] Starting crawl of URL: https://example.com
2025-02-06 17:33:13,878 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 17:33:13,879 [DEBUG] Base URL: https://example.com
2025-02-06 17:33:13,881 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 17:33:13,882 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 17:33:13,883 [DEBUG] Initial domain set to: example.com
2025-02-06 17:33:13,884 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 17:33:13,886 [INFO] Successfully crawled https://example.com
2025-02-06 17:33:13,891 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:13,896 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x0000019CA72BE490>
2025-02-06 17:33:13,898 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x0000019CA76AC4D0>, 188635.4743627)])']
connector: <aiohttp.connector.TCPConnector object at 0x0000019CA72BDD10>
2025-02-06 17:33:13,903 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 17:33:13,905 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:13,918 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-06 17:33:13,920 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-06 17:33:13,921 [INFO] Starting crawl of URL: https://example.com
2025-02-06 17:33:14,454 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 17:33:14,455 [DEBUG] Base URL: https://example.com
2025-02-06 17:33:14,456 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 17:33:14,457 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 17:33:14,457 [DEBUG] Initial domain set to: example.com
2025-02-06 17:33:14,458 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 17:33:14,459 [INFO] Successfully crawled https://example.com
2025-02-06 17:33:14,470 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 17:33:14,471 [DEBUG] Base URL: https://example.com
2025-02-06 17:33:14,472 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 17:33:14,473 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 17:33:14,473 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 17:33:14,559 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:14,572 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:33:14,574 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:14,594 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:14,625 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 17:33:14,627 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:14,658 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-06 17:33:14,673 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-06 17:33:14,673 [INFO] Starting crawl of URL: https://example.com
2025-02-06 17:33:15,187 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 17:33:15,188 [DEBUG] Base URL: https://example.com
2025-02-06 17:33:15,189 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 17:33:15,190 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 17:33:15,191 [DEBUG] Initial domain set to: example.com
2025-02-06 17:33:15,191 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 17:33:15,193 [INFO] Successfully crawled https://example.com
2025-02-06 17:33:15,220 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:15,226 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x0000019CA712EE90>
2025-02-06 17:33:15,227 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x0000019CA76AEBD0>, 188636.78069)])']
connector: <aiohttp.connector.TCPConnector object at 0x0000019CA7698770>
2025-02-06 17:33:15,236 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 17:33:15,238 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:15,250 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2025-02-06 17:33:15,252 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2025-02-06 17:33:15,255 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2025-02-06 17:33:17,527 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [getaddrinfo failed]
2025-02-06 17:33:17,529 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [getaddrinfo failed]
2025-02-06 17:33:17,530 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [getaddrinfo failed]
2025-02-06 17:33:17,548 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:17,550 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 17:33:17,551 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:17,553 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-06 17:33:17,554 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-06 17:33:17,554 [INFO] Starting crawl of URL: https://example.com
2025-02-06 17:33:17,609 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2025-02-06 17:33:17,611 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2025-02-06 17:33:17,612 [INFO] Starting crawl of URL: https://example.org
2025-02-06 17:33:17,612 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2025-02-06 17:33:17,613 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2025-02-06 17:33:17,613 [INFO] Starting crawl of URL: https://example.net
2025-02-06 17:33:17,778 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 17:33:17,778 [DEBUG] Base URL: https://example.com
2025-02-06 17:33:17,779 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 17:33:17,779 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 17:33:17,779 [DEBUG] Initial domain set to: example.com
2025-02-06 17:33:17,779 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 17:33:17,779 [INFO] Successfully crawled https://example.com
2025-02-06 17:33:18,266 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 17:33:18,266 [DEBUG] Base URL: https://example.org
2025-02-06 17:33:18,266 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 17:33:18,266 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 17:33:18,266 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 17:33:18,267 [INFO] Successfully crawled https://example.org
2025-02-06 17:33:18,820 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 17:33:18,821 [DEBUG] Base URL: https://example.net
2025-02-06 17:33:18,821 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 17:33:18,821 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 17:33:18,821 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 17:33:18,821 [INFO] Successfully crawled https://example.net
2025-02-06 17:33:18,823 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:18,824 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x0000019CA72BE710>
2025-02-06 17:33:18,825 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x0000019CA76ACB30>, 188639.378933)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x0000019CA76AC650>, 188639.868642)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x0000019CA76ACDD0>, 188640.4232129)])']
connector: <aiohttp.connector.TCPConnector object at 0x0000019CA76989D0>
2025-02-06 17:33:18,827 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 17:33:18,828 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:18,830 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-06 17:33:18,830 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-06 17:33:18,831 [INFO] Starting crawl of URL: https://example.com
2025-02-06 17:33:19,017 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 17:33:19,017 [DEBUG] Base URL: https://example.com
2025-02-06 17:33:19,018 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 17:33:19,018 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 17:33:19,018 [DEBUG] Initial domain set to: example.com
2025-02-06 17:33:19,018 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 17:33:19,018 [INFO] Successfully crawled https://example.com
2025-02-06 17:33:19,021 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:19,043 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 17:33:19,043 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:19,046 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:33:19,988 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 17:33:20,044 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:20,047 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 17:33:20,047 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:20,049 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 17:33:20,109 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:20,112 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 17:33:20,113 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:20,115 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 17:33:20,115 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2025-02-06 17:33:20,115 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2025-02-06 17:33:20,115 [INFO] Starting crawl of URL: https://example.com/page
2025-02-06 17:33:20,630 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 17:33:20,632 [DEBUG] Base URL: https://example.com/page
2025-02-06 17:33:20,634 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 17:33:20,636 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 17:33:20,638 [DEBUG] Initial domain set to: example.com
2025-02-06 17:33:20,642 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 17:33:20,649 [INFO] Successfully crawled https://example.com/page
2025-02-06 17:33:20,652 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2025-02-06 17:33:20,654 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2025-02-06 17:33:20,658 [INFO] Starting crawl of URL: https://other-domain.com/page
2025-02-06 17:33:22,870 [DEBUG] 
Evaluating link: ./
2025-02-06 17:33:22,870 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,870 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2025-02-06 17:33:22,872 [DEBUG] Absolute link: https://other-domain.com/
2025-02-06 17:33:22,873 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,873 [DEBUG] 
Evaluating link: ./
2025-02-06 17:33:22,874 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,874 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2025-02-06 17:33:22,874 [DEBUG] Absolute link: https://other-domain.com/
2025-02-06 17:33:22,874 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,874 [DEBUG] 
Evaluating link: ./hiyou.php
2025-02-06 17:33:22,875 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,875 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2025-02-06 17:33:22,875 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2025-02-06 17:33:22,875 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,876 [DEBUG] 
Evaluating link: ./date.php
2025-02-06 17:33:22,876 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,876 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2025-02-06 17:33:22,876 [DEBUG] Absolute link: https://other-domain.com/date.php
2025-02-06 17:33:22,877 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,877 [DEBUG] 
Evaluating link: ./contact.php
2025-02-06 17:33:22,877 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,877 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2025-02-06 17:33:22,878 [DEBUG] Absolute link: https://other-domain.com/contact.php
2025-02-06 17:33:22,878 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,878 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-06 17:33:22,878 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,879 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-06 17:33:22,879 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-06 17:33:22,879 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2025-02-06 17:33:22,902 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,912 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-06 17:33:22,912 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 17:33:22,912 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,912 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 17:33:22,913 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 17:33:22,913 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-06 17:33:22,913 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-06 17:33:22,913 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,913 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-06 17:33:22,913 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-06 17:33:22,913 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-06 17:33:22,913 [DEBUG] 
Evaluating link: ./contact
2025-02-06 17:33:22,914 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,914 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2025-02-06 17:33:22,914 [DEBUG] Absolute link: https://other-domain.com/contact
2025-02-06 17:33:22,914 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,917 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,929 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-06 17:33:22,929 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 17:33:22,929 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,929 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 17:33:22,929 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 17:33:22,929 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-06 17:33:22,930 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-06 17:33:22,930 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,930 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-06 17:33:22,930 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-06 17:33:22,930 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-06 17:33:22,934 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,947 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2025-02-06 17:33:22,947 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-06 17:33:22,947 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,948 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-06 17:33:22,948 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-06 17:33:22,948 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2025-02-06 17:33:22,948 [DEBUG] 
Evaluating link: #indexes
2025-02-06 17:33:22,949 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,949 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2025-02-06 17:33:22,949 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,949 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,950 [DEBUG] 
Evaluating link: #a
2025-02-06 17:33:22,950 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,950 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2025-02-06 17:33:22,950 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,951 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,951 [DEBUG] 
Evaluating link: #b
2025-02-06 17:33:22,951 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,952 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2025-02-06 17:33:22,952 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,952 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,952 [DEBUG] 
Evaluating link: #c
2025-02-06 17:33:22,953 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,953 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2025-02-06 17:33:22,953 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,953 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,954 [DEBUG] 
Evaluating link: #d
2025-02-06 17:33:22,956 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,957 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2025-02-06 17:33:22,958 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,958 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,959 [DEBUG] 
Evaluating link: #e
2025-02-06 17:33:22,959 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,960 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2025-02-06 17:33:22,960 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,960 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,961 [DEBUG] 
Evaluating link: #f
2025-02-06 17:33:22,961 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,961 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2025-02-06 17:33:22,962 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,962 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,962 [DEBUG] 
Evaluating link: #g
2025-02-06 17:33:22,963 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,963 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2025-02-06 17:33:22,963 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,964 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,964 [DEBUG] 
Evaluating link: #h
2025-02-06 17:33:22,964 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,965 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2025-02-06 17:33:22,965 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,966 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,966 [DEBUG] 
Evaluating link: #i
2025-02-06 17:33:22,966 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,967 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2025-02-06 17:33:22,967 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,967 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,968 [DEBUG] 
Evaluating link: #j
2025-02-06 17:33:22,968 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,968 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2025-02-06 17:33:22,969 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,969 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,969 [DEBUG] 
Evaluating link: #k
2025-02-06 17:33:22,970 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,970 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2025-02-06 17:33:22,970 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,973 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,973 [DEBUG] 
Evaluating link: #l
2025-02-06 17:33:22,974 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,974 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2025-02-06 17:33:22,974 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,974 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,975 [DEBUG] 
Evaluating link: #m
2025-02-06 17:33:22,975 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,975 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2025-02-06 17:33:22,975 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,975 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,976 [DEBUG] 
Evaluating link: #n
2025-02-06 17:33:22,976 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,976 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2025-02-06 17:33:22,976 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,976 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,976 [DEBUG] 
Evaluating link: #o
2025-02-06 17:33:22,976 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,976 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2025-02-06 17:33:22,977 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,977 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,977 [DEBUG] 
Evaluating link: #p
2025-02-06 17:33:22,977 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,977 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2025-02-06 17:33:22,977 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,977 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,977 [DEBUG] 
Evaluating link: #q
2025-02-06 17:33:22,978 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,978 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2025-02-06 17:33:22,978 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,978 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,978 [DEBUG] 
Evaluating link: #r
2025-02-06 17:33:22,978 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,978 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2025-02-06 17:33:22,979 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,979 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,979 [DEBUG] 
Evaluating link: #s
2025-02-06 17:33:22,979 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,979 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2025-02-06 17:33:22,979 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,979 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,979 [DEBUG] 
Evaluating link: #t
2025-02-06 17:33:22,979 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,980 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2025-02-06 17:33:22,980 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,980 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,980 [DEBUG] 
Evaluating link: #u
2025-02-06 17:33:22,980 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,980 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2025-02-06 17:33:22,980 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,980 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,981 [DEBUG] 
Evaluating link: #v
2025-02-06 17:33:22,981 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,981 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2025-02-06 17:33:22,981 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,981 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,981 [DEBUG] 
Evaluating link: #w
2025-02-06 17:33:22,981 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,981 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2025-02-06 17:33:22,982 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,982 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,982 [DEBUG] 
Evaluating link: #x
2025-02-06 17:33:22,982 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,982 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2025-02-06 17:33:22,982 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,982 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,982 [DEBUG] 
Evaluating link: #y
2025-02-06 17:33:22,983 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,983 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2025-02-06 17:33:22,983 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,983 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,983 [DEBUG] 
Evaluating link: #z
2025-02-06 17:33:22,983 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,983 [DEBUG] Normalized URL: https://other-domain.com/page#z -> https://other-domain.com/page
2025-02-06 17:33:22,983 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,984 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,984 [DEBUG] 
Evaluating link: #A
2025-02-06 17:33:22,984 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,984 [DEBUG] Normalized URL: https://other-domain.com/page#A -> https://other-domain.com/page
2025-02-06 17:33:22,984 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,984 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,984 [DEBUG] 
Evaluating link: #B
2025-02-06 17:33:22,984 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,985 [DEBUG] Normalized URL: https://other-domain.com/page#B -> https://other-domain.com/page
2025-02-06 17:33:22,985 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,985 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,985 [DEBUG] 
Evaluating link: #C
2025-02-06 17:33:22,985 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,985 [DEBUG] Normalized URL: https://other-domain.com/page#C -> https://other-domain.com/page
2025-02-06 17:33:22,985 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,985 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,986 [DEBUG] 
Evaluating link: #D
2025-02-06 17:33:22,986 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:22,986 [DEBUG] Normalized URL: https://other-domain.com/page#D -> https://other-domain.com/page
2025-02-06 17:33:22,986 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:22,986 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:22,994 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,007 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-06 17:33:23,008 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 17:33:23,008 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,008 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 17:33:23,008 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 17:33:23,009 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-06 17:33:23,009 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-06 17:33:23,009 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,009 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-06 17:33:23,009 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-06 17:33:23,009 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-06 17:33:23,013 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,019 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2025-02-06 17:33:23,020 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-06 17:33:23,020 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,020 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-06 17:33:23,020 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-06 17:33:23,020 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2025-02-06 17:33:23,023 [DEBUG] 
Evaluating link: https://spriq.jp/
2025-02-06 17:33:23,023 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,024 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2025-02-06 17:33:23,024 [DEBUG] Absolute link: https://spriq.jp/
2025-02-06 17:33:23,024 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2025-02-06 17:33:23,030 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,044 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-06 17:33:23,051 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,067 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-06 17:33:23,068 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 17:33:23,068 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,068 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 17:33:23,069 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-06 17:33:23,069 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-06 17:33:23,069 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-06 17:33:23,070 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,070 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-06 17:33:23,073 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-06 17:33:23,073 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-06 17:33:23,073 [DEBUG] 
Evaluating link: ./contact
2025-02-06 17:33:23,074 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,075 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2025-02-06 17:33:23,075 [DEBUG] Absolute link: https://other-domain.com/contact
2025-02-06 17:33:23,075 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,075 [DEBUG] 
Evaluating link: ./
2025-02-06 17:33:23,075 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,075 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2025-02-06 17:33:23,076 [DEBUG] Absolute link: https://other-domain.com/
2025-02-06 17:33:23,076 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,076 [DEBUG] 
Evaluating link: ./hiyou.php
2025-02-06 17:33:23,076 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,076 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2025-02-06 17:33:23,076 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2025-02-06 17:33:23,076 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,076 [DEBUG] 
Evaluating link: ./date.php
2025-02-06 17:33:23,076 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,077 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2025-02-06 17:33:23,077 [DEBUG] Absolute link: https://other-domain.com/date.php
2025-02-06 17:33:23,077 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,077 [DEBUG] 
Evaluating link: #z
2025-02-06 17:33:23,077 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,077 [DEBUG] Normalized URL: https://other-domain.com/page#z -> https://other-domain.com/page
2025-02-06 17:33:23,077 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:23,077 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,078 [DEBUG] 
Evaluating link: #A
2025-02-06 17:33:23,078 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,078 [DEBUG] Normalized URL: https://other-domain.com/page#A -> https://other-domain.com/page
2025-02-06 17:33:23,078 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:23,078 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,078 [DEBUG] 
Evaluating link: #B
2025-02-06 17:33:23,078 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,079 [DEBUG] Normalized URL: https://other-domain.com/page#B -> https://other-domain.com/page
2025-02-06 17:33:23,079 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:23,079 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,079 [DEBUG] 
Evaluating link: #C
2025-02-06 17:33:23,079 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,079 [DEBUG] Normalized URL: https://other-domain.com/page#C -> https://other-domain.com/page
2025-02-06 17:33:23,079 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:23,079 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,079 [DEBUG] 
Evaluating link: #D
2025-02-06 17:33:23,080 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,080 [DEBUG] Normalized URL: https://other-domain.com/page#D -> https://other-domain.com/page
2025-02-06 17:33:23,080 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:23,080 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,080 [DEBUG] 
Evaluating link: #u
2025-02-06 17:33:23,080 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,080 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2025-02-06 17:33:23,080 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:23,081 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,081 [DEBUG] 
Evaluating link: #v
2025-02-06 17:33:23,081 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,081 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2025-02-06 17:33:23,081 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:23,081 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,081 [DEBUG] 
Evaluating link: #w
2025-02-06 17:33:23,081 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,081 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2025-02-06 17:33:23,082 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:23,082 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,082 [DEBUG] 
Evaluating link: #x
2025-02-06 17:33:23,082 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,082 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2025-02-06 17:33:23,082 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:23,082 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,082 [DEBUG] 
Evaluating link: #y
2025-02-06 17:33:23,083 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,083 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2025-02-06 17:33:23,083 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:23,083 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,083 [DEBUG] 
Evaluating link: #p
2025-02-06 17:33:23,083 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,083 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2025-02-06 17:33:23,083 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:23,084 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,084 [DEBUG] 
Evaluating link: #q
2025-02-06 17:33:23,084 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,084 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2025-02-06 17:33:23,084 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:23,084 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,084 [DEBUG] 
Evaluating link: #r
2025-02-06 17:33:23,084 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,085 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2025-02-06 17:33:23,085 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:23,085 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,085 [DEBUG] 
Evaluating link: #s
2025-02-06 17:33:23,085 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,085 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2025-02-06 17:33:23,085 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:23,085 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,085 [DEBUG] 
Evaluating link: #t
2025-02-06 17:33:23,085 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,086 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2025-02-06 17:33:23,086 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:23,086 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,086 [DEBUG] 
Evaluating link: #k
2025-02-06 17:33:23,086 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,086 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2025-02-06 17:33:23,086 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:23,086 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,087 [DEBUG] 
Evaluating link: #l
2025-02-06 17:33:23,087 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,087 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2025-02-06 17:33:23,090 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:23,090 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,091 [DEBUG] 
Evaluating link: #m
2025-02-06 17:33:23,091 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,091 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2025-02-06 17:33:23,092 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:23,092 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,092 [DEBUG] 
Evaluating link: #n
2025-02-06 17:33:23,093 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,093 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2025-02-06 17:33:23,093 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:23,094 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,094 [DEBUG] 
Evaluating link: #o
2025-02-06 17:33:23,094 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,095 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2025-02-06 17:33:23,095 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:23,095 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,095 [DEBUG] 
Evaluating link: #f
2025-02-06 17:33:23,096 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,096 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2025-02-06 17:33:23,096 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:23,097 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,097 [DEBUG] 
Evaluating link: #g
2025-02-06 17:33:23,097 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,098 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2025-02-06 17:33:23,098 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:23,098 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,099 [DEBUG] 
Evaluating link: #h
2025-02-06 17:33:23,099 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,099 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2025-02-06 17:33:23,100 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:23,100 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,100 [DEBUG] 
Evaluating link: #i
2025-02-06 17:33:23,100 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,101 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2025-02-06 17:33:23,101 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:23,101 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,102 [DEBUG] 
Evaluating link: #j
2025-02-06 17:33:23,102 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,102 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2025-02-06 17:33:23,102 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:23,103 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,103 [DEBUG] 
Evaluating link: #b
2025-02-06 17:33:23,103 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,104 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2025-02-06 17:33:23,106 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:23,106 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,107 [DEBUG] 
Evaluating link: #c
2025-02-06 17:33:23,107 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,107 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2025-02-06 17:33:23,108 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:23,108 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,108 [DEBUG] 
Evaluating link: #d
2025-02-06 17:33:23,108 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,109 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2025-02-06 17:33:23,109 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:23,109 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,110 [DEBUG] 
Evaluating link: #e
2025-02-06 17:33:23,110 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,110 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2025-02-06 17:33:23,110 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:23,111 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,111 [DEBUG] 
Evaluating link: #site
2025-02-06 17:33:23,111 [DEBUG] Base URL: https://other-domain.com/page
2025-02-06 17:33:23,112 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2025-02-06 17:33:23,112 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-06 17:33:23,112 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-06 17:33:23,112 [INFO] Successfully crawled https://other-domain.com/page
2025-02-06 17:33:23,136 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:23,142 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 17:33:23,143 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:23,146 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2025-02-06 17:33:23,147 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2025-02-06 17:33:23,147 [INFO] Starting crawl of URL: https://example.com/page0
2025-02-06 17:33:24,088 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 17:33:24,088 [DEBUG] Base URL: https://example.com/page0
2025-02-06 17:33:24,088 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 17:33:24,089 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 17:33:24,089 [DEBUG] Initial domain set to: example.com
2025-02-06 17:33:24,089 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 17:33:24,089 [INFO] Successfully crawled https://example.com/page0
2025-02-06 17:33:24,090 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2025-02-06 17:33:24,090 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2025-02-06 17:33:24,090 [INFO] Starting crawl of URL: https://example.com/page1
2025-02-06 17:33:24,347 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 17:33:24,347 [DEBUG] Base URL: https://example.com/page1
2025-02-06 17:33:24,347 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 17:33:24,348 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 17:33:24,348 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 17:33:24,348 [INFO] Successfully crawled https://example.com/page1
2025-02-06 17:33:24,348 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2025-02-06 17:33:24,348 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2025-02-06 17:33:24,348 [INFO] Starting crawl of URL: https://example.com/page2
2025-02-06 17:33:24,842 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 17:33:24,842 [DEBUG] Base URL: https://example.com/page2
2025-02-06 17:33:24,843 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 17:33:24,843 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 17:33:24,843 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 17:33:24,843 [INFO] Successfully crawled https://example.com/page2
2025-02-06 17:33:24,843 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2025-02-06 17:33:24,843 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2025-02-06 17:33:24,843 [INFO] Starting crawl of URL: https://example.com/page3
2025-02-06 17:33:25,670 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 17:33:25,670 [DEBUG] Base URL: https://example.com/page3
2025-02-06 17:33:25,671 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 17:33:25,671 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 17:33:25,671 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 17:33:25,672 [INFO] Successfully crawled https://example.com/page3
2025-02-06 17:33:25,672 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2025-02-06 17:33:25,672 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2025-02-06 17:33:25,672 [INFO] Starting crawl of URL: https://example.com/page4
2025-02-06 17:33:26,429 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 17:33:26,429 [DEBUG] Base URL: https://example.com/page4
2025-02-06 17:33:26,430 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 17:33:26,430 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 17:33:26,430 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 17:33:26,430 [INFO] Successfully crawled https://example.com/page4
2025-02-06 17:33:26,431 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 17:33:26,431 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2025-02-06 17:33:26,431 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2025-02-06 17:33:26,431 [INFO] Starting crawl of URL: https://example.com/page0
2025-02-06 17:33:26,638 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 17:33:26,639 [DEBUG] Base URL: https://example.com/page0
2025-02-06 17:33:26,639 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 17:33:26,639 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 17:33:26,639 [DEBUG] Initial domain set to: example.com
2025-02-06 17:33:26,639 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 17:33:26,639 [INFO] Successfully crawled https://example.com/page0
2025-02-06 17:33:26,639 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2025-02-06 17:33:26,639 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2025-02-06 17:33:26,640 [INFO] Starting crawl of URL: https://example.com/page1
2025-02-06 17:33:27,066 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 17:33:27,066 [DEBUG] Base URL: https://example.com/page1
2025-02-06 17:33:27,066 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 17:33:27,066 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 17:33:27,066 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 17:33:27,066 [INFO] Successfully crawled https://example.com/page1
2025-02-06 17:33:27,066 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2025-02-06 17:33:27,067 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2025-02-06 17:33:27,067 [INFO] Starting crawl of URL: https://example.com/page2
2025-02-06 17:33:27,577 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 17:33:27,577 [DEBUG] Base URL: https://example.com/page2
2025-02-06 17:33:27,577 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 17:33:27,578 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 17:33:27,578 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 17:33:27,578 [INFO] Successfully crawled https://example.com/page2
2025-02-06 17:33:27,578 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2025-02-06 17:33:27,578 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2025-02-06 17:33:27,578 [INFO] Starting crawl of URL: https://example.com/page3
2025-02-06 17:33:28,103 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 17:33:28,104 [DEBUG] Base URL: https://example.com/page3
2025-02-06 17:33:28,104 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 17:33:28,104 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 17:33:28,104 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 17:33:28,104 [INFO] Successfully crawled https://example.com/page3
2025-02-06 17:33:28,104 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2025-02-06 17:33:28,105 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2025-02-06 17:33:28,105 [INFO] Starting crawl of URL: https://example.com/page4
2025-02-06 17:33:28,616 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 17:33:28,617 [DEBUG] Base URL: https://example.com/page4
2025-02-06 17:33:28,617 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 17:33:28,617 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 17:33:28,618 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 17:33:28,618 [INFO] Successfully crawled https://example.com/page4
2025-02-06 17:33:28,636 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:28,640 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:28,656 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:28,659 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:28,679 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:28,684 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:28,710 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x0000019CA72BCA50>
2025-02-06 17:33:28,710 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x0000019CA76AD9D0>, 188636.0484244)])']
connector: <aiohttp.connector.TCPConnector object at 0x0000019CA723A060>
2025-02-06 17:33:28,711 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x0000019CA712FED0>
2025-02-06 17:33:28,711 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x0000019CA72BEAD0>
2025-02-06 17:33:28,711 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x0000019CA76D8F50>, 188642.2093789)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x0000019CA76DAA50>, 188643.1229581)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x0000019CA76DB950>, 188644.0040574)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x0000019CA76D9910>, 188644.4355297)])']
connector: <aiohttp.connector.TCPConnector object at 0x0000019CA76988A0>
2025-02-06 17:33:28,712 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x0000019CA72BEE90>
2025-02-06 17:33:28,712 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x0000019CA76CBFB0>, 188648.0311669)])']
connector: <aiohttp.connector.TCPConnector object at 0x0000019CA76995B0>
2025-02-06 17:33:28,712 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x0000019CA72BF890>
2025-02-06 17:33:28,713 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x0000019CA7B60AD0>, 188650.2178751)])']
connector: <aiohttp.connector.TCPConnector object at 0x0000019CA76996E0>
2025-02-06 17:33:28,737 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:28,739 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:28,755 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:28,757 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:28,774 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:28,780 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:28,805 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:28,808 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:28,827 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:28,831 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:28,859 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:28,863 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:28,887 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:28,995 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:33:28,996 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:29,003 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:29,011 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:33:29,012 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:29,014 [DEBUG] 
Evaluating link: https://example.com/doc1
2025-02-06 17:33:29,014 [DEBUG] Target domain: example.com
2025-02-06 17:33:29,014 [DEBUG] URL domain: example.com
2025-02-06 17:33:29,015 [DEBUG] URL approved for crawling: https://example.com/doc1
2025-02-06 17:33:29,017 [DEBUG] 
Evaluating link: https://example.com/excluded/page
2025-02-06 17:33:29,017 [DEBUG] Target domain: example.com
2025-02-06 17:33:29,017 [DEBUG] URL domain: example.com
2025-02-06 17:33:29,019 [DEBUG] URL approved for crawling: https://example.com/excluded/page
2025-02-06 17:33:29,045 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:29,052 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:33:29,053 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:29,072 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:29,078 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:33:29,079 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:29,088 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-06 17:33:29,138 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:29,142 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:33:29,143 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:29,170 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:29,173 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:33:29,175 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:29,179 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-06 17:33:29,228 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:29,231 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:33:29,235 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:29,237 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-06 17:33:29,281 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:29,287 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:33:29,289 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:29,292 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-06 17:33:29,352 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:29,354 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:33:29,355 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:29,357 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-06 17:33:29,407 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:29,409 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:33:29,410 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:29,412 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-06 17:33:29,460 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:29,463 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:33:29,469 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:29,472 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-06 17:33:29,528 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:29,541 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:29,991 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:29,993 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:29,995 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:33:30,875 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:30,986 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:31,054 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:33:31,318 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:31,461 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:33:31,686 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:33:31,911 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:33:32,093 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:33:32,120 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x0000019CA72BD450>
2025-02-06 17:33:32,121 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x0000019CA7BF8F50>, 188651.3049088)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x0000019CA7BF96D0>, 188651.5418074)])']
connector: <aiohttp.connector.TCPConnector object at 0x0000019CA769B490>
2025-02-06 17:33:32,151 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:33:32,203 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:33:32,254 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:33:32,303 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:33:32,369 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:33:32,424 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:33:33,286 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:34,018 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:34,022 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 17:33:34,022 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-06 17:33:34,022 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-06 17:33:34,023 [INFO] Starting crawl of URL: https://example.com
2025-02-06 17:33:34,232 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-06 17:33:34,232 [DEBUG] Base URL: https://example.com
2025-02-06 17:33:34,232 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-06 17:33:34,233 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-06 17:33:34,233 [DEBUG] Initial domain set to: example.com
2025-02-06 17:33:34,233 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-06 17:33:34,234 [INFO] Successfully crawled https://example.com
2025-02-06 17:33:34,234 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x0000019CA7B72C10>
2025-02-06 17:33:34,235 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x0000019CA85CAAB0>, 188655.8333521)])']
connector: <aiohttp.connector.TCPConnector object at 0x0000019CA7698D60>
2025-02-06 17:33:34,239 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2025-02-06 17:33:34,618 [INFO] Operation took 0.10 seconds
2025-02-06 17:33:35,191 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 17:33:35,192 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:33:35,194 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:42,758 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-3\test_full_site_crawl0\test_docs/index.html
2025-02-06 17:33:42,758 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-06 17:33:42,806 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:42,821 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 17:33:42,821 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:33:42,822 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:50,470 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-3\test_content_processing_pipeli0\test_docs/guide.html
2025-02-06 17:33:50,475 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-06 17:33:51,289 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:51,351 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 17:33:51,356 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:33:51,359 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:59,213 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-3\test_quality_checks0\test_docs/api.html
2025-02-06 17:33:59,215 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-06 17:33:59,728 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:33:59,797 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 17:33:59,799 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:33:59,802 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:34:07,293 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-3\test_document_organization0\test_docs/index.html
2025-02-06 17:34:07,293 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-06 17:34:07,326 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:34:07,340 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 17:34:07,340 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:34:07,341 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:34:15,217 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-3\test_search_functionality0\test_docs/index.html
2025-02-06 17:34:15,219 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-06 17:34:15,713 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:34:15,781 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-06 17:34:15,782 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:34:15,785 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:34:23,274 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-3\test_error_handling_and_recove0\test_docs/index.html
2025-02-06 17:34:23,275 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-06 17:34:23,311 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:34:23,317 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:34:23,335 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:34:23,337 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:34:23,339 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:34:23,340 [ERROR] Error during crawl: 'str' object has no attribute 'url'
2025-02-06 17:34:23,358 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:34:23,362 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:34:23,375 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:34:23,379 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:34:23,386 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 17:34:23,404 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:34:25,583 [INFO] Starting _process_content
2025-02-06 17:34:25,585 [INFO] Soup Head content: <head>
<title>Test Document</title>
<meta content="Test description" name="description"/>
</head>
2025-02-06 17:34:25,585 [INFO] Stylesheet links found: []
2025-02-06 17:34:25,610 [INFO] Starting _process_content
2025-02-06 17:34:25,635 [INFO] Starting _process_content
2025-02-06 17:34:25,637 [INFO] Soup Head content: None
2025-02-06 17:34:25,637 [INFO] Stylesheet links found: []
2025-02-06 17:34:25,639 [INFO] Starting _process_content
2025-02-06 17:34:25,640 [INFO] Soup Head content: None
2025-02-06 17:34:25,640 [INFO] Stylesheet links found: []
2025-02-06 17:34:25,649 [INFO] Starting _process_content
2025-02-06 17:34:25,651 [INFO] Soup Head content: <head>
<script>alert('test');</script>
<style>body { color: red; }</style>
</head>
2025-02-06 17:34:25,651 [INFO] Stylesheet links found: []
2025-02-06 17:34:25,654 [INFO] Starting _process_content
2025-02-06 17:34:25,655 [INFO] Soup Head content: None
2025-02-06 17:34:25,656 [INFO] Stylesheet links found: []
2025-02-06 17:34:25,661 [INFO] Starting _process_content
2025-02-06 17:34:25,668 [INFO] Soup Head content: None
2025-02-06 17:34:25,668 [INFO] Stylesheet links found: []
2025-02-06 17:34:25,676 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:34:25,711 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:34:25,717 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:34:25,737 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:34:25,740 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:34:25,768 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:34:25,771 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:34:25,804 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:34:25,808 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:34:25,832 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:34:25,835 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:34:25,858 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:34:25,861 [DEBUG] Using proactor: IocpProactor
2025-02-06 17:34:25,876 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:09:11,482 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 18:09:13,824 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:09:13,826 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:09:13,830 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:09:13,838 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:09:13,842 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:09:13,845 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:09:13,847 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 18:09:13,850 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:09:13,852 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:09:13,854 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 18:09:13,856 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:09:13,859 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:09:13,862 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:09:13,864 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:09:13,870 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:09:13,873 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:09:13,878 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:09:13,883 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:09:13,887 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:09:13,889 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:09:13,892 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 18:09:13,894 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:09:13,900 [INFO] Starting _process_content
2025-02-06 18:09:13,902 [INFO] Soup Head content: None
2025-02-06 18:09:13,903 [INFO] Stylesheet links found: []
2025-02-06 18:09:13,913 [INFO] Starting _process_content
2025-02-06 18:09:13,919 [INFO] Soup Head content: None
2025-02-06 18:09:13,921 [INFO] Stylesheet links found: []
2025-02-06 18:09:13,930 [INFO] Starting _process_content
2025-02-06 18:09:13,943 [INFO] Soup Head content: None
2025-02-06 18:09:13,943 [INFO] Stylesheet links found: []
2025-02-06 18:09:13,960 [INFO] Starting _process_content
2025-02-06 18:09:13,961 [INFO] Soup Head content: None
2025-02-06 18:09:13,961 [INFO] Stylesheet links found: []
2025-02-06 18:09:13,967 [INFO] Starting _process_content
2025-02-06 18:09:13,969 [INFO] Soup Head content: None
2025-02-06 18:09:13,970 [INFO] Stylesheet links found: []
2025-02-06 18:09:13,972 [INFO] Starting _process_content
2025-02-06 18:09:13,973 [INFO] Soup Head content: None
2025-02-06 18:09:13,973 [INFO] Stylesheet links found: []
2025-02-06 18:09:13,976 [INFO] Starting _process_content
2025-02-06 18:09:13,977 [INFO] Soup Head content: <head><title>Test Document</title></head>
2025-02-06 18:09:13,978 [INFO] Stylesheet links found: []
2025-02-06 18:09:13,983 [INFO] Starting _process_content
2025-02-06 18:09:13,985 [INFO] Soup Head content: <head>
<script type="text/javascript">var a = 1;</script>
</head>
2025-02-06 18:09:13,985 [INFO] Stylesheet links found: []
2025-02-06 18:09:13,991 [INFO] Starting _process_content
2025-02-06 18:09:13,992 [INFO] Soup Head content: <head>
<title>Script Test</title>
<script>alert('Hello');</script>
</head>
2025-02-06 18:09:13,993 [INFO] Stylesheet links found: []
2025-02-06 18:09:13,997 [INFO] Starting _process_content
2025-02-06 18:09:14,001 [INFO] Soup Head content: <head>
<title>Invalid JSON-LD</title>
<script type="application/ld+json">
                { invalid json }
                </script>
</head>
2025-02-06 18:09:14,002 [INFO] Stylesheet links found: []
2025-02-06 18:09:14,008 [INFO] Starting _process_content
2025-02-06 18:09:14,010 [INFO] Soup Head content: <head>
<script type="text/javascript">
                    console.log('JS Script');
                </script>
<script type="application/json">
                    {"key": "value"}
                </script>
</head>
2025-02-06 18:09:14,010 [INFO] Stylesheet links found: []
2025-02-06 18:09:14,019 [INFO] Starting _process_content
2025-02-06 18:09:14,021 [INFO] Soup Head content: None
2025-02-06 18:09:14,021 [INFO] Stylesheet links found: []
2025-02-06 18:09:14,027 [INFO] Starting _process_content
2025-02-06 18:09:14,028 [INFO] Soup Head content: None
2025-02-06 18:09:14,029 [INFO] Stylesheet links found: []
2025-02-06 18:09:14,036 [INFO] Starting _process_content
2025-02-06 18:09:14,036 [INFO] Soup Head content: None
2025-02-06 18:09:14,037 [INFO] Stylesheet links found: []
2025-02-06 18:09:14,042 [INFO] Starting _process_content
2025-02-06 18:09:14,043 [INFO] Soup Head content: None
2025-02-06 18:09:14,043 [INFO] Stylesheet links found: []
2025-02-06 18:09:14,050 [INFO] Starting _process_content
2025-02-06 18:09:14,051 [INFO] Soup Head content: None
2025-02-06 18:09:14,051 [INFO] Stylesheet links found: []
2025-02-06 18:09:14,058 [INFO] Starting _process_content
2025-02-06 18:09:14,060 [INFO] Soup Head content: None
2025-02-06 18:09:14,061 [INFO] Stylesheet links found: []
2025-02-06 18:09:14,068 [INFO] Starting _process_content
2025-02-06 18:09:14,070 [INFO] Soup Head content: None
2025-02-06 18:09:14,070 [INFO] Stylesheet links found: []
2025-02-06 18:09:14,074 [INFO] Starting _process_content
2025-02-06 18:09:14,075 [INFO] Soup Head content: None
2025-02-06 18:09:14,075 [INFO] Stylesheet links found: []
2025-02-06 18:09:14,083 [INFO] Starting _process_content
2025-02-06 18:09:14,085 [INFO] Soup Head content: None
2025-02-06 18:09:14,086 [INFO] Stylesheet links found: []
2025-02-06 18:09:14,093 [INFO] Starting _process_content
2025-02-06 18:09:14,095 [INFO] Soup Head content: None
2025-02-06 18:09:14,097 [INFO] Stylesheet links found: []
2025-02-06 18:09:14,107 [INFO] Starting _process_content
2025-02-06 18:09:14,109 [INFO] Soup Head content: <head>
<title>Meta Test</title>
<meta content="A test description." name="description"/>
<meta content="OG Meta Title" property="og:title"/>
</head>
2025-02-06 18:09:14,110 [INFO] Stylesheet links found: []
2025-02-06 18:09:14,120 [INFO] Starting _process_content
2025-02-06 18:09:14,122 [INFO] Soup Head content: <head>
<title>Complex Meta</title>
<meta content="A complex description." name="description"/>
<meta content="Dublin Core Title" name="dc.title"/>
<meta content="website" property="og:type"/>
<script type="application/ld+json">
                {
                    "@context": "https://schema.org",
                    "@type": "WebSite",
                    "name": "Example Site",
                    "url": "https://example.com"
                }
                </script>
</head>
2025-02-06 18:09:14,123 [INFO] Stylesheet links found: []
2025-02-06 18:09:14,136 [INFO] Starting _process_content
2025-02-06 18:09:14,138 [INFO] Soup Head content: <head>
<title>First Title</title>
<title>Second Title</title>
<meta content="First description" name="description"/>
<meta content="Second description" name="description"/>
<meta content="First OG title" property="og:title"/>
<meta content="Second OG title" property="og:title"/>
</head>
2025-02-06 18:09:14,138 [INFO] Stylesheet links found: []
2025-02-06 18:09:14,145 [INFO] Starting _process_content
2025-02-06 18:09:14,150 [INFO] Soup Head content: <head>
<title>Inside head</title>
<meta content="inside, head" name="keywords"/>
</head>
2025-02-06 18:09:14,150 [INFO] Stylesheet links found: []
2025-02-06 18:09:14,157 [INFO] Starting _process_content
2025-02-06 18:09:14,160 [INFO] Soup Head content: <head>
<title></title>
<meta content="" name="empty-content"/>
<meta name="no-content"/>
<meta content="no-name" name=""/>
<meta content="no-name-or-property"/>
<meta content="empty-property" property=""/>
</head>
2025-02-06 18:09:14,160 [INFO] Stylesheet links found: []
2025-02-06 18:09:14,172 [INFO] Starting _process_content
2025-02-06 18:09:14,175 [INFO] Soup Head content: <head>
<script type="application/ld+json">
                {
                    "@context": "https://schema.org",
                    "@type": "Article",
                    "headline": "Article Title",
                    "author": {
                        "@type": "Person",
                        "name": "Author Name"
                    },
                    "description": "Article description"
                }
                </script>
</head>
2025-02-06 18:09:14,176 [INFO] Stylesheet links found: []
2025-02-06 18:09:14,187 [INFO] Starting _process_content
2025-02-06 18:09:14,196 [INFO] Soup Head content: <head>
<!-- Standard meta tags -->
<title>Page Title</title>
<meta content="Page description" name="description"/>
<meta content="key1, key2, key3" name="keywords"/>
<meta content="Author Name" name="author"/>
<!-- OpenGraph meta tags -->
<meta content="OG Title" property="og:title"/>
<meta content="OG description" property="og:description"/>
<meta content="https://example.com/image.jpg" property="og:image"/>
<meta content="https://example.com/page" property="og:url"/>
<!-- Twitter Card meta tags -->
<meta content="summary" name="twitter:card"/>
<meta content="Twitter Title" name="twitter:title"/>
<meta content="Twitter description" name="twitter:description"/>
<meta content="https://example.com/twitter.jpg" name="twitter:image"/>
<!-- Dublin Core meta tags -->
<meta content="DC Title" name="dc.title"/>
<meta content="DC Creator" name="dc.creator"/>
<meta content="DC Subject" name="dc.subject"/>
<!-- Custom meta tags -->
<meta content="Custom value" name="custom-tag"/>
<meta content="Property value" property="custom:property"/>
</head>
2025-02-06 18:09:14,197 [INFO] Stylesheet links found: []
2025-02-06 18:09:14,206 [INFO] Starting _process_content
2025-02-06 18:09:14,208 [INFO] Soup Head content: <head>
<link href="https://example.com/styles.css" rel="stylesheet"/>
<script src="app.js"></script>
</head>
2025-02-06 18:09:14,209 [INFO] Stylesheet links found: [<link href="https://example.com/styles.css" rel="stylesheet"/>]
2025-02-06 18:09:14,209 [INFO] Stylesheet href: https://example.com/styles.css, processed URL: https://example.com/styles.css
2025-02-06 18:09:14,209 [INFO] Extracted stylesheet URL: https://example.com/styles.css
2025-02-06 18:09:14,210 [INFO] Stylesheets assets after append: ['https://example.com/styles.css']
2025-02-06 18:09:14,237 [INFO] Starting _process_content
2025-02-06 18:09:14,242 [INFO] Soup Head content: None
2025-02-06 18:09:14,242 [INFO] Stylesheet links found: []
2025-02-06 18:09:14,253 [INFO] Starting _process_content
2025-02-06 18:09:14,255 [INFO] Soup Head content: None
2025-02-06 18:09:14,258 [INFO] Stylesheet links found: [<link href="https://example.com/styles.css" rel="stylesheet"/>]
2025-02-06 18:09:14,258 [INFO] Stylesheet href: https://example.com/styles.css, processed URL: https://example.com/styles.css
2025-02-06 18:09:14,258 [INFO] Extracted stylesheet URL: https://example.com/styles.css
2025-02-06 18:09:14,258 [INFO] Stylesheets assets after append: ['https://example.com/styles.css']
2025-02-06 18:09:14,270 [INFO] Starting _process_content
2025-02-06 18:09:14,271 [INFO] Soup Head content: None
2025-02-06 18:09:14,272 [INFO] Stylesheet links found: []
2025-02-06 18:09:14,286 [INFO] Starting _process_content
2025-02-06 18:09:14,288 [INFO] Soup Head content: <head>
<base href="https://example.com/subdir/"/>
</head>
2025-02-06 18:09:14,288 [INFO] Stylesheet links found: []
2025-02-06 18:10:37,679 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 18:10:39,836 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:10:39,839 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:10:39,843 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:10:39,846 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:10:39,852 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:10:39,857 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:10:39,860 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 18:10:39,866 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:10:39,873 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:10:39,877 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 18:10:39,881 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:10:39,883 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:10:39,891 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:10:39,894 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:10:39,897 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:10:39,899 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:10:39,906 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:10:39,908 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:10:39,912 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:10:39,915 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:10:39,917 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 18:10:39,921 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:10:39,929 [INFO] Starting _process_content
2025-02-06 18:10:39,930 [INFO] Soup Head content: None
2025-02-06 18:10:39,930 [INFO] Stylesheet links found: []
2025-02-06 18:10:39,937 [INFO] Starting _process_content
2025-02-06 18:10:39,939 [INFO] Soup Head content: None
2025-02-06 18:10:39,939 [INFO] Stylesheet links found: []
2025-02-06 18:10:39,946 [INFO] Starting _process_content
2025-02-06 18:10:39,955 [INFO] Soup Head content: None
2025-02-06 18:10:39,955 [INFO] Stylesheet links found: []
2025-02-06 18:10:39,970 [INFO] Starting _process_content
2025-02-06 18:10:39,972 [INFO] Soup Head content: None
2025-02-06 18:10:39,972 [INFO] Stylesheet links found: []
2025-02-06 18:10:39,980 [INFO] Starting _process_content
2025-02-06 18:10:39,980 [INFO] Soup Head content: None
2025-02-06 18:10:39,981 [INFO] Stylesheet links found: []
2025-02-06 18:10:39,987 [INFO] Starting _process_content
2025-02-06 18:10:39,988 [INFO] Soup Head content: None
2025-02-06 18:10:39,988 [INFO] Stylesheet links found: []
2025-02-06 18:10:39,993 [INFO] Starting _process_content
2025-02-06 18:10:39,994 [INFO] Soup Head content: <head><title>Test Document</title></head>
2025-02-06 18:10:39,994 [INFO] Stylesheet links found: []
2025-02-06 18:10:39,998 [INFO] Starting _process_content
2025-02-06 18:10:40,004 [INFO] Soup Head content: <head>
<script type="text/javascript">var a = 1;</script>
</head>
2025-02-06 18:10:40,004 [INFO] Stylesheet links found: []
2025-02-06 18:10:40,013 [INFO] Starting _process_content
2025-02-06 18:10:40,014 [INFO] Soup Head content: <head>
<title>Script Test</title>
<script>alert('Hello');</script>
</head>
2025-02-06 18:10:40,015 [INFO] Stylesheet links found: []
2025-02-06 18:10:40,022 [INFO] Starting _process_content
2025-02-06 18:10:40,025 [INFO] Soup Head content: <head>
<title>Invalid JSON-LD</title>
<script type="application/ld+json">
                { invalid json }
                </script>
</head>
2025-02-06 18:10:40,026 [INFO] Stylesheet links found: []
2025-02-06 18:10:40,029 [INFO] Starting _process_content
2025-02-06 18:10:40,030 [INFO] Soup Head content: <head>
<script type="text/javascript">
                    console.log('JS Script');
                </script>
<script type="application/json">
                    {"key": "value"}
                </script>
</head>
2025-02-06 18:10:40,030 [INFO] Stylesheet links found: []
2025-02-06 18:10:40,037 [INFO] Starting _process_content
2025-02-06 18:10:40,039 [INFO] Soup Head content: None
2025-02-06 18:10:40,040 [INFO] Stylesheet links found: []
2025-02-06 18:10:40,044 [INFO] Starting _process_content
2025-02-06 18:10:40,044 [INFO] Soup Head content: None
2025-02-06 18:10:40,045 [INFO] Stylesheet links found: []
2025-02-06 18:10:40,053 [INFO] Starting _process_content
2025-02-06 18:10:40,055 [INFO] Soup Head content: None
2025-02-06 18:10:40,055 [INFO] Stylesheet links found: []
2025-02-06 18:10:40,072 [INFO] Starting _process_content
2025-02-06 18:10:40,074 [INFO] Soup Head content: None
2025-02-06 18:10:40,074 [INFO] Stylesheet links found: []
2025-02-06 18:10:40,082 [INFO] Starting _process_content
2025-02-06 18:10:40,082 [INFO] Soup Head content: None
2025-02-06 18:10:40,087 [INFO] Stylesheet links found: []
2025-02-06 18:10:40,091 [INFO] Starting _process_content
2025-02-06 18:10:40,092 [INFO] Soup Head content: None
2025-02-06 18:10:40,092 [INFO] Stylesheet links found: []
2025-02-06 18:10:40,098 [INFO] Starting _process_content
2025-02-06 18:10:40,103 [INFO] Soup Head content: None
2025-02-06 18:10:40,104 [INFO] Stylesheet links found: []
2025-02-06 18:10:40,113 [INFO] Starting _process_content
2025-02-06 18:10:40,115 [INFO] Soup Head content: None
2025-02-06 18:10:40,115 [INFO] Stylesheet links found: []
2025-02-06 18:10:40,122 [INFO] Starting _process_content
2025-02-06 18:10:40,125 [INFO] Soup Head content: None
2025-02-06 18:10:40,125 [INFO] Stylesheet links found: []
2025-02-06 18:10:40,129 [INFO] Starting _process_content
2025-02-06 18:10:40,130 [INFO] Soup Head content: None
2025-02-06 18:10:40,130 [INFO] Stylesheet links found: []
2025-02-06 18:10:40,139 [INFO] Starting _process_content
2025-02-06 18:10:40,140 [INFO] Soup Head content: <head>
<title>Meta Test</title>
<meta content="A test description." name="description"/>
<meta content="OG Meta Title" property="og:title"/>
</head>
2025-02-06 18:10:40,141 [INFO] Stylesheet links found: []
2025-02-06 18:10:40,146 [INFO] Starting _process_content
2025-02-06 18:10:40,147 [INFO] Soup Head content: <head>
<title>Complex Meta</title>
<meta content="A complex description." name="description"/>
<meta content="Dublin Core Title" name="dc.title"/>
<meta content="website" property="og:type"/>
<script type="application/ld+json">
                {
                    "@context": "https://schema.org",
                    "@type": "WebSite",
                    "name": "Example Site",
                    "url": "https://example.com"
                }
                </script>
</head>
2025-02-06 18:10:40,147 [INFO] Stylesheet links found: []
2025-02-06 18:10:40,156 [INFO] Starting _process_content
2025-02-06 18:10:40,159 [INFO] Soup Head content: <head>
<title>First Title</title>
<title>Second Title</title>
<meta content="First description" name="description"/>
<meta content="Second description" name="description"/>
<meta content="First OG title" property="og:title"/>
<meta content="Second OG title" property="og:title"/>
</head>
2025-02-06 18:10:40,160 [INFO] Stylesheet links found: []
2025-02-06 18:10:40,164 [INFO] Starting _process_content
2025-02-06 18:10:40,165 [INFO] Soup Head content: <head>
<title>Inside head</title>
<meta content="inside, head" name="keywords"/>
</head>
2025-02-06 18:10:40,165 [INFO] Stylesheet links found: []
2025-02-06 18:10:40,172 [INFO] Starting _process_content
2025-02-06 18:10:40,173 [INFO] Soup Head content: <head>
<title></title>
<meta content="" name="empty-content"/>
<meta name="no-content"/>
<meta content="no-name" name=""/>
<meta content="no-name-or-property"/>
<meta content="empty-property" property=""/>
</head>
2025-02-06 18:10:40,173 [INFO] Stylesheet links found: []
2025-02-06 18:10:40,178 [INFO] Starting _process_content
2025-02-06 18:10:40,179 [INFO] Soup Head content: <head>
<script type="application/ld+json">
                {
                    "@context": "https://schema.org",
                    "@type": "Article",
                    "headline": "Article Title",
                    "author": {
                        "@type": "Person",
                        "name": "Author Name"
                    },
                    "description": "Article description"
                }
                </script>
</head>
2025-02-06 18:10:40,180 [INFO] Stylesheet links found: []
2025-02-06 18:10:40,193 [INFO] Starting _process_content
2025-02-06 18:10:40,196 [INFO] Soup Head content: <head>
<!-- Standard meta tags -->
<title>Page Title</title>
<meta content="Page description" name="description"/>
<meta content="key1, key2, key3" name="keywords"/>
<meta content="Author Name" name="author"/>
<!-- OpenGraph meta tags -->
<meta content="OG Title" property="og:title"/>
<meta content="OG description" property="og:description"/>
<meta content="https://example.com/image.jpg" property="og:image"/>
<meta content="https://example.com/page" property="og:url"/>
<!-- Twitter Card meta tags -->
<meta content="summary" name="twitter:card"/>
<meta content="Twitter Title" name="twitter:title"/>
<meta content="Twitter description" name="twitter:description"/>
<meta content="https://example.com/twitter.jpg" name="twitter:image"/>
<!-- Dublin Core meta tags -->
<meta content="DC Title" name="dc.title"/>
<meta content="DC Creator" name="dc.creator"/>
<meta content="DC Subject" name="dc.subject"/>
<!-- Custom meta tags -->
<meta content="Custom value" name="custom-tag"/>
<meta content="Property value" property="custom:property"/>
</head>
2025-02-06 18:10:40,197 [INFO] Stylesheet links found: []
2025-02-06 18:10:40,207 [INFO] Starting _process_content
2025-02-06 18:10:40,209 [INFO] Soup Head content: <head>
<link href="https://example.com/styles.css" rel="stylesheet"/>
<script src="app.js"></script>
</head>
2025-02-06 18:10:40,209 [INFO] Stylesheet links found: [<link href="https://example.com/styles.css" rel="stylesheet"/>]
2025-02-06 18:10:40,209 [INFO] Stylesheet href: https://example.com/styles.css, processed URL: https://example.com/styles.css
2025-02-06 18:10:40,209 [INFO] Extracted stylesheet URL: https://example.com/styles.css
2025-02-06 18:10:40,210 [INFO] Stylesheets assets after append: ['https://example.com/styles.css']
2025-02-06 18:10:40,215 [INFO] Starting _process_content
2025-02-06 18:10:40,219 [INFO] Soup Head content: None
2025-02-06 18:10:40,220 [INFO] Stylesheet links found: []
2025-02-06 18:10:40,224 [INFO] Starting _process_content
2025-02-06 18:10:40,228 [INFO] Soup Head content: None
2025-02-06 18:10:40,229 [INFO] Stylesheet links found: [<link href="https://example.com/styles.css" rel="stylesheet"/>]
2025-02-06 18:10:40,229 [INFO] Stylesheet href: https://example.com/styles.css, processed URL: https://example.com/styles.css
2025-02-06 18:10:40,229 [INFO] Extracted stylesheet URL: https://example.com/styles.css
2025-02-06 18:10:40,229 [INFO] Stylesheets assets after append: ['https://example.com/styles.css']
2025-02-06 18:10:40,239 [INFO] Starting _process_content
2025-02-06 18:10:40,240 [INFO] Soup Head content: None
2025-02-06 18:10:40,240 [INFO] Stylesheet links found: []
2025-02-06 18:10:40,244 [INFO] Starting _process_content
2025-02-06 18:10:40,246 [INFO] Soup Head content: <head>
<base href="https://example.com/subdir/"/>
</head>
2025-02-06 18:10:40,246 [INFO] Stylesheet links found: []
2025-02-06 18:12:04,388 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 18:12:06,500 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:12:06,503 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:12:06,506 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:12:06,509 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:12:06,511 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:12:06,513 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:12:06,515 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 18:12:06,516 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:12:06,518 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:12:06,524 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 18:12:06,528 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:12:06,530 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:12:06,534 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:12:06,537 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:12:06,542 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:12:06,546 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:12:06,557 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:12:06,566 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:12:06,569 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:12:06,574 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:12:06,581 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 18:12:06,582 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:12:06,590 [INFO] Starting _process_content
2025-02-06 18:12:06,591 [INFO] Soup Head content: None
2025-02-06 18:12:06,591 [INFO] Stylesheet links found: []
2025-02-06 18:12:06,598 [INFO] Starting _process_content
2025-02-06 18:12:06,601 [INFO] Soup Head content: None
2025-02-06 18:12:06,601 [INFO] Stylesheet links found: []
2025-02-06 18:12:06,610 [INFO] Starting _process_content
2025-02-06 18:12:06,612 [INFO] Soup Head content: None
2025-02-06 18:12:06,612 [INFO] Stylesheet links found: []
2025-02-06 18:12:06,618 [INFO] Starting _process_content
2025-02-06 18:12:06,619 [INFO] Soup Head content: None
2025-02-06 18:12:06,620 [INFO] Stylesheet links found: []
2025-02-06 18:12:06,625 [INFO] Starting _process_content
2025-02-06 18:12:06,626 [INFO] Soup Head content: None
2025-02-06 18:12:06,626 [INFO] Stylesheet links found: []
2025-02-06 18:12:06,629 [INFO] Starting _process_content
2025-02-06 18:12:06,635 [INFO] Soup Head content: None
2025-02-06 18:12:06,635 [INFO] Stylesheet links found: []
2025-02-06 18:12:06,641 [INFO] Starting _process_content
2025-02-06 18:12:06,646 [INFO] Soup Head content: <head><title>Test Document</title></head>
2025-02-06 18:12:06,647 [INFO] Stylesheet links found: []
2025-02-06 18:12:06,651 [INFO] Starting _process_content
2025-02-06 18:12:06,653 [INFO] Soup Head content: <head>
<script type="text/javascript">var a = 1;</script>
</head>
2025-02-06 18:12:06,654 [INFO] Stylesheet links found: []
2025-02-06 18:12:06,657 [INFO] Starting _process_content
2025-02-06 18:12:06,660 [INFO] Soup Head content: <head>
<title>Script Test</title>
<script>alert('Hello');</script>
</head>
2025-02-06 18:12:06,661 [INFO] Stylesheet links found: []
2025-02-06 18:12:06,665 [INFO] Starting _process_content
2025-02-06 18:12:06,666 [INFO] Soup Head content: <head>
<title>Invalid JSON-LD</title>
<script type="application/ld+json">
                { invalid json }
                </script>
</head>
2025-02-06 18:12:06,666 [INFO] Stylesheet links found: []
2025-02-06 18:12:06,673 [INFO] Starting _process_content
2025-02-06 18:12:06,674 [INFO] Soup Head content: <head>
<script type="text/javascript">
                    console.log('JS Script');
                </script>
<script type="application/json">
                    {"key": "value"}
                </script>
</head>
2025-02-06 18:12:06,674 [INFO] Stylesheet links found: []
2025-02-06 18:12:06,684 [INFO] Starting _process_content
2025-02-06 18:12:06,685 [INFO] Soup Head content: None
2025-02-06 18:12:06,685 [INFO] Stylesheet links found: []
2025-02-06 18:12:06,692 [INFO] Starting _process_content
2025-02-06 18:12:06,693 [INFO] Soup Head content: None
2025-02-06 18:12:06,693 [INFO] Stylesheet links found: []
2025-02-06 18:12:06,697 [INFO] Starting _process_content
2025-02-06 18:12:06,698 [INFO] Soup Head content: None
2025-02-06 18:12:06,699 [INFO] Stylesheet links found: []
2025-02-06 18:12:06,704 [INFO] Starting _process_content
2025-02-06 18:12:06,706 [INFO] Soup Head content: None
2025-02-06 18:12:06,706 [INFO] Stylesheet links found: []
2025-02-06 18:12:06,714 [INFO] Starting _process_content
2025-02-06 18:12:06,715 [INFO] Soup Head content: None
2025-02-06 18:12:06,715 [INFO] Stylesheet links found: []
2025-02-06 18:12:06,726 [INFO] Starting _process_content
2025-02-06 18:12:06,729 [INFO] Soup Head content: None
2025-02-06 18:12:06,730 [INFO] Stylesheet links found: []
2025-02-06 18:12:06,735 [INFO] Starting _process_content
2025-02-06 18:12:06,739 [INFO] Soup Head content: None
2025-02-06 18:12:06,739 [INFO] Stylesheet links found: []
2025-02-06 18:12:06,742 [INFO] Starting _process_content
2025-02-06 18:12:06,747 [INFO] Soup Head content: None
2025-02-06 18:12:06,748 [INFO] Stylesheet links found: []
2025-02-06 18:12:06,758 [INFO] Starting _process_content
2025-02-06 18:12:06,761 [INFO] Soup Head content: None
2025-02-06 18:12:06,761 [INFO] Stylesheet links found: []
2025-02-06 18:12:06,770 [INFO] Starting _process_content
2025-02-06 18:12:06,773 [INFO] Soup Head content: None
2025-02-06 18:12:06,773 [INFO] Stylesheet links found: []
2025-02-06 18:12:06,782 [INFO] Starting _process_content
2025-02-06 18:12:06,783 [INFO] Soup Head content: <head>
<title>Meta Test</title>
<meta content="A test description." name="description"/>
<meta content="OG Meta Title" property="og:title"/>
</head>
2025-02-06 18:12:06,783 [INFO] Stylesheet links found: []
2025-02-06 18:12:06,791 [INFO] Starting _process_content
2025-02-06 18:12:06,792 [INFO] Soup Head content: <head>
<title>Complex Meta</title>
<meta content="A complex description." name="description"/>
<meta content="Dublin Core Title" name="dc.title"/>
<meta content="website" property="og:type"/>
<script type="application/ld+json">
                {
                    "@context": "https://schema.org",
                    "@type": "WebSite",
                    "name": "Example Site",
                    "url": "https://example.com"
                }
                </script>
</head>
2025-02-06 18:12:06,793 [INFO] Stylesheet links found: []
2025-02-06 18:12:06,801 [INFO] Starting _process_content
2025-02-06 18:12:06,802 [INFO] Soup Head content: <head>
<title>First Title</title>
<title>Second Title</title>
<meta content="First description" name="description"/>
<meta content="Second description" name="description"/>
<meta content="First OG title" property="og:title"/>
<meta content="Second OG title" property="og:title"/>
</head>
2025-02-06 18:12:06,803 [INFO] Stylesheet links found: []
2025-02-06 18:12:06,811 [INFO] Starting _process_content
2025-02-06 18:12:06,813 [INFO] Soup Head content: <head>
<title>Inside head</title>
<meta content="inside, head" name="keywords"/>
</head>
2025-02-06 18:12:06,813 [INFO] Stylesheet links found: []
2025-02-06 18:12:06,816 [INFO] Starting _process_content
2025-02-06 18:12:06,817 [INFO] Soup Head content: <head>
<title></title>
<meta content="" name="empty-content"/>
<meta name="no-content"/>
<meta content="no-name" name=""/>
<meta content="no-name-or-property"/>
<meta content="empty-property" property=""/>
</head>
2025-02-06 18:12:06,817 [INFO] Stylesheet links found: []
2025-02-06 18:12:06,824 [INFO] Starting _process_content
2025-02-06 18:12:06,825 [INFO] Soup Head content: <head>
<script type="application/ld+json">
                {
                    "@context": "https://schema.org",
                    "@type": "Article",
                    "headline": "Article Title",
                    "author": {
                        "@type": "Person",
                        "name": "Author Name"
                    },
                    "description": "Article description"
                }
                </script>
</head>
2025-02-06 18:12:06,825 [INFO] Stylesheet links found: []
2025-02-06 18:12:06,830 [INFO] Starting _process_content
2025-02-06 18:12:06,832 [INFO] Soup Head content: <head>
<!-- Standard meta tags -->
<title>Page Title</title>
<meta content="Page description" name="description"/>
<meta content="key1, key2, key3" name="keywords"/>
<meta content="Author Name" name="author"/>
<!-- OpenGraph meta tags -->
<meta content="OG Title" property="og:title"/>
<meta content="OG description" property="og:description"/>
<meta content="https://example.com/image.jpg" property="og:image"/>
<meta content="https://example.com/page" property="og:url"/>
<!-- Twitter Card meta tags -->
<meta content="summary" name="twitter:card"/>
<meta content="Twitter Title" name="twitter:title"/>
<meta content="Twitter description" name="twitter:description"/>
<meta content="https://example.com/twitter.jpg" name="twitter:image"/>
<!-- Dublin Core meta tags -->
<meta content="DC Title" name="dc.title"/>
<meta content="DC Creator" name="dc.creator"/>
<meta content="DC Subject" name="dc.subject"/>
<!-- Custom meta tags -->
<meta content="Custom value" name="custom-tag"/>
<meta content="Property value" property="custom:property"/>
</head>
2025-02-06 18:12:06,832 [INFO] Stylesheet links found: []
2025-02-06 18:12:06,838 [INFO] Starting _process_content
2025-02-06 18:12:06,840 [INFO] Soup Head content: <head>
<link href="https://example.com/styles.css" rel="stylesheet"/>
<script src="app.js"></script>
</head>
2025-02-06 18:12:06,840 [INFO] Stylesheet links found: [<link href="https://example.com/styles.css" rel="stylesheet"/>]
2025-02-06 18:12:06,840 [INFO] Stylesheet href: https://example.com/styles.css, processed URL: https://example.com/styles.css
2025-02-06 18:12:06,841 [INFO] Extracted stylesheet URL: https://example.com/styles.css
2025-02-06 18:12:06,841 [INFO] Stylesheets assets after append: ['https://example.com/styles.css']
2025-02-06 18:12:06,848 [INFO] Starting _process_content
2025-02-06 18:12:06,849 [INFO] Soup Head content: None
2025-02-06 18:12:06,849 [INFO] Stylesheet links found: []
2025-02-06 18:12:06,855 [INFO] Starting _process_content
2025-02-06 18:12:06,856 [INFO] Soup Head content: None
2025-02-06 18:12:06,857 [INFO] Stylesheet links found: [<link href="https://example.com/styles.css" rel="stylesheet"/>]
2025-02-06 18:12:06,857 [INFO] Stylesheet href: https://example.com/styles.css, processed URL: https://example.com/styles.css
2025-02-06 18:12:06,857 [INFO] Extracted stylesheet URL: https://example.com/styles.css
2025-02-06 18:12:06,857 [INFO] Stylesheets assets after append: ['https://example.com/styles.css']
2025-02-06 18:12:06,863 [INFO] Starting _process_content
2025-02-06 18:12:06,864 [INFO] Soup Head content: None
2025-02-06 18:12:06,864 [INFO] Stylesheet links found: []
2025-02-06 18:12:06,868 [INFO] Starting _process_content
2025-02-06 18:12:06,871 [INFO] Soup Head content: <head>
<base href="https://example.com/subdir/"/>
</head>
2025-02-06 18:12:06,871 [INFO] Stylesheet links found: []
2025-02-06 18:13:39,014 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 18:13:41,324 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:13:41,326 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:13:41,329 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:13:41,335 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:13:41,339 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:13:41,345 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:13:41,350 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 18:13:41,352 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:13:41,354 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:13:41,359 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 18:13:41,361 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:13:41,366 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:13:41,371 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:13:41,375 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:13:41,378 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:13:41,383 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:13:41,387 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:13:41,391 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:13:41,394 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:13:41,399 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:13:41,401 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 18:13:41,405 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:13:41,409 [INFO] Starting _process_content
2025-02-06 18:13:41,410 [INFO] Soup Head content: None
2025-02-06 18:13:41,411 [INFO] Stylesheet links found: []
2025-02-06 18:13:41,418 [INFO] Starting _process_content
2025-02-06 18:13:41,425 [INFO] Soup Head content: None
2025-02-06 18:13:41,426 [INFO] Stylesheet links found: []
2025-02-06 18:13:41,436 [INFO] Starting _process_content
2025-02-06 18:13:41,440 [INFO] Soup Head content: None
2025-02-06 18:13:41,440 [INFO] Stylesheet links found: []
2025-02-06 18:13:41,454 [INFO] Starting _process_content
2025-02-06 18:13:41,455 [INFO] Soup Head content: None
2025-02-06 18:13:41,456 [INFO] Stylesheet links found: []
2025-02-06 18:13:41,462 [INFO] Starting _process_content
2025-02-06 18:13:41,466 [INFO] Soup Head content: None
2025-02-06 18:13:41,466 [INFO] Stylesheet links found: []
2025-02-06 18:13:41,473 [INFO] Starting _process_content
2025-02-06 18:13:41,474 [INFO] Soup Head content: None
2025-02-06 18:13:41,474 [INFO] Stylesheet links found: []
2025-02-06 18:13:41,484 [INFO] Starting _process_content
2025-02-06 18:13:41,485 [INFO] Soup Head content: <head><title>Test Document</title></head>
2025-02-06 18:13:41,485 [INFO] Stylesheet links found: []
2025-02-06 18:13:41,490 [INFO] Starting _process_content
2025-02-06 18:13:41,491 [INFO] Soup Head content: <head>
<script type="text/javascript">var a = 1;</script>
</head>
2025-02-06 18:13:41,491 [INFO] Stylesheet links found: []
2025-02-06 18:13:41,495 [INFO] Starting _process_content
2025-02-06 18:13:41,499 [INFO] Soup Head content: <head>
<title>Script Test</title>
<script>alert('Hello');</script>
</head>
2025-02-06 18:13:41,500 [INFO] Stylesheet links found: []
2025-02-06 18:13:41,507 [INFO] Starting _process_content
2025-02-06 18:13:41,508 [INFO] Soup Head content: <head>
<title>Invalid JSON-LD</title>
<script type="application/ld+json">
                { invalid json }
                </script>
</head>
2025-02-06 18:13:41,509 [INFO] Stylesheet links found: []
2025-02-06 18:13:41,517 [INFO] Starting _process_content
2025-02-06 18:13:41,519 [INFO] Soup Head content: <head>
<script type="text/javascript">
                    console.log('JS Script');
                </script>
<script type="application/json">
                    {"key": "value"}
                </script>
</head>
2025-02-06 18:13:41,519 [INFO] Stylesheet links found: []
2025-02-06 18:13:41,523 [INFO] Starting _process_content
2025-02-06 18:13:41,524 [INFO] Soup Head content: None
2025-02-06 18:13:41,525 [INFO] Stylesheet links found: []
2025-02-06 18:13:41,529 [INFO] Starting _process_content
2025-02-06 18:13:41,532 [INFO] Soup Head content: None
2025-02-06 18:13:41,532 [INFO] Stylesheet links found: []
2025-02-06 18:13:41,540 [INFO] Starting _process_content
2025-02-06 18:13:41,541 [INFO] Soup Head content: None
2025-02-06 18:13:41,541 [INFO] Stylesheet links found: []
2025-02-06 18:13:41,545 [INFO] Starting _process_content
2025-02-06 18:13:41,546 [INFO] Soup Head content: None
2025-02-06 18:13:41,550 [INFO] Stylesheet links found: []
2025-02-06 18:13:41,558 [INFO] Starting _process_content
2025-02-06 18:13:41,559 [INFO] Soup Head content: None
2025-02-06 18:13:41,559 [INFO] Stylesheet links found: []
2025-02-06 18:13:41,569 [INFO] Starting _process_content
2025-02-06 18:13:41,573 [INFO] Soup Head content: None
2025-02-06 18:13:41,574 [INFO] Stylesheet links found: []
2025-02-06 18:13:41,586 [INFO] Starting _process_content
2025-02-06 18:13:41,589 [INFO] Soup Head content: None
2025-02-06 18:13:41,590 [INFO] Stylesheet links found: []
2025-02-06 18:13:41,600 [INFO] Starting _process_content
2025-02-06 18:13:41,602 [INFO] Soup Head content: None
2025-02-06 18:13:41,603 [INFO] Stylesheet links found: []
2025-02-06 18:13:41,608 [INFO] Starting _process_content
2025-02-06 18:13:41,610 [INFO] Soup Head content: None
2025-02-06 18:13:41,610 [INFO] Stylesheet links found: []
2025-02-06 18:13:41,619 [INFO] Starting _process_content
2025-02-06 18:13:41,624 [INFO] Soup Head content: None
2025-02-06 18:13:41,624 [INFO] Stylesheet links found: []
2025-02-06 18:13:41,638 [INFO] Starting _process_content
2025-02-06 18:13:41,640 [INFO] Soup Head content: <head>
<title>Meta Test</title>
<meta content="A test description." name="description"/>
<meta content="OG Meta Title" property="og:title"/>
</head>
2025-02-06 18:13:41,640 [INFO] Stylesheet links found: []
2025-02-06 18:13:41,654 [INFO] Starting _process_content
2025-02-06 18:13:41,657 [INFO] Soup Head content: <head>
<title>Complex Meta</title>
<meta content="A complex description." name="description"/>
<meta content="Dublin Core Title" name="dc.title"/>
<meta content="website" property="og:type"/>
<script type="application/ld+json">
                {
                    "@context": "https://schema.org",
                    "@type": "WebSite",
                    "name": "Example Site",
                    "url": "https://example.com"
                }
                </script>
</head>
2025-02-06 18:13:41,657 [INFO] Stylesheet links found: []
2025-02-06 18:13:41,676 [INFO] Starting _process_content
2025-02-06 18:13:41,678 [INFO] Soup Head content: <head>
<title>First Title</title>
<title>Second Title</title>
<meta content="First description" name="description"/>
<meta content="Second description" name="description"/>
<meta content="First OG title" property="og:title"/>
<meta content="Second OG title" property="og:title"/>
</head>
2025-02-06 18:13:41,679 [INFO] Stylesheet links found: []
2025-02-06 18:13:41,691 [INFO] Starting _process_content
2025-02-06 18:13:41,693 [INFO] Soup Head content: <head>
<title>Inside head</title>
<meta content="inside, head" name="keywords"/>
</head>
2025-02-06 18:13:41,694 [INFO] Stylesheet links found: []
2025-02-06 18:13:41,710 [INFO] Starting _process_content
2025-02-06 18:13:41,712 [INFO] Soup Head content: <head>
<title></title>
<meta content="" name="empty-content"/>
<meta name="no-content"/>
<meta content="no-name" name=""/>
<meta content="no-name-or-property"/>
<meta content="empty-property" property=""/>
</head>
2025-02-06 18:13:41,715 [INFO] Stylesheet links found: []
2025-02-06 18:13:41,728 [INFO] Starting _process_content
2025-02-06 18:13:41,734 [INFO] Soup Head content: <head>
<script type="application/ld+json">
                {
                    "@context": "https://schema.org",
                    "@type": "Article",
                    "headline": "Article Title",
                    "author": {
                        "@type": "Person",
                        "name": "Author Name"
                    },
                    "description": "Article description"
                }
                </script>
</head>
2025-02-06 18:13:41,734 [INFO] Stylesheet links found: []
2025-02-06 18:13:41,742 [INFO] Starting _process_content
2025-02-06 18:13:41,745 [INFO] Soup Head content: <head>
<!-- Standard meta tags -->
<title>Page Title</title>
<meta content="Page description" name="description"/>
<meta content="key1, key2, key3" name="keywords"/>
<meta content="Author Name" name="author"/>
<!-- OpenGraph meta tags -->
<meta content="OG Title" property="og:title"/>
<meta content="OG description" property="og:description"/>
<meta content="https://example.com/image.jpg" property="og:image"/>
<meta content="https://example.com/page" property="og:url"/>
<!-- Twitter Card meta tags -->
<meta content="summary" name="twitter:card"/>
<meta content="Twitter Title" name="twitter:title"/>
<meta content="Twitter description" name="twitter:description"/>
<meta content="https://example.com/twitter.jpg" name="twitter:image"/>
<!-- Dublin Core meta tags -->
<meta content="DC Title" name="dc.title"/>
<meta content="DC Creator" name="dc.creator"/>
<meta content="DC Subject" name="dc.subject"/>
<!-- Custom meta tags -->
<meta content="Custom value" name="custom-tag"/>
<meta content="Property value" property="custom:property"/>
</head>
2025-02-06 18:13:41,745 [INFO] Stylesheet links found: []
2025-02-06 18:13:41,753 [INFO] Starting _process_content
2025-02-06 18:13:41,757 [INFO] Soup Head content: <head>
<link href="https://example.com/styles.css" rel="stylesheet"/>
<script src="app.js"></script>
</head>
2025-02-06 18:13:41,758 [INFO] Stylesheet links found: [<link href="https://example.com/styles.css" rel="stylesheet"/>]
2025-02-06 18:13:41,758 [INFO] Stylesheet href: https://example.com/styles.css, processed URL: https://example.com/styles.css
2025-02-06 18:13:41,758 [INFO] Extracted stylesheet URL: https://example.com/styles.css
2025-02-06 18:13:41,758 [INFO] Stylesheets assets after append: ['https://example.com/styles.css']
2025-02-06 18:13:41,767 [INFO] Starting _process_content
2025-02-06 18:13:41,771 [INFO] Soup Head content: None
2025-02-06 18:13:41,771 [INFO] Stylesheet links found: []
2025-02-06 18:13:41,775 [INFO] Starting _process_content
2025-02-06 18:13:41,777 [INFO] Soup Head content: None
2025-02-06 18:13:41,777 [INFO] Stylesheet links found: [<link href="https://example.com/styles.css" rel="stylesheet"/>]
2025-02-06 18:13:41,777 [INFO] Stylesheet href: https://example.com/styles.css, processed URL: https://example.com/styles.css
2025-02-06 18:13:41,777 [INFO] Extracted stylesheet URL: https://example.com/styles.css
2025-02-06 18:13:41,777 [INFO] Stylesheets assets after append: ['https://example.com/styles.css']
2025-02-06 18:13:41,786 [INFO] Starting _process_content
2025-02-06 18:13:41,787 [INFO] Soup Head content: None
2025-02-06 18:13:41,787 [INFO] Stylesheet links found: []
2025-02-06 18:13:41,798 [INFO] Starting _process_content
2025-02-06 18:13:41,801 [INFO] Soup Head content: <head>
<base href="https://example.com/subdir/"/>
</head>
2025-02-06 18:13:41,801 [INFO] Stylesheet links found: []
2025-02-06 18:14:43,017 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 18:14:43,335 [INFO] Starting _process_content
2025-02-06 18:14:43,337 [INFO] Soup Head content: <head>
<base href="https://example.com/subdir/"/>
</head>
2025-02-06 18:14:43,337 [INFO] Stylesheet links found: []
2025-02-06 18:15:32,356 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 18:15:32,538 [INFO] Starting _process_content
2025-02-06 18:15:32,540 [INFO] Soup Head content: <head>
<base href="https://example.com/subdir/"/>
</head>
2025-02-06 18:15:32,541 [INFO] Stylesheet links found: []
2025-02-06 18:16:20,399 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 18:16:20,541 [INFO] Starting _process_content
2025-02-06 18:16:20,542 [INFO] Soup Head content: <head>
<base href="https://example.com/subdir/"/>
</head>
2025-02-06 18:16:20,542 [INFO] Stylesheet links found: []
2025-02-06 18:16:20,543 [INFO] Processing link href: page.html, base_url: None
2025-02-06 18:16:20,544 [INFO] Processing link href: /absolute.html, base_url: None
2025-02-06 18:17:24,851 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 18:17:25,007 [INFO] Starting _process_content
2025-02-06 18:17:25,008 [INFO] Soup Head content: <head>
<base href="https://example.com/subdir/"/>
</head>
2025-02-06 18:17:25,008 [INFO] Stylesheet links found: []
2025-02-06 18:17:25,009 [INFO] Processing link href: page.html, base_url: None
2025-02-06 18:17:25,009 [INFO] Processing link href: /absolute.html, base_url: None
2025-02-06 18:18:26,042 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 18:18:26,199 [INFO] Starting _process_content
2025-02-06 18:18:26,201 [INFO] Soup Head content: <head>
<base href="https://example.com/subdir/"/>
</head>
2025-02-06 18:18:26,202 [INFO] Stylesheet links found: []
2025-02-06 18:18:26,203 [INFO] Processing link href: page.html, base_url: None
2025-02-06 18:18:26,204 [INFO] Processing link href: /absolute.html, base_url: None
2025-02-06 18:20:00,101 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 18:20:00,414 [INFO] Starting _process_content
2025-02-06 18:20:00,416 [INFO] Soup Head content: <head>
<base href="https://example.com/subdir/"/>
</head>
2025-02-06 18:20:00,417 [INFO] Stylesheet links found: []
2025-02-06 18:20:00,417 [INFO] Processing link href: page.html, base_url: None
2025-02-06 18:20:00,418 [INFO] Processing link href: /absolute.html, base_url: None
2025-02-06 18:21:21,240 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 18:21:21,584 [INFO] Starting _process_content
2025-02-06 18:21:21,590 [INFO] Soup Head content: <head>
<base href="https://example.com/subdir/"/>
</head>
2025-02-06 18:21:21,591 [INFO] Stylesheet links found: []
2025-02-06 18:21:21,593 [INFO] Processing link href: page.html, base_url: None
2025-02-06 18:21:21,598 [INFO] Processing link href: /absolute.html, base_url: None
2025-02-06 18:22:35,626 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 18:22:35,748 [INFO] Starting _process_content
2025-02-06 18:22:35,752 [INFO] Soup Head content: <head>
<base href="https://example.com/subdir/"/>
</head>
2025-02-06 18:22:35,752 [INFO] Stylesheet links found: []
2025-02-06 18:22:35,753 [INFO] Processing link href: page.html, base_url: https://example.com/subdir/
2025-02-06 18:22:35,753 [INFO] Sanitizing relative URL: page.html, base_url: https://example.com/subdir/
2025-02-06 18:22:35,754 [INFO] Processing link href: /absolute.html, base_url: https://example.com/subdir/
2025-02-06 18:22:35,754 [INFO] Sanitizing relative URL: /absolute.html, base_url: https://example.com/subdir/
2025-02-06 18:24:22,249 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 18:24:23,250 [INFO] Starting _process_content
2025-02-06 18:24:23,255 [INFO] Soup Head content: <head>
<base href="https://example.com/subdir/"/>
</head>
2025-02-06 18:24:23,256 [INFO] Stylesheet links found: []
2025-02-06 18:24:23,257 [INFO] Processing link href: page.html, base_url: https://example.com/subdir/
2025-02-06 18:24:23,257 [INFO] Sanitizing relative URL: page.html, base_url: https://example.com/subdir/
2025-02-06 18:24:23,258 [INFO] Processing link href: /absolute.html, base_url: https://example.com/subdir/
2025-02-06 18:24:23,258 [INFO] Sanitizing relative URL: /absolute.html, base_url: https://example.com/subdir/
2025-02-06 18:28:11,760 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 18:28:14,101 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:28:14,106 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:28:14,132 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:28:14,135 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:28:14,140 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:28:14,149 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:28:14,153 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 18:28:14,156 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:28:14,162 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:28:14,167 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 18:28:14,170 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:28:14,174 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:28:14,182 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:28:14,184 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:28:14,188 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:28:14,190 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:28:14,195 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:28:14,200 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:28:14,204 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:28:14,207 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:28:14,210 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 18:28:14,213 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:28:14,220 [INFO] Starting _process_content
2025-02-06 18:28:14,234 [INFO] Starting _process_content
2025-02-06 18:28:14,253 [INFO] Starting _process_content
2025-02-06 18:28:14,273 [INFO] Starting _process_content
2025-02-06 18:28:14,283 [INFO] Starting _process_content
2025-02-06 18:28:14,290 [INFO] Starting _process_content
2025-02-06 18:28:14,299 [INFO] Starting _process_content
2025-02-06 18:28:14,316 [INFO] Starting _process_content
2025-02-06 18:28:14,322 [INFO] Starting _process_content
2025-02-06 18:28:14,333 [INFO] Starting _process_content
2025-02-06 18:28:14,339 [INFO] Starting _process_content
2025-02-06 18:28:14,350 [INFO] Starting _process_content
2025-02-06 18:28:14,357 [INFO] Starting _process_content
2025-02-06 18:28:14,369 [INFO] Starting _process_content
2025-02-06 18:28:14,373 [INFO] Starting _process_content
2025-02-06 18:28:14,381 [INFO] Starting _process_content
2025-02-06 18:28:14,387 [INFO] Starting _process_content
2025-02-06 18:28:14,392 [INFO] Starting _process_content
2025-02-06 18:28:14,401 [INFO] Starting _process_content
2025-02-06 18:28:14,406 [INFO] Starting _process_content
2025-02-06 18:28:14,415 [INFO] Starting _process_content
2025-02-06 18:28:14,422 [INFO] Starting _process_content
2025-02-06 18:28:14,432 [INFO] Starting _process_content
2025-02-06 18:28:14,439 [INFO] Starting _process_content
2025-02-06 18:28:14,454 [INFO] Starting _process_content
2025-02-06 18:28:14,459 [INFO] Starting _process_content
2025-02-06 18:28:14,485 [INFO] Starting _process_content
2025-02-06 18:28:14,500 [INFO] Starting _process_content
2025-02-06 18:28:14,508 [INFO] Starting _process_content
2025-02-06 18:28:14,520 [INFO] Starting _process_content
2025-02-06 18:28:14,529 [INFO] Starting _process_content
2025-02-06 18:28:14,538 [INFO] Starting _process_content
2025-02-06 18:28:14,548 [INFO] Starting _process_content
2025-02-06 18:28:14,552 [INFO] Sanitizing relative URL: page.html, base_url: https://example.com/subdir/
2025-02-06 18:28:14,552 [INFO] Resolved URL using urljoin: https://example.com/subdir/page.html
2025-02-06 18:28:14,552 [INFO] Sanitizing relative URL: /absolute.html, base_url: https://example.com/subdir/
2025-02-06 18:28:14,553 [INFO] Resolved URL using urljoin: https://example.com/absolute.html
2025-02-06 18:29:20,158 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 18:29:22,245 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:29:22,251 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:29:22,255 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:29:22,259 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:29:22,270 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:29:22,273 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:29:22,276 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 18:29:22,288 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:29:22,291 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:29:22,293 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 18:29:22,298 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:29:22,303 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:29:22,309 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:29:22,312 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:29:22,319 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:29:22,322 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:29:22,331 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:29:22,334 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:29:22,342 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:29:22,348 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:29:22,351 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 18:29:22,356 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:29:22,367 [INFO] Starting _process_content
2025-02-06 18:29:22,377 [INFO] Starting _process_content
2025-02-06 18:29:22,393 [INFO] Starting _process_content
2025-02-06 18:29:22,457 [INFO] Starting _process_content
2025-02-06 18:29:22,480 [INFO] Starting _process_content
2025-02-06 18:29:22,489 [INFO] Starting _process_content
2025-02-06 18:29:22,501 [INFO] Starting _process_content
2025-02-06 18:29:22,528 [INFO] Starting _process_content
2025-02-06 18:29:22,551 [INFO] Starting _process_content
2025-02-06 18:29:22,580 [INFO] Starting _process_content
2025-02-06 18:29:22,599 [INFO] Starting _process_content
2025-02-06 18:29:22,626 [INFO] Starting _process_content
2025-02-06 18:29:22,636 [INFO] Starting _process_content
2025-02-06 18:29:22,657 [INFO] Starting _process_content
2025-02-06 18:29:22,677 [INFO] Starting _process_content
2025-02-06 18:29:22,690 [INFO] Starting _process_content
2025-02-06 18:29:22,722 [INFO] Starting _process_content
2025-02-06 18:29:22,743 [INFO] Starting _process_content
2025-02-06 18:29:22,769 [INFO] Starting _process_content
2025-02-06 18:29:22,785 [INFO] Starting _process_content
2025-02-06 18:29:22,805 [INFO] Starting _process_content
2025-02-06 18:29:22,824 [INFO] Starting _process_content
2025-02-06 18:29:22,838 [INFO] Starting _process_content
2025-02-06 18:29:22,844 [INFO] Starting _process_content
2025-02-06 18:29:22,860 [INFO] Starting _process_content
2025-02-06 18:29:22,872 [INFO] Starting _process_content
2025-02-06 18:29:22,884 [INFO] Starting _process_content
2025-02-06 18:29:22,890 [INFO] Starting _process_content
2025-02-06 18:29:22,903 [INFO] Starting _process_content
2025-02-06 18:29:22,914 [INFO] Starting _process_content
2025-02-06 18:29:22,922 [INFO] Starting _process_content
2025-02-06 18:29:22,927 [INFO] Starting _process_content
2025-02-06 18:29:22,940 [INFO] Starting _process_content
2025-02-06 18:29:22,943 [INFO] Sanitizing relative URL: page.html, base_url: https://example.com/subdir/
2025-02-06 18:29:22,943 [INFO] Resolved URL using urljoin: https://example.com/subdir/page.html
2025-02-06 18:29:22,943 [INFO] Sanitizing relative URL: /absolute.html, base_url: https://example.com/subdir/
2025-02-06 18:29:22,944 [INFO] Resolved URL using urljoin: https://example.com/absolute.html
2025-02-06 18:30:30,290 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-06 18:30:32,574 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:30:32,576 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:30:32,579 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:30:32,585 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:30:32,590 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:30:32,593 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:30:32,598 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 18:30:32,602 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:30:32,605 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:30:32,608 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 18:30:32,610 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:30:32,615 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:30:32,621 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:30:32,623 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:30:32,628 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:30:32,635 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:30:32,641 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:30:32,644 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:30:32,655 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:30:32,658 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:30:32,661 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-06 18:30:32,670 [DEBUG] Using proactor: IocpProactor
2025-02-06 18:30:32,808 [INFO] Starting _process_content
2025-02-06 18:30:32,857 [INFO] Starting _process_content
2025-02-06 18:30:32,882 [INFO] Starting _process_content
2025-02-06 18:30:32,904 [INFO] Starting _process_content
2025-02-06 18:30:32,915 [INFO] Starting _process_content
2025-02-06 18:30:32,923 [INFO] Starting _process_content
2025-02-06 18:30:32,929 [INFO] Starting _process_content
2025-02-06 18:30:32,939 [INFO] Starting _process_content
2025-02-06 18:30:32,952 [INFO] Starting _process_content
2025-02-06 18:30:32,962 [INFO] Starting _process_content
2025-02-06 18:30:32,974 [INFO] Starting _process_content
2025-02-06 18:30:32,989 [INFO] Starting _process_content
2025-02-06 18:30:32,999 [INFO] Starting _process_content
2025-02-06 18:30:33,007 [INFO] Starting _process_content
2025-02-06 18:30:33,013 [INFO] Starting _process_content
2025-02-06 18:30:33,021 [INFO] Starting _process_content
2025-02-06 18:30:33,026 [INFO] Starting _process_content
2025-02-06 18:30:33,036 [INFO] Starting _process_content
2025-02-06 18:30:33,042 [INFO] Starting _process_content
2025-02-06 18:30:33,052 [INFO] Starting _process_content
2025-02-06 18:30:33,060 [INFO] Starting _process_content
2025-02-06 18:30:33,071 [INFO] Starting _process_content
2025-02-06 18:30:33,078 [INFO] Starting _process_content
2025-02-06 18:30:33,126 [INFO] Starting _process_content
2025-02-06 18:30:33,166 [INFO] Starting _process_content
2025-02-06 18:30:33,189 [INFO] Starting _process_content
2025-02-06 18:30:33,222 [INFO] Starting _process_content
2025-02-06 18:30:33,244 [INFO] Starting _process_content
2025-02-06 18:30:33,259 [INFO] Starting _process_content
2025-02-06 18:30:33,275 [INFO] Starting _process_content
2025-02-06 18:30:33,288 [INFO] Starting _process_content
2025-02-06 18:30:33,314 [INFO] Starting _process_content
2025-02-06 18:30:33,343 [INFO] Starting _process_content
2025-02-06 18:30:33,354 [INFO] Sanitizing relative URL: page.html, base_url: https://example.com/subdir/
2025-02-06 18:30:33,355 [INFO] Resolved URL using urljoin: https://example.com/subdir/page.html
2025-02-06 18:30:33,355 [INFO] Sanitizing relative URL: /absolute.html, base_url: https://example.com/subdir/
2025-02-06 18:30:33,355 [INFO] Resolved URL using urljoin: https://example.com/absolute.html
2025-02-07 08:24:33,006 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 08:24:37,643 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:24:37,685 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:24:37,689 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:24:37,754 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:24:37,759 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:24:37,763 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:24:37,766 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:24:37,768 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:24:37,770 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:24:37,773 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:24:37,775 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:24:37,781 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:24:37,787 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:24:37,790 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:24:37,801 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:24:37,808 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:24:37,816 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:24:37,821 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:24:37,825 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:24:37,830 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:24:37,834 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:24:37,837 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:26:34,613 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 08:26:37,005 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:26:37,008 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:26:37,012 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:26:37,017 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:26:37,024 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:26:37,027 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:26:37,039 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:26:37,051 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:26:37,056 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:26:37,063 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:26:37,069 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:26:37,072 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:26:37,077 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:26:37,080 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:26:37,085 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:26:37,090 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:26:37,095 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:26:37,098 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:26:37,106 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:26:37,109 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:26:37,111 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:26:37,114 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:27:40,461 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 08:27:42,640 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:27:42,642 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:27:42,648 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:27:42,655 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:27:42,659 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:27:42,662 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:27:42,664 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:27:42,668 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:27:42,672 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:27:42,678 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:27:42,713 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:27:42,716 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:27:42,751 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:27:42,756 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:27:42,798 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:27:42,801 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:27:42,859 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:27:42,862 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:27:42,883 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:27:42,889 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:27:42,893 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:27:42,897 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:28:54,478 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 08:28:56,637 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:28:56,640 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:28:56,645 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:28:56,649 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:28:56,658 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:28:56,661 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:28:56,665 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:28:56,677 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:28:56,680 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:28:56,690 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:28:56,695 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:28:56,698 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:28:56,709 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:28:56,711 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:28:56,725 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:28:56,732 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:28:56,779 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:28:56,781 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:28:56,793 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:28:56,803 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:28:56,807 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:28:56,815 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:30:01,086 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 08:30:02,999 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:30:03,002 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:30:03,009 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:30:03,013 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:30:03,017 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:30:03,019 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:30:03,024 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:30:03,027 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:30:03,029 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:30:03,033 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:30:03,036 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:30:03,041 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:30:03,049 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:30:03,052 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:30:03,061 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:30:03,063 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:30:03,070 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:30:03,075 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:30:03,083 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:30:03,086 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:30:03,096 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:30:03,098 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:34:05,065 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 08:34:06,337 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:34:06,339 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:34:06,342 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:34:06,351 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:34:06,355 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:34:06,358 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:34:06,365 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:34:06,367 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:34:06,374 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:34:06,383 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:34:06,391 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:34:06,398 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:34:06,407 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:34:06,416 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:34:06,422 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:34:06,425 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:34:06,439 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:34:06,441 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:34:06,451 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:34:06,456 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:34:06,458 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:34:06,465 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:36:59,091 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 08:37:00,889 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:00,891 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:00,894 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:00,900 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:00,903 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:00,905 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:00,907 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:37:00,909 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:00,911 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:00,915 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:37:00,919 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:00,921 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:00,927 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:00,928 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:00,935 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:00,937 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:00,942 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:00,944 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:00,951 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:00,954 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:00,956 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:37:00,957 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:19,833 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 08:37:21,613 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:21,616 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:21,621 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:21,625 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:21,633 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:21,635 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:21,637 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:37:21,639 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:21,641 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:21,650 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:37:21,659 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:21,672 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:21,685 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:21,690 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:21,699 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:21,702 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:21,705 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:21,708 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:21,719 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:21,724 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:21,727 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:37:21,733 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:37,664 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 08:37:39,199 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:39,201 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:39,203 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:39,206 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:39,209 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:39,213 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:39,215 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:37:39,217 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:39,220 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:39,222 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:37:39,224 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:39,225 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:39,232 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:39,234 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:39,241 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:39,246 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:39,250 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:39,255 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:39,258 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:39,263 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:39,266 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:37:39,268 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:40,054 [INFO] Starting _process_content
2025-02-07 08:37:40,057 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:40,204 [INFO] Starting _process_content
2025-02-07 08:37:40,207 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:40,262 [INFO] Starting _process_content
2025-02-07 08:37:40,269 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:40,306 [INFO] Starting _process_content
2025-02-07 08:37:40,307 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:40,350 [INFO] Starting _process_content
2025-02-07 08:37:40,352 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:40,402 [INFO] Starting _process_content
2025-02-07 08:37:40,403 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:40,422 [INFO] Starting _process_content
2025-02-07 08:37:40,423 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:40,447 [INFO] Starting _process_content
2025-02-07 08:37:40,449 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:40,473 [INFO] Starting _process_content
2025-02-07 08:37:40,474 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:40,502 [INFO] Starting _process_content
2025-02-07 08:37:40,504 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:40,523 [INFO] Starting _process_content
2025-02-07 08:37:40,524 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:40,551 [INFO] Starting _process_content
2025-02-07 08:37:40,552 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:40,557 [INFO] Starting _process_content
2025-02-07 08:37:40,557 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:40,581 [INFO] Starting _process_content
2025-02-07 08:37:40,581 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:40,612 [INFO] Starting _process_content
2025-02-07 08:37:40,614 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:40,641 [INFO] Starting _process_content
2025-02-07 08:37:40,649 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:40,696 [INFO] Starting _process_content
2025-02-07 08:37:40,699 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:40,737 [INFO] Starting _process_content
2025-02-07 08:37:40,738 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:40,770 [INFO] Starting _process_content
2025-02-07 08:37:40,772 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:40,807 [INFO] Starting _process_content
2025-02-07 08:37:40,816 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:40,848 [INFO] Starting _process_content
2025-02-07 08:37:40,850 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:40,896 [INFO] Starting _process_content
2025-02-07 08:37:40,898 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:40,904 [INFO] Starting _process_content
2025-02-07 08:37:40,907 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:40,919 [INFO] Starting _process_content
2025-02-07 08:37:40,922 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:40,949 [INFO] Starting _process_content
2025-02-07 08:37:40,956 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:40,971 [INFO] Starting _process_content
2025-02-07 08:37:40,971 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:40,980 [INFO] Starting _process_content
2025-02-07 08:37:40,984 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:40,991 [INFO] Starting _process_content
2025-02-07 08:37:40,997 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:41,001 [INFO] Starting _process_content
2025-02-07 08:37:41,004 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:41,011 [INFO] Starting _process_content
2025-02-07 08:37:41,012 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:41,019 [INFO] Starting _process_content
2025-02-07 08:37:41,020 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:41,029 [INFO] Starting _process_content
2025-02-07 08:37:41,031 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:41,055 [INFO] Starting _process_content
2025-02-07 08:37:41,056 [INFO] Sanitizing relative URL: page.html, base_url: https://example.com/subdir/
2025-02-07 08:37:41,057 [INFO] Resolved URL using urljoin: https://example.com/subdir/page.html
2025-02-07 08:37:41,057 [INFO] Sanitizing relative URL: /absolute.html, base_url: https://example.com/subdir/
2025-02-07 08:37:41,057 [INFO] Resolved URL using urljoin: https://example.com/absolute.html
2025-02-07 08:37:41,057 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:41,080 [INFO] Starting _process_content
2025-02-07 08:37:41,082 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:41,112 [INFO] Starting _process_content
2025-02-07 08:37:41,114 [INFO] Sanitizing relative URL: #section2, base_url: https://example.com/page
2025-02-07 08:37:41,114 [INFO] Resolved URL using urljoin: https://example.com/page#section2
2025-02-07 08:37:41,115 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:41,137 [INFO] Starting _process_content
2025-02-07 08:37:41,139 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:41,164 [INFO] Starting _process_content
2025-02-07 08:37:41,166 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:41,188 [INFO] Starting _process_content
2025-02-07 08:37:41,189 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:41,232 [INFO] Starting _process_content
2025-02-07 08:37:41,234 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:41,283 [INFO] Starting _process_content
2025-02-07 08:37:41,284 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:41,399 [INFO] Starting _process_content
2025-02-07 08:37:41,400 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:41,467 [INFO] Starting _process_content
2025-02-07 08:37:41,470 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:41,506 [INFO] Starting _process_content
2025-02-07 08:37:41,507 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:41,547 [INFO] Starting _process_content
2025-02-07 08:37:41,547 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:41,571 [INFO] Starting _process_content
2025-02-07 08:37:41,572 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:41,606 [INFO] Starting _process_content
2025-02-07 08:37:41,611 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:41,639 [INFO] Starting _process_content
2025-02-07 08:37:41,640 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:41,713 [INFO] Starting _process_content
2025-02-07 08:37:41,714 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:41,820 [INFO] Starting _process_content
2025-02-07 08:37:41,854 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:41,856 [INFO] Operation took 0.12 seconds
2025-02-07 08:37:41,882 [INFO] Starting _process_content
2025-02-07 08:37:41,883 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:41,919 [INFO] Starting _process_content
2025-02-07 08:37:41,920 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:42,098 [DEBUG] IDNA encoding failed for example.com@evil.com: Codepoint U+0040 at position 4 of 'com@evil' not allowed
2025-02-07 08:37:42,196 [DEBUG] IDNA encoding failed for user@example.com: Codepoint U+0040 at position 5 of 'user@example' not allowed
2025-02-07 08:37:42,196 [DEBUG] IDNA encoding failed for :pass@example.com: Empty domain
2025-02-07 08:37:42,251 [DEBUG] IDNA encoding failed for .example.com: Empty Label
2025-02-07 08:37:42,252 [DEBUG] IDNA encoding failed for example..com: Empty Label
2025-02-07 08:37:42,253 [DEBUG] IDNA encoding failed for example.com..: Empty Label
2025-02-07 08:37:42,772 [INFO] test_operation took 0.10 seconds
2025-02-07 08:37:42,976 [INFO] inner took 0.10 seconds
2025-02-07 08:37:42,976 [INFO] outer took 0.20 seconds
2025-02-07 08:37:42,976 [INFO] error_operation took 0.00 seconds
2025-02-07 08:37:43,105 [INFO] test_operation took 0.00 seconds
2025-02-07 08:37:43,283 [INFO] Starting _process_content
2025-02-07 08:37:43,284 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:43,299 [INFO] Starting _process_content
2025-02-07 08:37:43,300 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:43,322 [INFO] Starting _process_content
2025-02-07 08:37:43,328 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:43,353 [INFO] Starting _process_content
2025-02-07 08:37:43,361 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:43,371 [INFO] Starting _process_content
2025-02-07 08:37:43,374 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:43,387 [INFO] Starting _process_content
2025-02-07 08:37:43,388 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:43,419 [INFO] Starting _process_content
2025-02-07 08:37:43,426 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:43,450 [INFO] Starting _process_content
2025-02-07 08:37:43,451 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:37:43,467 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:37:43,468 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:43,480 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 08:37:43,483 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 08:37:43,484 [INFO] Starting crawl of URL: https://example.com
2025-02-07 08:37:43,743 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:37:43,743 [DEBUG] Base URL: https://example.com
2025-02-07 08:37:43,743 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:37:43,743 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:37:43,744 [DEBUG] Initial domain set to: example.com
2025-02-07 08:37:43,744 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:37:43,744 [INFO] Successfully crawled https://example.com
2025-02-07 08:37:43,746 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:43,747 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x0000014024F9B0E0>
2025-02-07 08:37:43,748 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000140251E4890>, 242913.7783728)])']
connector: <aiohttp.connector.TCPConnector object at 0x0000014024F9ACF0>
2025-02-07 08:37:43,748 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:37:43,749 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:43,751 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 08:37:43,751 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 08:37:43,751 [INFO] Starting crawl of URL: https://example.com
2025-02-07 08:37:43,804 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2025-02-07 08:37:43,804 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2025-02-07 08:37:43,804 [INFO] Starting crawl of URL: https://example.org
2025-02-07 08:37:43,940 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:37:43,940 [DEBUG] Base URL: https://example.com
2025-02-07 08:37:43,941 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:37:43,941 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:37:43,941 [DEBUG] Initial domain set to: example.com
2025-02-07 08:37:43,942 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:37:43,942 [INFO] Successfully crawled https://example.com
2025-02-07 08:37:44,607 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:37:44,608 [DEBUG] Base URL: https://example.org
2025-02-07 08:37:44,609 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:37:44,609 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:37:44,609 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:37:44,609 [INFO] Successfully crawled https://example.org
2025-02-07 08:37:44,612 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:44,614 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x0000014025080550>
2025-02-07 08:37:44,615 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000140251E50D0>, 242913.9756523)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001402514E2D0>, 242914.6427749)])']
connector: <aiohttp.connector.TCPConnector object at 0x0000014025080690>
2025-02-07 08:37:44,616 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:37:44,616 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:44,619 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 08:37:44,619 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 08:37:44,619 [INFO] Starting crawl of URL: https://example.com
2025-02-07 08:37:44,820 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:37:44,821 [DEBUG] Base URL: https://example.com
2025-02-07 08:37:44,821 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:37:44,822 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:37:44,822 [DEBUG] Initial domain set to: example.com
2025-02-07 08:37:44,822 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:37:44,823 [INFO] Successfully crawled https://example.com
2025-02-07 08:37:44,826 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:44,828 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x0000014025080E10>
2025-02-07 08:37:44,828 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000140251E5070>, 242914.8577322)])']
connector: <aiohttp.connector.TCPConnector object at 0x0000014025080050>
2025-02-07 08:37:44,830 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:37:44,830 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:44,833 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 08:37:44,833 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 08:37:44,833 [INFO] Starting crawl of URL: https://example.com
2025-02-07 08:37:45,064 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:37:45,064 [DEBUG] Base URL: https://example.com
2025-02-07 08:37:45,064 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:37:45,065 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:37:45,065 [DEBUG] Initial domain set to: example.com
2025-02-07 08:37:45,065 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:37:45,065 [INFO] Successfully crawled https://example.com
2025-02-07 08:37:45,067 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:37:45,067 [DEBUG] Base URL: https://example.com
2025-02-07 08:37:45,068 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:37:45,068 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:37:45,068 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:37:45,125 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:45,127 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:37:45,128 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:45,133 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:45,136 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:37:45,136 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:45,141 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 08:37:45,142 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 08:37:45,142 [INFO] Starting crawl of URL: https://example.com
2025-02-07 08:37:45,370 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:37:45,370 [DEBUG] Base URL: https://example.com
2025-02-07 08:37:45,370 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:37:45,371 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:37:45,371 [DEBUG] Initial domain set to: example.com
2025-02-07 08:37:45,372 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:37:45,372 [INFO] Successfully crawled https://example.com
2025-02-07 08:37:45,374 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:45,376 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x0000014024F27B10>
2025-02-07 08:37:45,376 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000140251E7F50>, 242915.4073793)])']
connector: <aiohttp.connector.TCPConnector object at 0x00000140250C5480>
2025-02-07 08:37:45,377 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:37:45,377 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:45,379 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2025-02-07 08:37:45,379 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2025-02-07 08:37:45,379 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2025-02-07 08:37:47,467 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [getaddrinfo failed]
2025-02-07 08:37:47,472 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [getaddrinfo failed]
2025-02-07 08:37:47,472 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [getaddrinfo failed]
2025-02-07 08:37:47,492 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:47,495 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:37:47,495 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:47,498 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 08:37:47,499 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 08:37:47,499 [INFO] Starting crawl of URL: https://example.com
2025-02-07 08:37:47,579 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2025-02-07 08:37:47,580 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2025-02-07 08:37:47,580 [INFO] Starting crawl of URL: https://example.org
2025-02-07 08:37:47,580 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2025-02-07 08:37:47,580 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2025-02-07 08:37:47,581 [INFO] Starting crawl of URL: https://example.net
2025-02-07 08:37:47,704 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:37:47,704 [DEBUG] Base URL: https://example.com
2025-02-07 08:37:47,705 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:37:47,705 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:37:47,705 [DEBUG] Initial domain set to: example.com
2025-02-07 08:37:47,705 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:37:47,705 [INFO] Successfully crawled https://example.com
2025-02-07 08:37:48,361 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:37:48,361 [DEBUG] Base URL: https://example.org
2025-02-07 08:37:48,362 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:37:48,362 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:37:48,362 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:37:48,362 [INFO] Successfully crawled https://example.org
2025-02-07 08:37:48,988 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:37:48,988 [DEBUG] Base URL: https://example.net
2025-02-07 08:37:48,988 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:37:48,988 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:37:48,988 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:37:48,989 [INFO] Successfully crawled https://example.net
2025-02-07 08:37:48,990 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:48,992 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x0000014025082710>
2025-02-07 08:37:48,992 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000140251E5D30>, 242917.7399781)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000140251E6A50>, 242918.3987517)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000140251E60F0>, 242919.0248673)])']
connector: <aiohttp.connector.TCPConnector object at 0x00000140250C5F30>
2025-02-07 08:37:48,993 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:37:48,993 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:48,995 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 08:37:48,995 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 08:37:48,995 [INFO] Starting crawl of URL: https://example.com
2025-02-07 08:37:49,173 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:37:49,174 [DEBUG] Base URL: https://example.com
2025-02-07 08:37:49,174 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:37:49,174 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:37:49,174 [DEBUG] Initial domain set to: example.com
2025-02-07 08:37:49,174 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:37:49,174 [INFO] Successfully crawled https://example.com
2025-02-07 08:37:49,176 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:49,196 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:37:49,196 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:49,199 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:37:50,126 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:37:50,188 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:50,191 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:37:50,191 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:50,194 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:37:50,260 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:50,262 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:37:50,262 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:50,264 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:37:50,264 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2025-02-07 08:37:50,265 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2025-02-07 08:37:50,265 [INFO] Starting crawl of URL: https://example.com/page
2025-02-07 08:37:51,005 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:37:51,005 [DEBUG] Base URL: https://example.com/page
2025-02-07 08:37:51,005 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:37:51,005 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:37:51,005 [DEBUG] Initial domain set to: example.com
2025-02-07 08:37:51,005 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:37:51,006 [INFO] Successfully crawled https://example.com/page
2025-02-07 08:37:51,006 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2025-02-07 08:37:51,006 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2025-02-07 08:37:51,006 [INFO] Starting crawl of URL: https://other-domain.com/page
2025-02-07 08:37:53,371 [DEBUG] 
Evaluating link: ./
2025-02-07 08:37:53,371 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,371 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2025-02-07 08:37:53,371 [DEBUG] Absolute link: https://other-domain.com/
2025-02-07 08:37:53,371 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,371 [DEBUG] 
Evaluating link: ./
2025-02-07 08:37:53,372 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,372 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2025-02-07 08:37:53,372 [DEBUG] Absolute link: https://other-domain.com/
2025-02-07 08:37:53,372 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,372 [DEBUG] 
Evaluating link: ./hiyou.php
2025-02-07 08:37:53,372 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,372 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2025-02-07 08:37:53,372 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2025-02-07 08:37:53,372 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,373 [DEBUG] 
Evaluating link: ./date.php
2025-02-07 08:37:53,373 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,373 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2025-02-07 08:37:53,373 [DEBUG] Absolute link: https://other-domain.com/date.php
2025-02-07 08:37:53,373 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,373 [DEBUG] 
Evaluating link: ./contact.php
2025-02-07 08:37:53,373 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,373 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2025-02-07 08:37:53,373 [DEBUG] Absolute link: https://other-domain.com/contact.php
2025-02-07 08:37:53,373 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,374 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 08:37:53,374 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,374 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 08:37:53,374 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 08:37:53,374 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2025-02-07 08:37:53,538 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,543 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-07 08:37:53,543 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 08:37:53,544 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,544 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 08:37:53,544 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 08:37:53,544 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-07 08:37:53,544 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-07 08:37:53,544 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,544 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 08:37:53,544 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 08:37:53,545 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-07 08:37:53,545 [DEBUG] 
Evaluating link: ./contact
2025-02-07 08:37:53,545 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,545 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2025-02-07 08:37:53,545 [DEBUG] Absolute link: https://other-domain.com/contact
2025-02-07 08:37:53,545 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,557 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,571 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-07 08:37:53,571 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 08:37:53,571 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,571 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 08:37:53,571 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 08:37:53,572 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-07 08:37:53,572 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-07 08:37:53,572 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,572 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 08:37:53,572 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 08:37:53,573 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-07 08:37:53,576 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,590 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2025-02-07 08:37:53,590 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 08:37:53,591 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,591 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 08:37:53,591 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 08:37:53,592 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2025-02-07 08:37:53,592 [DEBUG] 
Evaluating link: #indexes
2025-02-07 08:37:53,592 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,592 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2025-02-07 08:37:53,592 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,593 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,593 [DEBUG] 
Evaluating link: #a
2025-02-07 08:37:53,593 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,593 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2025-02-07 08:37:53,594 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,594 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,594 [DEBUG] 
Evaluating link: #b
2025-02-07 08:37:53,594 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,595 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2025-02-07 08:37:53,595 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,595 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,595 [DEBUG] 
Evaluating link: #c
2025-02-07 08:37:53,595 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,596 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2025-02-07 08:37:53,599 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,599 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,599 [DEBUG] 
Evaluating link: #d
2025-02-07 08:37:53,600 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,600 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2025-02-07 08:37:53,600 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,601 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,601 [DEBUG] 
Evaluating link: #e
2025-02-07 08:37:53,601 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,601 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2025-02-07 08:37:53,601 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,602 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,602 [DEBUG] 
Evaluating link: #f
2025-02-07 08:37:53,603 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,603 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2025-02-07 08:37:53,603 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,603 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,604 [DEBUG] 
Evaluating link: #g
2025-02-07 08:37:53,604 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,604 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2025-02-07 08:37:53,604 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,604 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,605 [DEBUG] 
Evaluating link: #h
2025-02-07 08:37:53,605 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,605 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2025-02-07 08:37:53,605 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,605 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,605 [DEBUG] 
Evaluating link: #i
2025-02-07 08:37:53,605 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,605 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2025-02-07 08:37:53,605 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,606 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,606 [DEBUG] 
Evaluating link: #j
2025-02-07 08:37:53,606 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,606 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2025-02-07 08:37:53,606 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,606 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,606 [DEBUG] 
Evaluating link: #k
2025-02-07 08:37:53,606 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,607 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2025-02-07 08:37:53,607 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,607 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,607 [DEBUG] 
Evaluating link: #l
2025-02-07 08:37:53,607 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,607 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2025-02-07 08:37:53,607 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,607 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,607 [DEBUG] 
Evaluating link: #m
2025-02-07 08:37:53,608 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,608 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2025-02-07 08:37:53,608 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,608 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,608 [DEBUG] 
Evaluating link: #n
2025-02-07 08:37:53,608 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,608 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2025-02-07 08:37:53,608 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,608 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,609 [DEBUG] 
Evaluating link: #o
2025-02-07 08:37:53,609 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,609 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2025-02-07 08:37:53,609 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,609 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,609 [DEBUG] 
Evaluating link: #p
2025-02-07 08:37:53,609 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,609 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2025-02-07 08:37:53,609 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,610 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,610 [DEBUG] 
Evaluating link: #q
2025-02-07 08:37:53,610 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,610 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2025-02-07 08:37:53,610 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,610 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,610 [DEBUG] 
Evaluating link: #r
2025-02-07 08:37:53,610 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,610 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2025-02-07 08:37:53,611 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,611 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,611 [DEBUG] 
Evaluating link: #s
2025-02-07 08:37:53,611 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,611 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2025-02-07 08:37:53,611 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,611 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,611 [DEBUG] 
Evaluating link: #t
2025-02-07 08:37:53,612 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,612 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2025-02-07 08:37:53,612 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,612 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,612 [DEBUG] 
Evaluating link: #u
2025-02-07 08:37:53,612 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,612 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2025-02-07 08:37:53,612 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,615 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,615 [DEBUG] 
Evaluating link: #v
2025-02-07 08:37:53,615 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,616 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2025-02-07 08:37:53,616 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,616 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,616 [DEBUG] 
Evaluating link: #w
2025-02-07 08:37:53,617 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,617 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2025-02-07 08:37:53,617 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,618 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,618 [DEBUG] 
Evaluating link: #x
2025-02-07 08:37:53,618 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,618 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2025-02-07 08:37:53,618 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,619 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,619 [DEBUG] 
Evaluating link: #y
2025-02-07 08:37:53,619 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,619 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2025-02-07 08:37:53,620 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,620 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,620 [DEBUG] 
Evaluating link: #z
2025-02-07 08:37:53,620 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,620 [DEBUG] Normalized URL: https://other-domain.com/page#z -> https://other-domain.com/page
2025-02-07 08:37:53,621 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,621 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,621 [DEBUG] 
Evaluating link: #A
2025-02-07 08:37:53,621 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,622 [DEBUG] Normalized URL: https://other-domain.com/page#A -> https://other-domain.com/page
2025-02-07 08:37:53,622 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,622 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,622 [DEBUG] 
Evaluating link: #B
2025-02-07 08:37:53,622 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,623 [DEBUG] Normalized URL: https://other-domain.com/page#B -> https://other-domain.com/page
2025-02-07 08:37:53,623 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,623 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,623 [DEBUG] 
Evaluating link: #C
2025-02-07 08:37:53,623 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,624 [DEBUG] Normalized URL: https://other-domain.com/page#C -> https://other-domain.com/page
2025-02-07 08:37:53,624 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,624 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,624 [DEBUG] 
Evaluating link: #D
2025-02-07 08:37:53,624 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,625 [DEBUG] Normalized URL: https://other-domain.com/page#D -> https://other-domain.com/page
2025-02-07 08:37:53,625 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,625 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,633 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,639 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-07 08:37:53,639 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 08:37:53,639 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,640 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 08:37:53,640 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 08:37:53,640 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-07 08:37:53,640 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-07 08:37:53,640 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,640 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 08:37:53,640 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 08:37:53,640 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-07 08:37:53,643 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,652 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2025-02-07 08:37:53,652 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 08:37:53,652 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,653 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 08:37:53,653 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 08:37:53,653 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2025-02-07 08:37:53,653 [DEBUG] 
Evaluating link: https://spriq.jp/
2025-02-07 08:37:53,653 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,653 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2025-02-07 08:37:53,653 [DEBUG] Absolute link: https://spriq.jp/
2025-02-07 08:37:53,653 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2025-02-07 08:37:53,656 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,662 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-07 08:37:53,669 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,675 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-07 08:37:53,675 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 08:37:53,675 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,675 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 08:37:53,675 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 08:37:53,676 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-07 08:37:53,676 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-07 08:37:53,676 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,676 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 08:37:53,676 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 08:37:53,676 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-07 08:37:53,676 [DEBUG] 
Evaluating link: ./contact
2025-02-07 08:37:53,676 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,677 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2025-02-07 08:37:53,677 [DEBUG] Absolute link: https://other-domain.com/contact
2025-02-07 08:37:53,677 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,677 [DEBUG] 
Evaluating link: ./
2025-02-07 08:37:53,677 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,677 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2025-02-07 08:37:53,677 [DEBUG] Absolute link: https://other-domain.com/
2025-02-07 08:37:53,677 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,678 [DEBUG] 
Evaluating link: ./hiyou.php
2025-02-07 08:37:53,678 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,678 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2025-02-07 08:37:53,678 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2025-02-07 08:37:53,678 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,678 [DEBUG] 
Evaluating link: ./date.php
2025-02-07 08:37:53,678 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,678 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2025-02-07 08:37:53,678 [DEBUG] Absolute link: https://other-domain.com/date.php
2025-02-07 08:37:53,678 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,679 [DEBUG] 
Evaluating link: #z
2025-02-07 08:37:53,679 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,679 [DEBUG] Normalized URL: https://other-domain.com/page#z -> https://other-domain.com/page
2025-02-07 08:37:53,679 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,681 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,681 [DEBUG] 
Evaluating link: #A
2025-02-07 08:37:53,682 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,682 [DEBUG] Normalized URL: https://other-domain.com/page#A -> https://other-domain.com/page
2025-02-07 08:37:53,682 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,682 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,682 [DEBUG] 
Evaluating link: #B
2025-02-07 08:37:53,682 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,683 [DEBUG] Normalized URL: https://other-domain.com/page#B -> https://other-domain.com/page
2025-02-07 08:37:53,683 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,683 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,683 [DEBUG] 
Evaluating link: #C
2025-02-07 08:37:53,683 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,683 [DEBUG] Normalized URL: https://other-domain.com/page#C -> https://other-domain.com/page
2025-02-07 08:37:53,683 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,684 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,684 [DEBUG] 
Evaluating link: #D
2025-02-07 08:37:53,684 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,684 [DEBUG] Normalized URL: https://other-domain.com/page#D -> https://other-domain.com/page
2025-02-07 08:37:53,684 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,684 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,684 [DEBUG] 
Evaluating link: #u
2025-02-07 08:37:53,684 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,684 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2025-02-07 08:37:53,685 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,685 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,685 [DEBUG] 
Evaluating link: #v
2025-02-07 08:37:53,685 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,685 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2025-02-07 08:37:53,685 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,686 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,686 [DEBUG] 
Evaluating link: #w
2025-02-07 08:37:53,686 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,686 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2025-02-07 08:37:53,687 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,687 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,687 [DEBUG] 
Evaluating link: #x
2025-02-07 08:37:53,687 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,687 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2025-02-07 08:37:53,687 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,687 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,687 [DEBUG] 
Evaluating link: #y
2025-02-07 08:37:53,687 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,688 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2025-02-07 08:37:53,688 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,688 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,688 [DEBUG] 
Evaluating link: #p
2025-02-07 08:37:53,688 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,688 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2025-02-07 08:37:53,688 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,688 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,688 [DEBUG] 
Evaluating link: #q
2025-02-07 08:37:53,689 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,689 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2025-02-07 08:37:53,689 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,689 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,689 [DEBUG] 
Evaluating link: #r
2025-02-07 08:37:53,689 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,689 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2025-02-07 08:37:53,689 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,689 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,690 [DEBUG] 
Evaluating link: #s
2025-02-07 08:37:53,690 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,690 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2025-02-07 08:37:53,690 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,690 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,690 [DEBUG] 
Evaluating link: #t
2025-02-07 08:37:53,690 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,690 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2025-02-07 08:37:53,691 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,691 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,691 [DEBUG] 
Evaluating link: #k
2025-02-07 08:37:53,691 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,691 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2025-02-07 08:37:53,691 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,691 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,691 [DEBUG] 
Evaluating link: #l
2025-02-07 08:37:53,692 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,692 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2025-02-07 08:37:53,692 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,692 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,693 [DEBUG] 
Evaluating link: #m
2025-02-07 08:37:53,693 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,693 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2025-02-07 08:37:53,693 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,693 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,693 [DEBUG] 
Evaluating link: #n
2025-02-07 08:37:53,693 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,694 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2025-02-07 08:37:53,694 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,694 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,694 [DEBUG] 
Evaluating link: #o
2025-02-07 08:37:53,694 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,694 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2025-02-07 08:37:53,694 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,694 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,694 [DEBUG] 
Evaluating link: #f
2025-02-07 08:37:53,695 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,695 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2025-02-07 08:37:53,695 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,695 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,695 [DEBUG] 
Evaluating link: #g
2025-02-07 08:37:53,695 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,695 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2025-02-07 08:37:53,695 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,695 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,695 [DEBUG] 
Evaluating link: #h
2025-02-07 08:37:53,697 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,697 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2025-02-07 08:37:53,698 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,698 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,699 [DEBUG] 
Evaluating link: #i
2025-02-07 08:37:53,700 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,700 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2025-02-07 08:37:53,700 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,700 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,700 [DEBUG] 
Evaluating link: #j
2025-02-07 08:37:53,700 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,700 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2025-02-07 08:37:53,701 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,701 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,701 [DEBUG] 
Evaluating link: #b
2025-02-07 08:37:53,701 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,702 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2025-02-07 08:37:53,702 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,702 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,702 [DEBUG] 
Evaluating link: #c
2025-02-07 08:37:53,702 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,702 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2025-02-07 08:37:53,702 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,703 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,703 [DEBUG] 
Evaluating link: #d
2025-02-07 08:37:53,703 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,703 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2025-02-07 08:37:53,703 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,703 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,703 [DEBUG] 
Evaluating link: #e
2025-02-07 08:37:53,703 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,703 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2025-02-07 08:37:53,703 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,704 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,704 [DEBUG] 
Evaluating link: #site
2025-02-07 08:37:53,704 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:37:53,704 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2025-02-07 08:37:53,704 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:37:53,704 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:37:53,705 [INFO] Successfully crawled https://other-domain.com/page
2025-02-07 08:37:53,721 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:53,723 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:37:53,724 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:53,726 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2025-02-07 08:37:53,727 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2025-02-07 08:37:53,727 [INFO] Starting crawl of URL: https://example.com/page0
2025-02-07 08:37:54,139 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:37:54,139 [DEBUG] Base URL: https://example.com/page0
2025-02-07 08:37:54,139 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:37:54,139 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:37:54,139 [DEBUG] Initial domain set to: example.com
2025-02-07 08:37:54,139 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:37:54,140 [INFO] Successfully crawled https://example.com/page0
2025-02-07 08:37:54,140 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2025-02-07 08:37:54,140 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2025-02-07 08:37:54,140 [INFO] Starting crawl of URL: https://example.com/page1
2025-02-07 08:37:54,534 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:37:54,535 [DEBUG] Base URL: https://example.com/page1
2025-02-07 08:37:54,535 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:37:54,535 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:37:54,536 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:37:54,536 [INFO] Successfully crawled https://example.com/page1
2025-02-07 08:37:54,536 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2025-02-07 08:37:54,536 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2025-02-07 08:37:54,536 [INFO] Starting crawl of URL: https://example.com/page2
2025-02-07 08:37:55,043 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:37:55,044 [DEBUG] Base URL: https://example.com/page2
2025-02-07 08:37:55,044 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:37:55,044 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:37:55,044 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:37:55,044 [INFO] Successfully crawled https://example.com/page2
2025-02-07 08:37:55,044 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2025-02-07 08:37:55,045 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2025-02-07 08:37:55,045 [INFO] Starting crawl of URL: https://example.com/page3
2025-02-07 08:37:56,065 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:37:56,065 [DEBUG] Base URL: https://example.com/page3
2025-02-07 08:37:56,066 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:37:56,066 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:37:56,066 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:37:56,066 [INFO] Successfully crawled https://example.com/page3
2025-02-07 08:37:56,067 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2025-02-07 08:37:56,067 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2025-02-07 08:37:56,067 [INFO] Starting crawl of URL: https://example.com/page4
2025-02-07 08:37:56,791 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:37:56,792 [DEBUG] Base URL: https://example.com/page4
2025-02-07 08:37:56,792 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:37:56,792 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:37:56,792 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:37:56,792 [INFO] Successfully crawled https://example.com/page4
2025-02-07 08:37:56,792 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:37:56,792 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2025-02-07 08:37:56,793 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2025-02-07 08:37:56,793 [INFO] Starting crawl of URL: https://example.com/page0
2025-02-07 08:37:56,968 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:37:56,969 [DEBUG] Base URL: https://example.com/page0
2025-02-07 08:37:56,969 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:37:56,969 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:37:56,969 [DEBUG] Initial domain set to: example.com
2025-02-07 08:37:56,969 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:37:56,969 [INFO] Successfully crawled https://example.com/page0
2025-02-07 08:37:56,969 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2025-02-07 08:37:56,969 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2025-02-07 08:37:56,970 [INFO] Starting crawl of URL: https://example.com/page1
2025-02-07 08:37:57,389 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:37:57,390 [DEBUG] Base URL: https://example.com/page1
2025-02-07 08:37:57,390 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:37:57,390 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:37:57,390 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:37:57,390 [INFO] Successfully crawled https://example.com/page1
2025-02-07 08:37:57,390 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2025-02-07 08:37:57,390 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2025-02-07 08:37:57,390 [INFO] Starting crawl of URL: https://example.com/page2
2025-02-07 08:37:57,887 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:37:57,888 [DEBUG] Base URL: https://example.com/page2
2025-02-07 08:37:57,888 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:37:57,888 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:37:57,888 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:37:57,888 [INFO] Successfully crawled https://example.com/page2
2025-02-07 08:37:57,888 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2025-02-07 08:37:57,888 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2025-02-07 08:37:57,888 [INFO] Starting crawl of URL: https://example.com/page3
2025-02-07 08:37:58,404 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:37:58,404 [DEBUG] Base URL: https://example.com/page3
2025-02-07 08:37:58,404 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:37:58,404 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:37:58,404 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:37:58,404 [INFO] Successfully crawled https://example.com/page3
2025-02-07 08:37:58,404 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2025-02-07 08:37:58,405 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2025-02-07 08:37:58,405 [INFO] Starting crawl of URL: https://example.com/page4
2025-02-07 08:37:58,936 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:37:58,936 [DEBUG] Base URL: https://example.com/page4
2025-02-07 08:37:58,937 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:37:58,937 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:37:58,937 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:37:58,937 [INFO] Successfully crawled https://example.com/page4
2025-02-07 08:37:58,950 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x0000014025082490>
2025-02-07 08:37:58,950 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000140251E6B70>, 242915.1005681)])']
connector: <aiohttp.connector.TCPConnector object at 0x00000140250C4E90>
2025-02-07 08:37:58,951 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x0000014024F27610>
2025-02-07 08:37:58,951 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x0000014025080CD0>
2025-02-07 08:37:58,951 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001402514F0B0>, 242921.0423648)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000140251E4830>, 242921.8712197)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000140251E40B0>, 242922.6073413)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000140251E6450>, 242923.3722991)])']
connector: <aiohttp.connector.TCPConnector object at 0x00000140250C6190>
2025-02-07 08:37:58,967 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:58,972 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,031 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,033 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,048 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,050 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,068 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,071 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,087 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,089 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,105 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,107 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,126 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,129 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,151 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,153 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,170 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,172 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,189 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,304 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:37:59,306 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,312 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,320 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:37:59,321 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,323 [DEBUG] 
Evaluating link: https://example.com/doc1
2025-02-07 08:37:59,323 [DEBUG] Target domain: example.com
2025-02-07 08:37:59,323 [DEBUG] URL domain: example.com
2025-02-07 08:37:59,325 [DEBUG] URL approved for crawling: https://example.com/doc1
2025-02-07 08:37:59,326 [DEBUG] 
Evaluating link: https://example.com/excluded/page
2025-02-07 08:37:59,327 [DEBUG] Target domain: example.com
2025-02-07 08:37:59,327 [DEBUG] URL domain: example.com
2025-02-07 08:37:59,327 [DEBUG] URL approved for crawling: https://example.com/excluded/page
2025-02-07 08:37:59,388 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,396 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:37:59,398 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,555 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,653 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:37:59,654 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,657 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-07 08:37:59,765 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,768 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:37:59,768 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,786 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,789 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:37:59,792 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,796 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-07 08:37:59,855 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,862 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:37:59,863 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,865 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-07 08:37:59,905 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,911 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:37:59,912 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,915 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-07 08:37:59,981 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,984 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:37:59,986 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:37:59,989 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-07 08:38:00,118 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:00,122 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:38:00,128 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:00,131 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-07 08:38:00,204 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:00,210 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:38:00,212 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:00,214 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-07 08:38:00,271 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:00,286 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:00,833 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:00,836 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:00,837 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:38:00,865 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:00,867 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:00,869 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:38:00,904 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:00,915 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:38:00,967 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x0000014025080690>
2025-02-07 08:38:00,967 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000140252BFDD0>, 242930.4960098)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000140252BE810>, 242930.7595014)])']
connector: <aiohttp.connector.TCPConnector object at 0x00000140250C5CD0>
2025-02-07 08:38:01,001 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:38:01,045 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:38:01,104 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:38:01,154 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:38:01,188 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:38:01,248 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:38:01,296 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:38:01,346 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:38:01,381 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:38:02,157 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:03,191 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:03,193 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:38:03,194 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 08:38:03,194 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 08:38:03,194 [INFO] Starting crawl of URL: https://example.com
2025-02-07 08:38:03,464 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:38:03,464 [DEBUG] Base URL: https://example.com
2025-02-07 08:38:03,464 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:38:03,464 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:38:03,464 [DEBUG] Initial domain set to: example.com
2025-02-07 08:38:03,465 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:38:03,465 [INFO] Successfully crawled https://example.com
2025-02-07 08:38:03,465 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x0000014025776350>
2025-02-07 08:38:03,465 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x0000014026442750>, 242933.5012344)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001402528C510>
2025-02-07 08:38:03,468 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2025-02-07 08:38:03,827 [INFO] Operation took 0.10 seconds
2025-02-07 08:38:04,544 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x0000014025082AD0>
2025-02-07 08:38:04,545 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x0000014025252AB0>, 242926.8289963)])']
connector: <aiohttp.connector.TCPConnector object at 0x00000140250C6B10>
2025-02-07 08:38:04,545 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x00000140250834D0>
2025-02-07 08:38:04,546 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x0000014025386150>, 242928.9732659)])']
connector: <aiohttp.connector.TCPConnector object at 0x00000140250C6C40>
2025-02-07 08:38:04,729 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:38:04,730 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:38:04,731 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:12,516 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-4\test_full_site_crawl0\test_docs/index.html
2025-02-07 08:38:12,517 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-07 08:38:12,600 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:12,611 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:38:12,614 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:38:12,615 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:20,092 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-4\test_content_processing_pipeli0\test_docs/guide.html
2025-02-07 08:38:20,093 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-07 08:38:20,117 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:20,131 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:38:20,131 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:38:20,132 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:27,641 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-4\test_quality_checks0\test_docs/api.html
2025-02-07 08:38:27,641 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-07 08:38:27,669 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:27,680 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:38:27,680 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:38:27,683 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:35,148 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-4\test_document_organization0\test_docs/index.html
2025-02-07 08:38:35,148 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-07 08:38:35,188 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:35,203 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:38:35,203 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:38:35,204 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:42,667 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-4\test_search_functionality0\test_docs/index.html
2025-02-07 08:38:42,668 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-07 08:38:42,706 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:42,720 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:38:42,721 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:38:42,722 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:50,344 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-4\test_error_handling_and_recove0\test_docs/index.html
2025-02-07 08:38:50,345 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-07 08:38:50,391 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:50,399 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:50,449 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:50,466 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:50,468 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:38:50,469 [ERROR] Error during crawl: 'str' object has no attribute 'url'
2025-02-07 08:38:50,492 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:50,499 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:50,519 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:50,522 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:50,538 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:38:50,566 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:53,654 [INFO] Sanitizing relative URL: /test, base_url: https://example.com
2025-02-07 08:38:53,654 [INFO] Resolved URL using urljoin: https://example.com/test
2025-02-07 08:38:53,793 [INFO] Starting _process_content
2025-02-07 08:38:53,796 [INFO] Sanitizing relative URL: /test, base_url: https://example.com/test
2025-02-07 08:38:53,796 [INFO] Resolved URL using urljoin: https://example.com/test
2025-02-07 08:38:53,797 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:38:53,816 [INFO] Starting _process_content
2025-02-07 08:38:53,836 [INFO] Starting _process_content
2025-02-07 08:38:53,838 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:38:53,860 [INFO] Starting _process_content
2025-02-07 08:38:53,862 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:38:53,868 [INFO] Starting _process_content
2025-02-07 08:38:53,869 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:38:53,877 [INFO] Starting _process_content
2025-02-07 08:38:53,880 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:38:53,999 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:54,117 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:54,120 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:54,146 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:54,152 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:54,194 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:54,198 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:54,232 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:54,235 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:54,268 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:54,271 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:54,302 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:54,305 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:38:54,346 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:42,226 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 08:49:43,675 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:43,677 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:43,685 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:43,688 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:43,692 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:43,694 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:43,698 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:49:43,701 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:43,704 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:43,706 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:49:43,709 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:43,711 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:43,718 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:43,720 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:43,726 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:43,727 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:43,735 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:43,738 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:43,743 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:43,745 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:43,751 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:49:43,753 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:58,044 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 08:49:59,692 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:59,694 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:59,698 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:59,703 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:59,707 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:59,710 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:59,715 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:49:59,717 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:59,722 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:59,725 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:49:59,727 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:59,733 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:59,740 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:59,743 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:59,746 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:59,751 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:59,756 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:59,759 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:59,762 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:59,764 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:49:59,769 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:49:59,773 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:50:43,810 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 08:50:45,514 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:50:45,516 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:50:45,518 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:50:45,525 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:50:45,527 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:50:45,529 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:50:45,531 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:50:45,533 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:50:45,535 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:50:45,539 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:50:45,543 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:50:45,545 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:50:45,551 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:50:45,553 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:50:45,561 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:50:45,565 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:50:45,568 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:50:45,569 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:50:45,578 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:50:45,580 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:50:45,581 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:50:45,583 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:11,184 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 08:51:12,715 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:12,717 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:12,720 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:12,722 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:12,725 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:12,727 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:12,732 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:51:12,733 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:12,735 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:12,737 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:51:12,739 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:12,741 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:12,745 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:12,748 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:12,752 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:12,754 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:12,759 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:12,761 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:12,768 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:12,772 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:12,774 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:51:12,775 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:35,071 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 08:51:36,764 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:36,766 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:36,769 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:36,772 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:36,778 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:36,780 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:36,781 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:51:36,783 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:36,785 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:36,787 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:51:36,789 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:36,794 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:36,798 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:36,803 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:36,805 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:36,813 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:36,817 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:36,819 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:36,822 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:36,829 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:36,833 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:51:36,835 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:38,170 [INFO] Starting _process_content
2025-02-07 08:51:38,175 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:38,230 [INFO] Starting _process_content
2025-02-07 08:51:38,231 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:38,317 [INFO] Starting _process_content
2025-02-07 08:51:38,319 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:38,350 [INFO] Starting _process_content
2025-02-07 08:51:38,350 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:38,371 [INFO] Starting _process_content
2025-02-07 08:51:38,376 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:38,410 [INFO] Starting _process_content
2025-02-07 08:51:38,411 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:38,428 [INFO] Starting _process_content
2025-02-07 08:51:38,429 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:38,453 [INFO] Starting _process_content
2025-02-07 08:51:38,454 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:38,483 [INFO] Starting _process_content
2025-02-07 08:51:38,485 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:38,504 [INFO] Starting _process_content
2025-02-07 08:51:38,509 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:38,532 [INFO] Starting _process_content
2025-02-07 08:51:38,533 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:38,585 [INFO] Starting _process_content
2025-02-07 08:51:38,586 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:38,595 [INFO] Starting _process_content
2025-02-07 08:51:38,596 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:38,627 [INFO] Starting _process_content
2025-02-07 08:51:38,628 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:38,679 [INFO] Starting _process_content
2025-02-07 08:51:38,681 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:38,729 [INFO] Starting _process_content
2025-02-07 08:51:38,730 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:38,782 [INFO] Starting _process_content
2025-02-07 08:51:38,785 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:38,837 [INFO] Starting _process_content
2025-02-07 08:51:38,843 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:38,894 [INFO] Starting _process_content
2025-02-07 08:51:38,897 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:38,932 [INFO] Starting _process_content
2025-02-07 08:51:38,934 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:38,970 [INFO] Starting _process_content
2025-02-07 08:51:38,978 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:39,003 [INFO] Starting _process_content
2025-02-07 08:51:39,009 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:39,014 [INFO] Starting _process_content
2025-02-07 08:51:39,016 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:39,026 [INFO] Starting _process_content
2025-02-07 08:51:39,028 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:39,039 [INFO] Starting _process_content
2025-02-07 08:51:39,043 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:39,052 [INFO] Starting _process_content
2025-02-07 08:51:39,059 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:39,069 [INFO] Starting _process_content
2025-02-07 08:51:39,076 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:39,085 [INFO] Starting _process_content
2025-02-07 08:51:39,092 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:39,101 [INFO] Starting _process_content
2025-02-07 08:51:39,108 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:39,117 [INFO] Starting _process_content
2025-02-07 08:51:39,126 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:39,146 [INFO] Starting _process_content
2025-02-07 08:51:39,151 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:39,164 [INFO] Starting _process_content
2025-02-07 08:51:39,168 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:39,216 [INFO] Starting _process_content
2025-02-07 08:51:39,217 [INFO] Sanitizing relative URL: page.html, base_url: https://example.com/subdir/
2025-02-07 08:51:39,217 [INFO] Resolved URL using urljoin: https://example.com/subdir/page.html
2025-02-07 08:51:39,218 [INFO] Sanitizing relative URL: /absolute.html, base_url: https://example.com/subdir/
2025-02-07 08:51:39,218 [INFO] Resolved URL using urljoin: https://example.com/absolute.html
2025-02-07 08:51:39,218 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:39,261 [INFO] Starting _process_content
2025-02-07 08:51:39,263 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:39,298 [INFO] Starting _process_content
2025-02-07 08:51:39,299 [INFO] Sanitizing relative URL: #section2, base_url: https://example.com/page
2025-02-07 08:51:39,299 [INFO] Resolved URL using urljoin: https://example.com/page#section2
2025-02-07 08:51:39,300 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:39,343 [INFO] Starting _process_content
2025-02-07 08:51:39,351 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:39,385 [INFO] Starting _process_content
2025-02-07 08:51:39,386 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:39,431 [INFO] Starting _process_content
2025-02-07 08:51:39,433 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:39,452 [INFO] Starting _process_content
2025-02-07 08:51:39,455 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:39,484 [INFO] Starting _process_content
2025-02-07 08:51:39,486 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:39,528 [INFO] Starting _process_content
2025-02-07 08:51:39,529 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:39,551 [INFO] Starting _process_content
2025-02-07 08:51:39,552 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:39,584 [INFO] Starting _process_content
2025-02-07 08:51:39,584 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:39,617 [INFO] Starting _process_content
2025-02-07 08:51:39,617 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:39,648 [INFO] Starting _process_content
2025-02-07 08:51:39,649 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:39,679 [INFO] Starting _process_content
2025-02-07 08:51:39,684 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:39,733 [INFO] Starting _process_content
2025-02-07 08:51:39,733 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:39,794 [INFO] Starting _process_content
2025-02-07 08:51:39,796 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:39,929 [INFO] Starting _process_content
2025-02-07 08:51:39,968 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:39,976 [INFO] Operation took 0.13 seconds
2025-02-07 08:51:39,998 [INFO] Starting _process_content
2025-02-07 08:51:39,998 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:40,027 [INFO] Starting _process_content
2025-02-07 08:51:40,030 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:40,146 [DEBUG] IDNA encoding failed for example.com@evil.com: Codepoint U+0040 at position 4 of 'com@evil' not allowed
2025-02-07 08:51:40,293 [DEBUG] IDNA encoding failed for user@example.com: Codepoint U+0040 at position 5 of 'user@example' not allowed
2025-02-07 08:51:40,293 [DEBUG] IDNA encoding failed for :pass@example.com: Empty domain
2025-02-07 08:51:40,365 [DEBUG] IDNA encoding failed for .example.com: Empty Label
2025-02-07 08:51:40,365 [DEBUG] IDNA encoding failed for example..com: Empty Label
2025-02-07 08:51:40,366 [DEBUG] IDNA encoding failed for example.com..: Empty Label
2025-02-07 08:51:40,714 [INFO] test_operation took 0.10 seconds
2025-02-07 08:51:40,915 [INFO] inner took 0.10 seconds
2025-02-07 08:51:40,916 [INFO] outer took 0.20 seconds
2025-02-07 08:51:40,916 [INFO] error_operation took 0.00 seconds
2025-02-07 08:51:41,026 [INFO] test_operation took 0.00 seconds
2025-02-07 08:51:41,079 [INFO] Starting _process_content
2025-02-07 08:51:41,080 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:41,096 [INFO] Starting _process_content
2025-02-07 08:51:41,097 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:41,116 [INFO] Starting _process_content
2025-02-07 08:51:41,117 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:41,142 [INFO] Starting _process_content
2025-02-07 08:51:41,146 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:41,161 [INFO] Starting _process_content
2025-02-07 08:51:41,162 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:41,179 [INFO] Starting _process_content
2025-02-07 08:51:41,180 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:41,194 [INFO] Starting _process_content
2025-02-07 08:51:41,195 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:41,218 [INFO] Starting _process_content
2025-02-07 08:51:41,226 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:51:41,277 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:51:41,277 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:41,281 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 08:51:41,281 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 08:51:41,282 [INFO] Starting crawl of URL: https://example.com
2025-02-07 08:51:41,759 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:51:41,759 [DEBUG] Base URL: https://example.com
2025-02-07 08:51:41,759 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:51:41,760 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:51:41,760 [DEBUG] Initial domain set to: example.com
2025-02-07 08:51:41,760 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:51:41,760 [INFO] Successfully crawled https://example.com
2025-02-07 08:51:41,761 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:41,763 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001896B9970E0>
2025-02-07 08:51:41,763 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001896BC95130>, 243751.7963301)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001896B996CF0>
2025-02-07 08:51:41,764 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:51:41,764 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:41,766 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 08:51:41,766 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 08:51:41,766 [INFO] Starting crawl of URL: https://example.com
2025-02-07 08:51:41,841 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2025-02-07 08:51:41,841 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2025-02-07 08:51:41,841 [INFO] Starting crawl of URL: https://example.org
2025-02-07 08:51:42,115 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:51:42,115 [DEBUG] Base URL: https://example.com
2025-02-07 08:51:42,115 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:51:42,115 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:51:42,115 [DEBUG] Initial domain set to: example.com
2025-02-07 08:51:42,115 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:51:42,116 [INFO] Successfully crawled https://example.com
2025-02-07 08:51:42,529 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:51:42,530 [DEBUG] Base URL: https://example.org
2025-02-07 08:51:42,530 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:51:42,530 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:51:42,530 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:51:42,530 [INFO] Successfully crawled https://example.org
2025-02-07 08:51:42,532 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:42,533 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001896BAB1F90>
2025-02-07 08:51:42,533 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001896BC909B0>, 243752.1524258)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001896BC97AD0>, 243752.5669455)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001896BAB0550>
2025-02-07 08:51:42,536 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:51:42,537 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:42,539 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 08:51:42,539 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 08:51:42,540 [INFO] Starting crawl of URL: https://example.com
2025-02-07 08:51:42,908 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:51:42,908 [DEBUG] Base URL: https://example.com
2025-02-07 08:51:42,909 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:51:42,909 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:51:42,909 [DEBUG] Initial domain set to: example.com
2025-02-07 08:51:42,909 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:51:42,910 [INFO] Successfully crawled https://example.com
2025-02-07 08:51:42,912 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:42,913 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001896BAB1D10>
2025-02-07 08:51:42,914 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001896BC90950>, 243752.9445775)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001896BAB20D0>
2025-02-07 08:51:42,915 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:51:42,916 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:42,919 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 08:51:42,920 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 08:51:42,920 [INFO] Starting crawl of URL: https://example.com
2025-02-07 08:51:43,265 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:51:43,265 [DEBUG] Base URL: https://example.com
2025-02-07 08:51:43,265 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:51:43,266 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:51:43,266 [DEBUG] Initial domain set to: example.com
2025-02-07 08:51:43,266 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:51:43,266 [INFO] Successfully crawled https://example.com
2025-02-07 08:51:43,269 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:51:43,270 [DEBUG] Base URL: https://example.com
2025-02-07 08:51:43,270 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:51:43,270 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:51:43,270 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:51:43,296 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:43,299 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:51:43,299 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:43,306 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:43,308 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:51:43,308 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:43,310 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 08:51:43,310 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 08:51:43,310 [INFO] Starting crawl of URL: https://example.com
2025-02-07 08:51:43,664 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:51:43,665 [DEBUG] Base URL: https://example.com
2025-02-07 08:51:43,665 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:51:43,665 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:51:43,665 [DEBUG] Initial domain set to: example.com
2025-02-07 08:51:43,665 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:51:43,665 [INFO] Successfully crawled https://example.com
2025-02-07 08:51:43,668 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:43,670 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001896B92B250>
2025-02-07 08:51:43,670 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001896BC93830>, 243753.7021166)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001896BB16190>
2025-02-07 08:51:43,671 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:51:43,671 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:43,673 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2025-02-07 08:51:43,673 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2025-02-07 08:51:43,673 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2025-02-07 08:51:45,765 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [getaddrinfo failed]
2025-02-07 08:51:45,768 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [getaddrinfo failed]
2025-02-07 08:51:45,768 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [getaddrinfo failed]
2025-02-07 08:51:45,777 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:45,780 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:51:45,780 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:45,783 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 08:51:45,784 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 08:51:45,784 [INFO] Starting crawl of URL: https://example.com
2025-02-07 08:51:45,829 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2025-02-07 08:51:45,829 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2025-02-07 08:51:45,829 [INFO] Starting crawl of URL: https://example.org
2025-02-07 08:51:45,829 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2025-02-07 08:51:45,830 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2025-02-07 08:51:45,830 [INFO] Starting crawl of URL: https://example.net
2025-02-07 08:51:46,110 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:51:46,110 [DEBUG] Base URL: https://example.com
2025-02-07 08:51:46,110 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:51:46,110 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:51:46,111 [DEBUG] Initial domain set to: example.com
2025-02-07 08:51:46,111 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:51:46,111 [INFO] Successfully crawled https://example.com
2025-02-07 08:51:46,482 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:51:46,483 [DEBUG] Base URL: https://example.org
2025-02-07 08:51:46,483 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:51:46,483 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:51:46,483 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:51:46,484 [INFO] Successfully crawled https://example.org
2025-02-07 08:51:47,049 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:51:47,050 [DEBUG] Base URL: https://example.net
2025-02-07 08:51:47,050 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:51:47,050 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:51:47,050 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:51:47,051 [INFO] Successfully crawled https://example.net
2025-02-07 08:51:47,053 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:47,054 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001896BAB2C10>
2025-02-07 08:51:47,055 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001896BC91490>, 243756.1472111)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001896BC93E30>, 243756.5180699)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001896BC91AF0>, 243757.0852661)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001896BB16C40>
2025-02-07 08:51:47,056 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:51:47,056 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:47,058 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 08:51:47,058 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 08:51:47,058 [INFO] Starting crawl of URL: https://example.com
2025-02-07 08:51:47,400 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:51:47,400 [DEBUG] Base URL: https://example.com
2025-02-07 08:51:47,401 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:51:47,401 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:51:47,401 [DEBUG] Initial domain set to: example.com
2025-02-07 08:51:47,401 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:51:47,401 [INFO] Successfully crawled https://example.com
2025-02-07 08:51:47,403 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:47,421 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:51:47,422 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:47,424 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:51:48,258 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:51:48,305 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:48,307 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:51:48,307 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:48,309 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:51:48,366 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:48,368 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:51:48,369 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:48,371 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:51:48,371 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2025-02-07 08:51:48,371 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2025-02-07 08:51:48,371 [INFO] Starting crawl of URL: https://example.com/page
2025-02-07 08:51:48,924 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:51:48,925 [DEBUG] Base URL: https://example.com/page
2025-02-07 08:51:48,925 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:51:48,925 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:51:48,925 [DEBUG] Initial domain set to: example.com
2025-02-07 08:51:48,926 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:51:48,926 [INFO] Successfully crawled https://example.com/page
2025-02-07 08:51:48,926 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2025-02-07 08:51:48,927 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2025-02-07 08:51:48,927 [INFO] Starting crawl of URL: https://other-domain.com/page
2025-02-07 08:51:51,117 [DEBUG] 
Evaluating link: ./
2025-02-07 08:51:51,117 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,117 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2025-02-07 08:51:51,117 [DEBUG] Absolute link: https://other-domain.com/
2025-02-07 08:51:51,117 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,117 [DEBUG] 
Evaluating link: ./
2025-02-07 08:51:51,117 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,118 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2025-02-07 08:51:51,118 [DEBUG] Absolute link: https://other-domain.com/
2025-02-07 08:51:51,118 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,118 [DEBUG] 
Evaluating link: ./hiyou.php
2025-02-07 08:51:51,118 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,118 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2025-02-07 08:51:51,118 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2025-02-07 08:51:51,118 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,118 [DEBUG] 
Evaluating link: ./date.php
2025-02-07 08:51:51,119 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,119 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2025-02-07 08:51:51,119 [DEBUG] Absolute link: https://other-domain.com/date.php
2025-02-07 08:51:51,119 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,119 [DEBUG] 
Evaluating link: ./contact.php
2025-02-07 08:51:51,119 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,119 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2025-02-07 08:51:51,119 [DEBUG] Absolute link: https://other-domain.com/contact.php
2025-02-07 08:51:51,119 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,120 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 08:51:51,120 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,120 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 08:51:51,120 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 08:51:51,120 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2025-02-07 08:51:51,138 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,146 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-07 08:51:51,147 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 08:51:51,147 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,147 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 08:51:51,147 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 08:51:51,147 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-07 08:51:51,147 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-07 08:51:51,147 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,148 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 08:51:51,148 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 08:51:51,148 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-07 08:51:51,148 [DEBUG] 
Evaluating link: ./contact
2025-02-07 08:51:51,148 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,148 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2025-02-07 08:51:51,148 [DEBUG] Absolute link: https://other-domain.com/contact
2025-02-07 08:51:51,148 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,152 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,158 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-07 08:51:51,158 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 08:51:51,159 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,159 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 08:51:51,160 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 08:51:51,161 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-07 08:51:51,161 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-07 08:51:51,161 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,162 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 08:51:51,162 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 08:51:51,162 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-07 08:51:51,165 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,171 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2025-02-07 08:51:51,172 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 08:51:51,172 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,172 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 08:51:51,172 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 08:51:51,172 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2025-02-07 08:51:51,172 [DEBUG] 
Evaluating link: #indexes
2025-02-07 08:51:51,172 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,172 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2025-02-07 08:51:51,173 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,173 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,173 [DEBUG] 
Evaluating link: #a
2025-02-07 08:51:51,173 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,173 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2025-02-07 08:51:51,173 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,173 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,174 [DEBUG] 
Evaluating link: #b
2025-02-07 08:51:51,174 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,174 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2025-02-07 08:51:51,174 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,175 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,175 [DEBUG] 
Evaluating link: #c
2025-02-07 08:51:51,177 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,178 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2025-02-07 08:51:51,179 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,179 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,179 [DEBUG] 
Evaluating link: #d
2025-02-07 08:51:51,179 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,180 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2025-02-07 08:51:51,180 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,180 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,180 [DEBUG] 
Evaluating link: #e
2025-02-07 08:51:51,180 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,180 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2025-02-07 08:51:51,181 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,181 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,181 [DEBUG] 
Evaluating link: #f
2025-02-07 08:51:51,181 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,181 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2025-02-07 08:51:51,181 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,181 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,181 [DEBUG] 
Evaluating link: #g
2025-02-07 08:51:51,182 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,182 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2025-02-07 08:51:51,182 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,182 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,183 [DEBUG] 
Evaluating link: #h
2025-02-07 08:51:51,183 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,183 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2025-02-07 08:51:51,184 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,184 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,184 [DEBUG] 
Evaluating link: #i
2025-02-07 08:51:51,184 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,185 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2025-02-07 08:51:51,185 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,185 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,185 [DEBUG] 
Evaluating link: #j
2025-02-07 08:51:51,186 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,186 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2025-02-07 08:51:51,186 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,186 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,187 [DEBUG] 
Evaluating link: #k
2025-02-07 08:51:51,187 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,187 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2025-02-07 08:51:51,187 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,187 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,188 [DEBUG] 
Evaluating link: #l
2025-02-07 08:51:51,188 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,188 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2025-02-07 08:51:51,188 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,189 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,189 [DEBUG] 
Evaluating link: #m
2025-02-07 08:51:51,189 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,189 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2025-02-07 08:51:51,189 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,189 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,190 [DEBUG] 
Evaluating link: #n
2025-02-07 08:51:51,190 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,190 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2025-02-07 08:51:51,190 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,191 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,191 [DEBUG] 
Evaluating link: #o
2025-02-07 08:51:51,191 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,191 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2025-02-07 08:51:51,192 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,192 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,194 [DEBUG] 
Evaluating link: #p
2025-02-07 08:51:51,195 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,195 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2025-02-07 08:51:51,195 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,196 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,196 [DEBUG] 
Evaluating link: #q
2025-02-07 08:51:51,196 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,196 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2025-02-07 08:51:51,196 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,197 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,197 [DEBUG] 
Evaluating link: #r
2025-02-07 08:51:51,197 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,197 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2025-02-07 08:51:51,197 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,197 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,197 [DEBUG] 
Evaluating link: #s
2025-02-07 08:51:51,197 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,197 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2025-02-07 08:51:51,198 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,198 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,198 [DEBUG] 
Evaluating link: #t
2025-02-07 08:51:51,198 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,198 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2025-02-07 08:51:51,198 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,198 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,198 [DEBUG] 
Evaluating link: #u
2025-02-07 08:51:51,198 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,199 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2025-02-07 08:51:51,199 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,199 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,199 [DEBUG] 
Evaluating link: #v
2025-02-07 08:51:51,199 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,199 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2025-02-07 08:51:51,199 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,199 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,200 [DEBUG] 
Evaluating link: #w
2025-02-07 08:51:51,200 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,200 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2025-02-07 08:51:51,200 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,200 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,200 [DEBUG] 
Evaluating link: #x
2025-02-07 08:51:51,200 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,200 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2025-02-07 08:51:51,200 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,201 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,201 [DEBUG] 
Evaluating link: #y
2025-02-07 08:51:51,201 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,201 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2025-02-07 08:51:51,201 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,201 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,201 [DEBUG] 
Evaluating link: #z
2025-02-07 08:51:51,201 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,201 [DEBUG] Normalized URL: https://other-domain.com/page#z -> https://other-domain.com/page
2025-02-07 08:51:51,202 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,202 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,202 [DEBUG] 
Evaluating link: #A
2025-02-07 08:51:51,202 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,202 [DEBUG] Normalized URL: https://other-domain.com/page#A -> https://other-domain.com/page
2025-02-07 08:51:51,202 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,202 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,202 [DEBUG] 
Evaluating link: #B
2025-02-07 08:51:51,202 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,203 [DEBUG] Normalized URL: https://other-domain.com/page#B -> https://other-domain.com/page
2025-02-07 08:51:51,203 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,203 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,203 [DEBUG] 
Evaluating link: #C
2025-02-07 08:51:51,203 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,203 [DEBUG] Normalized URL: https://other-domain.com/page#C -> https://other-domain.com/page
2025-02-07 08:51:51,203 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,203 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,204 [DEBUG] 
Evaluating link: #D
2025-02-07 08:51:51,204 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,204 [DEBUG] Normalized URL: https://other-domain.com/page#D -> https://other-domain.com/page
2025-02-07 08:51:51,204 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,204 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,207 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,216 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-07 08:51:51,217 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 08:51:51,217 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,217 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 08:51:51,217 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 08:51:51,217 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-07 08:51:51,217 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-07 08:51:51,217 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,218 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 08:51:51,218 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 08:51:51,218 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-07 08:51:51,221 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,233 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2025-02-07 08:51:51,233 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 08:51:51,233 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,234 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 08:51:51,234 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 08:51:51,234 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2025-02-07 08:51:51,234 [DEBUG] 
Evaluating link: https://spriq.jp/
2025-02-07 08:51:51,234 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,235 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2025-02-07 08:51:51,235 [DEBUG] Absolute link: https://spriq.jp/
2025-02-07 08:51:51,235 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2025-02-07 08:51:51,240 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,257 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-07 08:51:51,266 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,280 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-07 08:51:51,280 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 08:51:51,281 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,281 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 08:51:51,281 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 08:51:51,281 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-07 08:51:51,282 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-07 08:51:51,282 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,282 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 08:51:51,282 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 08:51:51,283 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-07 08:51:51,283 [DEBUG] 
Evaluating link: ./contact
2025-02-07 08:51:51,283 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,284 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2025-02-07 08:51:51,284 [DEBUG] Absolute link: https://other-domain.com/contact
2025-02-07 08:51:51,284 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,284 [DEBUG] 
Evaluating link: ./
2025-02-07 08:51:51,284 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,285 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2025-02-07 08:51:51,285 [DEBUG] Absolute link: https://other-domain.com/
2025-02-07 08:51:51,285 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,285 [DEBUG] 
Evaluating link: ./hiyou.php
2025-02-07 08:51:51,285 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,286 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2025-02-07 08:51:51,286 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2025-02-07 08:51:51,286 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,286 [DEBUG] 
Evaluating link: ./date.php
2025-02-07 08:51:51,287 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,287 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2025-02-07 08:51:51,287 [DEBUG] Absolute link: https://other-domain.com/date.php
2025-02-07 08:51:51,287 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,287 [DEBUG] 
Evaluating link: #z
2025-02-07 08:51:51,288 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,288 [DEBUG] Normalized URL: https://other-domain.com/page#z -> https://other-domain.com/page
2025-02-07 08:51:51,288 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,288 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,288 [DEBUG] 
Evaluating link: #A
2025-02-07 08:51:51,288 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,288 [DEBUG] Normalized URL: https://other-domain.com/page#A -> https://other-domain.com/page
2025-02-07 08:51:51,288 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,288 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,289 [DEBUG] 
Evaluating link: #B
2025-02-07 08:51:51,289 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,289 [DEBUG] Normalized URL: https://other-domain.com/page#B -> https://other-domain.com/page
2025-02-07 08:51:51,289 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,289 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,289 [DEBUG] 
Evaluating link: #C
2025-02-07 08:51:51,289 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,289 [DEBUG] Normalized URL: https://other-domain.com/page#C -> https://other-domain.com/page
2025-02-07 08:51:51,289 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,289 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,290 [DEBUG] 
Evaluating link: #D
2025-02-07 08:51:51,290 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,290 [DEBUG] Normalized URL: https://other-domain.com/page#D -> https://other-domain.com/page
2025-02-07 08:51:51,290 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,290 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,290 [DEBUG] 
Evaluating link: #u
2025-02-07 08:51:51,290 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,290 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2025-02-07 08:51:51,290 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,290 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,291 [DEBUG] 
Evaluating link: #v
2025-02-07 08:51:51,291 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,291 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2025-02-07 08:51:51,291 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,291 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,291 [DEBUG] 
Evaluating link: #w
2025-02-07 08:51:51,291 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,291 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2025-02-07 08:51:51,291 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,291 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,292 [DEBUG] 
Evaluating link: #x
2025-02-07 08:51:51,292 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,292 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2025-02-07 08:51:51,292 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,293 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,295 [DEBUG] 
Evaluating link: #y
2025-02-07 08:51:51,295 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,296 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2025-02-07 08:51:51,296 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,296 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,296 [DEBUG] 
Evaluating link: #p
2025-02-07 08:51:51,296 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,297 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2025-02-07 08:51:51,297 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,297 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,297 [DEBUG] 
Evaluating link: #q
2025-02-07 08:51:51,297 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,297 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2025-02-07 08:51:51,297 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,298 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,298 [DEBUG] 
Evaluating link: #r
2025-02-07 08:51:51,298 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,298 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2025-02-07 08:51:51,298 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,298 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,298 [DEBUG] 
Evaluating link: #s
2025-02-07 08:51:51,298 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,298 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2025-02-07 08:51:51,298 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,299 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,299 [DEBUG] 
Evaluating link: #t
2025-02-07 08:51:51,299 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,299 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2025-02-07 08:51:51,299 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,299 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,299 [DEBUG] 
Evaluating link: #k
2025-02-07 08:51:51,299 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,299 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2025-02-07 08:51:51,300 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,300 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,300 [DEBUG] 
Evaluating link: #l
2025-02-07 08:51:51,300 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,300 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2025-02-07 08:51:51,300 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,300 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,300 [DEBUG] 
Evaluating link: #m
2025-02-07 08:51:51,300 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,300 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2025-02-07 08:51:51,301 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,301 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,301 [DEBUG] 
Evaluating link: #n
2025-02-07 08:51:51,301 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,301 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2025-02-07 08:51:51,301 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,301 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,301 [DEBUG] 
Evaluating link: #o
2025-02-07 08:51:51,301 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,302 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2025-02-07 08:51:51,302 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,302 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,302 [DEBUG] 
Evaluating link: #f
2025-02-07 08:51:51,302 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,302 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2025-02-07 08:51:51,302 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,302 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,302 [DEBUG] 
Evaluating link: #g
2025-02-07 08:51:51,302 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,303 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2025-02-07 08:51:51,303 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,303 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,303 [DEBUG] 
Evaluating link: #h
2025-02-07 08:51:51,303 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,303 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2025-02-07 08:51:51,303 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,303 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,303 [DEBUG] 
Evaluating link: #i
2025-02-07 08:51:51,303 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,304 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2025-02-07 08:51:51,304 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,304 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,304 [DEBUG] 
Evaluating link: #j
2025-02-07 08:51:51,304 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,304 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2025-02-07 08:51:51,304 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,304 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,304 [DEBUG] 
Evaluating link: #b
2025-02-07 08:51:51,304 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,305 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2025-02-07 08:51:51,305 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,305 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,305 [DEBUG] 
Evaluating link: #c
2025-02-07 08:51:51,305 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,305 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2025-02-07 08:51:51,305 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,305 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,305 [DEBUG] 
Evaluating link: #d
2025-02-07 08:51:51,305 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,306 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2025-02-07 08:51:51,306 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,306 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,306 [DEBUG] 
Evaluating link: #e
2025-02-07 08:51:51,306 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,306 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2025-02-07 08:51:51,306 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,306 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,306 [DEBUG] 
Evaluating link: #site
2025-02-07 08:51:51,306 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 08:51:51,307 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2025-02-07 08:51:51,307 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 08:51:51,307 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 08:51:51,307 [INFO] Successfully crawled https://other-domain.com/page
2025-02-07 08:51:51,322 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:51,324 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:51:51,325 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:51,330 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2025-02-07 08:51:51,331 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2025-02-07 08:51:51,331 [INFO] Starting crawl of URL: https://example.com/page0
2025-02-07 08:51:52,350 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:51:52,351 [DEBUG] Base URL: https://example.com/page0
2025-02-07 08:51:52,351 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:51:52,351 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:51:52,351 [DEBUG] Initial domain set to: example.com
2025-02-07 08:51:52,351 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:51:52,351 [INFO] Successfully crawled https://example.com/page0
2025-02-07 08:51:52,351 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2025-02-07 08:51:52,351 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2025-02-07 08:51:52,352 [INFO] Starting crawl of URL: https://example.com/page1
2025-02-07 08:51:52,998 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:51:52,999 [DEBUG] Base URL: https://example.com/page1
2025-02-07 08:51:52,999 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:51:52,999 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:51:52,999 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:51:52,999 [INFO] Successfully crawled https://example.com/page1
2025-02-07 08:51:52,999 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2025-02-07 08:51:52,999 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2025-02-07 08:51:53,000 [INFO] Starting crawl of URL: https://example.com/page2
2025-02-07 08:51:53,394 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:51:53,394 [DEBUG] Base URL: https://example.com/page2
2025-02-07 08:51:53,394 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:51:53,395 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:51:53,395 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:51:53,395 [INFO] Successfully crawled https://example.com/page2
2025-02-07 08:51:53,395 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2025-02-07 08:51:53,396 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2025-02-07 08:51:53,396 [INFO] Starting crawl of URL: https://example.com/page3
2025-02-07 08:51:54,553 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:51:54,553 [DEBUG] Base URL: https://example.com/page3
2025-02-07 08:51:54,553 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:51:54,553 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:51:54,553 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:51:54,554 [INFO] Successfully crawled https://example.com/page3
2025-02-07 08:51:54,554 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2025-02-07 08:51:54,554 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2025-02-07 08:51:54,554 [INFO] Starting crawl of URL: https://example.com/page4
2025-02-07 08:51:55,399 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:51:55,399 [DEBUG] Base URL: https://example.com/page4
2025-02-07 08:51:55,399 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:51:55,399 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:51:55,399 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:51:55,399 [INFO] Successfully crawled https://example.com/page4
2025-02-07 08:51:55,400 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:51:55,400 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2025-02-07 08:51:55,400 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2025-02-07 08:51:55,400 [INFO] Starting crawl of URL: https://example.com/page0
2025-02-07 08:51:55,908 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:51:55,908 [DEBUG] Base URL: https://example.com/page0
2025-02-07 08:51:55,908 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:51:55,909 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:51:55,909 [DEBUG] Initial domain set to: example.com
2025-02-07 08:51:55,909 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:51:55,909 [INFO] Successfully crawled https://example.com/page0
2025-02-07 08:51:55,909 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2025-02-07 08:51:55,909 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2025-02-07 08:51:55,909 [INFO] Starting crawl of URL: https://example.com/page1
2025-02-07 08:51:56,073 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:51:56,073 [DEBUG] Base URL: https://example.com/page1
2025-02-07 08:51:56,074 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:51:56,074 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:51:56,074 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:51:56,075 [INFO] Successfully crawled https://example.com/page1
2025-02-07 08:51:56,075 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2025-02-07 08:51:56,076 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2025-02-07 08:51:56,076 [INFO] Starting crawl of URL: https://example.com/page2
2025-02-07 08:51:56,726 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:51:56,726 [DEBUG] Base URL: https://example.com/page2
2025-02-07 08:51:56,726 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:51:56,727 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:51:56,727 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:51:56,727 [INFO] Successfully crawled https://example.com/page2
2025-02-07 08:51:56,727 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2025-02-07 08:51:56,727 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2025-02-07 08:51:56,727 [INFO] Starting crawl of URL: https://example.com/page3
2025-02-07 08:51:57,136 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:51:57,136 [DEBUG] Base URL: https://example.com/page3
2025-02-07 08:51:57,136 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:51:57,136 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:51:57,136 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:51:57,136 [INFO] Successfully crawled https://example.com/page3
2025-02-07 08:51:57,137 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2025-02-07 08:51:57,137 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2025-02-07 08:51:57,138 [INFO] Starting crawl of URL: https://example.com/page4
2025-02-07 08:51:57,750 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:51:57,750 [DEBUG] Base URL: https://example.com/page4
2025-02-07 08:51:57,750 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:51:57,751 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:51:57,751 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:51:57,751 [INFO] Successfully crawled https://example.com/page4
2025-02-07 08:51:57,762 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:57,766 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:57,793 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:57,798 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:57,825 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001896BAB2990>
2025-02-07 08:51:57,826 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001896BC92450>, 243753.3026004)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001896BB14D60>
2025-02-07 08:51:57,826 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001896AD17610>
2025-02-07 08:51:57,827 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001896BAB2210>
2025-02-07 08:51:57,827 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001896BC94890>, 243758.9609715)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001896BC91DF0>, 243759.5554642)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001896BC91910>, 243760.3798503)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001896BC923F0>, 243761.1002144)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001896BB16EA0>
2025-02-07 08:51:57,828 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001896BAB3250>
2025-02-07 08:51:57,828 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001896BC36E10>, 243765.4358442)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001896BB17820>
2025-02-07 08:51:57,828 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001896BAB3B10>
2025-02-07 08:51:57,829 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001896BEC9A30>, 243767.7877313)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001896BB17950>
2025-02-07 08:51:57,843 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:57,846 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:57,866 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:57,869 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:57,914 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:57,917 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,041 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,045 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,135 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,151 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,198 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,201 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,233 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,242 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,267 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,377 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:51:58,381 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,385 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,393 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:51:58,394 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,396 [DEBUG] 
Evaluating link: https://example.com/doc1
2025-02-07 08:51:58,396 [DEBUG] Target domain: example.com
2025-02-07 08:51:58,396 [DEBUG] URL domain: example.com
2025-02-07 08:51:58,397 [DEBUG] URL approved for crawling: https://example.com/doc1
2025-02-07 08:51:58,397 [DEBUG] 
Evaluating link: https://example.com/excluded/page
2025-02-07 08:51:58,397 [DEBUG] Target domain: example.com
2025-02-07 08:51:58,397 [DEBUG] URL domain: example.com
2025-02-07 08:51:58,397 [DEBUG] URL approved for crawling: https://example.com/excluded/page
2025-02-07 08:51:58,422 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,426 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:51:58,428 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,458 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,466 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:51:58,467 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,472 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-07 08:51:58,549 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,559 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:51:58,566 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,618 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,626 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:51:58,627 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,631 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-07 08:51:58,697 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,699 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:51:58,700 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,709 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-07 08:51:58,797 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,801 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:51:58,808 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,810 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-07 08:51:58,849 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,851 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:51:58,854 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,858 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-07 08:51:58,898 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,900 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:51:58,901 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,908 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-07 08:51:58,950 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,956 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:51:58,957 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:58,959 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-07 08:51:58,994 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:59,001 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:59,455 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:59,457 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:59,459 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:51:59,477 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:59,479 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:59,481 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:51:59,509 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:51:59,517 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:51:59,574 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:51:59,615 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:51:59,631 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001896B92BD90>
2025-02-07 08:51:59,631 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001896BDC3110>, 243769.2065419)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001896BDC0A70>, 243769.4411021)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001896BB16780>
2025-02-07 08:51:59,659 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:51:59,695 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:51:59,732 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:51:59,780 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:51:59,827 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:51:59,858 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:51:59,889 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:52:00,516 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:01,395 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:01,398 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:52:01,399 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 08:52:01,399 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 08:52:01,399 [INFO] Starting crawl of URL: https://example.com
2025-02-07 08:52:01,754 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 08:52:01,754 [DEBUG] Base URL: https://example.com
2025-02-07 08:52:01,754 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 08:52:01,754 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 08:52:01,754 [DEBUG] Initial domain set to: example.com
2025-02-07 08:52:01,755 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 08:52:01,755 [INFO] Successfully crawled https://example.com
2025-02-07 08:52:01,755 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000001896B92B250>
2025-02-07 08:52:01,755 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000001896BF31850>, 243771.790853)])']
connector: <aiohttp.connector.TCPConnector object at 0x000001896BEDDE00>
2025-02-07 08:52:01,758 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2025-02-07 08:52:02,145 [INFO] Operation took 0.10 seconds
2025-02-07 08:52:02,687 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:52:02,687 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:52:02,688 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:10,146 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-5\test_full_site_crawl0\test_docs/index.html
2025-02-07 08:52:10,146 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-07 08:52:10,180 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:10,187 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:52:10,187 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:52:10,188 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:17,630 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-5\test_content_processing_pipeli0\test_docs/guide.html
2025-02-07 08:52:17,631 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-07 08:52:17,687 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:17,700 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:52:17,704 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:52:17,705 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:25,154 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-5\test_quality_checks0\test_docs/api.html
2025-02-07 08:52:25,154 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-07 08:52:25,189 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:25,203 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:52:25,203 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:52:25,204 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:32,732 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-5\test_document_organization0\test_docs/index.html
2025-02-07 08:52:32,733 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-07 08:52:32,761 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:32,774 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:52:32,774 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:52:32,775 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:40,240 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-5\test_search_functionality0\test_docs/index.html
2025-02-07 08:52:40,241 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-07 08:52:40,272 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:40,285 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 08:52:40,286 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:52:40,287 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:47,715 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-5\test_error_handling_and_recove0\test_docs/index.html
2025-02-07 08:52:47,715 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-07 08:52:47,742 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:47,746 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:47,765 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:47,768 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:47,770 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:52:47,771 [ERROR] Error during crawl: 'str' object has no attribute 'url'
2025-02-07 08:52:47,804 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:47,813 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:47,830 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:47,832 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:47,836 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 08:52:47,861 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:50,169 [INFO] Sanitizing relative URL: /test, base_url: https://example.com
2025-02-07 08:52:50,169 [INFO] Resolved URL using urljoin: https://example.com/test
2025-02-07 08:52:50,252 [INFO] Starting _process_content
2025-02-07 08:52:50,259 [INFO] Sanitizing relative URL: /test, base_url: https://example.com/test
2025-02-07 08:52:50,260 [INFO] Resolved URL using urljoin: https://example.com/test
2025-02-07 08:52:50,260 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:52:50,287 [INFO] Starting _process_content
2025-02-07 08:52:50,317 [INFO] Starting _process_content
2025-02-07 08:52:50,318 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:52:50,344 [INFO] Starting _process_content
2025-02-07 08:52:50,345 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:52:50,349 [INFO] Starting _process_content
2025-02-07 08:52:50,350 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:52:50,365 [INFO] Starting _process_content
2025-02-07 08:52:50,370 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 08:52:50,417 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:50,444 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:50,450 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:50,478 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:50,480 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:50,501 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:50,503 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:50,519 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:50,525 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:50,543 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:50,548 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:50,564 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:50,566 [DEBUG] Using proactor: IocpProactor
2025-02-07 08:52:50,580 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:56:48,340 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 10:56:49,962 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:56:49,966 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:56:49,970 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:56:49,976 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:56:49,980 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:56:49,983 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:56:49,985 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 10:56:49,986 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:56:49,988 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:56:49,992 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 10:56:49,996 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:56:49,998 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:56:50,004 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:56:50,006 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:56:50,012 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:56:50,015 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:56:50,019 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:56:50,021 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:56:50,028 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:56:50,030 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:56:50,032 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 10:56:50,034 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:17,459 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 10:57:19,007 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:19,009 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:19,012 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:19,017 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:19,020 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:19,023 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:19,024 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 10:57:19,026 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:19,028 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:19,035 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 10:57:19,038 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:19,040 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:19,052 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:19,055 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:19,058 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:19,077 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:19,101 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:19,105 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:19,110 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:19,112 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:19,122 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 10:57:19,127 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:43,969 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 10:57:45,866 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:45,868 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:45,871 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:45,876 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:45,881 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:45,883 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:45,885 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 10:57:45,886 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:45,889 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:45,893 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 10:57:45,896 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:45,898 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:45,903 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:45,908 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:45,915 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:45,918 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:45,928 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:45,933 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:45,944 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:45,947 [DEBUG] Using proactor: IocpProactor
2025-02-07 10:57:45,951 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 10:57:45,960 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:02:08,432 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:02:10,764 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:02:10,767 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:02:10,780 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:02:10,785 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:02:10,796 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:02:10,799 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:02:10,812 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:02:10,815 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:02:10,828 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:02:10,835 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:02:10,843 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:02:10,847 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:02:10,853 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:02:10,861 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:02:10,865 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:02:10,869 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:02:10,884 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:02:10,888 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:02:10,901 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:02:10,904 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:02:10,913 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:02:10,918 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:02:58,605 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:03:00,912 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:00,915 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:00,918 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:00,922 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:00,927 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:00,932 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:00,934 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:03:00,935 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:00,938 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:00,940 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:03:00,943 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:00,953 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:00,976 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:00,985 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:00,989 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:00,998 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:01,009 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:01,019 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:01,034 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:01,037 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:01,040 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:03:01,044 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:50,822 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:03:53,025 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:53,032 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:53,036 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:53,050 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:53,056 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:53,068 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:53,072 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:03:53,085 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:53,088 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:53,092 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:03:53,106 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:53,115 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:53,123 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:53,133 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:53,139 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:53,142 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:53,155 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:53,158 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:53,167 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:53,170 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:03:53,173 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:03:53,179 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:04:42,066 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:04:44,308 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:04:44,310 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:04:44,317 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:04:44,321 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:04:44,324 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:04:44,327 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:04:44,331 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:04:44,334 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:04:44,337 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:04:44,340 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:04:44,344 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:04:44,348 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:04:44,355 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:04:44,358 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:04:44,368 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:04:44,372 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:04:44,385 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:04:44,388 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:04:44,392 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:04:44,399 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:04:44,402 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:04:44,404 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:06:02,748 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:06:05,483 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:06:05,489 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:06:05,493 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:06:05,497 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:06:05,506 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:06:05,510 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:06:05,513 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:06:05,515 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:06:05,526 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:06:05,531 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:06:05,544 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:06:05,551 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:06:05,564 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:06:05,574 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:06:05,581 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:06:05,591 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:06:05,595 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:06:05,598 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:06:05,607 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:06:05,610 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:06:05,613 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:06:05,616 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:07:24,864 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:07:27,682 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:07:27,686 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:07:27,700 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:07:27,732 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:07:27,748 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:07:27,752 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:07:27,765 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:07:27,767 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:07:27,770 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:07:27,773 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:07:27,784 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:07:27,789 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:07:27,803 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:07:27,807 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:07:27,816 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:07:27,820 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:07:27,829 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:07:27,832 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:07:27,836 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:07:27,839 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:07:27,847 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:07:27,849 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:08:17,289 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:08:19,894 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:08:19,897 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:08:19,902 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:08:19,906 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:08:19,912 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:08:19,915 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:08:19,917 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:08:19,919 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:08:19,922 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:08:19,928 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:08:19,932 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:08:19,935 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:08:19,940 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:08:19,948 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:08:19,955 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:08:19,965 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:08:19,973 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:08:19,981 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:08:19,985 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:08:19,987 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:08:19,990 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:08:19,997 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:10:59,229 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:11:00,930 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:11:00,932 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:11:00,936 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:11:00,939 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:11:00,947 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:11:00,949 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:11:00,951 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:11:00,953 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:11:00,954 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:11:00,957 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:11:00,964 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:11:00,966 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:11:00,973 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:11:00,979 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:11:00,982 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:11:00,987 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:11:00,995 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:11:00,998 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:11:01,004 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:11:01,006 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:11:01,013 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:11:01,015 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:11:31,490 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:12:27,229 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:12:28,990 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:12:28,994 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:12:28,997 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:12:29,000 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:12:29,003 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:12:29,004 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:12:29,006 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:12:29,010 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:12:29,012 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:12:29,015 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:12:29,017 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:12:29,019 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:12:29,022 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:12:29,024 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:12:29,031 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:12:29,033 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:12:29,038 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:12:29,040 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:12:29,048 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:12:29,052 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:12:29,054 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:12:29,055 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:13:22,940 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:13:24,934 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:13:24,936 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:13:24,942 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:13:24,947 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:13:24,950 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:13:24,952 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:13:24,954 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:13:24,959 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:13:24,962 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:13:24,967 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:13:24,969 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:13:24,971 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:13:24,979 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:13:24,981 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:13:24,986 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:13:24,988 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:13:24,996 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:13:24,999 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:13:25,002 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:13:25,004 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:13:25,009 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:13:25,012 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:14:17,402 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:14:19,211 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:14:19,213 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:14:19,216 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:14:19,219 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:14:19,228 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:14:19,232 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:14:19,235 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:14:19,237 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:14:19,246 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:14:19,249 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:14:19,254 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:14:19,262 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:14:19,270 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:14:19,278 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:14:19,284 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:14:19,286 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:14:19,299 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:14:19,302 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:14:19,312 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:14:19,317 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:14:19,320 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:14:19,329 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:14:19,478 [INFO] Starting _process_content
2025-02-07 11:14:19,483 [ERROR] Exception in _process_content: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    formatted_markdown = self._format_structure_to_markdown(self.result.structure)
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 899, in _format_structure_to_markdown
    level = section.get('level', 1)
            ^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
2025-02-07 11:15:34,632 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:15:36,335 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:15:36,341 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:15:36,353 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:15:36,357 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:15:36,362 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:15:36,369 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:15:36,372 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:15:36,376 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:15:36,386 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:15:36,393 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:15:36,405 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:15:36,408 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:15:36,412 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:15:36,420 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:15:36,428 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:15:36,438 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:15:36,446 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:15:36,455 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:15:36,462 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:15:36,471 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:15:36,473 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:15:36,476 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:15:36,669 [INFO] Starting _process_content
2025-02-07 11:15:36,678 [INFO] Starting _process_content
2025-02-07 11:15:36,691 [INFO] Starting _process_content
2025-02-07 11:15:36,706 [INFO] Starting _process_content
2025-02-07 11:15:36,714 [INFO] Starting _process_content
2025-02-07 11:15:36,723 [INFO] Starting _process_content
2025-02-07 11:15:36,727 [INFO] Starting _process_content
2025-02-07 11:15:36,737 [INFO] Starting _process_content
2025-02-07 11:15:36,744 [INFO] Starting _process_content
2025-02-07 11:15:36,753 [INFO] Starting _process_content
2025-02-07 11:15:36,758 [INFO] Starting _process_content
2025-02-07 11:15:36,762 [INFO] Starting _process_content
2025-02-07 11:15:36,774 [INFO] Starting _process_content
2025-02-07 11:15:36,778 [INFO] Starting _process_content
2025-02-07 11:15:36,788 [INFO] Starting _process_content
2025-02-07 11:15:36,794 [INFO] Starting _process_content
2025-02-07 11:15:36,803 [INFO] Starting _process_content
2025-02-07 11:15:36,811 [INFO] Starting _process_content
2025-02-07 11:15:36,823 [INFO] Starting _process_content
2025-02-07 11:15:36,830 [INFO] Starting _process_content
2025-02-07 11:15:36,855 [INFO] Starting _process_content
2025-02-07 11:15:36,871 [INFO] Starting _process_content
2025-02-07 11:15:36,888 [INFO] Starting _process_content
2025-02-07 11:15:36,903 [INFO] Starting _process_content
2025-02-07 11:15:36,921 [INFO] Starting _process_content
2025-02-07 11:15:36,930 [INFO] Starting _process_content
2025-02-07 11:15:36,947 [INFO] Starting _process_content
2025-02-07 11:15:36,963 [INFO] Starting _process_content
2025-02-07 11:15:36,979 [INFO] Starting _process_content
2025-02-07 11:15:36,996 [INFO] Starting _process_content
2025-02-07 11:15:37,014 [INFO] Starting _process_content
2025-02-07 11:15:37,028 [INFO] Starting _process_content
2025-02-07 11:15:37,038 [INFO] Starting _process_content
2025-02-07 11:15:37,042 [INFO] Sanitizing relative URL: page.html, base_url: https://example.com/subdir/
2025-02-07 11:15:37,042 [INFO] Resolved URL using urljoin: https://example.com/subdir/page.html
2025-02-07 11:15:37,042 [INFO] Sanitizing relative URL: /absolute.html, base_url: https://example.com/subdir/
2025-02-07 11:15:37,042 [INFO] Resolved URL using urljoin: https://example.com/absolute.html
2025-02-07 11:16:31,272 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:16:33,001 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:16:33,004 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:16:33,008 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:16:33,015 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:16:33,020 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:16:33,023 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:16:33,026 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:16:33,028 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:16:33,036 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:16:33,042 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:16:33,044 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:16:33,050 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:16:33,057 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:16:33,059 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:16:33,068 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:16:33,073 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:16:33,077 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:16:33,082 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:16:33,089 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:16:33,091 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:16:33,093 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:16:33,099 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:16:33,219 [INFO] Starting _process_content
2025-02-07 11:16:33,228 [INFO] Starting _process_content
2025-02-07 11:16:33,253 [INFO] Starting _process_content
2025-02-07 11:16:33,291 [INFO] Starting _process_content
2025-02-07 11:16:33,308 [INFO] Starting _process_content
2025-02-07 11:16:33,318 [INFO] Starting _process_content
2025-02-07 11:16:33,324 [INFO] Starting _process_content
2025-02-07 11:16:33,337 [INFO] Starting _process_content
2025-02-07 11:16:33,351 [INFO] Starting _process_content
2025-02-07 11:16:33,360 [INFO] Starting _process_content
2025-02-07 11:16:33,373 [INFO] Starting _process_content
2025-02-07 11:16:33,377 [INFO] Starting _process_content
2025-02-07 11:16:33,394 [INFO] Starting _process_content
2025-02-07 11:16:33,407 [INFO] Starting _process_content
2025-02-07 11:16:33,415 [INFO] Starting _process_content
2025-02-07 11:16:33,423 [INFO] Starting _process_content
2025-02-07 11:16:33,427 [INFO] Starting _process_content
2025-02-07 11:16:33,443 [INFO] Starting _process_content
2025-02-07 11:16:33,455 [INFO] Starting _process_content
2025-02-07 11:16:33,460 [INFO] Starting _process_content
2025-02-07 11:16:33,486 [INFO] Starting _process_content
2025-02-07 11:16:33,507 [INFO] Starting _process_content
2025-02-07 11:16:33,518 [INFO] Starting _process_content
2025-02-07 11:16:33,524 [INFO] Starting _process_content
2025-02-07 11:16:33,541 [INFO] Starting _process_content
2025-02-07 11:16:33,551 [INFO] Starting _process_content
2025-02-07 11:16:33,671 [INFO] Starting _process_content
2025-02-07 11:16:33,708 [INFO] Starting _process_content
2025-02-07 11:16:33,718 [INFO] Starting _process_content
2025-02-07 11:16:33,733 [INFO] Starting _process_content
2025-02-07 11:16:33,752 [INFO] Starting _process_content
2025-02-07 11:16:33,768 [INFO] Starting _process_content
2025-02-07 11:17:29,846 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:17:31,460 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:17:31,463 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:17:31,467 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:17:31,472 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:17:31,483 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:17:31,489 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:17:31,497 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:17:31,499 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:17:31,506 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:17:31,515 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:17:31,517 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:17:31,519 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:17:31,523 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:17:31,533 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:17:31,537 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:17:31,539 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:17:31,550 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:17:31,552 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:17:31,555 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:17:31,562 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:17:31,566 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:17:31,570 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:17:31,804 [INFO] Starting _process_content
2025-02-07 11:17:31,818 [INFO] Starting _process_content
2025-02-07 11:17:31,830 [INFO] Starting _process_content
2025-02-07 11:17:31,846 [INFO] Starting _process_content
2025-02-07 11:17:31,853 [INFO] Starting _process_content
2025-02-07 11:17:31,862 [INFO] Starting _process_content
2025-02-07 11:17:31,870 [INFO] Starting _process_content
2025-02-07 11:17:31,881 [INFO] Starting _process_content
2025-02-07 11:17:31,887 [INFO] Starting _process_content
2025-02-07 11:17:31,897 [INFO] Starting _process_content
2025-02-07 11:17:31,905 [INFO] Starting _process_content
2025-02-07 11:17:31,917 [INFO] Starting _process_content
2025-02-07 11:17:31,930 [INFO] Starting _process_content
2025-02-07 11:17:31,938 [INFO] Starting _process_content
2025-02-07 11:17:31,948 [INFO] Starting _process_content
2025-02-07 11:17:31,955 [INFO] Starting _process_content
2025-02-07 11:17:31,967 [INFO] Starting _process_content
2025-02-07 11:17:31,976 [INFO] Starting _process_content
2025-02-07 11:17:31,985 [INFO] Starting _process_content
2025-02-07 11:17:31,989 [INFO] Starting _process_content
2025-02-07 11:17:32,000 [INFO] Starting _process_content
2025-02-07 11:17:32,006 [INFO] Starting _process_content
2025-02-07 11:17:32,017 [INFO] Starting _process_content
2025-02-07 11:17:32,029 [INFO] Starting _process_content
2025-02-07 11:17:32,038 [INFO] Starting _process_content
2025-02-07 11:17:32,053 [INFO] Starting _process_content
2025-02-07 11:17:32,103 [INFO] Starting _process_content
2025-02-07 11:17:32,128 [INFO] Starting _process_content
2025-02-07 11:17:32,143 [INFO] Starting _process_content
2025-02-07 11:17:32,165 [INFO] Starting _process_content
2025-02-07 11:17:32,180 [INFO] Starting _process_content
2025-02-07 11:17:32,197 [INFO] Starting _process_content
2025-02-07 11:18:29,509 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:18:31,270 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:18:31,273 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:18:31,276 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:18:31,279 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:18:31,281 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:18:31,283 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:18:31,286 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:18:31,290 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:18:31,292 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:18:31,294 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:18:31,297 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:18:31,299 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:18:31,305 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:18:31,312 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:18:31,328 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:18:31,330 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:18:31,345 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:18:31,353 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:18:31,362 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:18:31,369 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:18:31,375 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:18:31,377 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:18:31,480 [INFO] Starting _process_content
2025-02-07 11:18:31,492 [INFO] Starting _process_content
2025-02-07 11:18:31,502 [INFO] Starting _process_content
2025-02-07 11:18:31,519 [INFO] Starting _process_content
2025-02-07 11:18:31,532 [INFO] Starting _process_content
2025-02-07 11:18:31,545 [INFO] Starting _process_content
2025-02-07 11:18:31,552 [INFO] Starting _process_content
2025-02-07 11:18:31,569 [INFO] Starting _process_content
2025-02-07 11:18:31,592 [INFO] Starting _process_content
2025-02-07 11:18:31,600 [INFO] Starting _process_content
2025-02-07 11:18:31,610 [INFO] Starting _process_content
2025-02-07 11:18:31,616 [INFO] Starting _process_content
2025-02-07 11:18:31,633 [INFO] Starting _process_content
2025-02-07 11:18:31,643 [INFO] Starting _process_content
2025-02-07 11:18:31,647 [INFO] Starting _process_content
2025-02-07 11:18:31,653 [INFO] Starting _process_content
2025-02-07 11:18:31,660 [INFO] Starting _process_content
2025-02-07 11:18:31,667 [INFO] Starting _process_content
2025-02-07 11:18:31,677 [INFO] Starting _process_content
2025-02-07 11:18:31,684 [INFO] Starting _process_content
2025-02-07 11:18:31,693 [INFO] Starting _process_content
2025-02-07 11:18:31,699 [INFO] Starting _process_content
2025-02-07 11:18:31,708 [INFO] Starting _process_content
2025-02-07 11:18:31,716 [INFO] Starting _process_content
2025-02-07 11:18:31,726 [INFO] Starting _process_content
2025-02-07 11:18:31,731 [INFO] Starting _process_content
2025-02-07 11:18:31,741 [INFO] Starting _process_content
2025-02-07 11:18:31,750 [INFO] Starting _process_content
2025-02-07 11:18:31,760 [INFO] Starting _process_content
2025-02-07 11:18:31,767 [INFO] Starting _process_content
2025-02-07 11:18:31,776 [INFO] Starting _process_content
2025-02-07 11:18:31,782 [INFO] Starting _process_content
2025-02-07 11:19:35,513 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:19:37,218 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:19:37,220 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:19:37,228 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:19:37,231 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:19:37,235 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:19:37,237 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:19:37,248 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:19:37,253 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:19:37,261 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:19:37,264 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:19:37,266 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:19:37,270 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:19:37,280 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:19:37,283 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:19:37,287 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:19:37,301 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:19:37,305 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:19:37,313 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:19:37,317 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:19:37,320 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:19:37,330 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:19:37,335 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:19:37,416 [INFO] Starting _process_content
2025-02-07 11:19:37,420 [INFO] Starting _process_content
2025-02-07 11:19:37,433 [INFO] Starting _process_content
2025-02-07 11:19:37,442 [INFO] Starting _process_content
2025-02-07 11:19:37,450 [INFO] Starting _process_content
2025-02-07 11:19:37,454 [INFO] Starting _process_content
2025-02-07 11:19:37,463 [INFO] Starting _process_content
2025-02-07 11:19:37,470 [INFO] Starting _process_content
2025-02-07 11:19:37,481 [INFO] Starting _process_content
2025-02-07 11:19:37,485 [INFO] Starting _process_content
2025-02-07 11:19:37,491 [INFO] Starting _process_content
2025-02-07 11:19:37,497 [INFO] Starting _process_content
2025-02-07 11:19:37,502 [INFO] Starting _process_content
2025-02-07 11:19:37,507 [INFO] Starting _process_content
2025-02-07 11:19:37,515 [INFO] Starting _process_content
2025-02-07 11:19:37,518 [INFO] Starting _process_content
2025-02-07 11:19:37,521 [INFO] Starting _process_content
2025-02-07 11:19:37,530 [INFO] Starting _process_content
2025-02-07 11:19:37,534 [INFO] Starting _process_content
2025-02-07 11:19:37,541 [INFO] Starting _process_content
2025-02-07 11:19:37,551 [INFO] Starting _process_content
2025-02-07 11:19:37,561 [INFO] Starting _process_content
2025-02-07 11:19:37,570 [INFO] Starting _process_content
2025-02-07 11:19:37,584 [INFO] Starting _process_content
2025-02-07 11:19:37,599 [INFO] Starting _process_content
2025-02-07 11:19:37,613 [INFO] Starting _process_content
2025-02-07 11:19:37,634 [INFO] Starting _process_content
2025-02-07 11:19:37,647 [INFO] Starting _process_content
2025-02-07 11:19:37,653 [INFO] Starting _process_content
2025-02-07 11:19:37,669 [INFO] Starting _process_content
2025-02-07 11:19:37,684 [INFO] Starting _process_content
2025-02-07 11:19:37,697 [INFO] Starting _process_content
2025-02-07 11:20:37,359 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:20:39,362 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:20:39,364 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:20:39,370 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:20:39,374 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:20:39,379 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:20:39,382 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:20:39,388 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:20:39,389 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:20:39,391 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:20:39,396 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:20:39,399 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:20:39,404 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:20:39,410 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:20:39,412 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:20:39,416 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:20:39,421 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:20:39,427 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:20:39,429 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:20:39,433 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:20:39,438 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:20:39,440 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:20:39,444 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:20:39,528 [INFO] Starting _process_content
2025-02-07 11:20:39,536 [INFO] Starting _process_content
2025-02-07 11:20:39,545 [INFO] Starting _process_content
2025-02-07 11:20:39,555 [INFO] Starting _process_content
2025-02-07 11:20:39,562 [INFO] Starting _process_content
2025-02-07 11:20:39,570 [INFO] Starting _process_content
2025-02-07 11:20:39,577 [INFO] Starting _process_content
2025-02-07 11:20:39,591 [INFO] Starting _process_content
2025-02-07 11:20:39,606 [INFO] Starting _process_content
2025-02-07 11:20:39,623 [INFO] Starting _process_content
2025-02-07 11:20:39,629 [INFO] Starting _process_content
2025-02-07 11:20:39,640 [INFO] Starting _process_content
2025-02-07 11:20:39,654 [INFO] Starting _process_content
2025-02-07 11:20:39,661 [INFO] Starting _process_content
2025-02-07 11:20:39,673 [INFO] Starting _process_content
2025-02-07 11:20:39,683 [INFO] Starting _process_content
2025-02-07 11:20:39,693 [INFO] Starting _process_content
2025-02-07 11:20:39,707 [INFO] Starting _process_content
2025-02-07 11:20:39,723 [INFO] Starting _process_content
2025-02-07 11:20:39,741 [INFO] Starting _process_content
2025-02-07 11:20:39,755 [INFO] Starting _process_content
2025-02-07 11:20:39,767 [INFO] Starting _process_content
2025-02-07 11:20:39,782 [INFO] Starting _process_content
2025-02-07 11:20:39,794 [INFO] Starting _process_content
2025-02-07 11:20:39,809 [INFO] Starting _process_content
2025-02-07 11:20:39,829 [INFO] Starting _process_content
2025-02-07 11:20:39,844 [INFO] Starting _process_content
2025-02-07 11:20:39,866 [INFO] Starting _process_content
2025-02-07 11:20:39,889 [INFO] Starting _process_content
2025-02-07 11:20:39,895 [INFO] Starting _process_content
2025-02-07 11:20:39,907 [INFO] Starting _process_content
2025-02-07 11:20:39,912 [INFO] Starting _process_content
2025-02-07 11:21:33,003 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:21:34,867 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:21:34,869 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:21:34,872 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:21:34,875 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:21:34,877 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:21:34,882 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:21:34,884 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:21:34,886 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:21:34,888 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:21:34,890 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:21:34,892 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:21:34,894 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:21:34,902 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:21:34,904 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:21:34,910 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:21:34,915 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:21:34,920 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:21:34,925 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:21:34,927 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:21:34,934 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:21:34,938 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:21:34,940 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:21:35,018 [INFO] Starting _process_content
2025-02-07 11:21:35,025 [INFO] Starting _process_content
2025-02-07 11:21:35,038 [INFO] Starting _process_content
2025-02-07 11:21:35,044 [INFO] Starting _process_content
2025-02-07 11:21:35,054 [INFO] Starting _process_content
2025-02-07 11:21:35,065 [INFO] Starting _process_content
2025-02-07 11:21:35,074 [INFO] Starting _process_content
2025-02-07 11:21:35,082 [INFO] Starting _process_content
2025-02-07 11:21:35,087 [INFO] Starting _process_content
2025-02-07 11:21:35,091 [INFO] Starting _process_content
2025-02-07 11:21:35,097 [INFO] Starting _process_content
2025-02-07 11:21:35,108 [INFO] Starting _process_content
2025-02-07 11:21:35,125 [INFO] Starting _process_content
2025-02-07 11:21:35,142 [INFO] Starting _process_content
2025-02-07 11:21:35,157 [INFO] Starting _process_content
2025-02-07 11:21:35,168 [INFO] Starting _process_content
2025-02-07 11:21:35,172 [INFO] Starting _process_content
2025-02-07 11:21:35,177 [INFO] Starting _process_content
2025-02-07 11:21:35,192 [INFO] Starting _process_content
2025-02-07 11:21:35,203 [INFO] Starting _process_content
2025-02-07 11:21:35,211 [INFO] Starting _process_content
2025-02-07 11:21:35,224 [INFO] Starting _process_content
2025-02-07 11:21:35,234 [INFO] Starting _process_content
2025-02-07 11:21:35,250 [INFO] Starting _process_content
2025-02-07 11:21:35,261 [INFO] Starting _process_content
2025-02-07 11:21:35,274 [INFO] Starting _process_content
2025-02-07 11:21:35,285 [INFO] Starting _process_content
2025-02-07 11:21:35,294 [INFO] Starting _process_content
2025-02-07 11:21:35,311 [INFO] Starting _process_content
2025-02-07 11:21:35,333 [INFO] Starting _process_content
2025-02-07 11:21:35,342 [INFO] Starting _process_content
2025-02-07 11:21:35,360 [INFO] Starting _process_content
2025-02-07 11:22:42,864 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:22:45,124 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:22:45,128 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:22:45,148 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:22:45,156 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:22:45,165 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:22:45,172 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:22:45,175 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:22:45,180 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:22:45,189 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:22:45,196 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:22:45,200 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:22:45,204 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:22:45,210 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:22:45,214 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:22:45,218 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:22:45,222 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:22:45,227 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:22:45,231 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:22:45,237 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:22:45,240 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:22:45,241 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:22:45,246 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:22:45,347 [INFO] Starting _process_content
2025-02-07 11:22:45,355 [INFO] Starting _process_content
2025-02-07 11:22:45,368 [INFO] Starting _process_content
2025-02-07 11:22:45,385 [INFO] Starting _process_content
2025-02-07 11:22:45,400 [INFO] Starting _process_content
2025-02-07 11:22:45,410 [INFO] Starting _process_content
2025-02-07 11:22:45,421 [INFO] Starting _process_content
2025-02-07 11:22:45,426 [INFO] Starting _process_content
2025-02-07 11:22:45,434 [INFO] Starting _process_content
2025-02-07 11:22:45,442 [INFO] Starting _process_content
2025-02-07 11:22:45,447 [INFO] Starting _process_content
2025-02-07 11:22:45,461 [INFO] Starting _process_content
2025-02-07 11:22:45,481 [INFO] Starting _process_content
2025-02-07 11:22:45,497 [INFO] Starting _process_content
2025-02-07 11:22:45,515 [INFO] Starting _process_content
2025-02-07 11:22:45,526 [INFO] Starting _process_content
2025-02-07 11:22:45,532 [INFO] Starting _process_content
2025-02-07 11:22:45,553 [INFO] Starting _process_content
2025-02-07 11:22:45,570 [INFO] Starting _process_content
2025-02-07 11:22:45,588 [INFO] Starting _process_content
2025-02-07 11:22:45,592 [INFO] Starting _process_content
2025-02-07 11:22:45,604 [INFO] Starting _process_content
2025-02-07 11:22:45,613 [INFO] Starting _process_content
2025-02-07 11:22:45,632 [INFO] Starting _process_content
2025-02-07 11:22:45,648 [INFO] Starting _process_content
2025-02-07 11:22:45,660 [INFO] Starting _process_content
2025-02-07 11:22:45,672 [INFO] Starting _process_content
2025-02-07 11:22:45,686 [INFO] Starting _process_content
2025-02-07 11:22:45,700 [INFO] Starting _process_content
2025-02-07 11:22:45,742 [INFO] Starting _process_content
2025-02-07 11:22:45,756 [INFO] Starting _process_content
2025-02-07 11:22:45,773 [INFO] Starting _process_content
2025-02-07 11:23:44,188 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:23:46,500 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:23:46,504 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:23:46,512 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:23:46,519 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:23:46,529 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:23:46,532 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:23:46,543 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:23:46,547 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:23:46,553 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:23:46,561 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:23:46,569 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:23:46,576 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:23:46,583 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:23:46,587 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:23:46,600 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:23:46,603 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:23:46,618 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:23:46,620 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:23:46,651 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:23:46,659 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:23:46,662 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:23:46,665 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:23:46,800 [INFO] Starting _process_content
2025-02-07 11:23:46,809 [INFO] Starting _process_content
2025-02-07 11:23:46,818 [INFO] Starting _process_content
2025-02-07 11:23:46,832 [INFO] Starting _process_content
2025-02-07 11:23:46,841 [INFO] Starting _process_content
2025-02-07 11:23:46,849 [INFO] Starting _process_content
2025-02-07 11:23:46,862 [INFO] Starting _process_content
2025-02-07 11:23:46,868 [INFO] Starting _process_content
2025-02-07 11:23:46,883 [INFO] Starting _process_content
2025-02-07 11:23:46,894 [INFO] Starting _process_content
2025-02-07 11:23:46,903 [INFO] Starting _process_content
2025-02-07 11:23:46,917 [INFO] Starting _process_content
2025-02-07 11:23:46,927 [INFO] Starting _process_content
2025-02-07 11:23:46,932 [INFO] Starting _process_content
2025-02-07 11:23:46,935 [INFO] Starting _process_content
2025-02-07 11:23:46,951 [INFO] Starting _process_content
2025-02-07 11:23:46,967 [INFO] Starting _process_content
2025-02-07 11:23:46,981 [INFO] Starting _process_content
2025-02-07 11:23:47,001 [INFO] Starting _process_content
2025-02-07 11:23:47,018 [INFO] Starting _process_content
2025-02-07 11:23:47,035 [INFO] Starting _process_content
2025-02-07 11:23:47,052 [INFO] Starting _process_content
2025-02-07 11:23:47,076 [INFO] Starting _process_content
2025-02-07 11:23:47,085 [INFO] Starting _process_content
2025-02-07 11:23:47,101 [INFO] Starting _process_content
2025-02-07 11:23:47,120 [INFO] Starting _process_content
2025-02-07 11:23:47,137 [INFO] Starting _process_content
2025-02-07 11:23:47,153 [INFO] Starting _process_content
2025-02-07 11:23:47,169 [INFO] Starting _process_content
2025-02-07 11:23:47,184 [INFO] Starting _process_content
2025-02-07 11:23:47,210 [INFO] Starting _process_content
2025-02-07 11:23:47,225 [INFO] Starting _process_content
2025-02-07 11:25:10,842 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:25:13,812 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:25:13,815 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:25:13,818 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:25:13,821 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:25:13,831 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:25:13,835 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:25:13,840 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:25:13,849 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:25:13,855 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:25:13,867 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:25:13,874 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:25:13,878 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:25:13,887 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:25:13,894 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:25:13,902 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:25:13,907 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:25:13,920 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:25:13,922 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:25:13,934 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:25:13,942 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:25:13,965 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:25:13,968 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:25:14,158 [INFO] Starting _process_content
2025-02-07 11:25:14,169 [INFO] Starting _process_content
2025-02-07 11:25:14,182 [INFO] Starting _process_content
2025-02-07 11:25:14,188 [INFO] Starting _process_content
2025-02-07 11:25:14,199 [INFO] Starting _process_content
2025-02-07 11:25:14,208 [INFO] Starting _process_content
2025-02-07 11:25:14,218 [INFO] Starting _process_content
2025-02-07 11:25:14,232 [INFO] Starting _process_content
2025-02-07 11:25:14,238 [INFO] Starting _process_content
2025-02-07 11:25:14,250 [INFO] Starting _process_content
2025-02-07 11:25:14,260 [INFO] Starting _process_content
2025-02-07 11:25:14,269 [INFO] Starting _process_content
2025-02-07 11:25:14,275 [INFO] Starting _process_content
2025-02-07 11:25:14,282 [INFO] Starting _process_content
2025-02-07 11:25:14,286 [INFO] Starting _process_content
2025-02-07 11:25:14,292 [INFO] Starting _process_content
2025-02-07 11:25:14,300 [INFO] Starting _process_content
2025-02-07 11:25:14,314 [INFO] Starting _process_content
2025-02-07 11:25:14,325 [INFO] Starting _process_content
2025-02-07 11:25:14,334 [INFO] Starting _process_content
2025-02-07 11:25:14,348 [INFO] Starting _process_content
2025-02-07 11:25:14,353 [INFO] Starting _process_content
2025-02-07 11:25:14,361 [INFO] Starting _process_content
2025-02-07 11:25:14,368 [INFO] Starting _process_content
2025-02-07 11:25:14,375 [INFO] Starting _process_content
2025-02-07 11:25:14,408 [INFO] Starting _process_content
2025-02-07 11:25:14,424 [INFO] Starting _process_content
2025-02-07 11:25:14,444 [INFO] Starting _process_content
2025-02-07 11:25:14,468 [INFO] Starting _process_content
2025-02-07 11:25:14,485 [INFO] Starting _process_content
2025-02-07 11:25:14,501 [INFO] Starting _process_content
2025-02-07 11:25:14,517 [INFO] Starting _process_content
2025-02-07 11:32:51,288 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:32:53,095 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:32:53,097 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:32:53,100 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:32:53,103 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:32:53,107 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:32:53,114 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:32:53,117 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:32:53,121 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:32:53,128 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:32:53,132 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:32:53,136 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:32:53,140 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:32:53,149 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:32:53,152 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:32:53,155 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:32:53,157 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:32:53,165 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:32:53,170 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:32:53,173 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:32:53,181 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:32:53,186 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:32:53,189 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:32:53,316 [INFO] Starting _process_content
2025-02-07 11:32:53,317 [ERROR] Exception in _process_content: name 'local_base_url' is not defined
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 858, in _process_content
    self._extract_assets(soup, self.result, local_base_url)
                                            ^^^^^^^^^^^^^^
NameError: name 'local_base_url' is not defined
2025-02-07 11:35:11,064 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:35:12,999 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:35:13,002 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:35:13,005 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:35:13,011 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:35:13,017 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:35:13,019 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:35:13,024 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:35:13,025 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:35:13,031 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:35:13,033 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:35:13,095 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:35:13,098 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:35:13,173 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:35:13,186 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:35:13,218 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:35:13,220 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:35:13,223 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:35:13,225 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:35:13,232 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:35:13,235 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:35:13,237 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:35:13,239 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:35:13,392 [INFO] Starting _process_content
2025-02-07 11:35:13,407 [INFO] Starting _process_content
2025-02-07 11:35:13,422 [INFO] Starting _process_content
2025-02-07 11:35:13,437 [INFO] Starting _process_content
2025-02-07 11:35:13,452 [INFO] Starting _process_content
2025-02-07 11:35:13,469 [INFO] Starting _process_content
2025-02-07 11:35:13,482 [INFO] Starting _process_content
2025-02-07 11:35:13,501 [INFO] Starting _process_content
2025-02-07 11:35:13,523 [INFO] Starting _process_content
2025-02-07 11:35:13,585 [INFO] Starting _process_content
2025-02-07 11:35:13,657 [INFO] Starting _process_content
2025-02-07 11:35:13,710 [INFO] Starting _process_content
2025-02-07 11:35:13,720 [INFO] Starting _process_content
2025-02-07 11:35:13,736 [INFO] Starting _process_content
2025-02-07 11:35:13,747 [INFO] Starting _process_content
2025-02-07 11:35:13,757 [INFO] Starting _process_content
2025-02-07 11:35:13,767 [INFO] Starting _process_content
2025-02-07 11:35:13,791 [INFO] Starting _process_content
2025-02-07 11:35:13,829 [INFO] Starting _process_content
2025-02-07 11:35:13,874 [INFO] Starting _process_content
2025-02-07 11:35:13,906 [INFO] Starting _process_content
2025-02-07 11:35:13,934 [INFO] Starting _process_content
2025-02-07 11:35:13,944 [INFO] Starting _process_content
2025-02-07 11:35:13,954 [INFO] Starting _process_content
2025-02-07 11:35:13,967 [INFO] Starting _process_content
2025-02-07 11:35:13,979 [INFO] Starting _process_content
2025-02-07 11:35:13,993 [INFO] Starting _process_content
2025-02-07 11:35:14,004 [INFO] Starting _process_content
2025-02-07 11:35:14,019 [INFO] Starting _process_content
2025-02-07 11:35:14,030 [INFO] Starting _process_content
2025-02-07 11:35:14,039 [INFO] Starting _process_content
2025-02-07 11:35:14,048 [INFO] Starting _process_content
2025-02-07 11:37:37,680 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:37:39,417 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:37:39,419 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:37:39,426 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:37:39,429 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:37:39,432 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:37:39,435 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:37:39,437 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:37:39,439 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:37:39,443 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:37:39,447 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:37:39,450 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:37:39,453 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:37:39,493 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:37:39,497 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:37:39,511 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:37:39,514 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:37:39,521 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:37:39,528 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:37:39,581 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:37:39,585 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:37:39,587 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:37:39,601 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:37:39,750 [INFO] Starting _process_content
2025-02-07 11:37:39,762 [INFO] Starting _process_content
2025-02-07 11:37:39,770 [INFO] Starting _process_content
2025-02-07 11:37:39,777 [ERROR] Exception in _process_content: name 'href' is not defined
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 864, in _process_content
    self._process_links(body, base_url_for_links)
    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 522, in _process_links
    if not href:
           ^^^^
NameError: name 'href' is not defined
2025-02-07 11:39:32,908 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:39:34,803 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:39:34,807 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:39:34,810 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:39:34,813 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:39:34,819 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:39:34,822 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:39:34,824 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:39:34,826 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:39:34,828 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:39:34,831 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:39:34,836 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:39:34,841 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:39:34,854 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:39:34,857 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:39:34,862 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:39:34,871 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:39:34,874 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:39:34,876 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:39:34,879 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:39:34,883 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:39:34,894 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:39:34,897 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:39:35,010 [INFO] Starting _process_content
2025-02-07 11:39:35,018 [INFO] Starting _process_content
2025-02-07 11:39:35,025 [INFO] Starting _process_content
2025-02-07 11:39:35,028 [ERROR] Exception in _process_content: name 'href' is not defined
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 857, in _process_content
    self._process_links(body, base_url)
    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 522, in _process_links
    if not href:
           ^^^^
NameError: name 'href' is not defined
2025-02-07 11:40:54,852 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:40:56,783 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:40:56,791 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:40:56,797 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:40:56,812 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:40:56,827 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:40:56,831 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:40:56,842 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:40:56,844 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:40:56,847 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:40:56,851 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:40:56,858 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:40:56,862 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:40:56,868 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:40:56,876 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:40:56,881 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:40:56,884 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:40:56,893 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:40:56,898 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:40:56,907 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:40:56,910 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:40:56,914 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:40:56,916 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:40:57,025 [INFO] Starting _process_content
2025-02-07 11:40:57,033 [INFO] Starting _process_content
2025-02-07 11:40:57,044 [INFO] Starting _process_content
2025-02-07 11:40:57,057 [INFO] Starting _process_content
2025-02-07 11:40:57,067 [INFO] Starting _process_content
2025-02-07 11:40:57,093 [INFO] Starting _process_content
2025-02-07 11:40:57,135 [INFO] Starting _process_content
2025-02-07 11:40:57,152 [INFO] Starting _process_content
2025-02-07 11:40:57,164 [INFO] Starting _process_content
2025-02-07 11:40:57,176 [INFO] Starting _process_content
2025-02-07 11:40:57,192 [INFO] Starting _process_content
2025-02-07 11:40:57,207 [INFO] Starting _process_content
2025-02-07 11:40:57,214 [INFO] Starting _process_content
2025-02-07 11:40:57,226 [INFO] Starting _process_content
2025-02-07 11:40:57,245 [INFO] Starting _process_content
2025-02-07 11:40:57,262 [INFO] Starting _process_content
2025-02-07 11:40:57,276 [INFO] Starting _process_content
2025-02-07 11:40:57,283 [INFO] Starting _process_content
2025-02-07 11:40:57,297 [INFO] Starting _process_content
2025-02-07 11:40:57,315 [INFO] Starting _process_content
2025-02-07 11:40:57,341 [INFO] Starting _process_content
2025-02-07 11:40:57,349 [INFO] Starting _process_content
2025-02-07 11:40:57,366 [INFO] Starting _process_content
2025-02-07 11:40:57,393 [INFO] Starting _process_content
2025-02-07 11:40:57,411 [INFO] Starting _process_content
2025-02-07 11:40:57,435 [INFO] Starting _process_content
2025-02-07 11:40:57,445 [INFO] Starting _process_content
2025-02-07 11:40:57,451 [INFO] Starting _process_content
2025-02-07 11:40:57,463 [INFO] Starting _process_content
2025-02-07 11:40:57,468 [INFO] Starting _process_content
2025-02-07 11:40:57,481 [INFO] Starting _process_content
2025-02-07 11:40:57,490 [INFO] Starting _process_content
2025-02-07 11:43:47,515 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:43:49,471 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:43:49,474 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:43:49,481 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:43:49,486 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:43:49,500 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:43:49,505 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:43:49,513 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:43:49,516 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:43:49,519 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:43:49,533 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:43:49,537 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:43:49,540 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:43:49,556 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:43:49,566 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:43:49,572 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:43:49,582 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:43:49,588 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:43:49,595 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:43:49,601 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:43:49,604 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:43:49,611 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:43:49,615 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:43:49,704 [INFO] Starting _process_content
2025-02-07 11:43:49,713 [INFO] Starting _process_content
2025-02-07 11:43:49,722 [INFO] Starting _process_content
2025-02-07 11:43:49,732 [INFO] Starting _process_content
2025-02-07 11:43:49,739 [INFO] Starting _process_content
2025-02-07 11:43:49,749 [INFO] Starting _process_content
2025-02-07 11:43:49,755 [INFO] Starting _process_content
2025-02-07 11:43:49,765 [INFO] Starting _process_content
2025-02-07 11:43:49,772 [INFO] Starting _process_content
2025-02-07 11:43:49,781 [INFO] Starting _process_content
2025-02-07 11:43:49,789 [INFO] Starting _process_content
2025-02-07 11:43:49,803 [INFO] Starting _process_content
2025-02-07 11:43:49,853 [INFO] Starting _process_content
2025-02-07 11:43:49,868 [INFO] Starting _process_content
2025-02-07 11:43:49,873 [INFO] Starting _process_content
2025-02-07 11:43:49,886 [INFO] Starting _process_content
2025-02-07 11:43:49,897 [INFO] Starting _process_content
2025-02-07 11:43:49,903 [INFO] Starting _process_content
2025-02-07 11:43:49,912 [INFO] Starting _process_content
2025-02-07 11:43:49,920 [INFO] Starting _process_content
2025-02-07 11:43:49,932 [INFO] Starting _process_content
2025-02-07 11:43:49,956 [INFO] Starting _process_content
2025-02-07 11:43:49,988 [INFO] Starting _process_content
2025-02-07 11:43:50,003 [INFO] Starting _process_content
2025-02-07 11:43:50,022 [INFO] Starting _process_content
2025-02-07 11:43:50,047 [INFO] Starting _process_content
2025-02-07 11:43:50,065 [INFO] Starting _process_content
2025-02-07 11:43:50,113 [INFO] Starting _process_content
2025-02-07 11:43:50,129 [INFO] Starting _process_content
2025-02-07 11:43:50,168 [INFO] Starting _process_content
2025-02-07 11:43:50,188 [INFO] Starting _process_content
2025-02-07 11:43:50,206 [INFO] Starting _process_content
2025-02-07 11:45:02,194 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:45:04,015 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:45:04,016 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:45:04,019 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:45:04,023 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:45:04,028 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:45:04,031 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:45:04,034 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:45:04,038 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:45:04,041 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:45:04,045 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:45:04,047 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:45:04,052 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:45:04,057 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:45:04,061 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:45:04,064 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:45:04,067 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:45:04,070 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:45:04,075 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:45:04,079 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:45:04,083 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:45:04,085 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:45:04,087 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:45:04,205 [INFO] Starting _process_content
2025-02-07 11:45:04,218 [INFO] Starting _process_content
2025-02-07 11:45:04,229 [INFO] Starting _process_content
2025-02-07 11:45:04,239 [INFO] Starting _process_content
2025-02-07 11:45:04,246 [INFO] Starting _process_content
2025-02-07 11:45:04,254 [INFO] Starting _process_content
2025-02-07 11:45:04,264 [INFO] Starting _process_content
2025-02-07 11:45:04,274 [INFO] Starting _process_content
2025-02-07 11:45:04,286 [INFO] Starting _process_content
2025-02-07 11:45:04,297 [INFO] Starting _process_content
2025-02-07 11:45:04,304 [INFO] Starting _process_content
2025-02-07 11:45:04,314 [INFO] Starting _process_content
2025-02-07 11:45:04,329 [INFO] Starting _process_content
2025-02-07 11:45:04,336 [INFO] Starting _process_content
2025-02-07 11:45:04,345 [INFO] Starting _process_content
2025-02-07 11:45:04,348 [INFO] Starting _process_content
2025-02-07 11:45:04,360 [INFO] Starting _process_content
2025-02-07 11:45:04,381 [INFO] Starting _process_content
2025-02-07 11:45:04,386 [INFO] Starting _process_content
2025-02-07 11:45:04,406 [INFO] Starting _process_content
2025-02-07 11:45:04,415 [INFO] Starting _process_content
2025-02-07 11:45:04,424 [INFO] Starting _process_content
2025-02-07 11:45:04,436 [INFO] Starting _process_content
2025-02-07 11:45:04,447 [INFO] Starting _process_content
2025-02-07 11:45:04,453 [INFO] Starting _process_content
2025-02-07 11:45:04,463 [INFO] Starting _process_content
2025-02-07 11:45:04,470 [INFO] Starting _process_content
2025-02-07 11:45:04,479 [INFO] Starting _process_content
2025-02-07 11:45:04,487 [INFO] Starting _process_content
2025-02-07 11:45:04,496 [INFO] Starting _process_content
2025-02-07 11:45:04,594 [INFO] Starting _process_content
2025-02-07 11:45:04,618 [INFO] Starting _process_content
2025-02-07 11:46:05,296 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:46:07,320 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:46:07,323 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:46:07,327 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:46:07,353 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:46:07,357 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:46:07,369 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:46:07,371 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:46:07,372 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:46:07,375 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:46:07,381 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:46:07,384 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:46:07,388 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:46:07,393 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:46:07,397 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:46:07,402 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:46:07,405 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:46:07,410 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:46:07,417 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:46:07,422 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:46:07,424 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:46:07,427 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:46:07,435 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:46:07,569 [INFO] Starting _process_content
2025-02-07 11:46:07,585 [INFO] Starting _process_content
2025-02-07 11:46:07,600 [INFO] Starting _process_content
2025-02-07 11:46:07,609 [INFO] Starting _process_content
2025-02-07 11:46:07,623 [INFO] Starting _process_content
2025-02-07 11:46:07,637 [INFO] Starting _process_content
2025-02-07 11:46:07,658 [INFO] Starting _process_content
2025-02-07 11:46:07,691 [INFO] Starting _process_content
2025-02-07 11:46:07,706 [INFO] Starting _process_content
2025-02-07 11:46:07,717 [INFO] Starting _process_content
2025-02-07 11:46:07,726 [INFO] Starting _process_content
2025-02-07 11:46:07,741 [INFO] Starting _process_content
2025-02-07 11:46:07,757 [INFO] Starting _process_content
2025-02-07 11:46:07,769 [INFO] Starting _process_content
2025-02-07 11:46:07,784 [INFO] Starting _process_content
2025-02-07 11:46:07,792 [INFO] Starting _process_content
2025-02-07 11:46:07,803 [INFO] Starting _process_content
2025-02-07 11:46:07,809 [INFO] Starting _process_content
2025-02-07 11:46:07,825 [INFO] Starting _process_content
2025-02-07 11:46:07,838 [INFO] Starting _process_content
2025-02-07 11:46:07,853 [INFO] Starting _process_content
2025-02-07 11:46:07,859 [INFO] Starting _process_content
2025-02-07 11:46:07,890 [INFO] Starting _process_content
2025-02-07 11:46:07,902 [INFO] Starting _process_content
2025-02-07 11:46:07,909 [INFO] Starting _process_content
2025-02-07 11:46:07,924 [INFO] Starting _process_content
2025-02-07 11:46:07,938 [INFO] Starting _process_content
2025-02-07 11:46:07,958 [INFO] Starting _process_content
2025-02-07 11:46:07,974 [INFO] Starting _process_content
2025-02-07 11:46:07,988 [INFO] Starting _process_content
2025-02-07 11:46:07,992 [INFO] Starting _process_content
2025-02-07 11:46:08,001 [INFO] Starting _process_content
2025-02-07 11:46:08,007 [INFO] Starting _process_content
2025-02-07 11:46:08,008 [INFO] Sanitizing relative URL: page.html, base_url: https://example.com/subdir/
2025-02-07 11:46:08,009 [INFO] Resolved URL using urljoin: https://example.com/subdir/page.html
2025-02-07 11:46:08,009 [INFO] Sanitizing relative URL: /absolute.html, base_url: https://example.com/subdir/
2025-02-07 11:46:08,009 [INFO] Resolved URL using urljoin: https://example.com/absolute.html
2025-02-07 11:46:08,016 [INFO] Starting _process_content
2025-02-07 11:46:08,021 [INFO] Starting _process_content
2025-02-07 11:46:08,022 [INFO] Sanitizing relative URL: #section2, base_url: https://example.com/page
2025-02-07 11:46:08,022 [INFO] Resolved URL using urljoin: https://example.com/page#section2
2025-02-07 11:46:08,026 [INFO] Starting _process_content
2025-02-07 11:48:12,805 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:48:14,716 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:48:14,718 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:48:14,721 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:48:14,724 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:48:14,730 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:48:14,732 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:48:14,735 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:48:14,738 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:48:14,741 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:48:14,747 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:48:14,749 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:48:14,753 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:48:14,757 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:48:14,763 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:48:14,768 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:48:14,770 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:48:14,774 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:48:14,780 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:48:14,786 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:48:14,789 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:48:14,796 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:48:14,799 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:48:14,903 [INFO] Starting _process_content
2025-02-07 11:48:14,908 [INFO] Starting _process_content
2025-02-07 11:48:14,918 [INFO] Starting _process_content
2025-02-07 11:48:14,924 [INFO] Starting _process_content
2025-02-07 11:48:14,935 [INFO] Starting _process_content
2025-02-07 11:48:14,938 [INFO] Starting _process_content
2025-02-07 11:48:14,948 [INFO] Starting _process_content
2025-02-07 11:48:14,971 [INFO] Starting _process_content
2025-02-07 11:48:14,983 [INFO] Starting _process_content
2025-02-07 11:48:14,991 [INFO] Starting _process_content
2025-02-07 11:48:15,006 [INFO] Starting _process_content
2025-02-07 11:48:15,016 [INFO] Starting _process_content
2025-02-07 11:48:15,022 [INFO] Starting _process_content
2025-02-07 11:48:15,031 [INFO] Starting _process_content
2025-02-07 11:48:15,037 [INFO] Starting _process_content
2025-02-07 11:48:15,047 [INFO] Starting _process_content
2025-02-07 11:48:15,054 [INFO] Starting _process_content
2025-02-07 11:48:15,065 [INFO] Starting _process_content
2025-02-07 11:48:15,072 [INFO] Starting _process_content
2025-02-07 11:48:15,086 [INFO] Starting _process_content
2025-02-07 11:48:15,099 [INFO] Starting _process_content
2025-02-07 11:48:15,115 [INFO] Starting _process_content
2025-02-07 11:48:15,130 [INFO] Starting _process_content
2025-02-07 11:48:15,139 [INFO] Starting _process_content
2025-02-07 11:48:15,147 [INFO] Starting _process_content
2025-02-07 11:48:15,153 [INFO] Starting _process_content
2025-02-07 11:48:15,164 [INFO] Starting _process_content
2025-02-07 11:48:15,190 [INFO] Starting _process_content
2025-02-07 11:48:15,205 [INFO] Starting _process_content
2025-02-07 11:48:15,220 [INFO] Starting _process_content
2025-02-07 11:48:15,236 [INFO] Starting _process_content
2025-02-07 11:48:15,250 [INFO] Starting _process_content
2025-02-07 11:48:15,256 [INFO] Starting _process_content
2025-02-07 11:48:15,257 [INFO] Sanitizing relative URL: page.html, base_url: https://example.com/subdir/
2025-02-07 11:48:15,264 [INFO] Resolved URL using urljoin: https://example.com/subdir/page.html
2025-02-07 11:48:15,265 [INFO] Sanitizing relative URL: /absolute.html, base_url: https://example.com/subdir/
2025-02-07 11:48:15,265 [INFO] Resolved URL using urljoin: https://example.com/absolute.html
2025-02-07 11:48:15,272 [INFO] Starting _process_content
2025-02-07 11:48:15,282 [INFO] Starting _process_content
2025-02-07 11:48:15,285 [INFO] Sanitizing relative URL: #section2, base_url: https://example.com/page
2025-02-07 11:48:15,285 [INFO] Resolved URL using urljoin: https://example.com/page#section2
2025-02-07 11:48:15,289 [INFO] Starting _process_content
2025-02-07 11:48:15,298 [INFO] Starting _process_content
2025-02-07 11:48:15,303 [INFO] Starting _process_content
2025-02-07 11:48:15,307 [INFO] Starting _process_content
2025-02-07 11:48:15,319 [INFO] Starting _process_content
2025-02-07 11:48:15,330 [INFO] Starting _process_content
2025-02-07 11:50:51,469 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:50:53,778 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:50:53,782 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:50:53,786 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:50:53,790 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:50:53,800 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:50:53,803 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:50:53,806 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:50:53,808 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:50:53,815 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:50:53,817 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:50:53,820 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:50:53,822 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:50:53,831 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:50:53,833 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:50:53,839 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:50:53,841 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:50:53,850 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:50:53,855 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:50:53,858 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:50:53,868 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:50:53,872 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:50:53,874 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:50:54,036 [INFO] Starting _process_content
2025-02-07 11:50:54,049 [INFO] Starting _process_content
2025-02-07 11:50:54,055 [INFO] Starting _process_content
2025-02-07 11:50:54,067 [INFO] Starting _process_content
2025-02-07 11:50:54,073 [INFO] Starting _process_content
2025-02-07 11:50:54,081 [INFO] Starting _process_content
2025-02-07 11:50:54,087 [INFO] Starting _process_content
2025-02-07 11:50:54,091 [INFO] Starting _process_content
2025-02-07 11:50:54,103 [INFO] Starting _process_content
2025-02-07 11:50:54,107 [INFO] Starting _process_content
2025-02-07 11:50:54,119 [INFO] Starting _process_content
2025-02-07 11:50:54,124 [INFO] Starting _process_content
2025-02-07 11:50:54,135 [INFO] Starting _process_content
2025-02-07 11:50:54,138 [INFO] Starting _process_content
2025-02-07 11:50:54,146 [INFO] Starting _process_content
2025-02-07 11:50:54,150 [INFO] Starting _process_content
2025-02-07 11:50:54,154 [INFO] Starting _process_content
2025-02-07 11:50:54,158 [INFO] Starting _process_content
2025-02-07 11:50:54,170 [INFO] Starting _process_content
2025-02-07 11:50:54,174 [INFO] Starting _process_content
2025-02-07 11:50:54,186 [INFO] Starting _process_content
2025-02-07 11:50:54,191 [INFO] Starting _process_content
2025-02-07 11:50:54,201 [INFO] Starting _process_content
2025-02-07 11:50:54,205 [INFO] Starting _process_content
2025-02-07 11:50:54,215 [INFO] Starting _process_content
2025-02-07 11:50:54,220 [INFO] Starting _process_content
2025-02-07 11:50:54,229 [INFO] Starting _process_content
2025-02-07 11:50:54,239 [INFO] Starting _process_content
2025-02-07 11:50:54,251 [INFO] Starting _process_content
2025-02-07 11:50:54,255 [INFO] Starting _process_content
2025-02-07 11:50:54,265 [INFO] Starting _process_content
2025-02-07 11:50:54,272 [INFO] Starting _process_content
2025-02-07 11:50:54,281 [INFO] Starting _process_content
2025-02-07 11:50:54,285 [INFO] Sanitizing relative URL: page.html, base_url: https://example.com/subdir/
2025-02-07 11:50:54,285 [INFO] Resolved URL using urljoin: https://example.com/subdir/page.html
2025-02-07 11:50:54,286 [INFO] Sanitizing relative URL: /absolute.html, base_url: https://example.com/subdir/
2025-02-07 11:50:54,286 [INFO] Resolved URL using urljoin: https://example.com/absolute.html
2025-02-07 11:50:54,300 [INFO] Starting _process_content
2025-02-07 11:50:54,307 [INFO] Starting _process_content
2025-02-07 11:50:54,316 [INFO] Sanitizing relative URL: #section2, base_url: https://example.com/page
2025-02-07 11:50:54,316 [INFO] Resolved URL using urljoin: https://example.com/page#section2
2025-02-07 11:50:54,324 [INFO] Starting _process_content
2025-02-07 11:50:54,339 [INFO] Starting _process_content
2025-02-07 11:50:54,357 [INFO] Starting _process_content
2025-02-07 11:50:54,369 [INFO] Starting _process_content
2025-02-07 11:50:54,380 [INFO] Starting _process_content
2025-02-07 11:50:54,390 [INFO] Starting _process_content
2025-02-07 11:52:20,042 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 11:52:22,470 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:52:22,472 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:52:22,477 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:52:22,482 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:52:22,485 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:52:22,490 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:52:22,494 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:52:22,497 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:52:22,499 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:52:22,502 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:52:22,503 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:52:22,505 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:52:22,512 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:52:22,515 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:52:22,518 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:52:22,522 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:52:22,527 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:52:22,531 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:52:22,536 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:52:22,538 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:52:22,540 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 11:52:22,542 [DEBUG] Using proactor: IocpProactor
2025-02-07 11:52:22,633 [INFO] Starting _process_content
2025-02-07 11:52:22,637 [INFO] Starting _process_content
2025-02-07 11:52:22,646 [INFO] Starting _process_content
2025-02-07 12:16:43,294 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 12:16:45,613 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:16:45,615 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:16:45,619 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:16:45,623 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:16:45,626 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:16:45,628 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:16:45,630 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 12:16:45,631 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:16:45,634 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:16:45,639 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 12:16:45,641 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:16:45,646 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:16:45,650 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:16:45,655 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:16:45,659 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:16:45,664 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:16:45,667 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:16:45,673 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:16:45,678 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:16:45,680 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:16:45,682 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 12:16:45,686 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:17:31,127 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 12:17:33,523 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:17:33,526 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:17:33,529 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:17:33,532 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:17:33,535 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:17:33,537 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:17:33,540 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 12:17:33,544 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:17:33,549 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:17:33,554 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 12:17:33,565 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:17:33,570 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:17:33,583 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:17:33,589 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:17:33,597 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:17:33,599 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:17:33,602 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:17:33,605 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:17:33,614 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:17:33,618 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:17:33,621 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 12:17:33,625 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:17:33,818 [INFO] Starting _process_content
2025-02-07 12:17:33,830 [ERROR] Exception in _process_content: 'ContentProcessor' object has no attribute 'headings'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 877, in _process_content
    self.result.content["headings"] = self.headings
                                      ^^^^^^^^^^^^^
AttributeError: 'ContentProcessor' object has no attribute 'headings'
2025-02-07 12:17:33,851 [INFO] Starting _process_content
2025-02-07 12:17:33,860 [ERROR] Exception in _process_content: 'ContentProcessor' object has no attribute 'headings'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 877, in _process_content
    self.result.content["headings"] = self.headings
                                      ^^^^^^^^^^^^^
AttributeError: 'ContentProcessor' object has no attribute 'headings'
2025-02-07 12:17:33,878 [INFO] Starting _process_content
2025-02-07 12:17:33,888 [ERROR] Exception in _process_content: 'ContentProcessor' object has no attribute 'headings'
Traceback (most recent call last):
  File "C:\Users\Andu\Documents\DEV\lib2docScrape\src\processors\content_processor.py", line 877, in _process_content
    self.result.content["headings"] = self.headings
                                      ^^^^^^^^^^^^^
AttributeError: 'ContentProcessor' object has no attribute 'headings'
2025-02-07 12:19:22,203 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 12:19:24,504 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:19:24,507 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:19:24,510 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:19:24,513 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:19:24,515 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:19:24,518 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:19:24,521 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 12:19:24,523 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:19:24,526 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:19:24,528 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 12:19:24,533 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:19:24,535 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:19:24,542 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:19:24,545 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:19:24,549 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:19:24,553 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:19:24,559 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:19:24,562 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:19:24,567 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:19:24,570 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:19:24,573 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 12:19:24,576 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:19:24,665 [INFO] Starting _process_content
2025-02-07 12:19:24,671 [INFO] Starting _process_content
2025-02-07 12:19:24,682 [INFO] Starting _process_content
2025-02-07 12:21:01,143 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 12:21:03,991 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:21:03,999 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:21:04,002 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:21:04,006 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:21:04,024 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:21:04,034 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:21:04,037 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 12:21:04,048 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:21:04,051 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:21:04,055 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 12:21:04,058 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:21:04,070 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:21:04,084 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:21:04,086 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:21:04,089 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:21:04,091 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:21:04,102 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:21:04,103 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:21:04,106 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:21:04,116 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:21:04,118 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 12:21:04,120 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:21:04,205 [INFO] Starting _process_content
2025-02-07 12:21:04,216 [INFO] Starting _process_content
2025-02-07 12:21:04,222 [INFO] Starting _process_content
2025-02-07 12:23:00,631 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 12:23:03,788 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:03,791 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:03,806 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:03,819 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:03,823 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:03,827 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:03,832 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 12:23:03,837 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:03,841 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:03,851 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 12:23:03,854 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:03,859 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:03,868 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:03,875 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:03,906 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:03,909 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:03,919 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:03,923 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:03,927 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:03,935 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:03,940 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 12:23:03,961 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:04,085 [INFO] Starting _process_content
2025-02-07 12:23:04,092 [INFO] Starting _process_content
2025-02-07 12:23:04,106 [INFO] Starting _process_content
2025-02-07 12:23:51,613 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 12:23:53,735 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:53,737 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:53,740 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:53,743 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:53,746 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:53,749 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:53,754 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 12:23:53,756 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:53,758 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:53,760 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 12:23:53,764 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:53,767 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:53,772 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:53,774 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:53,778 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:53,780 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:53,785 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:53,788 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:53,791 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:53,794 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:53,797 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 12:23:53,799 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:23:53,867 [INFO] Starting _process_content
2025-02-07 12:23:53,875 [INFO] Starting _process_content
2025-02-07 12:23:53,880 [INFO] Starting _process_content
2025-02-07 12:26:37,586 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 12:26:40,524 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:26:40,530 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:26:40,534 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:26:40,538 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:26:40,547 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:26:40,550 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:26:40,553 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 12:26:40,557 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:26:40,566 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:26:40,571 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 12:26:40,581 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:26:40,585 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:26:40,590 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:26:40,596 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:26:40,600 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:26:40,604 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:26:40,611 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:26:40,615 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:26:40,619 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:26:40,622 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:26:40,625 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 12:26:40,630 [DEBUG] Using proactor: IocpProactor
2025-02-07 12:26:40,739 [INFO] Starting _process_content
2025-02-07 12:26:40,748 [INFO] Starting _process_content
2025-02-07 12:26:40,756 [INFO] Starting _process_content
2025-02-07 15:47:58,159 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 15:48:01,061 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:48:01,063 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:48:01,066 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:48:01,118 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:48:01,122 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:48:01,128 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:48:01,131 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 15:48:01,133 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:48:01,139 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:48:01,145 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 15:48:01,149 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:48:01,152 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:48:01,157 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:48:01,165 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:48:01,169 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:48:01,172 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:48:01,179 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:48:01,268 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:48:01,332 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:48:01,339 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:48:01,346 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 15:48:01,350 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:01,106 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 15:56:02,637 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:02,641 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:02,646 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:02,650 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:02,654 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:02,660 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:02,662 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 15:56:02,664 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:02,669 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:02,671 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 15:56:02,677 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:02,680 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:02,685 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:02,687 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:02,696 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:02,698 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:02,701 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:02,704 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:02,712 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:02,715 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:02,717 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 15:56:02,719 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:44,341 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 15:56:45,870 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:45,872 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:45,875 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:45,879 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:45,883 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:45,886 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:45,889 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 15:56:45,891 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:45,893 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:45,896 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 15:56:45,900 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:45,903 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:45,908 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:45,911 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:45,914 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:45,919 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:45,924 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:45,927 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:45,930 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:45,939 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:56:45,941 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 15:56:45,942 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:57:21,074 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 15:57:22,687 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:57:22,689 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:57:22,692 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:57:22,696 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:57:22,702 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:57:22,705 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:57:22,707 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 15:57:22,710 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:57:22,713 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:57:22,719 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 15:57:22,722 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:57:22,726 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:57:22,731 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:57:22,738 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:57:22,741 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:57:22,744 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:57:22,751 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:57:22,755 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:57:22,759 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:57:22,762 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:57:22,764 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 15:57:22,768 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:57:22,878 [INFO] Starting _process_content
2025-02-07 15:57:22,888 [INFO] Starting _process_content
2025-02-07 15:57:22,895 [INFO] Starting _process_content
2025-02-07 15:59:19,862 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 15:59:21,427 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:59:21,429 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:59:21,433 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:59:21,438 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:59:21,443 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:59:21,445 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:59:21,448 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 15:59:21,450 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:59:21,454 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:59:21,458 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 15:59:21,460 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:59:21,464 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:59:21,471 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:59:21,476 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:59:21,481 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:59:21,489 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:59:21,496 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:59:21,505 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:59:21,512 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:59:21,517 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:59:21,525 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 15:59:21,528 [DEBUG] Using proactor: IocpProactor
2025-02-07 15:59:21,674 [INFO] Starting _process_content
2025-02-07 15:59:21,684 [INFO] Starting _process_content
2025-02-07 15:59:21,696 [INFO] Starting _process_content
2025-02-07 16:00:13,168 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 16:00:14,482 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:00:14,484 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:00:14,488 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:00:14,494 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:00:14,498 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:00:14,500 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:00:14,503 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:00:14,506 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:00:14,509 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:00:14,513 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:00:14,515 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:00:14,518 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:00:14,525 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:00:14,530 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:00:14,533 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:00:14,537 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:00:14,540 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:00:14,546 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:00:14,549 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:00:14,551 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:00:14,555 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:00:14,558 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:00:14,848 [INFO] Starting _process_content
2025-02-07 16:00:14,857 [INFO] Starting _process_content
2025-02-07 16:00:14,863 [INFO] Starting _process_content
2025-02-07 16:02:25,592 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 16:02:27,490 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:02:27,492 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:02:27,497 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:02:27,501 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:02:27,504 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:02:27,507 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:02:27,510 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:02:27,512 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:02:27,516 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:02:27,519 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:02:27,522 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:02:27,527 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:02:27,535 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:02:27,538 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:02:27,542 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:02:27,548 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:02:27,553 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:02:27,556 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:02:27,559 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:02:27,566 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:02:27,568 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:02:27,570 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:02:27,675 [INFO] Starting _process_content
2025-02-07 16:02:27,684 [INFO] Starting _process_content
2025-02-07 16:02:27,690 [INFO] Starting _process_content
2025-02-07 16:02:27,700 [INFO] Starting _process_content
2025-02-07 16:02:27,704 [INFO] Starting _process_content
2025-02-07 16:03:23,066 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 16:03:24,378 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:03:24,380 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:03:24,383 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:03:24,387 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:03:24,390 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:03:24,396 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:03:24,398 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:03:24,400 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:03:24,405 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:03:24,410 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:03:24,413 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:03:24,416 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:03:24,421 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:03:24,426 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:03:24,432 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:03:24,434 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:03:24,438 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:03:24,442 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:03:24,449 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:03:24,451 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:03:24,453 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:03:24,455 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:03:24,565 [INFO] Starting _process_content
2025-02-07 16:03:24,570 [INFO] Starting _process_content
2025-02-07 16:03:24,581 [INFO] Starting _process_content
2025-02-07 16:03:24,588 [INFO] Starting _process_content
2025-02-07 16:03:24,597 [INFO] Starting _process_content
2025-02-07 16:04:34,603 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 16:04:36,113 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:04:36,115 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:04:36,119 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:04:36,122 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:04:36,128 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:04:36,131 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:04:36,133 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:04:36,136 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:04:36,139 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:04:36,146 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:04:36,149 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:04:36,153 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:04:36,159 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:04:36,164 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:04:36,168 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:04:36,171 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:04:36,179 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:04:36,182 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:04:36,185 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:04:36,188 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:04:36,192 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:04:36,196 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:04:36,297 [INFO] Starting _process_content
2025-02-07 16:04:36,303 [INFO] Starting _process_content
2025-02-07 16:04:36,313 [INFO] Starting _process_content
2025-02-07 16:04:36,319 [INFO] Starting _process_content
2025-02-07 16:04:36,329 [INFO] Starting _process_content
2025-02-07 16:05:25,467 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 16:05:26,865 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:05:26,867 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:05:26,871 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:05:26,875 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:05:26,880 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:05:26,883 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:05:26,885 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:05:26,887 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:05:26,891 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:05:26,895 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:05:26,899 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:05:26,902 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:05:26,907 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:05:26,910 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:05:26,915 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:05:26,918 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:05:26,922 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:05:26,925 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:05:26,932 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:05:26,935 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:05:26,937 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:05:26,939 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:05:27,039 [INFO] Starting _process_content
2025-02-07 16:05:27,049 [INFO] Starting _process_content
2025-02-07 16:05:27,055 [INFO] Starting _process_content
2025-02-07 16:05:27,065 [INFO] Starting _process_content
2025-02-07 16:05:27,070 [INFO] Starting _process_content
2025-02-07 16:05:27,074 [INFO] Starting _process_content
2025-02-07 16:06:23,385 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 16:06:25,046 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:06:25,048 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:06:25,052 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:06:25,057 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:06:25,061 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:06:25,063 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:06:25,065 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:06:25,067 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:06:25,069 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:06:25,074 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:06:25,076 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:06:25,079 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:06:25,083 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:06:25,086 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:06:25,092 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:06:25,095 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:06:25,099 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:06:25,102 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:06:25,108 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:06:25,114 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:06:25,116 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:06:25,118 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:06:25,225 [INFO] Starting _process_content
2025-02-07 16:06:25,234 [INFO] Starting _process_content
2025-02-07 16:06:25,246 [INFO] Starting _process_content
2025-02-07 16:06:25,254 [INFO] Starting _process_content
2025-02-07 16:06:25,264 [INFO] Starting _process_content
2025-02-07 16:06:25,269 [INFO] Starting _process_content
2025-02-07 16:06:25,279 [INFO] Starting _process_content
2025-02-07 16:06:25,284 [INFO] Starting _process_content
2025-02-07 16:06:25,295 [INFO] Starting _process_content
2025-02-07 16:06:25,301 [INFO] Starting _process_content
2025-02-07 16:06:25,312 [INFO] Starting _process_content
2025-02-07 16:06:25,318 [INFO] Starting _process_content
2025-02-07 16:06:25,328 [INFO] Starting _process_content
2025-02-07 16:06:25,333 [INFO] Starting _process_content
2025-02-07 16:06:25,341 [INFO] Starting _process_content
2025-02-07 16:06:25,347 [INFO] Starting _process_content
2025-02-07 16:06:25,353 [INFO] Starting _process_content
2025-02-07 16:06:25,365 [INFO] Starting _process_content
2025-02-07 16:06:25,378 [INFO] Starting _process_content
2025-02-07 16:06:25,384 [INFO] Starting _process_content
2025-02-07 16:07:21,889 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 16:07:24,039 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:07:24,042 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:07:24,048 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:07:24,058 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:07:24,063 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:07:24,073 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:07:24,076 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:07:24,080 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:07:24,090 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:07:24,096 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:07:24,106 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:07:24,110 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:07:24,114 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:07:24,122 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:07:24,128 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:07:24,137 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:07:24,142 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:07:24,147 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:07:24,155 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:07:24,173 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:07:24,175 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:07:24,177 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:07:24,312 [INFO] Starting _process_content
2025-02-07 16:07:24,326 [INFO] Starting _process_content
2025-02-07 16:07:24,338 [INFO] Starting _process_content
2025-02-07 16:07:24,346 [INFO] Starting _process_content
2025-02-07 16:07:24,358 [INFO] Starting _process_content
2025-02-07 16:07:24,365 [INFO] Starting _process_content
2025-02-07 16:07:24,375 [INFO] Starting _process_content
2025-02-07 16:07:24,385 [INFO] Starting _process_content
2025-02-07 16:07:24,393 [INFO] Starting _process_content
2025-02-07 16:07:24,404 [INFO] Starting _process_content
2025-02-07 16:07:24,412 [INFO] Starting _process_content
2025-02-07 16:07:24,426 [INFO] Starting _process_content
2025-02-07 16:07:24,436 [INFO] Starting _process_content
2025-02-07 16:07:24,448 [INFO] Starting _process_content
2025-02-07 16:07:24,464 [INFO] Starting _process_content
2025-02-07 16:07:24,487 [INFO] Starting _process_content
2025-02-07 16:07:24,494 [INFO] Starting _process_content
2025-02-07 16:07:24,508 [INFO] Starting _process_content
2025-02-07 16:07:24,525 [INFO] Starting _process_content
2025-02-07 16:07:24,536 [INFO] Starting _process_content
2025-02-07 16:08:49,235 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 16:08:51,425 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:08:51,428 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:08:51,431 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:08:51,438 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:08:51,446 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:08:51,452 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:08:51,457 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:08:51,460 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:08:51,465 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:08:51,470 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:08:51,476 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:08:51,480 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:08:51,491 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:08:51,496 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:08:51,507 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:08:51,512 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:08:51,524 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:08:51,526 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:08:51,529 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:08:51,532 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:08:51,541 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:08:51,543 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:08:51,699 [INFO] Starting _process_content
2025-02-07 16:08:51,712 [INFO] Starting _process_content
2025-02-07 16:08:51,727 [INFO] Starting _process_content
2025-02-07 16:08:51,740 [INFO] Starting _process_content
2025-02-07 16:08:51,754 [INFO] Starting _process_content
2025-02-07 16:08:51,762 [INFO] Starting _process_content
2025-02-07 16:08:51,770 [INFO] Starting _process_content
2025-02-07 16:08:51,779 [INFO] Starting _process_content
2025-02-07 16:08:51,791 [INFO] Starting _process_content
2025-02-07 16:08:51,805 [INFO] Starting _process_content
2025-02-07 16:08:51,819 [INFO] Starting _process_content
2025-02-07 16:08:51,829 [INFO] Starting _process_content
2025-02-07 16:08:51,840 [INFO] Starting _process_content
2025-02-07 16:08:51,844 [INFO] Starting _process_content
2025-02-07 16:08:51,852 [INFO] Starting _process_content
2025-02-07 16:08:51,861 [INFO] Starting _process_content
2025-02-07 16:08:51,872 [INFO] Starting _process_content
2025-02-07 16:08:51,878 [INFO] Starting _process_content
2025-02-07 16:08:51,889 [INFO] Starting _process_content
2025-02-07 16:08:51,897 [INFO] Starting _process_content
2025-02-07 16:08:51,910 [INFO] Starting _process_content
2025-02-07 16:10:09,880 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 16:10:11,930 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:10:11,933 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:10:11,938 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:10:11,947 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:10:11,956 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:10:11,965 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:10:11,970 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:10:11,972 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:10:11,979 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:10:11,982 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:10:11,985 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:10:11,989 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:10:11,999 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:10:12,003 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:10:12,006 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:10:12,014 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:10:12,019 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:10:12,022 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:10:12,029 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:10:12,033 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:10:12,040 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:10:12,045 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:10:12,267 [INFO] Starting _process_content
2025-02-07 16:10:12,280 [INFO] Starting _process_content
2025-02-07 16:10:12,295 [INFO] Starting _process_content
2025-02-07 16:10:12,315 [INFO] Starting _process_content
2025-02-07 16:10:12,328 [INFO] Starting _process_content
2025-02-07 16:10:12,337 [INFO] Starting _process_content
2025-02-07 16:10:12,351 [INFO] Starting _process_content
2025-02-07 16:10:12,365 [INFO] Starting _process_content
2025-02-07 16:10:12,372 [INFO] Starting _process_content
2025-02-07 16:10:12,397 [INFO] Starting _process_content
2025-02-07 16:10:12,415 [INFO] Starting _process_content
2025-02-07 16:10:12,434 [INFO] Starting _process_content
2025-02-07 16:10:12,451 [INFO] Starting _process_content
2025-02-07 16:10:12,471 [INFO] Starting _process_content
2025-02-07 16:10:12,483 [INFO] Starting _process_content
2025-02-07 16:10:12,489 [INFO] Starting _process_content
2025-02-07 16:10:12,502 [INFO] Starting _process_content
2025-02-07 16:10:12,520 [INFO] Starting _process_content
2025-02-07 16:10:12,537 [INFO] Starting _process_content
2025-02-07 16:10:12,556 [INFO] Starting _process_content
2025-02-07 16:10:12,584 [INFO] Starting _process_content
2025-02-07 16:10:12,602 [INFO] Starting _process_content
2025-02-07 16:10:12,619 [INFO] Starting _process_content
2025-02-07 16:10:12,647 [INFO] Starting _process_content
2025-02-07 16:10:12,679 [INFO] Starting _process_content
2025-02-07 16:10:12,704 [INFO] Starting _process_content
2025-02-07 16:10:12,731 [INFO] Starting _process_content
2025-02-07 16:10:12,755 [INFO] Starting _process_content
2025-02-07 16:10:12,782 [INFO] Starting _process_content
2025-02-07 16:11:08,478 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 16:11:10,448 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:11:10,452 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:11:10,459 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:11:10,466 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:11:10,474 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:11:10,479 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:11:10,485 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:11:10,490 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:11:10,495 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:11:10,500 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:11:10,505 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:11:10,511 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:11:10,518 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:11:10,527 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:11:10,534 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:11:10,542 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:11:10,550 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:11:10,559 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:11:10,564 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:11:10,567 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:11:10,572 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:11:10,575 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:11:10,775 [INFO] Starting _process_content
2025-02-07 16:11:10,782 [INFO] Starting _process_content
2025-02-07 16:11:10,808 [INFO] Starting _process_content
2025-02-07 16:11:10,828 [INFO] Starting _process_content
2025-02-07 16:11:10,844 [INFO] Starting _process_content
2025-02-07 16:11:10,858 [INFO] Starting _process_content
2025-02-07 16:11:10,866 [INFO] Starting _process_content
2025-02-07 16:11:10,882 [INFO] Starting _process_content
2025-02-07 16:11:10,899 [INFO] Starting _process_content
2025-02-07 16:11:10,913 [INFO] Starting _process_content
2025-02-07 16:11:10,931 [INFO] Starting _process_content
2025-02-07 16:11:10,948 [INFO] Starting _process_content
2025-02-07 16:11:10,973 [INFO] Starting _process_content
2025-02-07 16:11:10,993 [INFO] Starting _process_content
2025-02-07 16:11:11,002 [INFO] Starting _process_content
2025-02-07 16:11:11,015 [INFO] Starting _process_content
2025-02-07 16:11:11,035 [INFO] Starting _process_content
2025-02-07 16:11:11,065 [INFO] Starting _process_content
2025-02-07 16:11:11,079 [INFO] Starting _process_content
2025-02-07 16:11:11,092 [INFO] Starting _process_content
2025-02-07 16:11:11,118 [INFO] Starting _process_content
2025-02-07 16:11:11,141 [INFO] Starting _process_content
2025-02-07 16:11:11,163 [INFO] Starting _process_content
2025-02-07 16:11:11,183 [INFO] Starting _process_content
2025-02-07 16:11:11,200 [INFO] Starting _process_content
2025-02-07 16:11:11,216 [INFO] Starting _process_content
2025-02-07 16:11:11,233 [INFO] Starting _process_content
2025-02-07 16:11:11,248 [INFO] Starting _process_content
2025-02-07 16:11:11,279 [INFO] Starting _process_content
2025-02-07 16:12:32,490 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 16:12:34,613 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:12:34,616 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:12:34,621 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:12:34,627 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:12:34,636 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:12:34,642 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:12:34,646 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:12:34,648 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:12:34,653 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:12:34,661 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:12:34,665 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:12:34,671 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:12:34,681 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:12:34,684 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:12:34,687 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:12:34,689 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:12:34,700 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:12:34,704 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:12:34,711 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:12:34,715 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:12:34,718 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:12:34,721 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:12:34,873 [INFO] Starting _process_content
2025-02-07 16:12:34,887 [INFO] Starting _process_content
2025-02-07 16:12:34,902 [INFO] Starting _process_content
2025-02-07 16:12:34,914 [INFO] Starting _process_content
2025-02-07 16:12:34,923 [INFO] Starting _process_content
2025-02-07 16:12:34,935 [INFO] Starting _process_content
2025-02-07 16:12:34,948 [INFO] Starting _process_content
2025-02-07 16:12:34,953 [INFO] Starting _process_content
2025-02-07 16:12:34,965 [INFO] Starting _process_content
2025-02-07 16:12:34,976 [INFO] Starting _process_content
2025-02-07 16:12:34,988 [INFO] Starting _process_content
2025-02-07 16:12:35,003 [INFO] Starting _process_content
2025-02-07 16:12:35,015 [INFO] Starting _process_content
2025-02-07 16:12:35,022 [INFO] Starting _process_content
2025-02-07 16:12:35,036 [INFO] Starting _process_content
2025-02-07 16:12:35,046 [INFO] Starting _process_content
2025-02-07 16:12:35,056 [INFO] Starting _process_content
2025-02-07 16:12:35,071 [INFO] Starting _process_content
2025-02-07 16:12:35,086 [INFO] Starting _process_content
2025-02-07 16:12:35,100 [INFO] Starting _process_content
2025-02-07 16:12:35,116 [INFO] Starting _process_content
2025-02-07 16:12:35,122 [INFO] Starting _process_content
2025-02-07 16:12:35,136 [INFO] Starting _process_content
2025-02-07 16:12:35,152 [INFO] Starting _process_content
2025-02-07 16:12:35,163 [INFO] Starting _process_content
2025-02-07 16:12:35,170 [INFO] Starting _process_content
2025-02-07 16:12:35,185 [INFO] Starting _process_content
2025-02-07 16:12:35,204 [INFO] Starting _process_content
2025-02-07 16:12:35,235 [INFO] Starting _process_content
2025-02-07 16:13:37,992 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 16:13:39,956 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:13:39,958 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:13:39,961 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:13:39,967 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:13:39,970 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:13:39,973 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:13:39,975 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:13:39,977 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:13:39,982 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:13:39,986 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:13:39,988 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:13:39,993 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:13:40,000 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:13:40,004 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:13:40,008 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:13:40,011 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:13:40,019 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:13:40,022 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:13:40,025 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:13:40,027 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:13:40,033 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:13:40,035 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:13:40,137 [INFO] Starting _process_content
2025-02-07 16:13:40,142 [INFO] Starting _process_content
2025-02-07 16:13:40,154 [INFO] Starting _process_content
2025-02-07 16:13:40,161 [INFO] Starting _process_content
2025-02-07 16:13:40,172 [INFO] Starting _process_content
2025-02-07 16:13:40,177 [INFO] Starting _process_content
2025-02-07 16:13:40,186 [INFO] Starting _process_content
2025-02-07 16:13:40,191 [INFO] Starting _process_content
2025-02-07 16:13:40,202 [INFO] Starting _process_content
2025-02-07 16:13:40,209 [INFO] Starting _process_content
2025-02-07 16:13:40,220 [INFO] Starting _process_content
2025-02-07 16:13:40,227 [INFO] Starting _process_content
2025-02-07 16:13:40,237 [INFO] Starting _process_content
2025-02-07 16:13:40,242 [INFO] Starting _process_content
2025-02-07 16:13:40,252 [INFO] Starting _process_content
2025-02-07 16:13:40,257 [INFO] Starting _process_content
2025-02-07 16:13:40,261 [INFO] Starting _process_content
2025-02-07 16:13:40,273 [INFO] Starting _process_content
2025-02-07 16:13:40,277 [INFO] Starting _process_content
2025-02-07 16:13:40,288 [INFO] Starting _process_content
2025-02-07 16:13:40,294 [INFO] Starting _process_content
2025-02-07 16:13:40,305 [INFO] Starting _process_content
2025-02-07 16:13:40,311 [INFO] Starting _process_content
2025-02-07 16:13:40,323 [INFO] Starting _process_content
2025-02-07 16:13:40,335 [INFO] Starting _process_content
2025-02-07 16:13:40,342 [INFO] Starting _process_content
2025-02-07 16:13:40,354 [INFO] Starting _process_content
2025-02-07 16:13:40,361 [INFO] Starting _process_content
2025-02-07 16:13:40,376 [INFO] Starting _process_content
2025-02-07 16:15:09,938 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 16:15:11,662 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:15:11,665 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:15:11,668 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:15:11,671 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:15:11,675 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:15:11,679 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:15:11,682 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:15:11,684 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:15:11,687 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:15:11,691 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:15:11,696 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:15:11,700 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:15:11,705 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:15:11,708 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:15:11,715 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:15:11,718 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:15:11,722 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:15:11,724 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:15:11,731 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:15:11,734 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:15:11,736 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:15:11,739 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:15:11,841 [INFO] Starting _process_content
2025-02-07 16:15:11,852 [INFO] Starting _process_content
2025-02-07 16:15:11,858 [INFO] Starting _process_content
2025-02-07 16:15:11,870 [INFO] Starting _process_content
2025-02-07 16:15:11,875 [INFO] Starting _process_content
2025-02-07 16:15:11,886 [INFO] Starting _process_content
2025-02-07 16:15:11,890 [INFO] Starting _process_content
2025-02-07 16:15:11,901 [INFO] Starting _process_content
2025-02-07 16:15:11,906 [INFO] Starting _process_content
2025-02-07 16:15:11,917 [INFO] Starting _process_content
2025-02-07 16:15:11,923 [INFO] Starting _process_content
2025-02-07 16:15:11,934 [INFO] Starting _process_content
2025-02-07 16:15:11,940 [INFO] Starting _process_content
2025-02-07 16:15:11,950 [INFO] Starting _process_content
2025-02-07 16:15:11,955 [INFO] Starting _process_content
2025-02-07 16:15:11,963 [INFO] Starting _process_content
2025-02-07 16:15:11,969 [INFO] Starting _process_content
2025-02-07 16:15:11,974 [INFO] Starting _process_content
2025-02-07 16:15:11,985 [INFO] Starting _process_content
2025-02-07 16:15:11,990 [INFO] Starting _process_content
2025-02-07 16:15:12,002 [INFO] Starting _process_content
2025-02-07 16:15:12,008 [INFO] Starting _process_content
2025-02-07 16:15:12,020 [INFO] Starting _process_content
2025-02-07 16:15:12,031 [INFO] Starting _process_content
2025-02-07 16:15:12,038 [INFO] Starting _process_content
2025-02-07 16:15:12,050 [INFO] Starting _process_content
2025-02-07 16:15:12,056 [INFO] Starting _process_content
2025-02-07 16:15:12,068 [INFO] Starting _process_content
2025-02-07 16:15:12,084 [INFO] Starting _process_content
2025-02-07 16:16:24,265 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 16:16:26,369 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:16:26,371 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:16:26,377 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:16:26,380 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:16:26,384 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:16:26,386 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:16:26,388 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:16:26,392 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:16:26,395 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:16:26,398 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:16:26,403 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:16:26,409 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:16:26,413 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:16:26,417 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:16:26,420 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:16:26,425 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:16:26,430 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:16:26,433 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:16:26,436 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:16:26,438 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:16:26,443 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:16:26,445 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:16:26,546 [INFO] Starting _process_content
2025-02-07 16:16:26,551 [INFO] Starting _process_content
2025-02-07 16:16:26,562 [INFO] Starting _process_content
2025-02-07 16:16:26,570 [INFO] Starting _process_content
2025-02-07 16:16:26,582 [INFO] Starting _process_content
2025-02-07 16:16:26,587 [INFO] Starting _process_content
2025-02-07 16:16:26,597 [INFO] Starting _process_content
2025-02-07 16:16:26,602 [INFO] Starting _process_content
2025-02-07 16:16:26,613 [INFO] Starting _process_content
2025-02-07 16:16:26,619 [INFO] Starting _process_content
2025-02-07 16:16:26,631 [INFO] Starting _process_content
2025-02-07 16:16:26,637 [INFO] Starting _process_content
2025-02-07 16:16:26,649 [INFO] Starting _process_content
2025-02-07 16:16:26,653 [INFO] Starting _process_content
2025-02-07 16:16:26,663 [INFO] Starting _process_content
2025-02-07 16:16:26,667 [INFO] Starting _process_content
2025-02-07 16:16:26,676 [INFO] Starting _process_content
2025-02-07 16:16:26,683 [INFO] Starting _process_content
2025-02-07 16:16:26,689 [INFO] Starting _process_content
2025-02-07 16:16:26,699 [INFO] Starting _process_content
2025-02-07 16:16:26,705 [INFO] Starting _process_content
2025-02-07 16:16:26,716 [INFO] Starting _process_content
2025-02-07 16:16:26,729 [INFO] Starting _process_content
2025-02-07 16:16:26,735 [INFO] Starting _process_content
2025-02-07 16:16:26,746 [INFO] Starting _process_content
2025-02-07 16:16:26,753 [INFO] Starting _process_content
2025-02-07 16:16:26,764 [INFO] Starting _process_content
2025-02-07 16:16:26,776 [INFO] Starting _process_content
2025-02-07 16:16:26,788 [INFO] Starting _process_content
2025-02-07 16:17:51,988 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 16:17:53,762 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:17:53,764 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:17:53,767 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:17:53,770 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:17:53,775 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:17:53,779 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:17:53,781 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:17:53,785 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:17:53,788 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:17:53,790 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:17:53,796 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:17:53,799 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:17:53,805 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:17:53,811 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:17:53,816 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:17:53,818 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:17:53,821 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:17:53,826 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:17:53,832 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:17:53,835 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:17:53,837 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:17:53,838 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:17:53,948 [INFO] Starting _process_content
2025-02-07 16:17:53,953 [INFO] Starting _process_content
2025-02-07 16:17:53,963 [INFO] Starting _process_content
2025-02-07 16:17:53,973 [INFO] Starting _process_content
2025-02-07 16:17:53,994 [INFO] Starting _process_content
2025-02-07 16:17:54,006 [INFO] Starting _process_content
2025-02-07 16:17:54,022 [INFO] Starting _process_content
2025-02-07 16:17:54,034 [INFO] Starting _process_content
2025-02-07 16:17:54,048 [INFO] Starting _process_content
2025-02-07 16:17:54,057 [INFO] Starting _process_content
2025-02-07 16:17:54,071 [INFO] Starting _process_content
2025-02-07 16:17:54,087 [INFO] Starting _process_content
2025-02-07 16:17:54,101 [INFO] Starting _process_content
2025-02-07 16:17:54,111 [INFO] Starting _process_content
2025-02-07 16:17:54,120 [INFO] Starting _process_content
2025-02-07 16:17:54,131 [INFO] Starting _process_content
2025-02-07 16:17:54,136 [INFO] Starting _process_content
2025-02-07 16:17:54,147 [INFO] Starting _process_content
2025-02-07 16:17:54,156 [INFO] Starting _process_content
2025-02-07 16:17:54,180 [INFO] Starting _process_content
2025-02-07 16:17:54,197 [INFO] Starting _process_content
2025-02-07 16:17:54,210 [INFO] Starting _process_content
2025-02-07 16:17:54,230 [INFO] Starting _process_content
2025-02-07 16:17:54,239 [INFO] Starting _process_content
2025-02-07 16:17:54,253 [INFO] Starting _process_content
2025-02-07 16:17:54,266 [INFO] Starting _process_content
2025-02-07 16:17:54,276 [INFO] Starting _process_content
2025-02-07 16:17:54,288 [INFO] Starting _process_content
2025-02-07 16:17:54,305 [INFO] Starting _process_content
2025-02-07 16:19:41,106 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 16:19:42,816 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:19:42,818 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:19:42,821 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:19:42,828 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:19:42,832 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:19:42,834 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:19:42,836 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:19:42,839 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:19:42,844 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:19:42,847 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:19:42,852 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:19:42,856 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:19:42,863 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:19:42,866 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:19:42,869 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:19:42,874 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:19:42,879 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:19:42,882 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:19:42,885 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:19:42,887 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:19:42,893 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:19:42,895 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:19:43,008 [INFO] Starting _process_content
2025-02-07 16:19:43,017 [INFO] Starting _process_content
2025-02-07 16:19:43,029 [INFO] Starting _process_content
2025-02-07 16:19:43,037 [INFO] Starting _process_content
2025-02-07 16:19:43,047 [INFO] Starting _process_content
2025-02-07 16:19:43,052 [INFO] Starting _process_content
2025-02-07 16:19:43,061 [INFO] Starting _process_content
2025-02-07 16:19:43,067 [INFO] Starting _process_content
2025-02-07 16:19:43,076 [INFO] Starting _process_content
2025-02-07 16:19:43,084 [INFO] Starting _process_content
2025-02-07 16:19:43,095 [INFO] Starting _process_content
2025-02-07 16:19:43,101 [INFO] Starting _process_content
2025-02-07 16:19:43,111 [INFO] Starting _process_content
2025-02-07 16:19:43,116 [INFO] Starting _process_content
2025-02-07 16:19:43,125 [INFO] Starting _process_content
2025-02-07 16:19:43,133 [INFO] Starting _process_content
2025-02-07 16:19:43,137 [INFO] Starting _process_content
2025-02-07 16:19:43,150 [INFO] Starting _process_content
2025-02-07 16:19:43,159 [INFO] Starting _process_content
2025-02-07 16:19:43,166 [INFO] Starting _process_content
2025-02-07 16:19:43,178 [INFO] Starting _process_content
2025-02-07 16:19:43,187 [INFO] Starting _process_content
2025-02-07 16:19:43,199 [INFO] Starting _process_content
2025-02-07 16:19:43,211 [INFO] Starting _process_content
2025-02-07 16:19:43,218 [INFO] Starting _process_content
2025-02-07 16:19:43,230 [INFO] Starting _process_content
2025-02-07 16:19:43,236 [INFO] Starting _process_content
2025-02-07 16:19:43,248 [INFO] Starting _process_content
2025-02-07 16:19:43,262 [INFO] Starting _process_content
2025-02-07 16:21:14,418 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 16:21:16,170 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:21:16,173 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:21:16,176 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:21:16,179 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:21:16,182 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:21:16,189 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:21:16,191 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:21:16,193 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:21:16,197 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:21:16,199 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:21:16,205 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:21:16,208 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:21:16,213 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:21:16,215 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:21:16,224 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:21:16,226 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:21:16,229 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:21:16,232 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:21:16,239 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:21:16,242 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:21:16,244 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:21:16,245 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:21:16,346 [INFO] Starting _process_content
2025-02-07 16:21:16,355 [INFO] Starting _process_content
2025-02-07 16:21:16,363 [INFO] Starting _process_content
2025-02-07 16:21:16,374 [INFO] Starting _process_content
2025-02-07 16:21:16,379 [INFO] Starting _process_content
2025-02-07 16:21:16,389 [INFO] Starting _process_content
2025-02-07 16:21:16,393 [INFO] Starting _process_content
2025-02-07 16:21:16,399 [INFO] Starting _process_content
2025-02-07 16:21:16,409 [INFO] Starting _process_content
2025-02-07 16:21:16,415 [INFO] Starting _process_content
2025-02-07 16:21:16,426 [INFO] Starting _process_content
2025-02-07 16:21:16,432 [INFO] Starting _process_content
2025-02-07 16:21:16,443 [INFO] Starting _process_content
2025-02-07 16:21:16,448 [INFO] Starting _process_content
2025-02-07 16:21:16,458 [INFO] Starting _process_content
2025-02-07 16:21:16,462 [INFO] Starting _process_content
2025-02-07 16:21:16,472 [INFO] Starting _process_content
2025-02-07 16:21:16,478 [INFO] Starting _process_content
2025-02-07 16:21:16,488 [INFO] Starting _process_content
2025-02-07 16:21:16,494 [INFO] Starting _process_content
2025-02-07 16:21:16,506 [INFO] Starting _process_content
2025-02-07 16:21:16,512 [INFO] Starting _process_content
2025-02-07 16:21:16,523 [INFO] Starting _process_content
2025-02-07 16:21:16,532 [INFO] Starting _process_content
2025-02-07 16:21:16,546 [INFO] Starting _process_content
2025-02-07 16:21:16,558 [INFO] Starting _process_content
2025-02-07 16:21:16,564 [INFO] Starting _process_content
2025-02-07 16:21:16,576 [INFO] Starting _process_content
2025-02-07 16:21:16,592 [INFO] Starting _process_content
2025-02-07 16:22:18,295 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 16:22:19,764 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:22:19,766 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:22:19,769 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:22:19,773 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:22:19,778 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:22:19,781 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:22:19,783 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:22:19,786 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:22:19,789 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:22:19,793 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:22:19,797 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:22:19,799 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:22:19,804 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:22:19,807 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:22:19,814 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:22:19,817 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:22:19,821 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:22:19,826 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:22:19,832 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:22:19,834 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:22:19,837 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 16:22:19,839 [DEBUG] Using proactor: IocpProactor
2025-02-07 16:22:19,936 [INFO] Starting _process_content
2025-02-07 16:22:19,946 [INFO] Starting _process_content
2025-02-07 16:22:19,954 [INFO] Starting _process_content
2025-02-07 16:22:19,966 [INFO] Starting _process_content
2025-02-07 16:22:19,971 [INFO] Starting _process_content
2025-02-07 16:22:19,981 [INFO] Starting _process_content
2025-02-07 16:22:19,986 [INFO] Starting _process_content
2025-02-07 16:22:19,996 [INFO] Starting _process_content
2025-02-07 16:22:20,001 [INFO] Starting _process_content
2025-02-07 16:22:20,012 [INFO] Starting _process_content
2025-02-07 16:22:20,019 [INFO] Starting _process_content
2025-02-07 16:22:20,031 [INFO] Starting _process_content
2025-02-07 16:22:20,031 [INFO] Joining base_url: https://example.com/scripts/ with url: app.js
2025-02-07 16:22:20,032 [INFO] Joined URL: https://example.com/scripts/app.js
2025-02-07 16:22:20,032 [INFO] Joining base_url: https://example.com/scripts/ with url: user.js
2025-02-07 16:22:20,032 [INFO] Joined URL: https://example.com/scripts/user.js
2025-02-07 16:22:20,037 [INFO] Starting _process_content
2025-02-07 16:22:20,046 [INFO] Starting _process_content
2025-02-07 16:22:20,051 [INFO] Starting _process_content
2025-02-07 16:22:20,055 [INFO] Starting _process_content
2025-02-07 16:22:20,065 [INFO] Starting _process_content
2025-02-07 16:22:20,072 [INFO] Starting _process_content
2025-02-07 16:22:20,084 [INFO] Starting _process_content
2025-02-07 16:22:20,095 [INFO] Starting _process_content
2025-02-07 16:22:20,101 [INFO] Starting _process_content
2025-02-07 16:22:20,112 [INFO] Starting _process_content
2025-02-07 16:22:20,118 [INFO] Starting _process_content
2025-02-07 16:22:20,130 [INFO] Starting _process_content
2025-02-07 16:22:20,136 [INFO] Starting _process_content
2025-02-07 16:22:20,148 [INFO] Starting _process_content
2025-02-07 16:22:20,154 [INFO] Starting _process_content
2025-02-07 16:22:20,166 [INFO] Starting _process_content
2025-02-07 16:22:20,181 [INFO] Starting _process_content
2025-02-07 16:22:20,182 [INFO] Returning URL without joining: https://example.com/styles.css
2025-02-07 16:22:20,182 [INFO] Joining base_url: https://example.com with url: app.js
2025-02-07 16:22:20,182 [INFO] Joined URL: https://example.com/app.js
2025-02-07 16:22:20,183 [INFO] Joining base_url: https://example.com with url: video.mp4
2025-02-07 16:22:20,183 [INFO] Joined URL: https://example.com/video.mp4
2025-02-07 16:22:20,183 [INFO] Joining base_url: https://example.com with url: audio.mp3
2025-02-07 16:22:20,183 [INFO] Joined URL: https://example.com/audio.mp3
2025-02-07 16:22:20,187 [INFO] Joining base_url: https://example.com with url: media.webm
2025-02-07 16:22:20,187 [INFO] Joined URL: https://example.com/media.webm
2025-02-07 18:03:44,614 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 18:03:45,850 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:03:45,854 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:03:45,857 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:03:45,860 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:03:45,864 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:03:45,865 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:03:45,868 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:03:45,871 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:03:45,874 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:03:45,878 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:03:45,882 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:03:45,884 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:03:45,890 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:03:45,893 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:03:45,898 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:03:45,900 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:03:45,907 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:03:45,910 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:03:45,915 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:03:45,917 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:03:45,922 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:03:45,925 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:03:46,012 [INFO] Starting _process_content
2025-02-07 18:03:46,013 [ERROR] Unexpected error in content processing: 'ContentProcessor' object has no attribute '_process_lists'
2025-02-07 18:04:39,365 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 18:04:40,795 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:40,799 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:40,803 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:40,805 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:40,808 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:40,810 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:40,813 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:04:40,819 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:40,822 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:40,827 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:04:40,830 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:40,836 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:40,841 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:40,846 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:40,852 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:40,855 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:40,861 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:40,863 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:40,869 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:40,872 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:40,877 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:04:40,879 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:40,983 [INFO] Starting _process_content
2025-02-07 18:04:40,985 [ERROR] Unexpected error in content processing: 'ContentProcessor' object has no attribute '_process_tables'
2025-02-07 18:04:57,045 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 18:04:58,483 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:58,485 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:58,488 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:58,491 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:58,495 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:58,497 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:58,500 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:04:58,502 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:58,504 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:58,507 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:04:58,511 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:58,514 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:58,520 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:58,522 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:58,528 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:58,533 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:58,537 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:58,539 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:58,542 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:58,544 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:58,549 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:04:58,552 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:04:58,635 [INFO] Starting _process_content
2025-02-07 18:04:58,641 [INFO] Starting _process_content
2025-02-07 18:04:58,651 [INFO] Starting _process_content
2025-02-07 18:04:58,657 [INFO] Starting _process_content
2025-02-07 18:04:58,661 [INFO] Starting _process_content
2025-02-07 18:04:58,669 [INFO] Starting _process_content
2025-02-07 18:04:58,675 [INFO] Starting _process_content
2025-02-07 18:04:58,679 [INFO] Starting _process_content
2025-02-07 18:04:58,688 [INFO] Starting _process_content
2025-02-07 18:04:58,694 [INFO] Starting _process_content
2025-02-07 18:04:58,702 [INFO] Starting _process_content
2025-02-07 18:04:58,710 [INFO] Starting _process_content
2025-02-07 18:04:58,710 [INFO] Joining base_url: https://example.com/scripts/ with url: app.js
2025-02-07 18:04:58,710 [INFO] Joined URL: https://example.com/scripts/app.js
2025-02-07 18:04:58,711 [INFO] Joining base_url: https://example.com/scripts/ with url: user.js
2025-02-07 18:04:58,711 [INFO] Joined URL: https://example.com/scripts/user.js
2025-02-07 18:04:58,719 [INFO] Starting _process_content
2025-02-07 18:04:58,725 [INFO] Starting _process_content
2025-02-07 18:04:58,729 [INFO] Starting _process_content
2025-02-07 18:04:58,737 [INFO] Starting _process_content
2025-02-07 18:04:58,744 [INFO] Starting _process_content
2025-02-07 18:04:58,753 [INFO] Starting _process_content
2025-02-07 18:04:58,759 [INFO] Starting _process_content
2025-02-07 18:04:58,769 [INFO] Starting _process_content
2025-02-07 18:05:20,067 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 18:05:21,423 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:21,428 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:21,432 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:21,434 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:21,437 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:21,439 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:21,444 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:05:21,446 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:21,449 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:21,452 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:05:21,458 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:21,463 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:21,468 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:21,470 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:21,480 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:21,482 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:21,487 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:21,489 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:21,496 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:21,498 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:21,501 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:05:21,504 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:21,597 [INFO] Starting _process_content
2025-02-07 18:05:21,604 [INFO] Starting _process_content
2025-02-07 18:05:21,613 [INFO] Starting _process_content
2025-02-07 18:05:21,621 [INFO] Starting _process_content
2025-02-07 18:05:21,629 [INFO] Starting _process_content
2025-02-07 18:05:21,636 [INFO] Starting _process_content
2025-02-07 18:05:21,639 [INFO] Starting _process_content
2025-02-07 18:05:21,648 [INFO] Starting _process_content
2025-02-07 18:05:21,655 [INFO] Starting _process_content
2025-02-07 18:05:21,664 [INFO] Starting _process_content
2025-02-07 18:05:21,672 [INFO] Starting _process_content
2025-02-07 18:05:21,681 [INFO] Starting _process_content
2025-02-07 18:05:21,684 [INFO] Joining base_url: https://example.com/scripts/ with url: app.js
2025-02-07 18:05:21,685 [INFO] Joined URL: https://example.com/scripts/app.js
2025-02-07 18:05:21,685 [INFO] Joining base_url: https://example.com/scripts/ with url: user.js
2025-02-07 18:05:21,685 [INFO] Joined URL: https://example.com/scripts/user.js
2025-02-07 18:05:21,689 [INFO] Starting _process_content
2025-02-07 18:05:21,696 [INFO] Starting _process_content
2025-02-07 18:05:21,702 [INFO] Starting _process_content
2025-02-07 18:05:21,705 [INFO] Starting _process_content
2025-02-07 18:05:21,713 [INFO] Starting _process_content
2025-02-07 18:05:21,718 [INFO] Starting _process_content
2025-02-07 18:05:21,722 [INFO] Starting _process_content
2025-02-07 18:05:21,731 [INFO] Starting _process_content
2025-02-07 18:05:37,052 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 18:05:38,553 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:38,555 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:38,561 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:38,564 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:38,568 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:38,570 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:38,573 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:05:38,578 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:38,581 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:38,584 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:05:38,586 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:38,596 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:38,602 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:38,610 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:38,616 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:38,621 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:38,637 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:38,645 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:38,649 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:38,652 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:38,655 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:05:38,657 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:05:38,772 [INFO] Starting _process_content
2025-02-07 18:05:38,787 [INFO] Starting _process_content
2025-02-07 18:05:38,812 [INFO] Starting _process_content
2025-02-07 18:05:38,827 [INFO] Starting _process_content
2025-02-07 18:05:38,836 [INFO] Starting _process_content
2025-02-07 18:05:38,847 [INFO] Starting _process_content
2025-02-07 18:05:38,852 [INFO] Starting _process_content
2025-02-07 18:05:38,863 [INFO] Starting _process_content
2025-02-07 18:05:38,872 [INFO] Starting _process_content
2025-02-07 18:05:38,887 [INFO] Starting _process_content
2025-02-07 18:05:38,899 [INFO] Starting _process_content
2025-02-07 18:05:38,905 [INFO] Starting _process_content
2025-02-07 18:05:38,911 [INFO] Joining base_url: https://example.com/scripts/ with url: app.js
2025-02-07 18:05:38,912 [INFO] Joined URL: https://example.com/scripts/app.js
2025-02-07 18:05:38,912 [INFO] Joining base_url: https://example.com/scripts/ with url: user.js
2025-02-07 18:05:38,912 [INFO] Joined URL: https://example.com/scripts/user.js
2025-02-07 18:05:38,917 [INFO] Starting _process_content
2025-02-07 18:05:38,922 [INFO] Starting _process_content
2025-02-07 18:05:38,931 [INFO] Starting _process_content
2025-02-07 18:05:38,936 [INFO] Starting _process_content
2025-02-07 18:05:38,947 [INFO] Starting _process_content
2025-02-07 18:05:38,954 [INFO] Starting _process_content
2025-02-07 18:05:38,963 [INFO] Starting _process_content
2025-02-07 18:05:38,970 [INFO] Starting _process_content
2025-02-07 18:57:48,263 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 18:58:10,239 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 18:58:11,907 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:11,909 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:11,911 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:11,918 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:11,921 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:11,923 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:11,925 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:11,926 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:11,928 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:11,933 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:11,936 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:11,938 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:11,944 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:11,950 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:11,954 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:11,956 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:11,964 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:11,968 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:11,973 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:11,975 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:11,977 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:11,978 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:14,840 [INFO] Operation took 0.11 seconds
2025-02-07 18:58:15,039 [DEBUG] IDNA encoding failed for example.com@evil.com: Codepoint U+0040 at position 4 of 'com@evil' not allowed
2025-02-07 18:58:15,199 [DEBUG] IDNA encoding failed for user@example.com: Codepoint U+0040 at position 5 of 'user@example' not allowed
2025-02-07 18:58:15,199 [DEBUG] IDNA encoding failed for :pass@example.com: Empty domain
2025-02-07 18:58:15,266 [DEBUG] IDNA encoding failed for .example.com: Empty Label
2025-02-07 18:58:15,266 [DEBUG] IDNA encoding failed for example..com: Empty Label
2025-02-07 18:58:15,267 [DEBUG] IDNA encoding failed for example.com..: Empty Label
2025-02-07 18:58:15,605 [INFO] test_operation took 0.10 seconds
2025-02-07 18:58:15,806 [INFO] inner took 0.10 seconds
2025-02-07 18:58:15,806 [INFO] outer took 0.20 seconds
2025-02-07 18:58:15,806 [INFO] error_operation took 0.00 seconds
2025-02-07 18:58:15,999 [INFO] test_operation took 0.00 seconds
2025-02-07 18:58:16,662 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 18:58:16,663 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:16,669 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 18:58:16,670 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 18:58:16,670 [INFO] Starting crawl of URL: https://example.com
2025-02-07 18:58:17,147 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 18:58:17,148 [DEBUG] Base URL: https://example.com
2025-02-07 18:58:17,148 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 18:58:17,149 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 18:58:17,149 [DEBUG] Initial domain set to: example.com
2025-02-07 18:58:17,149 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 18:58:17,150 [INFO] Successfully crawled https://example.com
2025-02-07 18:58:17,151 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:17,153 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x00000217CCF7EA50>
2025-02-07 18:58:17,154 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000217CD479BB0>, 280147.4076166)])']
connector: <aiohttp.connector.TCPConnector object at 0x00000217CCF7CEC0>
2025-02-07 18:58:17,155 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 18:58:17,156 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:17,159 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 18:58:17,160 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 18:58:17,160 [INFO] Starting crawl of URL: https://example.com
2025-02-07 18:58:17,234 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2025-02-07 18:58:17,234 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2025-02-07 18:58:17,234 [INFO] Starting crawl of URL: https://example.org
2025-02-07 18:58:17,564 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 18:58:17,565 [DEBUG] Base URL: https://example.com
2025-02-07 18:58:17,565 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 18:58:17,565 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 18:58:17,565 [DEBUG] Initial domain set to: example.com
2025-02-07 18:58:17,565 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 18:58:17,565 [INFO] Successfully crawled https://example.com
2025-02-07 18:58:17,909 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 18:58:17,910 [DEBUG] Base URL: https://example.org
2025-02-07 18:58:17,910 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 18:58:17,910 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 18:58:17,910 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 18:58:17,911 [INFO] Successfully crawled https://example.org
2025-02-07 18:58:17,913 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:17,915 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x00000217CCF43890>
2025-02-07 18:58:17,915 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000217CD478D70>, 280147.8255323)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000217CCFD4050>, 280148.1712642)])']
connector: <aiohttp.connector.TCPConnector object at 0x00000217CCF43610>
2025-02-07 18:58:17,916 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 18:58:17,916 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:17,918 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 18:58:17,918 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 18:58:17,918 [INFO] Starting crawl of URL: https://example.com
2025-02-07 18:58:18,271 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 18:58:18,271 [DEBUG] Base URL: https://example.com
2025-02-07 18:58:18,271 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 18:58:18,271 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 18:58:18,271 [DEBUG] Initial domain set to: example.com
2025-02-07 18:58:18,271 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 18:58:18,271 [INFO] Successfully crawled https://example.com
2025-02-07 18:58:18,273 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:18,275 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x00000217CCF43610>
2025-02-07 18:58:18,275 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000217CD4F8290>, 280148.5331306)])']
connector: <aiohttp.connector.TCPConnector object at 0x00000217CC312D50>
2025-02-07 18:58:18,276 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 18:58:18,277 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:18,279 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 18:58:18,280 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 18:58:18,280 [INFO] Starting crawl of URL: https://example.com
2025-02-07 18:58:18,736 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 18:58:18,736 [DEBUG] Base URL: https://example.com
2025-02-07 18:58:18,737 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 18:58:18,737 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 18:58:18,737 [DEBUG] Initial domain set to: example.com
2025-02-07 18:58:18,737 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 18:58:18,737 [INFO] Successfully crawled https://example.com
2025-02-07 18:58:18,739 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 18:58:18,740 [DEBUG] Base URL: https://example.com
2025-02-07 18:58:18,740 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 18:58:18,740 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 18:58:18,741 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 18:58:18,790 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:18,792 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:18,793 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:18,797 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:18,799 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 18:58:18,800 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:18,801 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 18:58:18,802 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 18:58:18,802 [INFO] Starting crawl of URL: https://example.com
2025-02-07 18:58:19,207 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 18:58:19,207 [DEBUG] Base URL: https://example.com
2025-02-07 18:58:19,207 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 18:58:19,207 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 18:58:19,207 [DEBUG] Initial domain set to: example.com
2025-02-07 18:58:19,208 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 18:58:19,208 [INFO] Successfully crawled https://example.com
2025-02-07 18:58:19,210 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:19,212 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x00000217CCF42210>
2025-02-07 18:58:19,212 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000217CD4FA990>, 280149.468161)])']
connector: <aiohttp.connector.TCPConnector object at 0x00000217CD428E90>
2025-02-07 18:58:19,213 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 18:58:19,213 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:19,215 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2025-02-07 18:58:19,215 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2025-02-07 18:58:19,215 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2025-02-07 18:58:21,321 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [getaddrinfo failed]
2025-02-07 18:58:21,323 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [getaddrinfo failed]
2025-02-07 18:58:21,323 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [getaddrinfo failed]
2025-02-07 18:58:21,341 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:21,344 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 18:58:21,345 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:21,348 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 18:58:21,348 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 18:58:21,348 [INFO] Starting crawl of URL: https://example.com
2025-02-07 18:58:21,412 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2025-02-07 18:58:21,412 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2025-02-07 18:58:21,412 [INFO] Starting crawl of URL: https://example.org
2025-02-07 18:58:21,413 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2025-02-07 18:58:21,413 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2025-02-07 18:58:21,413 [INFO] Starting crawl of URL: https://example.net
2025-02-07 18:58:21,701 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 18:58:21,701 [DEBUG] Base URL: https://example.com
2025-02-07 18:58:21,702 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 18:58:21,702 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 18:58:21,702 [DEBUG] Initial domain set to: example.com
2025-02-07 18:58:21,702 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 18:58:21,702 [INFO] Successfully crawled https://example.com
2025-02-07 18:58:22,082 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 18:58:22,082 [DEBUG] Base URL: https://example.org
2025-02-07 18:58:22,083 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 18:58:22,083 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 18:58:22,083 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 18:58:22,083 [INFO] Successfully crawled https://example.org
2025-02-07 18:58:22,646 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 18:58:22,646 [DEBUG] Base URL: https://example.net
2025-02-07 18:58:22,647 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 18:58:22,647 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 18:58:22,647 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 18:58:22,647 [INFO] Successfully crawled https://example.net
2025-02-07 18:58:22,654 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:22,655 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x00000217CCF41310>
2025-02-07 18:58:22,656 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000217CD4F87D0>, 280151.9637138)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000217CD4F84D0>, 280152.3439523)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000217CD4F8B30>, 280152.9076348)])']
connector: <aiohttp.connector.TCPConnector object at 0x00000217CD428D60>
2025-02-07 18:58:22,657 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 18:58:22,658 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:22,661 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 18:58:22,661 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 18:58:22,662 [INFO] Starting crawl of URL: https://example.com
2025-02-07 18:58:23,054 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 18:58:23,054 [DEBUG] Base URL: https://example.com
2025-02-07 18:58:23,054 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 18:58:23,054 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 18:58:23,055 [DEBUG] Initial domain set to: example.com
2025-02-07 18:58:23,055 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 18:58:23,055 [INFO] Successfully crawled https://example.com
2025-02-07 18:58:23,057 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:23,072 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 18:58:23,072 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:23,076 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:23,994 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 18:58:24,064 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:24,066 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 18:58:24,067 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:24,072 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 18:58:24,137 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:24,140 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 18:58:24,141 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:24,143 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 18:58:24,143 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2025-02-07 18:58:24,143 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2025-02-07 18:58:24,143 [INFO] Starting crawl of URL: https://example.com/page
2025-02-07 18:58:24,648 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 18:58:24,648 [DEBUG] Base URL: https://example.com/page
2025-02-07 18:58:24,648 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 18:58:24,648 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 18:58:24,649 [DEBUG] Initial domain set to: example.com
2025-02-07 18:58:24,649 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 18:58:24,649 [INFO] Successfully crawled https://example.com/page
2025-02-07 18:58:24,649 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2025-02-07 18:58:24,649 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2025-02-07 18:58:24,649 [INFO] Starting crawl of URL: https://other-domain.com/page
2025-02-07 18:58:27,196 [DEBUG] 
Evaluating link: ./
2025-02-07 18:58:27,196 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,196 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2025-02-07 18:58:27,196 [DEBUG] Absolute link: https://other-domain.com/
2025-02-07 18:58:27,196 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,197 [DEBUG] 
Evaluating link: ./
2025-02-07 18:58:27,197 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,197 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2025-02-07 18:58:27,197 [DEBUG] Absolute link: https://other-domain.com/
2025-02-07 18:58:27,197 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,197 [DEBUG] 
Evaluating link: ./hiyou.php
2025-02-07 18:58:27,197 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,197 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2025-02-07 18:58:27,197 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2025-02-07 18:58:27,198 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,199 [DEBUG] 
Evaluating link: ./date.php
2025-02-07 18:58:27,199 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,199 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2025-02-07 18:58:27,199 [DEBUG] Absolute link: https://other-domain.com/date.php
2025-02-07 18:58:27,200 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,200 [DEBUG] 
Evaluating link: ./contact.php
2025-02-07 18:58:27,200 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,201 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2025-02-07 18:58:27,201 [DEBUG] Absolute link: https://other-domain.com/contact.php
2025-02-07 18:58:27,201 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,201 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 18:58:27,202 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,202 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 18:58:27,202 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 18:58:27,203 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2025-02-07 18:58:27,345 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,356 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-07 18:58:27,356 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 18:58:27,356 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,357 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 18:58:27,357 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 18:58:27,357 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-07 18:58:27,357 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-07 18:58:27,358 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,358 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 18:58:27,358 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 18:58:27,358 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-07 18:58:27,359 [DEBUG] 
Evaluating link: ./contact
2025-02-07 18:58:27,359 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,359 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2025-02-07 18:58:27,359 [DEBUG] Absolute link: https://other-domain.com/contact
2025-02-07 18:58:27,360 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,366 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,379 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-07 18:58:27,379 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 18:58:27,380 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,380 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 18:58:27,380 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 18:58:27,380 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-07 18:58:27,381 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-07 18:58:27,383 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,383 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 18:58:27,383 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 18:58:27,383 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-07 18:58:27,390 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,400 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2025-02-07 18:58:27,400 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 18:58:27,401 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,401 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 18:58:27,401 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 18:58:27,401 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2025-02-07 18:58:27,402 [DEBUG] 
Evaluating link: #indexes
2025-02-07 18:58:27,402 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,403 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2025-02-07 18:58:27,404 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,404 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,404 [DEBUG] 
Evaluating link: #a
2025-02-07 18:58:27,405 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,405 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2025-02-07 18:58:27,405 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,405 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,406 [DEBUG] 
Evaluating link: #b
2025-02-07 18:58:27,406 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,406 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2025-02-07 18:58:27,406 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,406 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,406 [DEBUG] 
Evaluating link: #c
2025-02-07 18:58:27,407 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,407 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2025-02-07 18:58:27,407 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,407 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,407 [DEBUG] 
Evaluating link: #d
2025-02-07 18:58:27,408 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,408 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2025-02-07 18:58:27,408 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,408 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,408 [DEBUG] 
Evaluating link: #e
2025-02-07 18:58:27,408 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,409 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2025-02-07 18:58:27,409 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,409 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,409 [DEBUG] 
Evaluating link: #f
2025-02-07 18:58:27,409 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,410 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2025-02-07 18:58:27,410 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,410 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,410 [DEBUG] 
Evaluating link: #g
2025-02-07 18:58:27,411 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,411 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2025-02-07 18:58:27,411 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,411 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,411 [DEBUG] 
Evaluating link: #h
2025-02-07 18:58:27,412 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,412 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2025-02-07 18:58:27,412 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,412 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,413 [DEBUG] 
Evaluating link: #i
2025-02-07 18:58:27,413 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,413 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2025-02-07 18:58:27,413 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,414 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,416 [DEBUG] 
Evaluating link: #j
2025-02-07 18:58:27,418 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,419 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2025-02-07 18:58:27,422 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,422 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,423 [DEBUG] 
Evaluating link: #k
2025-02-07 18:58:27,423 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,423 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2025-02-07 18:58:27,423 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,424 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,424 [DEBUG] 
Evaluating link: #l
2025-02-07 18:58:27,424 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,424 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2025-02-07 18:58:27,424 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,425 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,425 [DEBUG] 
Evaluating link: #m
2025-02-07 18:58:27,425 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,425 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2025-02-07 18:58:27,425 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,425 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,425 [DEBUG] 
Evaluating link: #n
2025-02-07 18:58:27,426 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,426 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2025-02-07 18:58:27,426 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,426 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,426 [DEBUG] 
Evaluating link: #o
2025-02-07 18:58:27,426 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,426 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2025-02-07 18:58:27,426 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,427 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,427 [DEBUG] 
Evaluating link: #p
2025-02-07 18:58:27,427 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,427 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2025-02-07 18:58:27,427 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,427 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,427 [DEBUG] 
Evaluating link: #q
2025-02-07 18:58:27,427 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,427 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2025-02-07 18:58:27,427 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,428 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,428 [DEBUG] 
Evaluating link: #r
2025-02-07 18:58:27,428 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,428 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2025-02-07 18:58:27,428 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,428 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,428 [DEBUG] 
Evaluating link: #s
2025-02-07 18:58:27,428 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,428 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2025-02-07 18:58:27,429 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,429 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,429 [DEBUG] 
Evaluating link: #t
2025-02-07 18:58:27,429 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,429 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2025-02-07 18:58:27,429 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,429 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,429 [DEBUG] 
Evaluating link: #u
2025-02-07 18:58:27,429 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,430 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2025-02-07 18:58:27,430 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,430 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,430 [DEBUG] 
Evaluating link: #v
2025-02-07 18:58:27,430 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,430 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2025-02-07 18:58:27,430 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,430 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,430 [DEBUG] 
Evaluating link: #w
2025-02-07 18:58:27,430 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,432 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2025-02-07 18:58:27,433 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,433 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,433 [DEBUG] 
Evaluating link: #x
2025-02-07 18:58:27,433 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,434 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2025-02-07 18:58:27,434 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,435 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,435 [DEBUG] 
Evaluating link: #y
2025-02-07 18:58:27,435 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,435 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2025-02-07 18:58:27,435 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,435 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,436 [DEBUG] 
Evaluating link: #z
2025-02-07 18:58:27,436 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,437 [DEBUG] Normalized URL: https://other-domain.com/page#z -> https://other-domain.com/page
2025-02-07 18:58:27,438 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,438 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,438 [DEBUG] 
Evaluating link: #A
2025-02-07 18:58:27,438 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,438 [DEBUG] Normalized URL: https://other-domain.com/page#A -> https://other-domain.com/page
2025-02-07 18:58:27,439 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,439 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,439 [DEBUG] 
Evaluating link: #B
2025-02-07 18:58:27,439 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,439 [DEBUG] Normalized URL: https://other-domain.com/page#B -> https://other-domain.com/page
2025-02-07 18:58:27,439 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,439 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,439 [DEBUG] 
Evaluating link: #C
2025-02-07 18:58:27,439 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,439 [DEBUG] Normalized URL: https://other-domain.com/page#C -> https://other-domain.com/page
2025-02-07 18:58:27,440 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,440 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,440 [DEBUG] 
Evaluating link: #D
2025-02-07 18:58:27,440 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,440 [DEBUG] Normalized URL: https://other-domain.com/page#D -> https://other-domain.com/page
2025-02-07 18:58:27,440 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,440 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,443 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,450 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-07 18:58:27,450 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 18:58:27,450 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,450 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 18:58:27,450 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 18:58:27,451 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-07 18:58:27,451 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-07 18:58:27,451 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,451 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 18:58:27,452 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 18:58:27,452 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-07 18:58:27,456 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,461 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2025-02-07 18:58:27,461 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 18:58:27,461 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,461 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 18:58:27,461 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 18:58:27,461 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2025-02-07 18:58:27,462 [DEBUG] 
Evaluating link: https://spriq.jp/
2025-02-07 18:58:27,462 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,462 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2025-02-07 18:58:27,462 [DEBUG] Absolute link: https://spriq.jp/
2025-02-07 18:58:27,462 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2025-02-07 18:58:27,466 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,474 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-07 18:58:27,476 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,483 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-07 18:58:27,483 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 18:58:27,484 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,484 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 18:58:27,484 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 18:58:27,484 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-07 18:58:27,484 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-07 18:58:27,485 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,485 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 18:58:27,486 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 18:58:27,486 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-07 18:58:27,486 [DEBUG] 
Evaluating link: ./contact
2025-02-07 18:58:27,487 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,487 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2025-02-07 18:58:27,487 [DEBUG] Absolute link: https://other-domain.com/contact
2025-02-07 18:58:27,487 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,487 [DEBUG] 
Evaluating link: ./
2025-02-07 18:58:27,487 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,487 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2025-02-07 18:58:27,487 [DEBUG] Absolute link: https://other-domain.com/
2025-02-07 18:58:27,488 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,488 [DEBUG] 
Evaluating link: ./hiyou.php
2025-02-07 18:58:27,488 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,488 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2025-02-07 18:58:27,488 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2025-02-07 18:58:27,488 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,488 [DEBUG] 
Evaluating link: ./date.php
2025-02-07 18:58:27,488 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,488 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2025-02-07 18:58:27,488 [DEBUG] Absolute link: https://other-domain.com/date.php
2025-02-07 18:58:27,489 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,489 [DEBUG] 
Evaluating link: #z
2025-02-07 18:58:27,489 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,489 [DEBUG] Normalized URL: https://other-domain.com/page#z -> https://other-domain.com/page
2025-02-07 18:58:27,489 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,489 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,489 [DEBUG] 
Evaluating link: #A
2025-02-07 18:58:27,489 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,489 [DEBUG] Normalized URL: https://other-domain.com/page#A -> https://other-domain.com/page
2025-02-07 18:58:27,489 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,490 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,490 [DEBUG] 
Evaluating link: #B
2025-02-07 18:58:27,490 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,490 [DEBUG] Normalized URL: https://other-domain.com/page#B -> https://other-domain.com/page
2025-02-07 18:58:27,490 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,490 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,490 [DEBUG] 
Evaluating link: #C
2025-02-07 18:58:27,490 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,490 [DEBUG] Normalized URL: https://other-domain.com/page#C -> https://other-domain.com/page
2025-02-07 18:58:27,491 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,491 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,491 [DEBUG] 
Evaluating link: #D
2025-02-07 18:58:27,491 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,491 [DEBUG] Normalized URL: https://other-domain.com/page#D -> https://other-domain.com/page
2025-02-07 18:58:27,491 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,491 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,491 [DEBUG] 
Evaluating link: #u
2025-02-07 18:58:27,491 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,491 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2025-02-07 18:58:27,491 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,492 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,492 [DEBUG] 
Evaluating link: #v
2025-02-07 18:58:27,492 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,492 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2025-02-07 18:58:27,492 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,492 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,492 [DEBUG] 
Evaluating link: #w
2025-02-07 18:58:27,492 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,492 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2025-02-07 18:58:27,492 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,493 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,493 [DEBUG] 
Evaluating link: #x
2025-02-07 18:58:27,493 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,493 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2025-02-07 18:58:27,493 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,493 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,493 [DEBUG] 
Evaluating link: #y
2025-02-07 18:58:27,493 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,493 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2025-02-07 18:58:27,493 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,494 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,494 [DEBUG] 
Evaluating link: #p
2025-02-07 18:58:27,494 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,494 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2025-02-07 18:58:27,494 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,494 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,494 [DEBUG] 
Evaluating link: #q
2025-02-07 18:58:27,494 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,494 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2025-02-07 18:58:27,494 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,495 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,495 [DEBUG] 
Evaluating link: #r
2025-02-07 18:58:27,495 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,495 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2025-02-07 18:58:27,495 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,495 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,495 [DEBUG] 
Evaluating link: #s
2025-02-07 18:58:27,495 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,495 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2025-02-07 18:58:27,495 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,496 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,496 [DEBUG] 
Evaluating link: #t
2025-02-07 18:58:27,496 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,496 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2025-02-07 18:58:27,496 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,496 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,496 [DEBUG] 
Evaluating link: #k
2025-02-07 18:58:27,496 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,496 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2025-02-07 18:58:27,496 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,497 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,497 [DEBUG] 
Evaluating link: #l
2025-02-07 18:58:27,497 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,497 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2025-02-07 18:58:27,497 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,497 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,497 [DEBUG] 
Evaluating link: #m
2025-02-07 18:58:27,498 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,498 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2025-02-07 18:58:27,499 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,499 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,499 [DEBUG] 
Evaluating link: #n
2025-02-07 18:58:27,499 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,499 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2025-02-07 18:58:27,499 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,499 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,499 [DEBUG] 
Evaluating link: #o
2025-02-07 18:58:27,500 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,500 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2025-02-07 18:58:27,500 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,500 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,500 [DEBUG] 
Evaluating link: #f
2025-02-07 18:58:27,500 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,501 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2025-02-07 18:58:27,501 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,501 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,501 [DEBUG] 
Evaluating link: #g
2025-02-07 18:58:27,501 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,501 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2025-02-07 18:58:27,501 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,501 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,501 [DEBUG] 
Evaluating link: #h
2025-02-07 18:58:27,502 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,502 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2025-02-07 18:58:27,502 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,502 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,502 [DEBUG] 
Evaluating link: #i
2025-02-07 18:58:27,502 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,502 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2025-02-07 18:58:27,503 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,504 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,505 [DEBUG] 
Evaluating link: #j
2025-02-07 18:58:27,505 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,505 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2025-02-07 18:58:27,506 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,506 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,506 [DEBUG] 
Evaluating link: #b
2025-02-07 18:58:27,506 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,506 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2025-02-07 18:58:27,507 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,507 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,507 [DEBUG] 
Evaluating link: #c
2025-02-07 18:58:27,507 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,508 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2025-02-07 18:58:27,508 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,508 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,508 [DEBUG] 
Evaluating link: #d
2025-02-07 18:58:27,508 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,509 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2025-02-07 18:58:27,509 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,509 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,509 [DEBUG] 
Evaluating link: #e
2025-02-07 18:58:27,509 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,510 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2025-02-07 18:58:27,510 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,510 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,510 [DEBUG] 
Evaluating link: #site
2025-02-07 18:58:27,511 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 18:58:27,511 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2025-02-07 18:58:27,511 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 18:58:27,511 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 18:58:27,512 [INFO] Successfully crawled https://other-domain.com/page
2025-02-07 18:58:27,527 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:27,529 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 18:58:27,529 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:27,531 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2025-02-07 18:58:27,532 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2025-02-07 18:58:27,532 [INFO] Starting crawl of URL: https://example.com/page0
2025-02-07 18:58:28,541 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 18:58:28,541 [DEBUG] Base URL: https://example.com/page0
2025-02-07 18:58:28,541 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 18:58:28,541 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 18:58:28,541 [DEBUG] Initial domain set to: example.com
2025-02-07 18:58:28,541 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 18:58:28,542 [INFO] Successfully crawled https://example.com/page0
2025-02-07 18:58:28,542 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2025-02-07 18:58:28,542 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2025-02-07 18:58:28,542 [INFO] Starting crawl of URL: https://example.com/page1
2025-02-07 18:58:28,851 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 18:58:28,852 [DEBUG] Base URL: https://example.com/page1
2025-02-07 18:58:28,852 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 18:58:28,853 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 18:58:28,853 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 18:58:28,854 [INFO] Successfully crawled https://example.com/page1
2025-02-07 18:58:28,854 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2025-02-07 18:58:28,854 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2025-02-07 18:58:28,854 [INFO] Starting crawl of URL: https://example.com/page2
2025-02-07 18:58:29,887 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 18:58:29,887 [DEBUG] Base URL: https://example.com/page2
2025-02-07 18:58:29,887 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 18:58:29,887 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 18:58:29,887 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 18:58:29,887 [INFO] Successfully crawled https://example.com/page2
2025-02-07 18:58:29,888 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2025-02-07 18:58:29,888 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2025-02-07 18:58:29,888 [INFO] Starting crawl of URL: https://example.com/page3
2025-02-07 18:58:30,359 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 18:58:30,359 [DEBUG] Base URL: https://example.com/page3
2025-02-07 18:58:30,359 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 18:58:30,360 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 18:58:30,360 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 18:58:30,360 [INFO] Successfully crawled https://example.com/page3
2025-02-07 18:58:30,360 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2025-02-07 18:58:30,360 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2025-02-07 18:58:30,360 [INFO] Starting crawl of URL: https://example.com/page4
2025-02-07 18:58:31,117 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 18:58:31,118 [DEBUG] Base URL: https://example.com/page4
2025-02-07 18:58:31,118 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 18:58:31,118 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 18:58:31,118 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 18:58:31,119 [INFO] Successfully crawled https://example.com/page4
2025-02-07 18:58:31,119 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 18:58:31,119 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2025-02-07 18:58:31,120 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2025-02-07 18:58:31,120 [INFO] Starting crawl of URL: https://example.com/page0
2025-02-07 18:58:31,514 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 18:58:31,514 [DEBUG] Base URL: https://example.com/page0
2025-02-07 18:58:31,514 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 18:58:31,515 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 18:58:31,515 [DEBUG] Initial domain set to: example.com
2025-02-07 18:58:31,515 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 18:58:31,516 [INFO] Successfully crawled https://example.com/page0
2025-02-07 18:58:31,516 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2025-02-07 18:58:31,517 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2025-02-07 18:58:31,517 [INFO] Starting crawl of URL: https://example.com/page1
2025-02-07 18:58:31,792 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 18:58:31,793 [DEBUG] Base URL: https://example.com/page1
2025-02-07 18:58:31,793 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 18:58:31,793 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 18:58:31,793 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 18:58:31,793 [INFO] Successfully crawled https://example.com/page1
2025-02-07 18:58:31,794 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2025-02-07 18:58:31,794 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2025-02-07 18:58:31,795 [INFO] Starting crawl of URL: https://example.com/page2
2025-02-07 18:58:32,309 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 18:58:32,309 [DEBUG] Base URL: https://example.com/page2
2025-02-07 18:58:32,310 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 18:58:32,310 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 18:58:32,310 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 18:58:32,311 [INFO] Successfully crawled https://example.com/page2
2025-02-07 18:58:32,311 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2025-02-07 18:58:32,311 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2025-02-07 18:58:32,311 [INFO] Starting crawl of URL: https://example.com/page3
2025-02-07 18:58:32,849 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 18:58:32,849 [DEBUG] Base URL: https://example.com/page3
2025-02-07 18:58:32,850 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 18:58:32,850 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 18:58:32,850 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 18:58:32,850 [INFO] Successfully crawled https://example.com/page3
2025-02-07 18:58:32,850 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2025-02-07 18:58:32,850 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2025-02-07 18:58:32,850 [INFO] Starting crawl of URL: https://example.com/page4
2025-02-07 18:58:33,340 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 18:58:33,340 [DEBUG] Base URL: https://example.com/page4
2025-02-07 18:58:33,340 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 18:58:33,340 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 18:58:33,341 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 18:58:33,341 [INFO] Successfully crawled https://example.com/page4
2025-02-07 18:58:33,361 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:33,368 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:33,424 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:33,429 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:33,463 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:33,480 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:33,506 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:33,509 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:33,539 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x00000217CCF43250>
2025-02-07 18:58:33,540 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000217CD4F9790>, 280148.9987163)])']
connector: <aiohttp.connector.TCPConnector object at 0x00000217CCF7B820>
2025-02-07 18:58:33,540 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x00000217CCF43ED0>
2025-02-07 18:58:33,541 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x00000217CC312D50>
2025-02-07 18:58:33,541 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000217CD479730>, 280154.9104776)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000217CD478EF0>, 280155.6667399)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000217CD4792B0>, 280156.3142)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000217CD479430>, 280157.4253004)])']
connector: <aiohttp.connector.TCPConnector object at 0x00000217CD4288A0>
2025-02-07 18:58:33,547 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x00000217CD454F50>
2025-02-07 18:58:33,555 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000217CD403350>, 280161.3782522)])']
connector: <aiohttp.connector.TCPConnector object at 0x00000217CD429BA0>
2025-02-07 18:58:33,556 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x00000217CD455590>
2025-02-07 18:58:33,556 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000217CD8687D0>, 280163.6025248)])']
connector: <aiohttp.connector.TCPConnector object at 0x00000217CD429CD0>
2025-02-07 18:58:33,586 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:33,590 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:33,636 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:33,639 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:33,662 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:33,664 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:33,694 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:33,698 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:33,718 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:33,720 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:33,739 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:33,823 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:33,824 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:33,830 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:33,836 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:33,837 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:33,839 [DEBUG] 
Evaluating link: https://example.com/doc1
2025-02-07 18:58:33,839 [DEBUG] Target domain: example.com
2025-02-07 18:58:33,840 [DEBUG] URL domain: example.com
2025-02-07 18:58:33,840 [DEBUG] URL approved for crawling: https://example.com/doc1
2025-02-07 18:58:33,840 [DEBUG] 
Evaluating link: https://example.com/excluded/page
2025-02-07 18:58:33,840 [DEBUG] Target domain: example.com
2025-02-07 18:58:33,840 [DEBUG] URL domain: example.com
2025-02-07 18:58:33,840 [DEBUG] URL approved for crawling: https://example.com/excluded/page
2025-02-07 18:58:33,884 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:33,886 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:33,887 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:33,953 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:34,007 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:34,014 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:34,017 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-07 18:58:34,072 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:34,074 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:34,079 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:34,115 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:34,118 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:34,119 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:34,122 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-07 18:58:34,198 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:34,200 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:34,201 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:34,203 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-07 18:58:34,240 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:34,247 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:34,248 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:34,250 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-07 18:58:34,286 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:34,288 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:34,289 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:34,293 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-07 18:58:34,334 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:34,336 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:34,337 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:34,340 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-07 18:58:34,380 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:34,386 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:34,386 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:34,388 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-07 18:58:34,445 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:34,456 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:34,987 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:34,989 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:34,995 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:35,032 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:35,034 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:35,037 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:35,083 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:35,090 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:35,190 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:35,230 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:35,258 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:35,289 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:35,320 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x00000217CCF43C50>
2025-02-07 18:58:35,320 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000217CD4A5310>, 280164.8985442)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000217CD4A6A50>, 280165.1273313)])']
connector: <aiohttp.connector.TCPConnector object at 0x00000217CD42B100>
2025-02-07 18:58:35,351 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:35,401 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:35,471 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:35,532 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:35,587 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:36,518 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:37,442 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:37,446 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 18:58:37,446 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 18:58:37,446 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 18:58:37,446 [INFO] Starting crawl of URL: https://example.com
2025-02-07 18:58:37,792 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 18:58:37,792 [DEBUG] Base URL: https://example.com
2025-02-07 18:58:37,792 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 18:58:37,792 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 18:58:37,792 [DEBUG] Initial domain set to: example.com
2025-02-07 18:58:37,793 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 18:58:37,793 [INFO] Successfully crawled https://example.com
2025-02-07 18:58:37,793 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x00000217CD904E10>
2025-02-07 18:58:37,793 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000217CE360230>, 280168.0537036)])']
connector: <aiohttp.connector.TCPConnector object at 0x00000217CD4BC770>
2025-02-07 18:58:37,796 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2025-02-07 18:58:38,147 [INFO] Operation took 0.10 seconds
2025-02-07 18:58:39,050 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 18:58:39,050 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:39,051 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:46,776 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-6\test_full_site_crawl0\test_docs/index.html
2025-02-07 18:58:46,776 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-07 18:58:46,840 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:46,856 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 18:58:46,856 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:46,857 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:54,318 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-6\test_content_processing_pipeli0\test_docs/guide.html
2025-02-07 18:58:54,319 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-07 18:58:54,370 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:58:54,386 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 18:58:54,387 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:58:54,387 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:59:01,828 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-6\test_quality_checks0\test_docs/api.html
2025-02-07 18:59:01,828 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-07 18:59:01,858 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:59:01,869 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 18:59:01,870 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:59:01,871 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:59:09,278 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-6\test_document_organization0\test_docs/index.html
2025-02-07 18:59:09,279 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-07 18:59:09,305 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:59:09,319 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 18:59:09,319 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:59:09,320 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:59:16,744 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-6\test_search_functionality0\test_docs/index.html
2025-02-07 18:59:16,744 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-07 18:59:16,774 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:59:16,787 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 18:59:16,788 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:59:16,789 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:59:24,200 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-6\test_error_handling_and_recove0\test_docs/index.html
2025-02-07 18:59:24,201 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-07 18:59:24,262 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:59:24,272 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:59:24,355 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:59:24,376 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:59:24,386 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:59:24,387 [ERROR] Error during crawl: 'str' object has no attribute 'url'
2025-02-07 18:59:24,537 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:59:24,540 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:59:24,603 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:59:24,607 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:59:24,613 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 18:59:24,662 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:59:28,135 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:59:28,185 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:59:28,187 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:59:28,202 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:59:28,205 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:59:28,228 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:59:28,231 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:59:28,250 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:59:28,252 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:59:28,271 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:59:28,273 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:59:28,303 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:59:28,306 [DEBUG] Using proactor: IocpProactor
2025-02-07 18:59:28,337 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:04:47,331 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 19:04:48,999 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:04:49,002 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:04:49,007 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:04:49,017 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:04:49,051 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:04:49,055 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:04:49,065 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:04:49,071 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:04:49,080 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:04:49,083 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:04:49,085 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:04:49,087 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:04:49,099 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:04:49,102 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:04:49,132 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:04:49,135 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:04:49,146 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:04:49,149 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:04:49,157 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:04:49,167 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:04:49,170 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:04:49,181 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:32:35,913 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 19:32:48,135 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 19:33:01,517 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 19:33:52,033 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 19:33:53,386 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:33:53,389 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:33:53,394 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:33:53,448 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:33:53,452 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:33:53,455 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:33:53,461 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:33:53,464 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:33:53,467 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:33:53,472 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:33:53,478 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:33:53,480 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:33:53,485 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:33:53,489 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:33:53,496 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:33:53,501 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:33:53,506 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:33:53,507 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:33:53,510 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:33:53,513 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:33:53,516 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:33:53,518 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:29,430 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 19:34:30,744 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:30,746 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:30,750 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:30,753 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:30,756 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:30,759 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:30,761 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:34:30,763 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:30,765 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:30,769 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:34:30,771 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:30,775 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:30,778 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:30,782 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:30,788 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:30,792 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:30,795 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:30,798 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:30,804 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:30,807 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:30,810 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:34:30,811 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:51,536 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 19:34:52,669 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:52,671 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:52,674 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:52,677 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:52,681 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:52,684 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:52,686 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:34:52,687 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:52,691 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:52,698 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:34:52,700 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:52,702 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:52,707 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:52,711 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:52,716 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:52,719 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:52,725 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:52,740 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:52,745 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:52,748 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:34:52,751 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:34:52,755 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:10,903 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 19:35:12,006 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:12,008 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:12,012 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:12,016 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:12,020 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:12,023 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:12,025 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:35:12,028 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:12,031 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:12,034 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:35:12,036 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:12,039 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:12,042 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:12,046 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:12,049 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:12,054 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:12,058 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:12,063 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:12,069 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:12,072 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:12,075 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:35:12,079 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:33,480 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 19:35:34,403 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:34,407 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:34,412 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:34,415 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:34,420 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:34,424 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:34,427 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:35:34,432 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:34,434 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:34,437 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:35:34,443 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:34,445 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:34,452 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:34,457 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:34,461 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:34,465 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:34,470 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:34,474 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:34,483 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:34,487 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:35:34,493 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:35:34,498 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:04,279 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 19:36:05,232 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:05,234 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:05,237 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:05,240 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:05,242 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:05,247 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:05,250 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:36:05,251 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:05,254 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:05,258 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:36:05,262 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:05,264 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:05,270 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:05,273 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:05,278 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:05,281 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:05,288 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:05,292 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:05,299 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:05,305 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:05,308 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:36:05,312 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:48,343 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 19:36:49,277 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:49,279 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:49,282 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:49,285 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:49,290 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:49,292 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:49,293 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:36:49,295 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:49,296 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:49,299 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:36:49,301 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:49,304 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:49,308 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:49,309 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:49,312 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:49,315 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:49,317 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:49,320 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:49,324 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:49,329 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:36:49,331 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:36:49,332 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:29,868 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 19:37:30,826 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:30,828 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:30,832 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:30,835 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:30,838 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:30,839 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:30,841 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:37:30,843 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:30,845 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:30,849 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:37:30,851 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:30,853 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:30,856 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:30,859 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:30,862 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:30,864 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:30,868 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:30,871 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:30,876 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:30,877 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:30,880 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:37:30,883 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:49,849 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 19:37:50,736 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:50,738 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:50,741 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:50,745 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:50,748 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:50,750 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:50,752 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:37:50,753 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:50,756 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:50,758 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:37:50,760 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:50,763 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:50,768 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:50,771 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:50,773 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:50,775 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:50,780 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:50,782 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:50,787 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:50,789 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:37:50,791 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:37:50,792 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:03,351 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 19:38:04,272 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:04,274 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:04,277 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:04,279 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:04,284 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:04,286 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:04,288 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:38:04,289 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:04,291 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:04,294 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:38:04,296 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:04,298 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:04,303 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:04,307 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:04,310 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:04,312 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:04,316 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:04,319 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:04,321 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:04,323 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:04,325 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:38:04,328 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:27,790 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 19:38:28,746 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:28,748 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:28,751 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:28,754 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:28,757 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:28,759 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:28,761 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:38:28,763 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:28,765 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:28,768 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:38:28,770 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:28,772 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:28,777 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:28,779 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:28,782 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:28,784 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:28,787 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:28,789 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:28,793 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:28,795 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:38:28,797 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:38:28,798 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:03,770 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 19:39:04,774 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:04,777 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:04,780 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:04,783 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:04,786 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:04,788 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:04,790 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:39:04,793 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:04,795 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:04,797 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:39:04,799 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:04,801 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:04,805 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:04,807 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:04,811 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:04,816 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:04,819 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:04,821 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:04,824 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:04,830 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:04,832 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:39:04,833 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:32,984 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 19:39:33,892 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:33,894 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:33,897 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:33,900 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:33,903 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:33,905 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:33,907 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:39:33,908 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:33,910 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:33,914 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:39:33,917 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:33,919 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:33,925 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:33,927 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:33,929 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:33,934 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:33,937 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:33,940 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:33,943 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:33,945 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:39:33,949 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:39:33,952 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:40:14,416 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 19:40:15,375 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:40:15,378 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:40:15,381 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:40:15,385 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:40:15,389 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:40:15,391 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:40:15,394 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:40:15,396 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:40:15,398 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:40:15,401 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:40:15,404 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:40:15,407 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:40:15,413 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:40:15,419 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:40:15,421 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:40:15,424 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:40:15,430 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:40:15,435 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:40:15,438 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:40:15,441 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:40:15,444 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:40:15,445 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:40:59,560 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 19:41:00,501 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:00,503 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:00,506 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:00,509 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:00,512 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:00,513 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:00,517 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:41:00,519 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:00,521 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:00,523 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:41:00,525 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:00,527 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:00,531 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:00,533 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:00,536 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:00,539 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:00,543 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:00,545 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:00,547 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:00,550 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:00,553 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:41:00,555 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:17,475 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 19:41:18,379 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:18,381 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:18,386 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:18,388 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:18,392 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:18,393 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:18,395 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:41:18,397 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:18,399 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:18,402 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:41:18,405 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:18,407 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:18,410 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:18,413 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:18,416 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:18,419 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:18,425 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:18,427 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:18,429 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:18,431 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:18,436 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:41:18,437 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:56,433 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 19:41:57,337 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:57,339 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:57,342 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:57,345 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:57,349 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:57,351 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:57,353 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:41:57,354 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:57,356 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:57,358 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:41:57,360 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:57,363 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:57,368 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:57,370 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:57,373 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:57,375 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:57,378 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:57,381 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:57,384 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:57,387 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:41:57,390 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:41:57,391 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:16,365 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 19:42:17,287 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:17,289 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:17,292 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:17,297 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:17,301 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:17,303 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:17,305 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:42:17,307 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:17,309 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:17,312 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:42:17,315 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:17,321 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:17,325 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:17,329 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:17,336 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:17,339 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:17,342 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:17,346 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:17,351 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:17,355 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:17,358 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:42:17,361 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:37,599 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 19:42:38,475 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:38,478 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:38,481 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:38,484 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:38,487 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:38,489 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:38,493 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:42:38,494 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:38,496 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:38,498 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:42:38,501 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:38,502 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:38,506 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:38,509 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:38,511 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:38,516 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:38,519 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:38,520 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:38,524 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:38,528 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:42:38,530 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:42:38,531 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:13,875 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 19:43:14,734 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:14,736 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:14,739 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:14,743 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:14,746 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:14,748 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:14,749 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:43:14,751 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:14,753 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:14,755 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:43:14,759 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:14,761 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:14,764 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:14,766 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:14,769 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:14,771 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:14,775 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:14,777 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:14,782 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:14,784 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:14,786 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:43:14,788 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:37,523 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 19:43:38,421 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:38,423 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:38,426 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:38,429 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:38,432 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:38,434 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:38,437 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:43:38,439 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:38,440 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:38,443 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:43:38,445 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:38,447 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:38,451 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:38,454 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:38,457 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:38,460 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:38,463 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:38,465 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:38,469 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:38,472 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:43:38,473 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:43:38,475 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:45:52,997 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 19:46:17,148 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 19:46:18,743 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:18,745 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:18,748 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:18,754 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:18,758 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:18,761 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:18,763 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:46:18,765 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:18,770 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:18,773 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:46:18,779 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:18,782 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:18,789 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:18,796 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:18,799 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:18,806 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:18,814 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:18,817 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:18,823 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:18,826 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:18,828 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:46:18,830 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:20,579 [INFO] Operation took 0.13 seconds
2025-02-07 19:46:20,781 [DEBUG] IDNA encoding failed for example.com@evil.com: Codepoint U+0040 at position 4 of 'com@evil' not allowed
2025-02-07 19:46:20,978 [DEBUG] IDNA encoding failed for user@example.com: Codepoint U+0040 at position 5 of 'user@example' not allowed
2025-02-07 19:46:20,979 [DEBUG] IDNA encoding failed for :pass@example.com: Empty domain
2025-02-07 19:46:21,463 [DEBUG] IDNA encoding failed for .example.com: Empty Label
2025-02-07 19:46:21,463 [DEBUG] IDNA encoding failed for example..com: Empty Label
2025-02-07 19:46:21,466 [DEBUG] IDNA encoding failed for example.com..: Empty Label
2025-02-07 19:46:22,041 [INFO] test_operation took 0.11 seconds
2025-02-07 19:46:22,285 [INFO] inner took 0.10 seconds
2025-02-07 19:46:22,285 [INFO] outer took 0.24 seconds
2025-02-07 19:46:22,285 [INFO] error_operation took 0.00 seconds
2025-02-07 19:46:22,454 [INFO] test_operation took 0.00 seconds
2025-02-07 19:46:22,870 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 19:46:22,870 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:22,875 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 19:46:22,876 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 19:46:22,876 [INFO] Starting crawl of URL: https://example.com
2025-02-07 19:46:23,248 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 19:46:23,249 [DEBUG] Base URL: https://example.com
2025-02-07 19:46:23,249 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 19:46:23,249 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 19:46:23,250 [DEBUG] Initial domain set to: example.com
2025-02-07 19:46:23,250 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 19:46:23,250 [INFO] Successfully crawled https://example.com
2025-02-07 19:46:23,252 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:23,253 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x00000259593702F0>
2025-02-07 19:46:23,253 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000259593FC050>, 283033.5083718)])']
connector: <aiohttp.connector.TCPConnector object at 0x00000259591D3E00>
2025-02-07 19:46:23,255 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 19:46:23,255 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:23,257 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 19:46:23,258 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 19:46:23,258 [INFO] Starting crawl of URL: https://example.com
2025-02-07 19:46:23,315 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2025-02-07 19:46:23,316 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2025-02-07 19:46:23,317 [INFO] Starting crawl of URL: https://example.org
2025-02-07 19:46:23,606 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 19:46:23,607 [DEBUG] Base URL: https://example.com
2025-02-07 19:46:23,607 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 19:46:23,607 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 19:46:23,607 [DEBUG] Initial domain set to: example.com
2025-02-07 19:46:23,607 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 19:46:23,607 [INFO] Successfully crawled https://example.com
2025-02-07 19:46:24,126 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 19:46:24,127 [DEBUG] Base URL: https://example.org
2025-02-07 19:46:24,127 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 19:46:24,127 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 19:46:24,127 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 19:46:24,128 [INFO] Successfully crawled https://example.org
2025-02-07 19:46:24,186 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:24,188 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x00000259592EE0D0>
2025-02-07 19:46:24,188 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000259593FCE30>, 283033.8688572)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000259593CC170>, 283034.3878267)])']
connector: <aiohttp.connector.TCPConnector object at 0x00000259592ED6D0>
2025-02-07 19:46:24,196 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 19:46:24,197 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:24,200 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 19:46:24,200 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 19:46:24,200 [INFO] Starting crawl of URL: https://example.com
2025-02-07 19:46:24,599 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 19:46:24,599 [DEBUG] Base URL: https://example.com
2025-02-07 19:46:24,600 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 19:46:24,600 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 19:46:24,600 [DEBUG] Initial domain set to: example.com
2025-02-07 19:46:24,600 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 19:46:24,601 [INFO] Successfully crawled https://example.com
2025-02-07 19:46:24,650 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:24,651 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x00000259592EDE50>
2025-02-07 19:46:24,652 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000259593FD3D0>, 283034.860294)])']
connector: <aiohttp.connector.TCPConnector object at 0x00000259592EE210>
2025-02-07 19:46:24,700 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 19:46:24,701 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:24,734 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 19:46:24,734 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 19:46:24,735 [INFO] Starting crawl of URL: https://example.com
2025-02-07 19:46:25,128 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 19:46:25,129 [DEBUG] Base URL: https://example.com
2025-02-07 19:46:25,129 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 19:46:25,129 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 19:46:25,130 [DEBUG] Initial domain set to: example.com
2025-02-07 19:46:25,130 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 19:46:25,131 [INFO] Successfully crawled https://example.com
2025-02-07 19:46:25,133 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 19:46:25,133 [DEBUG] Base URL: https://example.com
2025-02-07 19:46:25,133 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 19:46:25,133 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 19:46:25,134 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 19:46:25,192 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:25,204 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:46:25,205 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:25,210 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:25,251 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 19:46:25,252 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:25,255 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 19:46:25,255 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 19:46:25,255 [INFO] Starting crawl of URL: https://example.com
2025-02-07 19:46:25,609 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 19:46:25,609 [DEBUG] Base URL: https://example.com
2025-02-07 19:46:25,610 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 19:46:25,610 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 19:46:25,610 [DEBUG] Initial domain set to: example.com
2025-02-07 19:46:25,610 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 19:46:25,611 [INFO] Successfully crawled https://example.com
2025-02-07 19:46:25,614 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:25,617 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x00000259592EDBD0>
2025-02-07 19:46:25,617 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000259593FF290>, 283035.8717343)])']
connector: <aiohttp.connector.TCPConnector object at 0x0000025959285F30>
2025-02-07 19:46:25,619 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 19:46:25,620 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:25,622 [DEBUG] Using raw URL string: https://invalid-url-that-does-not-exist.com (type: <class 'str'>)
2025-02-07 19:46:25,622 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='invalid-url-that-does-not-exist.com', path='', params='', query='', fragment='')
2025-02-07 19:46:25,622 [INFO] Starting crawl of URL: https://invalid-url-that-does-not-exist.com
2025-02-07 19:46:27,743 [WARNING] Retry 1/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [getaddrinfo failed]
2025-02-07 19:46:27,745 [WARNING] Retry 2/2 for https://invalid-url-that-does-not-exist.com: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [getaddrinfo failed]
2025-02-07 19:46:27,745 [ERROR] Error crawling https://invalid-url-that-does-not-exist.com: Failed after 2 retries: Cannot connect to host invalid-url-that-does-not-exist.com:443 ssl:default [getaddrinfo failed]
2025-02-07 19:46:27,756 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x00000259592EDE50>
2025-02-07 19:46:27,756 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000259593FD1F0>, 283035.3891052)])']
connector: <aiohttp.connector.TCPConnector object at 0x0000025959285220>
2025-02-07 19:46:27,768 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:27,816 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 19:46:27,817 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:27,820 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 19:46:27,821 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 19:46:27,821 [INFO] Starting crawl of URL: https://example.com
2025-02-07 19:46:27,972 [DEBUG] Using raw URL string: https://example.org (type: <class 'str'>)
2025-02-07 19:46:27,972 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.org', path='', params='', query='', fragment='')
2025-02-07 19:46:27,973 [INFO] Starting crawl of URL: https://example.org
2025-02-07 19:46:27,973 [DEBUG] Using raw URL string: https://example.net (type: <class 'str'>)
2025-02-07 19:46:27,973 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.net', path='', params='', query='', fragment='')
2025-02-07 19:46:27,974 [INFO] Starting crawl of URL: https://example.net
2025-02-07 19:46:28,250 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 19:46:28,250 [DEBUG] Base URL: https://example.com
2025-02-07 19:46:28,250 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 19:46:28,251 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 19:46:28,251 [DEBUG] Initial domain set to: example.com
2025-02-07 19:46:28,251 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 19:46:28,251 [INFO] Successfully crawled https://example.com
2025-02-07 19:46:28,751 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 19:46:28,751 [DEBUG] Base URL: https://example.org
2025-02-07 19:46:28,752 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 19:46:28,752 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 19:46:28,752 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 19:46:28,752 [INFO] Successfully crawled https://example.org
2025-02-07 19:46:29,322 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 19:46:29,323 [DEBUG] Base URL: https://example.net
2025-02-07 19:46:29,323 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 19:46:29,324 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 19:46:29,324 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 19:46:29,325 [INFO] Successfully crawled https://example.net
2025-02-07 19:46:29,345 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:29,347 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x0000025959116990>
2025-02-07 19:46:29,348 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000259596671D0>, 283038.5109289)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000259596641D0>, 283039.0128498)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000259596642F0>, 283039.5841327)])']
connector: <aiohttp.connector.TCPConnector object at 0x0000025959285810>
2025-02-07 19:46:29,349 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 19:46:29,350 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:29,353 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 19:46:29,353 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 19:46:29,354 [INFO] Starting crawl of URL: https://example.com
2025-02-07 19:46:29,707 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 19:46:29,707 [DEBUG] Base URL: https://example.com
2025-02-07 19:46:29,707 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 19:46:29,708 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 19:46:29,708 [DEBUG] Initial domain set to: example.com
2025-02-07 19:46:29,708 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 19:46:29,708 [INFO] Successfully crawled https://example.com
2025-02-07 19:46:29,713 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:29,782 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x00000259592ED950>
2025-02-07 19:46:29,800 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 19:46:29,800 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:29,803 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:46:31,177 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 19:46:31,333 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:31,343 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 19:46:31,344 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:31,386 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 19:46:31,731 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:31,765 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 19:46:31,805 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:31,808 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 19:46:31,811 [DEBUG] Using raw URL string: https://example.com/page (type: <class 'str'>)
2025-02-07 19:46:31,812 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page', params='', query='', fragment='')
2025-02-07 19:46:31,812 [INFO] Starting crawl of URL: https://example.com/page
2025-02-07 19:46:32,446 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 19:46:32,446 [DEBUG] Base URL: https://example.com/page
2025-02-07 19:46:32,446 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 19:46:32,446 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 19:46:32,446 [DEBUG] Initial domain set to: example.com
2025-02-07 19:46:32,446 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 19:46:32,447 [INFO] Successfully crawled https://example.com/page
2025-02-07 19:46:32,447 [DEBUG] Using raw URL string: https://other-domain.com/page (type: <class 'str'>)
2025-02-07 19:46:32,447 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='other-domain.com', path='/page', params='', query='', fragment='')
2025-02-07 19:46:32,447 [INFO] Starting crawl of URL: https://other-domain.com/page
2025-02-07 19:46:34,163 [DEBUG] 
Evaluating link: ./
2025-02-07 19:46:34,163 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,163 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2025-02-07 19:46:34,163 [DEBUG] Absolute link: https://other-domain.com/
2025-02-07 19:46:34,163 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,163 [DEBUG] 
Evaluating link: ./
2025-02-07 19:46:34,163 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,164 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2025-02-07 19:46:34,164 [DEBUG] Absolute link: https://other-domain.com/
2025-02-07 19:46:34,164 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,164 [DEBUG] 
Evaluating link: ./hiyou.php
2025-02-07 19:46:34,164 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,164 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2025-02-07 19:46:34,164 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2025-02-07 19:46:34,164 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,164 [DEBUG] 
Evaluating link: ./date.php
2025-02-07 19:46:34,164 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,165 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2025-02-07 19:46:34,165 [DEBUG] Absolute link: https://other-domain.com/date.php
2025-02-07 19:46:34,165 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,165 [DEBUG] 
Evaluating link: ./contact.php
2025-02-07 19:46:34,165 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,165 [DEBUG] Normalized URL: https://other-domain.com/contact.php -> https://other-domain.com/contact.php
2025-02-07 19:46:34,165 [DEBUG] Absolute link: https://other-domain.com/contact.php
2025-02-07 19:46:34,165 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,165 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 19:46:34,165 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,166 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 19:46:34,166 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 19:46:34,166 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2025-02-07 19:46:34,325 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,330 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-07 19:46:34,330 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 19:46:34,330 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,331 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 19:46:34,331 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 19:46:34,331 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-07 19:46:34,331 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-07 19:46:34,331 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,331 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 19:46:34,331 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 19:46:34,331 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-07 19:46:34,331 [DEBUG] 
Evaluating link: ./contact
2025-02-07 19:46:34,332 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,332 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2025-02-07 19:46:34,332 [DEBUG] Absolute link: https://other-domain.com/contact
2025-02-07 19:46:34,332 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,335 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,344 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-07 19:46:34,344 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 19:46:34,344 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,344 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 19:46:34,344 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 19:46:34,344 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-07 19:46:34,344 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-07 19:46:34,345 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,345 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 19:46:34,345 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 19:46:34,345 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-07 19:46:34,348 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,356 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2025-02-07 19:46:34,357 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 19:46:34,357 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,357 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 19:46:34,357 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 19:46:34,357 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2025-02-07 19:46:34,357 [DEBUG] 
Evaluating link: #indexes
2025-02-07 19:46:34,357 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,357 [DEBUG] Normalized URL: https://other-domain.com/page#indexes -> https://other-domain.com/page
2025-02-07 19:46:34,357 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,358 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,358 [DEBUG] 
Evaluating link: #a
2025-02-07 19:46:34,358 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,358 [DEBUG] Normalized URL: https://other-domain.com/page#a -> https://other-domain.com/page
2025-02-07 19:46:34,358 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,358 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,358 [DEBUG] 
Evaluating link: #b
2025-02-07 19:46:34,358 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,358 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2025-02-07 19:46:34,358 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,359 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,359 [DEBUG] 
Evaluating link: #c
2025-02-07 19:46:34,359 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,359 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2025-02-07 19:46:34,359 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,359 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,359 [DEBUG] 
Evaluating link: #d
2025-02-07 19:46:34,359 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,359 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2025-02-07 19:46:34,360 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,360 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,360 [DEBUG] 
Evaluating link: #e
2025-02-07 19:46:34,360 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,360 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2025-02-07 19:46:34,360 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,360 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,360 [DEBUG] 
Evaluating link: #f
2025-02-07 19:46:34,360 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,360 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2025-02-07 19:46:34,361 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,361 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,361 [DEBUG] 
Evaluating link: #g
2025-02-07 19:46:34,361 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,361 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2025-02-07 19:46:34,361 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,361 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,361 [DEBUG] 
Evaluating link: #h
2025-02-07 19:46:34,361 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,362 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2025-02-07 19:46:34,362 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,362 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,362 [DEBUG] 
Evaluating link: #i
2025-02-07 19:46:34,362 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,362 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2025-02-07 19:46:34,362 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,362 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,362 [DEBUG] 
Evaluating link: #j
2025-02-07 19:46:34,362 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,363 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2025-02-07 19:46:34,363 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,363 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,363 [DEBUG] 
Evaluating link: #k
2025-02-07 19:46:34,363 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,363 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2025-02-07 19:46:34,363 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,363 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,363 [DEBUG] 
Evaluating link: #l
2025-02-07 19:46:34,363 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,364 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2025-02-07 19:46:34,364 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,364 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,364 [DEBUG] 
Evaluating link: #m
2025-02-07 19:46:34,364 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,364 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2025-02-07 19:46:34,364 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,364 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,364 [DEBUG] 
Evaluating link: #n
2025-02-07 19:46:34,364 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,365 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2025-02-07 19:46:34,365 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,365 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,365 [DEBUG] 
Evaluating link: #o
2025-02-07 19:46:34,365 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,365 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2025-02-07 19:46:34,365 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,365 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,365 [DEBUG] 
Evaluating link: #p
2025-02-07 19:46:34,365 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,366 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2025-02-07 19:46:34,366 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,366 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,366 [DEBUG] 
Evaluating link: #q
2025-02-07 19:46:34,366 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,366 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2025-02-07 19:46:34,366 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,366 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,366 [DEBUG] 
Evaluating link: #r
2025-02-07 19:46:34,367 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,367 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2025-02-07 19:46:34,367 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,367 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,367 [DEBUG] 
Evaluating link: #s
2025-02-07 19:46:34,367 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,367 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2025-02-07 19:46:34,367 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,367 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,367 [DEBUG] 
Evaluating link: #t
2025-02-07 19:46:34,368 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,368 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2025-02-07 19:46:34,368 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,368 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,368 [DEBUG] 
Evaluating link: #u
2025-02-07 19:46:34,368 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,370 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2025-02-07 19:46:34,371 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,371 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,372 [DEBUG] 
Evaluating link: #v
2025-02-07 19:46:34,372 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,372 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2025-02-07 19:46:34,372 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,372 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,373 [DEBUG] 
Evaluating link: #w
2025-02-07 19:46:34,373 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,373 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2025-02-07 19:46:34,373 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,373 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,373 [DEBUG] 
Evaluating link: #x
2025-02-07 19:46:34,373 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,373 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2025-02-07 19:46:34,373 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,374 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,374 [DEBUG] 
Evaluating link: #y
2025-02-07 19:46:34,374 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,374 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2025-02-07 19:46:34,374 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,374 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,374 [DEBUG] 
Evaluating link: #z
2025-02-07 19:46:34,374 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,374 [DEBUG] Normalized URL: https://other-domain.com/page#z -> https://other-domain.com/page
2025-02-07 19:46:34,375 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,375 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,375 [DEBUG] 
Evaluating link: #A
2025-02-07 19:46:34,375 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,375 [DEBUG] Normalized URL: https://other-domain.com/page#A -> https://other-domain.com/page
2025-02-07 19:46:34,375 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,375 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,375 [DEBUG] 
Evaluating link: #B
2025-02-07 19:46:34,375 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,375 [DEBUG] Normalized URL: https://other-domain.com/page#B -> https://other-domain.com/page
2025-02-07 19:46:34,376 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,376 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,376 [DEBUG] 
Evaluating link: #C
2025-02-07 19:46:34,376 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,376 [DEBUG] Normalized URL: https://other-domain.com/page#C -> https://other-domain.com/page
2025-02-07 19:46:34,376 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,376 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,376 [DEBUG] 
Evaluating link: #D
2025-02-07 19:46:34,376 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,376 [DEBUG] Normalized URL: https://other-domain.com/page#D -> https://other-domain.com/page
2025-02-07 19:46:34,377 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,377 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,379 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,385 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-07 19:46:34,385 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 19:46:34,385 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,387 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 19:46:34,387 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 19:46:34,387 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-07 19:46:34,387 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-07 19:46:34,387 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,387 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 19:46:34,388 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 19:46:34,388 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-07 19:46:34,391 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,396 [DEBUG] Skipping different subdomain: getpocket.com (initial: example.com)
2025-02-07 19:46:34,396 [DEBUG] 
Evaluating link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 19:46:34,397 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,397 [DEBUG] Normalized URL: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/ -> https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 19:46:34,397 [DEBUG] Absolute link: https://timeline.line.me/social-plugin/share?url=https://www.other-domain.com/
2025-02-07 19:46:34,397 [DEBUG] Skipping different subdomain: timeline.line.me (initial: example.com)
2025-02-07 19:46:34,397 [DEBUG] 
Evaluating link: https://spriq.jp/
2025-02-07 19:46:34,397 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,397 [DEBUG] Normalized URL: https://spriq.jp/ -> https://spriq.jp/
2025-02-07 19:46:34,397 [DEBUG] Absolute link: https://spriq.jp/
2025-02-07 19:46:34,397 [DEBUG] Skipping different subdomain: spriq.jp (initial: example.com)
2025-02-07 19:46:34,400 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,409 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-07 19:46:34,411 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,417 [DEBUG] Skipping different subdomain: twitter.com (initial: example.com)
2025-02-07 19:46:34,417 [DEBUG] 
Evaluating link: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 19:46:34,417 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,417 [DEBUG] Normalized URL: https://www.facebook.com/sharer/sharer.php?u=https://www.other-domain.com/ -> https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 19:46:34,417 [DEBUG] Absolute link: https://facebook.com/sharer/sharer.php?u=https://www.other-domain.com/
2025-02-07 19:46:34,417 [DEBUG] Skipping different subdomain: facebook.com (initial: example.com)
2025-02-07 19:46:34,418 [DEBUG] 
Evaluating link: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/
2025-02-07 19:46:34,418 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,418 [DEBUG] Normalized URL: https://b.hatena.ne.jp/entry/panel/?mode=confirm&url=https://www.other-domain.com/ -> https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 19:46:34,418 [DEBUG] Absolute link: https://b.hatena.ne.jp/entry/panel?mode=confirm&url=https://www.other-domain.com/
2025-02-07 19:46:34,418 [DEBUG] Skipping different subdomain: b.hatena.ne.jp (initial: example.com)
2025-02-07 19:46:34,418 [DEBUG] 
Evaluating link: ./contact
2025-02-07 19:46:34,420 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,421 [DEBUG] Normalized URL: https://other-domain.com/contact -> https://other-domain.com/contact
2025-02-07 19:46:34,421 [DEBUG] Absolute link: https://other-domain.com/contact
2025-02-07 19:46:34,422 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,422 [DEBUG] 
Evaluating link: ./
2025-02-07 19:46:34,423 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,423 [DEBUG] Normalized URL: https://other-domain.com/ -> https://other-domain.com/
2025-02-07 19:46:34,423 [DEBUG] Absolute link: https://other-domain.com/
2025-02-07 19:46:34,423 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,423 [DEBUG] 
Evaluating link: ./hiyou.php
2025-02-07 19:46:34,423 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,424 [DEBUG] Normalized URL: https://other-domain.com/hiyou.php -> https://other-domain.com/hiyou.php
2025-02-07 19:46:34,424 [DEBUG] Absolute link: https://other-domain.com/hiyou.php
2025-02-07 19:46:34,424 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,424 [DEBUG] 
Evaluating link: ./date.php
2025-02-07 19:46:34,424 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,424 [DEBUG] Normalized URL: https://other-domain.com/date.php -> https://other-domain.com/date.php
2025-02-07 19:46:34,425 [DEBUG] Absolute link: https://other-domain.com/date.php
2025-02-07 19:46:34,425 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,425 [DEBUG] 
Evaluating link: #z
2025-02-07 19:46:34,425 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,425 [DEBUG] Normalized URL: https://other-domain.com/page#z -> https://other-domain.com/page
2025-02-07 19:46:34,425 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,425 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,426 [DEBUG] 
Evaluating link: #A
2025-02-07 19:46:34,426 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,426 [DEBUG] Normalized URL: https://other-domain.com/page#A -> https://other-domain.com/page
2025-02-07 19:46:34,426 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,426 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,426 [DEBUG] 
Evaluating link: #B
2025-02-07 19:46:34,426 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,426 [DEBUG] Normalized URL: https://other-domain.com/page#B -> https://other-domain.com/page
2025-02-07 19:46:34,427 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,427 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,427 [DEBUG] 
Evaluating link: #C
2025-02-07 19:46:34,427 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,427 [DEBUG] Normalized URL: https://other-domain.com/page#C -> https://other-domain.com/page
2025-02-07 19:46:34,427 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,428 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,428 [DEBUG] 
Evaluating link: #D
2025-02-07 19:46:34,428 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,428 [DEBUG] Normalized URL: https://other-domain.com/page#D -> https://other-domain.com/page
2025-02-07 19:46:34,428 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,428 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,428 [DEBUG] 
Evaluating link: #u
2025-02-07 19:46:34,428 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,429 [DEBUG] Normalized URL: https://other-domain.com/page#u -> https://other-domain.com/page
2025-02-07 19:46:34,429 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,429 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,429 [DEBUG] 
Evaluating link: #v
2025-02-07 19:46:34,429 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,429 [DEBUG] Normalized URL: https://other-domain.com/page#v -> https://other-domain.com/page
2025-02-07 19:46:34,429 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,430 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,430 [DEBUG] 
Evaluating link: #w
2025-02-07 19:46:34,430 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,430 [DEBUG] Normalized URL: https://other-domain.com/page#w -> https://other-domain.com/page
2025-02-07 19:46:34,430 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,430 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,430 [DEBUG] 
Evaluating link: #x
2025-02-07 19:46:34,431 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,431 [DEBUG] Normalized URL: https://other-domain.com/page#x -> https://other-domain.com/page
2025-02-07 19:46:34,431 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,431 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,431 [DEBUG] 
Evaluating link: #y
2025-02-07 19:46:34,431 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,431 [DEBUG] Normalized URL: https://other-domain.com/page#y -> https://other-domain.com/page
2025-02-07 19:46:34,432 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,432 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,432 [DEBUG] 
Evaluating link: #p
2025-02-07 19:46:34,432 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,432 [DEBUG] Normalized URL: https://other-domain.com/page#p -> https://other-domain.com/page
2025-02-07 19:46:34,432 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,432 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,432 [DEBUG] 
Evaluating link: #q
2025-02-07 19:46:34,433 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,433 [DEBUG] Normalized URL: https://other-domain.com/page#q -> https://other-domain.com/page
2025-02-07 19:46:34,433 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,433 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,433 [DEBUG] 
Evaluating link: #r
2025-02-07 19:46:34,433 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,433 [DEBUG] Normalized URL: https://other-domain.com/page#r -> https://other-domain.com/page
2025-02-07 19:46:34,433 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,433 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,434 [DEBUG] 
Evaluating link: #s
2025-02-07 19:46:34,434 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,434 [DEBUG] Normalized URL: https://other-domain.com/page#s -> https://other-domain.com/page
2025-02-07 19:46:34,434 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,434 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,434 [DEBUG] 
Evaluating link: #t
2025-02-07 19:46:34,435 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,437 [DEBUG] Normalized URL: https://other-domain.com/page#t -> https://other-domain.com/page
2025-02-07 19:46:34,438 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,438 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,438 [DEBUG] 
Evaluating link: #k
2025-02-07 19:46:34,439 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,439 [DEBUG] Normalized URL: https://other-domain.com/page#k -> https://other-domain.com/page
2025-02-07 19:46:34,440 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,440 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,440 [DEBUG] 
Evaluating link: #l
2025-02-07 19:46:34,440 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,440 [DEBUG] Normalized URL: https://other-domain.com/page#l -> https://other-domain.com/page
2025-02-07 19:46:34,441 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,441 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,441 [DEBUG] 
Evaluating link: #m
2025-02-07 19:46:34,441 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,441 [DEBUG] Normalized URL: https://other-domain.com/page#m -> https://other-domain.com/page
2025-02-07 19:46:34,441 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,441 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,442 [DEBUG] 
Evaluating link: #n
2025-02-07 19:46:34,442 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,442 [DEBUG] Normalized URL: https://other-domain.com/page#n -> https://other-domain.com/page
2025-02-07 19:46:34,442 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,442 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,443 [DEBUG] 
Evaluating link: #o
2025-02-07 19:46:34,443 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,443 [DEBUG] Normalized URL: https://other-domain.com/page#o -> https://other-domain.com/page
2025-02-07 19:46:34,443 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,444 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,444 [DEBUG] 
Evaluating link: #f
2025-02-07 19:46:34,444 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,444 [DEBUG] Normalized URL: https://other-domain.com/page#f -> https://other-domain.com/page
2025-02-07 19:46:34,445 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,445 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,445 [DEBUG] 
Evaluating link: #g
2025-02-07 19:46:34,445 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,445 [DEBUG] Normalized URL: https://other-domain.com/page#g -> https://other-domain.com/page
2025-02-07 19:46:34,446 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,446 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,446 [DEBUG] 
Evaluating link: #h
2025-02-07 19:46:34,446 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,447 [DEBUG] Normalized URL: https://other-domain.com/page#h -> https://other-domain.com/page
2025-02-07 19:46:34,447 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,447 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,448 [DEBUG] 
Evaluating link: #i
2025-02-07 19:46:34,448 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,448 [DEBUG] Normalized URL: https://other-domain.com/page#i -> https://other-domain.com/page
2025-02-07 19:46:34,448 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,449 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,449 [DEBUG] 
Evaluating link: #j
2025-02-07 19:46:34,449 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,449 [DEBUG] Normalized URL: https://other-domain.com/page#j -> https://other-domain.com/page
2025-02-07 19:46:34,449 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,450 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,450 [DEBUG] 
Evaluating link: #b
2025-02-07 19:46:34,450 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,450 [DEBUG] Normalized URL: https://other-domain.com/page#b -> https://other-domain.com/page
2025-02-07 19:46:34,450 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,450 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,450 [DEBUG] 
Evaluating link: #c
2025-02-07 19:46:34,450 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,450 [DEBUG] Normalized URL: https://other-domain.com/page#c -> https://other-domain.com/page
2025-02-07 19:46:34,450 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,451 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,451 [DEBUG] 
Evaluating link: #d
2025-02-07 19:46:34,451 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,451 [DEBUG] Normalized URL: https://other-domain.com/page#d -> https://other-domain.com/page
2025-02-07 19:46:34,451 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,451 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,451 [DEBUG] 
Evaluating link: #e
2025-02-07 19:46:34,451 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,453 [DEBUG] Normalized URL: https://other-domain.com/page#e -> https://other-domain.com/page
2025-02-07 19:46:34,454 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,454 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,455 [DEBUG] 
Evaluating link: #site
2025-02-07 19:46:34,455 [DEBUG] Base URL: https://other-domain.com/page
2025-02-07 19:46:34,456 [DEBUG] Normalized URL: https://other-domain.com/page#site -> https://other-domain.com/page
2025-02-07 19:46:34,456 [DEBUG] Absolute link: https://other-domain.com/page
2025-02-07 19:46:34,456 [DEBUG] Skipping different subdomain: other-domain.com (initial: example.com)
2025-02-07 19:46:34,457 [INFO] Successfully crawled https://other-domain.com/page
2025-02-07 19:46:34,471 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:34,476 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 19:46:34,477 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:34,479 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2025-02-07 19:46:34,479 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2025-02-07 19:46:34,480 [INFO] Starting crawl of URL: https://example.com/page0
2025-02-07 19:46:35,438 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 19:46:35,438 [DEBUG] Base URL: https://example.com/page0
2025-02-07 19:46:35,438 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 19:46:35,439 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 19:46:35,439 [DEBUG] Initial domain set to: example.com
2025-02-07 19:46:35,439 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 19:46:35,439 [INFO] Successfully crawled https://example.com/page0
2025-02-07 19:46:35,439 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2025-02-07 19:46:35,439 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2025-02-07 19:46:35,439 [INFO] Starting crawl of URL: https://example.com/page1
2025-02-07 19:46:35,759 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 19:46:35,760 [DEBUG] Base URL: https://example.com/page1
2025-02-07 19:46:35,760 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 19:46:35,760 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 19:46:35,760 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 19:46:35,760 [INFO] Successfully crawled https://example.com/page1
2025-02-07 19:46:35,760 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2025-02-07 19:46:35,760 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2025-02-07 19:46:35,760 [INFO] Starting crawl of URL: https://example.com/page2
2025-02-07 19:46:36,222 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 19:46:36,222 [DEBUG] Base URL: https://example.com/page2
2025-02-07 19:46:36,222 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 19:46:36,222 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 19:46:36,222 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 19:46:36,223 [INFO] Successfully crawled https://example.com/page2
2025-02-07 19:46:36,223 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2025-02-07 19:46:36,223 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2025-02-07 19:46:36,223 [INFO] Starting crawl of URL: https://example.com/page3
2025-02-07 19:46:37,218 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 19:46:37,218 [DEBUG] Base URL: https://example.com/page3
2025-02-07 19:46:37,218 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 19:46:37,219 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 19:46:37,219 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 19:46:37,219 [INFO] Successfully crawled https://example.com/page3
2025-02-07 19:46:37,220 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2025-02-07 19:46:37,220 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2025-02-07 19:46:37,220 [INFO] Starting crawl of URL: https://example.com/page4
2025-02-07 19:46:37,501 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 19:46:37,501 [DEBUG] Base URL: https://example.com/page4
2025-02-07 19:46:37,502 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 19:46:37,502 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 19:46:37,503 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 19:46:37,503 [INFO] Successfully crawled https://example.com/page4
2025-02-07 19:46:37,503 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=3, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 19:46:37,503 [DEBUG] Using raw URL string: https://example.com/page0 (type: <class 'str'>)
2025-02-07 19:46:37,504 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page0', params='', query='', fragment='')
2025-02-07 19:46:37,504 [INFO] Starting crawl of URL: https://example.com/page0
2025-02-07 19:46:37,827 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 19:46:37,827 [DEBUG] Base URL: https://example.com/page0
2025-02-07 19:46:37,827 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 19:46:37,827 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 19:46:37,827 [DEBUG] Initial domain set to: example.com
2025-02-07 19:46:37,827 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 19:46:37,828 [INFO] Successfully crawled https://example.com/page0
2025-02-07 19:46:37,828 [DEBUG] Using raw URL string: https://example.com/page1 (type: <class 'str'>)
2025-02-07 19:46:37,828 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page1', params='', query='', fragment='')
2025-02-07 19:46:37,828 [INFO] Starting crawl of URL: https://example.com/page1
2025-02-07 19:46:38,170 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 19:46:38,170 [DEBUG] Base URL: https://example.com/page1
2025-02-07 19:46:38,170 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 19:46:38,170 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 19:46:38,170 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 19:46:38,171 [INFO] Successfully crawled https://example.com/page1
2025-02-07 19:46:38,171 [DEBUG] Using raw URL string: https://example.com/page2 (type: <class 'str'>)
2025-02-07 19:46:38,171 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page2', params='', query='', fragment='')
2025-02-07 19:46:38,171 [INFO] Starting crawl of URL: https://example.com/page2
2025-02-07 19:46:38,660 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 19:46:38,661 [DEBUG] Base URL: https://example.com/page2
2025-02-07 19:46:38,661 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 19:46:38,661 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 19:46:38,661 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 19:46:38,661 [INFO] Successfully crawled https://example.com/page2
2025-02-07 19:46:38,661 [DEBUG] Using raw URL string: https://example.com/page3 (type: <class 'str'>)
2025-02-07 19:46:38,661 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page3', params='', query='', fragment='')
2025-02-07 19:46:38,661 [INFO] Starting crawl of URL: https://example.com/page3
2025-02-07 19:46:39,185 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 19:46:39,185 [DEBUG] Base URL: https://example.com/page3
2025-02-07 19:46:39,185 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 19:46:39,185 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 19:46:39,186 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 19:46:39,186 [INFO] Successfully crawled https://example.com/page3
2025-02-07 19:46:39,187 [DEBUG] Using raw URL string: https://example.com/page4 (type: <class 'str'>)
2025-02-07 19:46:39,187 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='/page4', params='', query='', fragment='')
2025-02-07 19:46:39,187 [INFO] Starting crawl of URL: https://example.com/page4
2025-02-07 19:46:39,701 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 19:46:39,702 [DEBUG] Base URL: https://example.com/page4
2025-02-07 19:46:39,702 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 19:46:39,702 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 19:46:39,702 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 19:46:39,702 [INFO] Successfully crawled https://example.com/page4
2025-02-07 19:46:39,713 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:39,718 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:39,734 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:39,740 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:39,757 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:39,760 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:39,778 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:39,784 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:39,802 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:39,807 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:39,825 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:39,828 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:39,844 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:39,849 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:39,871 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:39,874 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:39,889 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x0000025959117ED0>
2025-02-07 19:46:39,890 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000259593FF950>, 283042.7082863)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000259593FF6B0>, 283043.256173)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000259593FF530>, 283043.9210452)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000259593FF290>, 283044.3928952)])']
connector: <aiohttp.connector.TCPConnector object at 0x00000259592849D0>
2025-02-07 19:46:39,891 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x00000259592EE850>
2025-02-07 19:46:39,892 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x00000259592E5670>, 283047.7615499)])']
connector: <aiohttp.connector.TCPConnector object at 0x0000025959284E90>
2025-02-07 19:46:39,892 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x00000259592EE210>
2025-02-07 19:46:39,892 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000002595978DC10>, 283049.9624695)])']
connector: <aiohttp.connector.TCPConnector object at 0x0000025959285220>
2025-02-07 19:46:39,906 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:39,910 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:39,927 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:40,019 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:46:40,020 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:40,023 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:40,026 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:46:40,027 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:40,033 [DEBUG] 
Evaluating link: https://example.com/doc1
2025-02-07 19:46:40,034 [DEBUG] Target domain: example.com
2025-02-07 19:46:40,034 [DEBUG] URL domain: example.com
2025-02-07 19:46:40,034 [DEBUG] URL approved for crawling: https://example.com/doc1
2025-02-07 19:46:40,034 [DEBUG] 
Evaluating link: https://example.com/excluded/page
2025-02-07 19:46:40,034 [DEBUG] Target domain: example.com
2025-02-07 19:46:40,034 [DEBUG] URL domain: example.com
2025-02-07 19:46:40,034 [DEBUG] URL approved for crawling: https://example.com/excluded/page
2025-02-07 19:46:40,058 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:40,061 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:46:40,062 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:40,084 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:40,121 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:46:40,122 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:40,125 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-07 19:46:40,167 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:40,170 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:46:40,171 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:40,190 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:40,193 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:46:40,195 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:40,202 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-07 19:46:40,243 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:40,246 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:46:40,251 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:40,253 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-07 19:46:40,292 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:40,295 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:46:40,295 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:40,303 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-07 19:46:40,343 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:40,345 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:46:40,351 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:40,353 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-07 19:46:40,392 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:40,394 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:46:40,395 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:40,402 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-07 19:46:40,445 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:40,452 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:46:40,453 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:40,455 [ERROR] Error during crawl: 'Timer' object does not support the asynchronous context manager protocol
2025-02-07 19:46:40,502 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:40,511 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:41,027 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:41,033 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:41,035 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:46:41,059 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:41,068 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:41,070 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:46:41,110 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:41,122 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:46:41,160 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:46:41,206 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:46:41,239 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:46:41,287 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:46:41,326 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:46:41,384 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:46:41,604 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:46:41,671 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:46:41,719 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:46:43,158 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:44,690 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:44,700 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=10, max_pages=1000, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 19:46:44,701 [DEBUG] Using raw URL string: https://example.com (type: <class 'str'>)
2025-02-07 19:46:44,701 [DEBUG] Parsed URL components: ParseResult(scheme='https', netloc='example.com', path='', params='', query='', fragment='')
2025-02-07 19:46:44,701 [INFO] Starting crawl of URL: https://example.com
2025-02-07 19:46:45,057 [DEBUG] 
Evaluating link: https://www.iana.org/domains/example
2025-02-07 19:46:45,058 [DEBUG] Base URL: https://example.com
2025-02-07 19:46:45,058 [DEBUG] Normalized URL: https://www.iana.org/domains/example -> https://iana.org/domains/example
2025-02-07 19:46:45,058 [DEBUG] Absolute link: https://iana.org/domains/example
2025-02-07 19:46:45,058 [DEBUG] Initial domain set to: example.com
2025-02-07 19:46:45,058 [DEBUG] Skipping different subdomain: iana.org (initial: example.com)
2025-02-07 19:46:45,059 [INFO] Successfully crawled https://example.com
2025-02-07 19:46:45,059 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000002595A4CD810>
2025-02-07 19:46:45,060 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000002595A85CE90>, 283055.3198341)])']
connector: <aiohttp.connector.TCPConnector object at 0x0000025959652650>
2025-02-07 19:46:45,064 [INFO] HTTP Request: POST http://testserver/crawl "HTTP/1.1 200 OK"
2025-02-07 19:46:45,753 [INFO] Operation took 0.11 seconds
2025-02-07 19:46:46,518 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 19:46:46,519 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:46:46,520 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:53,997 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-7\test_full_site_crawl0\test_docs/index.html
2025-02-07 19:46:53,998 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-07 19:46:54,029 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:46:54,042 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 19:46:54,043 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:46:54,043 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:01,491 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-7\test_content_processing_pipeli0\test_docs/guide.html
2025-02-07 19:47:01,491 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-07 19:47:01,521 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:01,534 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 19:47:01,534 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:47:01,535 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:09,008 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-7\test_quality_checks0\test_docs/api.html
2025-02-07 19:47:09,008 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-07 19:47:09,033 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:09,047 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 19:47:09,048 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:47:09,049 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:16,466 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-7\test_document_organization0\test_docs/index.html
2025-02-07 19:47:16,466 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-07 19:47:16,494 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:16,509 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 19:47:16,509 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:47:16,510 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:24,058 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-7\test_search_functionality0\test_docs/index.html
2025-02-07 19:47:24,059 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-07 19:47:24,085 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:24,100 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=False, concurrent_requests=10)
2025-02-07 19:47:24,100 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:47:24,101 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:31,557 [ERROR] Could not find documentation URL for package: file://C:\Users\Andu\AppData\Local\Temp\pytest-of-Andu\pytest-7\test_error_handling_and_recove0\test_docs/index.html
2025-02-07 19:47:31,558 [ERROR] Error during crawl: 2 validation errors for QualityIssue
type
  Input should be 'content_length', 'heading_structure', 'link_count', 'code_block_length', 'metadata' or 'general' [type=enum, input_value='error', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/enum
level
  Field required [type=missing, input_value={'type': 'error', 'messag...ml', 'severity': 'high'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-07 19:47:31,595 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:31,600 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:31,616 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:31,621 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:31,626 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:47:31,627 [ERROR] Error during crawl: 'str' object has no attribute 'url'
2025-02-07 19:47:31,652 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:31,658 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:31,676 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:31,680 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:31,684 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 19:47:31,710 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:32,063 [ERROR] Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x00000259592EDA90>
2025-02-07 19:47:32,063 [ERROR] Unclosed connector
connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x000002595969C290>, 283050.9811042)])', 'deque([(<aiohttp.client_proto.ResponseHandler object at 0x000002595969C110>, 283051.2140682)])']
connector: <aiohttp.connector.TCPConnector object at 0x0000025959284510>
2025-02-07 19:47:33,984 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:34,020 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:34,023 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:34,049 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:34,055 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:34,082 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:34,089 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:34,121 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:34,124 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:34,157 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:34,162 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:34,188 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:34,192 [DEBUG] Using proactor: IocpProactor
2025-02-07 19:47:34,210 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:07:31,806 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:08:34,276 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:09:43,217 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:10:35,794 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:11:41,992 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:12:57,490 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:14:17,985 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:15:48,905 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:18:52,784 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:20:23,877 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:20:25,426 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:20:25,431 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:20:25,434 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:20:25,437 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:20:25,440 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:20:25,442 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:20:25,444 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 20:20:25,447 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:20:25,450 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:20:25,452 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 20:20:25,454 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:20:25,456 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:20:25,459 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:20:25,463 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:20:25,467 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:20:25,469 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:20:25,474 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:20:25,475 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:20:25,482 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:20:25,485 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:20:25,488 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 20:20:25,490 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:23:11,103 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:23:12,724 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:23:12,727 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:23:12,730 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:23:12,732 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:23:12,735 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:23:12,738 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:23:12,740 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 20:23:12,742 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:23:12,745 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:23:12,747 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 20:23:12,749 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:23:12,751 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:23:12,755 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:23:12,758 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:23:12,762 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:23:12,764 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:23:12,768 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:23:12,771 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:23:12,775 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:23:12,777 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:23:12,780 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 20:23:12,782 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:25:58,392 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:26:00,635 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:26:00,637 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:26:00,641 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:26:00,650 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:26:00,658 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:26:00,666 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:26:00,669 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 20:26:00,675 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:26:00,684 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:26:00,688 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 20:26:00,692 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:26:00,694 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:26:00,725 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:26:00,728 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:26:00,734 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:26:00,738 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:26:00,743 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:26:00,747 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:26:00,753 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:26:00,756 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:26:00,761 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 20:26:00,769 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:29:12,540 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:29:14,325 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:29:14,327 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:29:14,332 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:29:14,337 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:29:14,342 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:29:14,344 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:29:14,346 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 20:29:14,352 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:29:14,357 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:29:14,362 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 20:29:14,370 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:29:14,375 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:29:14,378 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:29:14,386 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:29:14,394 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:29:14,404 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:29:14,409 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:29:14,421 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:29:14,429 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:29:14,437 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:29:14,440 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 20:29:14,444 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:30:47,641 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:30:49,207 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:30:49,211 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:30:49,215 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:30:49,219 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:30:49,228 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:30:49,232 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:30:49,239 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 20:30:49,240 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:30:49,248 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:30:49,251 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 20:30:49,257 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:30:49,267 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:30:49,282 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:30:49,287 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:30:49,293 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:30:49,297 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:30:49,303 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:30:49,305 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:30:49,315 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:30:49,318 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:30:49,321 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 20:30:49,323 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:31:13,246 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:32:16,262 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:32:17,953 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:32:17,957 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:32:17,963 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:32:17,967 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:32:17,971 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:32:17,974 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:32:17,980 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 20:32:17,982 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:32:17,985 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:32:17,989 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 20:32:17,997 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:32:18,002 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:32:18,014 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:32:18,016 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:32:18,021 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:32:18,023 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:32:18,030 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:32:18,036 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:32:18,048 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:32:18,050 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:32:18,052 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 20:32:18,055 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:32:46,487 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:33:03,942 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:33:05,818 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:33:05,823 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:33:05,832 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:33:05,836 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:33:05,841 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:33:05,845 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:33:05,852 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 20:33:05,856 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:33:05,868 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:33:05,871 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 20:33:05,874 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:33:05,876 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:33:05,883 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:33:05,886 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:33:05,890 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:33:05,892 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:33:05,895 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:33:05,901 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:33:05,907 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:33:05,910 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:33:05,914 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 20:33:05,918 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:33:55,901 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:34:11,404 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:34:12,738 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:34:12,740 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:34:12,743 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:34:12,746 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:34:12,749 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:34:12,754 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:34:12,756 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 20:34:12,757 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:34:12,760 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:34:12,762 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 20:34:12,764 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:34:12,766 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:34:12,772 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:34:12,774 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:34:12,779 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:34:12,782 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:34:12,789 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:34:12,792 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:34:12,796 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:34:12,799 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:34:12,806 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 20:34:12,809 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:35:05,202 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:35:24,571 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:35:26,206 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:35:26,209 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:35:26,211 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:35:26,216 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:35:26,219 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:35:26,221 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:35:26,224 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 20:35:26,226 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:35:26,228 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:35:26,230 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 20:35:26,235 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:35:26,238 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:35:26,243 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:35:26,245 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:35:26,253 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:35:26,256 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:35:26,261 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:35:26,263 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:35:26,270 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:35:26,273 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:35:26,275 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 20:35:26,276 [DEBUG] Using proactor: IocpProactor
2025-02-07 20:35:26,771 [INFO] Operation took 0.17 seconds
2025-02-07 20:37:50,305 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:39:51,689 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:42:25,973 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:48:43,934 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:50:23,581 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:53:19,060 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:53:37,142 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:56:15,609 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:57:01,142 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 20:58:21,962 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 21:04:03,072 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 21:09:28,176 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 21:18:04,276 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 21:18:33,682 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 21:19:32,907 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 21:20:15,193 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 21:21:07,999 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 21:21:57,720 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 21:23:10,280 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 21:24:02,869 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 21:25:07,495 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 21:26:06,470 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 21:26:06,601 [DEBUG] IDNA encoding failed for example.com:abc: Invalid port number
2025-02-07 21:30:58,230 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 21:30:58,453 [DEBUG] IDNA encoding failed for example.com:abc: Invalid port number
2025-02-07 21:30:58,453 [DEBUG] IDNA encoding failed for example.com:65536: Invalid port number
2025-02-07 21:30:58,454 [DEBUG] IDNA encoding failed for example.com:-80: Invalid port number
2025-02-07 21:39:36,625 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 21:39:39,905 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:39:39,933 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:39:39,936 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:39:39,979 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:39:39,985 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:39:39,987 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:39:39,990 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 21:39:39,992 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:39:39,995 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:39:40,000 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 21:39:40,007 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:39:40,009 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:39:40,424 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:09,508 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 21:40:09,743 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:09,745 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:09,751 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:09,756 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:09,759 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:09,762 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:09,764 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 21:40:09,772 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:09,776 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:09,778 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 21:40:09,788 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:09,793 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:10,183 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:10,186 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:10,189 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:10,192 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:10,200 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:10,205 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:10,210 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:10,214 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:10,222 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 21:40:10,225 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:40,147 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 21:40:40,281 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:40,283 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:40,287 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:40,295 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:40,301 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:40,305 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:40,311 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 21:40:40,312 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:40,318 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:40,320 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 21:40:40,328 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:40,331 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:40,645 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:40,650 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:40,662 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:40,667 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:40,672 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:40,678 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:40,681 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:40,683 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:40:40,685 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 21:40:40,688 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:41:20,855 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 21:41:40,372 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 21:41:40,536 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:41:40,538 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:41:40,828 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:42:34,278 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 21:42:34,471 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:42:34,474 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:42:34,771 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:44:28,038 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 21:44:28,304 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:44:28,309 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:44:28,312 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:44:28,316 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:44:28,320 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:44:28,326 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:44:28,329 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 21:44:28,334 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:44:28,337 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:44:28,345 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 21:44:28,353 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:44:28,360 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:44:28,700 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:44:28,703 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:44:28,711 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:44:28,713 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:44:28,718 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:44:28,720 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:44:28,726 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:44:28,728 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:44:28,730 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 21:44:28,732 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:44:53,993 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 21:44:54,129 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:44:54,131 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:44:54,405 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:21,083 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 21:45:21,205 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:21,210 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:21,213 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:21,216 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:21,219 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:21,221 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:21,228 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 21:45:21,230 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:21,234 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:21,236 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 21:45:21,245 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:21,247 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:21,549 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:21,552 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:21,555 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:21,559 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:21,562 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:21,564 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:21,567 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:21,570 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:21,574 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 21:45:21,578 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:51,567 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 21:45:51,689 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:51,691 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:51,696 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:51,701 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:51,704 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:51,706 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:51,709 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 21:45:51,711 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:51,715 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:51,719 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 21:45:51,729 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:51,734 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:52,039 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:52,041 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:52,045 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:52,049 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:52,052 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:52,055 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:52,058 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:52,062 [DEBUG] Using proactor: IocpProactor
2025-02-07 21:45:52,065 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 21:45:52,068 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:05:41,764 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 22:05:42,057 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:05:42,060 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:05:42,066 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:05:42,071 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:05:42,076 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:05:42,079 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:05:42,082 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:05:42,084 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:05:42,087 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:05:42,090 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:05:42,099 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:05:42,103 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:05:42,694 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:05:42,697 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:05:42,700 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:05:42,719 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:05:42,722 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:05:42,755 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:06:29,540 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 22:06:29,875 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:06:29,877 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:06:29,885 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:06:29,888 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:06:29,892 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:06:29,895 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:06:29,899 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:06:29,905 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:06:29,908 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:06:29,913 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:06:29,921 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:06:29,923 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:06:30,324 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:06:30,327 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:06:30,332 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:06:30,343 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:06:30,346 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:06:30,368 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:07:09,941 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 22:07:10,239 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:07:10,242 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:07:10,250 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:07:10,257 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:07:10,265 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:07:10,270 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:07:10,274 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:07:10,278 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:07:10,282 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:07:10,285 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:07:10,297 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:07:10,302 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:07:10,698 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:07:10,701 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:07:10,703 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:07:10,717 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:07:10,719 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:07:10,741 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:07:36,890 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 22:07:37,137 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:07:37,140 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:07:37,143 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:07:37,149 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:07:37,208 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:07:37,214 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:07:37,225 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:07:37,275 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:07:37,280 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:07:37,283 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:07:37,344 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:07:37,357 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:07:37,861 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:07:37,864 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:07:37,869 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:07:37,899 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:07:37,905 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:07:37,932 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:08:23,489 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 22:08:23,663 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:08:23,666 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:08:23,669 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:08:23,672 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:08:23,679 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:08:23,682 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:08:23,685 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:08:23,687 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:08:23,693 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:08:23,701 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:08:23,702 [DEBUG] Backend backend1 selected.
2025-02-07 22:08:23,706 [DEBUG] Backend backend2 selected.
2025-02-07 22:08:23,709 [DEBUG] No backend selected.
2025-02-07 22:08:23,712 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:08:23,718 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:08:24,200 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:08:24,204 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:08:24,208 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:08:24,210 [DEBUG] Backend backend1 selected.
2025-02-07 22:08:24,210 [DEBUG] Backend backend2 selected.
2025-02-07 22:08:24,211 [DEBUG] Backend backend1 selected.
2025-02-07 22:08:24,237 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:08:24,243 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:08:24,255 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:08:54,016 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 22:08:54,207 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:08:54,211 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:08:54,223 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:08:54,231 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:08:54,239 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:08:54,242 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:08:54,244 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:08:54,246 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:08:54,250 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:08:54,253 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:08:54,254 [DEBUG] Backend backend1 selected.
2025-02-07 22:08:54,259 [DEBUG] Backend backend2 selected.
2025-02-07 22:08:54,259 [DEBUG] No backend selected.
2025-02-07 22:08:54,263 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:08:54,268 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:08:54,766 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:08:54,769 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:08:54,771 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:08:54,772 [DEBUG] Backend backend1 selected.
2025-02-07 22:08:54,773 [DEBUG] Backend backend2 selected.
2025-02-07 22:08:54,773 [DEBUG] Backend backend1 selected.
2025-02-07 22:08:54,791 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:08:54,794 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:08:54,825 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:09:25,893 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 22:09:26,004 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:09:26,006 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:09:26,009 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:09:26,011 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:09:26,014 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:09:26,020 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:09:26,022 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:09:26,023 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:09:26,025 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:09:26,027 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:09:26,028 [DEBUG] Backend backend1 score: 0.5
2025-02-07 22:09:26,028 [DEBUG] Backend backend1 score: 0.5
2025-02-07 22:09:26,028 [DEBUG] Best backend selected: backend1
2025-02-07 22:09:26,034 [DEBUG] Backend backend2 score: 0.44999999999999996
2025-02-07 22:09:26,035 [DEBUG] Backend backend2 score: 0.44999999999999996
2025-02-07 22:09:26,035 [DEBUG] Best backend selected: backend2
2025-02-07 22:09:26,036 [DEBUG] No backend selected.
2025-02-07 22:09:26,038 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:09:26,040 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:09:26,439 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:09:26,442 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:09:26,445 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:09:26,446 [ERROR] Error selecting backend backend1: 'BackendCriteria' object has no attribute 'domains'
2025-02-07 22:09:26,446 [ERROR] Error selecting backend backend2: 'BackendCriteria' object has no attribute 'domains'
2025-02-07 22:09:26,447 [ERROR] Error selecting backend backend3: 'BackendCriteria' object has no attribute 'domains'
2025-02-07 22:09:26,447 [ERROR] Error selecting backend backend4: 'BackendCriteria' object has no attribute 'domains'
2025-02-07 22:09:26,447 [DEBUG] No backend selected.
2025-02-07 22:09:26,484 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:09:26,489 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:09:26,520 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:09:46,133 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 22:09:46,268 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:09:46,270 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:09:46,273 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:09:46,276 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:09:46,282 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:09:46,285 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:09:46,288 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:09:46,291 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:09:46,293 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:09:46,295 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:09:46,300 [DEBUG] Backend backend1 score: 0.5
2025-02-07 22:09:46,300 [DEBUG] Backend backend1 score: 0.5
2025-02-07 22:09:46,301 [DEBUG] Best backend selected: backend1
2025-02-07 22:09:46,306 [DEBUG] Backend backend2 score: 0.44999999999999996
2025-02-07 22:09:46,307 [DEBUG] Backend backend2 score: 0.44999999999999996
2025-02-07 22:09:46,307 [DEBUG] Best backend selected: backend2
2025-02-07 22:09:46,308 [DEBUG] No backend selected.
2025-02-07 22:09:46,309 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:09:46,312 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:09:46,966 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:09:46,968 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:09:46,970 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:09:46,971 [ERROR] Error selecting backend backend1: 'BackendCriteria' object has no attribute 'domains'
2025-02-07 22:09:46,971 [ERROR] Error selecting backend backend2: 'BackendCriteria' object has no attribute 'domains'
2025-02-07 22:09:46,971 [ERROR] Error selecting backend backend3: 'BackendCriteria' object has no attribute 'domains'
2025-02-07 22:09:46,972 [ERROR] Error selecting backend backend4: 'BackendCriteria' object has no attribute 'domains'
2025-02-07 22:09:46,972 [DEBUG] No backend selected.
2025-02-07 22:09:46,987 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:09:46,990 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:09:47,009 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:10:13,372 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 22:10:13,569 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:10:13,580 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:10:13,585 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:10:13,595 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:10:13,600 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:10:13,610 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:10:13,613 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:10:13,615 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:10:13,618 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:10:13,626 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:10:13,629 [DEBUG] Backend backend1 score: 0.5
2025-02-07 22:10:13,629 [DEBUG] Backend backend1 score: 0.5
2025-02-07 22:10:13,629 [DEBUG] Best backend selected: backend1
2025-02-07 22:10:13,630 [DEBUG] Backend backend2 score: 0.44999999999999996
2025-02-07 22:10:13,631 [DEBUG] Backend backend2 score: 0.44999999999999996
2025-02-07 22:10:13,632 [DEBUG] Best backend selected: backend2
2025-02-07 22:10:13,633 [DEBUG] No backend selected.
2025-02-07 22:10:13,635 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:10:13,643 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:10:14,516 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:10:14,519 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:10:14,524 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:10:14,525 [ERROR] Error selecting backend backend1: 'BackendCriteria' object has no attribute 'domains'
2025-02-07 22:10:14,525 [ERROR] Error selecting backend backend2: 'BackendCriteria' object has no attribute 'domains'
2025-02-07 22:10:14,525 [ERROR] Error selecting backend backend3: 'BackendCriteria' object has no attribute 'domains'
2025-02-07 22:10:14,525 [ERROR] Error selecting backend backend4: 'BackendCriteria' object has no attribute 'domains'
2025-02-07 22:10:14,526 [DEBUG] No backend selected.
2025-02-07 22:10:14,547 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:10:14,549 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:10:14,577 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:11:47,097 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 22:11:47,211 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:11:47,213 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:11:47,220 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:11:47,224 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:11:47,231 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:11:47,256 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:11:47,261 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:11:47,264 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:11:47,271 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:11:47,273 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:11:47,274 [DEBUG] Backend backend1 score: 0.5
2025-02-07 22:11:47,275 [DEBUG] Backend backend1 score: 0.5
2025-02-07 22:11:47,275 [DEBUG] Best backend selected: backend1
2025-02-07 22:11:47,279 [DEBUG] Backend backend2 score: 0.44999999999999996
2025-02-07 22:11:47,279 [DEBUG] Backend backend2 score: 0.44999999999999996
2025-02-07 22:11:47,280 [DEBUG] Best backend selected: backend2
2025-02-07 22:11:47,280 [DEBUG] No backend selected.
2025-02-07 22:11:47,287 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:11:47,291 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:11:47,864 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:11:47,869 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:11:47,872 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:11:47,874 [DEBUG] Backend backend1 score: 0.5
2025-02-07 22:11:47,874 [DEBUG] Backend backend1 score: 0.5
2025-02-07 22:11:47,874 [DEBUG] Backend backend2 score: 0.35
2025-02-07 22:11:47,874 [DEBUG] Backend backend2 score: 0.35
2025-02-07 22:11:47,875 [DEBUG] Backend backend3 score: 0.31
2025-02-07 22:11:47,875 [DEBUG] Backend backend3 score: 0.31
2025-02-07 22:11:47,875 [DEBUG] Backend backend4 score: 0.5499999999999999
2025-02-07 22:11:47,875 [DEBUG] Backend backend4 score: 0.5499999999999999
2025-02-07 22:11:47,875 [DEBUG] Best backend selected: backend4
2025-02-07 22:11:47,901 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:11:47,904 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:11:47,923 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:12:59,721 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 22:12:59,826 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:12:59,828 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:12:59,834 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:12:59,838 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:12:59,842 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:12:59,845 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:12:59,851 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:12:59,859 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:12:59,862 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:12:59,863 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:12:59,870 [DEBUG] Backend backend1 score: 0.4
2025-02-07 22:12:59,870 [DEBUG] Backend backend1 score: 0.4
2025-02-07 22:12:59,870 [DEBUG] Best backend selected: backend1
2025-02-07 22:12:59,875 [DEBUG] Backend backend2 score: 0.35
2025-02-07 22:12:59,875 [DEBUG] Backend backend2 score: 0.35
2025-02-07 22:12:59,876 [DEBUG] Best backend selected: backend2
2025-02-07 22:12:59,876 [DEBUG] No backend selected.
2025-02-07 22:12:59,879 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:12:59,885 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:13:00,382 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:13:00,385 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:13:00,387 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:13:00,388 [DEBUG] Backend backend1 score: 0.8
2025-02-07 22:13:00,388 [DEBUG] Backend backend1 score: 0.8
2025-02-07 22:13:00,388 [DEBUG] Backend backend2 score: 0.35
2025-02-07 22:13:00,388 [DEBUG] Backend backend2 score: 0.35
2025-02-07 22:13:00,389 [DEBUG] Backend backend3 score: 0.31
2025-02-07 22:13:00,389 [DEBUG] Backend backend3 score: 0.31
2025-02-07 22:13:00,389 [DEBUG] Backend backend4 score: 0.4499999999999999
2025-02-07 22:13:00,389 [DEBUG] Backend backend4 score: 0.4499999999999999
2025-02-07 22:13:00,389 [DEBUG] Best backend selected: backend1
2025-02-07 22:13:00,390 [DEBUG] Backend backend2 score: 0.6499999999999999
2025-02-07 22:13:00,391 [DEBUG] Backend backend2 score: 0.6499999999999999
2025-02-07 22:13:00,392 [DEBUG] Best backend selected: backend2
2025-02-07 22:13:00,392 [DEBUG] Backend backend1 score: 0.0
2025-02-07 22:13:00,393 [DEBUG] Backend backend1 score: 0.0
2025-02-07 22:13:00,393 [DEBUG] Backend backend2 score: 0.35
2025-02-07 22:13:00,393 [DEBUG] Backend backend2 score: 0.35
2025-02-07 22:13:00,393 [DEBUG] Backend backend3 score: 0.31
2025-02-07 22:13:00,393 [DEBUG] Backend backend3 score: 0.31
2025-02-07 22:13:00,393 [DEBUG] Backend backend4 score: 0.04999999999999993
2025-02-07 22:13:00,393 [DEBUG] Backend backend4 score: 0.04999999999999993
2025-02-07 22:13:00,393 [DEBUG] Best backend selected: backend2
2025-02-07 22:13:00,406 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:13:00,410 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:13:00,438 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:14:10,196 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 22:14:10,346 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:14:10,351 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:14:10,357 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:14:10,361 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:14:10,367 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:14:10,371 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:14:10,376 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:14:10,378 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:14:10,384 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:14:10,388 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:14:10,389 [DEBUG] Backend backend1 score: 0.14999999999999997
2025-02-07 22:14:10,391 [DEBUG] Backend backend1 score: 0.14999999999999997
2025-02-07 22:14:10,395 [DEBUG] Best backend selected: backend1
2025-02-07 22:14:10,396 [DEBUG] Backend backend2 score: 0.12499999999999997
2025-02-07 22:14:10,397 [DEBUG] Backend backend2 score: 0.12499999999999997
2025-02-07 22:14:10,397 [DEBUG] Best backend selected: backend2
2025-02-07 22:14:10,397 [DEBUG] No backend selected.
2025-02-07 22:14:10,402 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:14:10,405 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:14:11,023 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:14:11,026 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:14:11,029 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:14:11,030 [DEBUG] Backend backend1 score: 0.55
2025-02-07 22:14:11,030 [DEBUG] Backend backend1 score: 0.55
2025-02-07 22:14:11,030 [DEBUG] Backend backend2 score: 0.12499999999999997
2025-02-07 22:14:11,030 [DEBUG] Backend backend2 score: 0.12499999999999997
2025-02-07 22:14:11,032 [DEBUG] Backend backend3 score: 0.10499999999999998
2025-02-07 22:14:11,032 [DEBUG] Backend backend3 score: 0.10499999999999998
2025-02-07 22:14:11,033 [DEBUG] Backend backend4 score: 0.175
2025-02-07 22:14:11,033 [DEBUG] Backend backend4 score: 0.175
2025-02-07 22:14:11,034 [DEBUG] Best backend selected: backend1
2025-02-07 22:14:11,034 [DEBUG] Backend backend2 score: 0.7250000000000001
2025-02-07 22:14:11,035 [DEBUG] Backend backend2 score: 0.7250000000000001
2025-02-07 22:14:11,035 [DEBUG] Best backend selected: backend2
2025-02-07 22:14:11,035 [DEBUG] Backend backend1 score: -0.25000000000000006
2025-02-07 22:14:11,036 [DEBUG] Backend backend1 score: 0
2025-02-07 22:14:11,036 [DEBUG] Backend backend2 score: 0.12499999999999997
2025-02-07 22:14:11,036 [DEBUG] Backend backend2 score: 0.12499999999999997
2025-02-07 22:14:11,037 [DEBUG] Backend backend3 score: 0.10499999999999998
2025-02-07 22:14:11,037 [DEBUG] Backend backend3 score: 0.10499999999999998
2025-02-07 22:14:11,037 [DEBUG] Backend backend4 score: -0.22500000000000003
2025-02-07 22:14:11,038 [DEBUG] Backend backend4 score: 0
2025-02-07 22:14:11,038 [DEBUG] Best backend selected: backend2
2025-02-07 22:14:11,055 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:14:11,057 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:14:11,078 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:15:18,123 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 22:15:18,238 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:15:18,241 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:15:18,244 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:15:18,248 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:15:18,256 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:15:18,259 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:15:18,263 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:15:18,265 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:15:18,274 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:15:18,277 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:15:18,279 [DEBUG] Backend backend1 score: -0.050000000000000044
2025-02-07 22:15:18,279 [DEBUG] Backend backend1 score: 0
2025-02-07 22:15:18,279 [DEBUG] Best backend selected: backend1
2025-02-07 22:15:18,286 [DEBUG] Backend backend2 score: -0.07500000000000001
2025-02-07 22:15:18,287 [DEBUG] Backend backend2 score: 0
2025-02-07 22:15:18,287 [DEBUG] Best backend selected: backend2
2025-02-07 22:15:18,288 [DEBUG] No backend selected.
2025-02-07 22:15:18,290 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:15:18,294 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:15:18,813 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:15:18,819 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:15:18,822 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:15:18,823 [DEBUG] Backend backend1 score: 0.35
2025-02-07 22:15:18,824 [DEBUG] Backend backend1 score: 0.35
2025-02-07 22:15:18,824 [DEBUG] Backend backend2 score: -0.07500000000000001
2025-02-07 22:15:18,824 [DEBUG] Backend backend2 score: 0
2025-02-07 22:15:18,824 [DEBUG] Backend backend3 score: -0.09500000000000003
2025-02-07 22:15:18,824 [DEBUG] Backend backend3 score: 0
2025-02-07 22:15:18,825 [DEBUG] Backend backend4 score: -0.025000000000000022
2025-02-07 22:15:18,825 [DEBUG] Backend backend4 score: 0
2025-02-07 22:15:18,825 [DEBUG] Best backend selected: backend1
2025-02-07 22:15:18,826 [DEBUG] Backend backend2 score: 0.7250000000000001
2025-02-07 22:15:18,826 [DEBUG] Backend backend2 score: 0.7250000000000001
2025-02-07 22:15:18,826 [DEBUG] Best backend selected: backend2
2025-02-07 22:15:18,827 [DEBUG] Backend backend1 score: -0.45000000000000007
2025-02-07 22:15:18,827 [DEBUG] Backend backend1 score: 0
2025-02-07 22:15:18,827 [DEBUG] Backend backend2 score: -0.07500000000000001
2025-02-07 22:15:18,828 [DEBUG] Backend backend2 score: 0
2025-02-07 22:15:18,828 [DEBUG] Backend backend3 score: -0.09500000000000003
2025-02-07 22:15:18,828 [DEBUG] Backend backend3 score: 0
2025-02-07 22:15:18,828 [DEBUG] Backend backend4 score: -0.42500000000000004
2025-02-07 22:15:18,829 [DEBUG] Backend backend4 score: 0
2025-02-07 22:15:18,829 [DEBUG] Best backend selected: backend1
2025-02-07 22:15:18,859 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:15:18,919 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:15:19,022 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:22:01,442 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 22:22:01,560 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:22:01,562 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:22:01,565 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:22:01,568 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:22:01,573 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:22:01,577 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:22:01,579 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:22:01,582 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:22:01,584 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:22:01,586 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:22:01,587 [DEBUG] Backend backend1 score: -0.050000000000000044
2025-02-07 22:22:01,587 [DEBUG] Backend backend1 score: 0
2025-02-07 22:22:01,588 [DEBUG] Best backend selected: backend1
2025-02-07 22:22:01,592 [DEBUG] Backend backend2 score: -0.07500000000000001
2025-02-07 22:22:01,592 [DEBUG] Backend backend2 score: 0
2025-02-07 22:22:01,592 [DEBUG] Best backend selected: backend2
2025-02-07 22:22:01,593 [DEBUG] No backend selected.
2025-02-07 22:22:01,594 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:22:01,598 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:22:01,997 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:22:02,000 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:22:02,003 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:22:02,009 [DEBUG] Backend backend1 score: 0.35
2025-02-07 22:22:02,009 [DEBUG] Backend backend1 score: 0.35
2025-02-07 22:22:02,009 [DEBUG] Backend backend2 score: -0.07500000000000001
2025-02-07 22:22:02,014 [DEBUG] Backend backend2 score: 0
2025-02-07 22:22:02,015 [DEBUG] Backend backend3 score: -0.09500000000000003
2025-02-07 22:22:02,015 [DEBUG] Backend backend3 score: 0
2025-02-07 22:22:02,015 [DEBUG] Backend backend4 score: -0.025000000000000022
2025-02-07 22:22:02,015 [DEBUG] Backend backend4 score: 0
2025-02-07 22:22:02,016 [DEBUG] Best backend selected: backend1
2025-02-07 22:22:02,016 [DEBUG] Backend backend2 score: 0.7250000000000001
2025-02-07 22:22:02,016 [DEBUG] Backend backend2 score: 0.7250000000000001
2025-02-07 22:22:02,017 [DEBUG] Best backend selected: backend2
2025-02-07 22:22:02,018 [DEBUG] Backend backend1 score: -0.45000000000000007
2025-02-07 22:22:02,018 [DEBUG] Backend backend1 score: 0
2025-02-07 22:22:02,019 [DEBUG] Backend backend2 score: -0.07500000000000001
2025-02-07 22:22:02,019 [DEBUG] Backend backend2 score: 0
2025-02-07 22:22:02,019 [DEBUG] Backend backend3 score: -0.09500000000000003
2025-02-07 22:22:02,019 [DEBUG] Backend backend3 score: 0
2025-02-07 22:22:02,020 [DEBUG] Backend backend4 score: -0.42500000000000004
2025-02-07 22:22:02,020 [DEBUG] Backend backend4 score: 0
2025-02-07 22:22:02,020 [DEBUG] Best backend selected: backend1
2025-02-07 22:22:02,054 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:22:02,060 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:22:02,093 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:22:50,595 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 22:22:50,732 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:22:50,734 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:22:50,737 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:22:50,747 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:22:50,754 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:22:50,757 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:22:50,764 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:22:50,766 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:22:50,772 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:22:50,774 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:22:50,780 [DEBUG] Backend backend1 score: -0.050000000000000044
2025-02-07 22:22:50,780 [DEBUG] Backend backend1 score: 0
2025-02-07 22:22:50,781 [DEBUG] Best backend selected: backend1
2025-02-07 22:22:50,787 [DEBUG] Backend backend2 score: -0.07500000000000001
2025-02-07 22:22:50,787 [DEBUG] Backend backend2 score: 0
2025-02-07 22:22:50,787 [DEBUG] Best backend selected: backend2
2025-02-07 22:22:50,787 [DEBUG] No backend selected.
2025-02-07 22:22:50,789 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:22:50,792 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:22:51,560 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:22:51,563 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:22:51,647 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 22:22:51,648 [DEBUG] Backend backend1 score: 0.35
2025-02-07 22:22:51,648 [DEBUG] Backend backend1 score: 0.35
2025-02-07 22:22:51,649 [DEBUG] Backend backend2 score: -0.07500000000000001
2025-02-07 22:22:51,649 [DEBUG] Backend backend2 score: 0
2025-02-07 22:22:51,649 [DEBUG] Backend backend3 score: -0.09500000000000003
2025-02-07 22:22:51,649 [DEBUG] Backend backend3 score: 0
2025-02-07 22:22:51,650 [DEBUG] Backend backend4 score: -0.025000000000000022
2025-02-07 22:22:51,650 [DEBUG] Backend backend4 score: 0
2025-02-07 22:22:51,650 [DEBUG] Best backend selected: backend1
2025-02-07 22:22:51,651 [DEBUG] Backend backend2 score: 0.7250000000000001
2025-02-07 22:22:51,651 [DEBUG] Backend backend2 score: 0.7250000000000001
2025-02-07 22:22:51,652 [DEBUG] Best backend selected: backend2
2025-02-07 22:22:51,652 [DEBUG] Backend backend1 score: -0.45000000000000007
2025-02-07 22:22:51,652 [DEBUG] Backend backend1 score: 0
2025-02-07 22:22:51,653 [DEBUG] Backend backend2 score: -0.07500000000000001
2025-02-07 22:22:51,653 [DEBUG] Backend backend2 score: 0
2025-02-07 22:22:51,653 [DEBUG] Backend backend3 score: -0.09500000000000003
2025-02-07 22:22:51,653 [DEBUG] Backend backend3 score: 0
2025-02-07 22:22:51,655 [DEBUG] Backend backend4 score: -0.42500000000000004
2025-02-07 22:22:51,656 [DEBUG] Backend backend4 score: 0
2025-02-07 22:22:51,656 [DEBUG] Best backend selected: backend1
2025-02-07 22:22:51,761 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:22:51,766 [DEBUG] Using proactor: IocpProactor
2025-02-07 22:22:51,829 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:16:06,447 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 23:16:06,674 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:16:06,677 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:16:06,680 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:16:06,736 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:16:06,739 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:16:06,744 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:16:06,746 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 23:16:06,748 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:16:06,750 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:16:06,752 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 23:16:06,784 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-07 23:16:06,785 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-07 23:16:06,785 [DEBUG] Best backend selected: backend1
2025-02-07 23:16:06,785 [DEBUG] Backend backend2 score: 0.25
2025-02-07 23:16:06,786 [DEBUG] Backend backend2 score: 0.25
2025-02-07 23:16:06,786 [DEBUG] Best backend selected: backend2
2025-02-07 23:16:06,786 [DEBUG] No backend selected.
2025-02-07 23:16:06,794 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:16:06,797 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:16:07,386 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:16:07,390 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:16:07,393 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 23:16:07,394 [DEBUG] Backend backend1 score: 0.7
2025-02-07 23:16:07,394 [DEBUG] Backend backend1 score: 0.7
2025-02-07 23:16:07,394 [DEBUG] Backend backend2 score: 0.25
2025-02-07 23:16:07,394 [DEBUG] Backend backend2 score: 0.25
2025-02-07 23:16:07,395 [DEBUG] Backend backend3 score: 0.21000000000000002
2025-02-07 23:16:07,395 [DEBUG] Backend backend3 score: 0.21000000000000002
2025-02-07 23:16:07,395 [DEBUG] Best backend selected: backend1
2025-02-07 23:16:07,395 [DEBUG] Backend backend2 score: 0.45
2025-02-07 23:16:07,396 [DEBUG] Backend backend2 score: 0.45
2025-02-07 23:16:07,396 [DEBUG] Best backend selected: backend2
2025-02-07 23:16:07,396 [DEBUG] Backend backend2 score: 0.25
2025-02-07 23:16:07,397 [DEBUG] Backend backend2 score: 0.25
2025-02-07 23:16:07,397 [DEBUG] Backend backend3 score: 0.21000000000000002
2025-02-07 23:16:07,397 [DEBUG] Backend backend3 score: 0.21000000000000002
2025-02-07 23:16:07,397 [DEBUG] Best backend selected: backend2
2025-02-07 23:16:07,420 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:16:07,424 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:16:07,483 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:38:43,958 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 23:38:44,145 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:38:44,148 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:38:44,152 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:38:44,157 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:38:44,490 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:38:44,493 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:38:44,496 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 23:38:44,498 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:38:44,503 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:38:44,505 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 23:38:44,759 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-07 23:38:44,759 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-07 23:38:44,759 [DEBUG] Best backend selected: backend1
2025-02-07 23:38:44,760 [DEBUG] Backend backend2 score: 0.25
2025-02-07 23:38:44,760 [DEBUG] Backend backend2 score: 0.25
2025-02-07 23:38:44,761 [DEBUG] Best backend selected: backend2
2025-02-07 23:38:44,761 [DEBUG] No backend selected.
2025-02-07 23:38:44,786 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:38:44,789 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:38:44,812 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:38:44,818 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:38:44,821 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 23:38:44,821 [DEBUG] Backend backend1 score: 0.7
2025-02-07 23:38:44,821 [DEBUG] Backend backend1 score: 0.7
2025-02-07 23:38:44,821 [DEBUG] Backend backend2 score: 0.25
2025-02-07 23:38:44,821 [DEBUG] Backend backend2 score: 0.25
2025-02-07 23:38:44,822 [DEBUG] Backend backend3 score: 0.21000000000000002
2025-02-07 23:38:44,822 [DEBUG] Backend backend3 score: 0.21000000000000002
2025-02-07 23:38:44,822 [DEBUG] Best backend selected: backend1
2025-02-07 23:38:44,822 [DEBUG] Backend backend2 score: 0.55
2025-02-07 23:38:44,822 [DEBUG] Backend backend2 score: 0.55
2025-02-07 23:38:44,822 [DEBUG] Best backend selected: backend2
2025-02-07 23:38:44,822 [DEBUG] Backend backend2 score: 0.25
2025-02-07 23:38:44,823 [DEBUG] Backend backend2 score: 0.25
2025-02-07 23:38:44,823 [DEBUG] Backend backend3 score: 0.21000000000000002
2025-02-07 23:38:44,823 [DEBUG] Backend backend3 score: 0.21000000000000002
2025-02-07 23:38:44,823 [DEBUG] Best backend selected: backend2
2025-02-07 23:38:44,839 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:38:44,842 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:38:44,889 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:52:02,262 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-07 23:52:02,476 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:52:02,478 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:52:02,483 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:52:02,486 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:52:02,489 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:52:02,492 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:52:02,494 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 23:52:02,495 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:52:02,499 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:52:02,503 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 23:52:02,579 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-07 23:52:02,580 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-07 23:52:02,580 [DEBUG] Best backend selected: backend1
2025-02-07 23:52:02,583 [DEBUG] Backend backend2 score: 0.25
2025-02-07 23:52:02,583 [DEBUG] Backend backend2 score: 0.25
2025-02-07 23:52:02,584 [DEBUG] Best backend selected: backend2
2025-02-07 23:52:02,584 [DEBUG] No backend selected.
2025-02-07 23:52:02,589 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:52:02,592 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:52:03,122 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:52:03,125 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:52:03,127 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-07 23:52:03,127 [DEBUG] Backend backend1 score: 0.8
2025-02-07 23:52:03,128 [DEBUG] Backend backend1 score: 0.8
2025-02-07 23:52:03,128 [DEBUG] Backend backend2 score: 0.25
2025-02-07 23:52:03,128 [DEBUG] Backend backend2 score: 0.25
2025-02-07 23:52:03,128 [DEBUG] Backend backend3 score: 0.21000000000000002
2025-02-07 23:52:03,128 [DEBUG] Backend backend3 score: 0.21000000000000002
2025-02-07 23:52:03,128 [DEBUG] Best backend selected: backend1
2025-02-07 23:52:03,128 [DEBUG] Backend backend2 score: 0.55
2025-02-07 23:52:03,128 [DEBUG] Backend backend2 score: 0.55
2025-02-07 23:52:03,129 [DEBUG] Best backend selected: backend2
2025-02-07 23:52:03,129 [DEBUG] Backend backend2 score: 0.25
2025-02-07 23:52:03,129 [DEBUG] Backend backend2 score: 0.25
2025-02-07 23:52:03,129 [DEBUG] Backend backend3 score: 0.21000000000000002
2025-02-07 23:52:03,129 [DEBUG] Backend backend3 score: 0.21000000000000002
2025-02-07 23:52:03,129 [DEBUG] Best backend selected: backend2
2025-02-07 23:52:03,150 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:52:03,153 [DEBUG] Using proactor: IocpProactor
2025-02-07 23:52:03,177 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:01:08,008 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 00:01:08,129 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:01:08,139 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:01:08,148 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:01:08,157 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:01:08,178 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:01:08,184 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:01:08,198 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 00:01:08,205 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:01:08,214 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:01:08,217 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 00:01:08,233 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 00:01:08,233 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 00:01:08,234 [DEBUG] Best backend selected: backend1
2025-02-08 00:01:08,234 [DEBUG] Backend backend2 score: 0.25
2025-02-08 00:01:08,234 [DEBUG] Backend backend2 score: 0.25
2025-02-08 00:01:08,235 [DEBUG] Best backend selected: backend2
2025-02-08 00:01:08,239 [DEBUG] No backend selected.
2025-02-08 00:01:08,245 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:01:08,247 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:01:08,577 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:01:39,896 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 00:01:40,001 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:01:40,003 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:01:40,009 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:01:40,013 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:01:40,016 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:01:40,019 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:01:40,023 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 00:01:40,026 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:01:40,031 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:01:40,033 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 00:01:40,061 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 00:01:40,062 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 00:01:40,062 [DEBUG] Best backend selected: backend1
2025-02-08 00:01:40,062 [DEBUG] Backend backend2 score: 0.25
2025-02-08 00:01:40,063 [DEBUG] Backend backend2 score: 0.25
2025-02-08 00:01:40,063 [DEBUG] Best backend selected: backend2
2025-02-08 00:01:40,063 [DEBUG] No backend selected.
2025-02-08 00:01:40,069 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:01:40,075 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:01:40,696 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:02:27,034 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 00:02:27,150 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:02:27,152 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:02:27,156 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:02:27,159 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:02:27,165 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:02:27,169 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:02:27,173 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 00:02:27,174 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:02:27,176 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:02:27,182 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 00:02:27,222 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 00:02:27,222 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 00:02:27,222 [DEBUG] Best backend selected: backend1
2025-02-08 00:02:27,223 [DEBUG] Backend backend2 score: 0.25
2025-02-08 00:02:27,223 [DEBUG] Backend backend2 score: 0.25
2025-02-08 00:02:27,223 [DEBUG] Best backend selected: backend2
2025-02-08 00:02:27,223 [DEBUG] No backend selected.
2025-02-08 00:02:27,231 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:02:27,234 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:02:27,953 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:02:55,847 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 00:02:55,949 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:02:55,952 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:02:55,956 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:02:55,960 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:02:55,964 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:02:55,967 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:02:55,970 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 00:02:55,971 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:02:55,975 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:02:55,977 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 00:02:56,023 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 00:02:56,024 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 00:02:56,024 [DEBUG] Best backend selected: backend1
2025-02-08 00:02:56,024 [DEBUG] Backend backend2 score: 0.25
2025-02-08 00:02:56,025 [DEBUG] Backend backend2 score: 0.25
2025-02-08 00:02:56,025 [DEBUG] Best backend selected: backend2
2025-02-08 00:02:56,026 [DEBUG] No backend selected.
2025-02-08 00:02:56,034 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:02:56,039 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:02:56,642 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:05:14,647 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 00:05:14,778 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:05:14,781 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:05:14,790 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:05:14,794 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:05:14,802 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:05:14,811 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:05:14,813 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 00:05:14,816 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:05:14,819 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:05:14,826 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 00:05:14,876 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 00:05:14,877 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 00:05:14,877 [DEBUG] Best backend selected: backend1
2025-02-08 00:05:14,877 [DEBUG] Backend backend2 score: 0.25
2025-02-08 00:05:14,878 [DEBUG] Backend backend2 score: 0.25
2025-02-08 00:05:14,878 [DEBUG] Best backend selected: backend2
2025-02-08 00:05:14,878 [DEBUG] No backend selected.
2025-02-08 00:05:14,880 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:05:14,883 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:05:15,659 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:07:49,455 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 00:07:49,567 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:07:49,569 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:07:49,573 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:07:49,580 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:07:49,585 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:07:49,588 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:07:49,590 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 00:07:49,597 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:07:49,599 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:07:49,601 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 00:07:49,673 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 00:07:49,673 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 00:07:49,673 [DEBUG] Best backend selected: backend1
2025-02-08 00:07:49,680 [DEBUG] Backend backend2 score: 0.25
2025-02-08 00:07:49,680 [DEBUG] Backend backend2 score: 0.25
2025-02-08 00:07:49,681 [DEBUG] Best backend selected: backend2
2025-02-08 00:07:49,682 [DEBUG] No backend selected.
2025-02-08 00:07:49,697 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:07:49,701 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:07:50,460 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:10:57,685 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 00:10:57,825 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:10:57,828 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:10:57,837 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:10:57,842 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:10:57,846 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:10:57,859 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:10:57,862 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 00:10:57,870 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:10:57,879 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:10:57,891 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 00:10:57,988 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 00:10:57,989 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 00:10:57,989 [DEBUG] Best backend selected: backend1
2025-02-08 00:10:57,990 [DEBUG] Backend backend2 score: 0.25
2025-02-08 00:10:57,990 [DEBUG] Backend backend2 score: 0.25
2025-02-08 00:10:57,990 [DEBUG] Best backend selected: backend2
2025-02-08 00:10:57,991 [DEBUG] No backend selected.
2025-02-08 00:10:57,997 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:10:58,003 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:10:58,644 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:14:02,672 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 00:14:02,784 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:14:02,786 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:14:02,789 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:14:02,796 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:14:02,799 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:14:02,801 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:14:02,803 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 00:14:02,805 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:14:02,811 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:14:02,814 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 00:14:02,855 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 00:14:02,856 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 00:14:02,856 [DEBUG] Best backend selected: backend1
2025-02-08 00:14:02,859 [DEBUG] Backend backend2 score: 0.25
2025-02-08 00:14:02,860 [DEBUG] Backend backend2 score: 0.25
2025-02-08 00:14:02,860 [DEBUG] Best backend selected: backend2
2025-02-08 00:14:02,865 [DEBUG] No backend selected.
2025-02-08 00:14:02,866 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:14:02,869 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:14:03,433 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:17:14,135 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 00:17:14,240 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:17:14,242 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:17:14,250 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:17:14,254 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:17:14,258 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:17:14,272 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:17:14,275 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 00:17:14,282 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:17:14,286 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:17:14,288 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 00:17:14,374 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 00:17:14,374 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 00:17:14,374 [DEBUG] Best backend selected: backend1
2025-02-08 00:17:14,375 [DEBUG] Backend backend2 score: 0.25
2025-02-08 00:17:14,375 [DEBUG] Backend backend2 score: 0.25
2025-02-08 00:17:14,376 [DEBUG] Best backend selected: backend2
2025-02-08 00:17:14,376 [DEBUG] No backend selected.
2025-02-08 00:17:14,381 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:17:14,384 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:17:15,402 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:35:31,193 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 00:35:31,304 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:35:31,306 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:35:31,309 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:35:31,312 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:35:31,319 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:35:31,324 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:35:31,326 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 00:35:31,327 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:35:31,333 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:35:31,336 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 00:35:31,384 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 00:35:31,384 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 00:35:31,384 [DEBUG] Best backend selected: backend1
2025-02-08 00:35:31,385 [DEBUG] Backend backend2 score: 0.25
2025-02-08 00:35:31,385 [DEBUG] Backend backend2 score: 0.25
2025-02-08 00:35:31,385 [DEBUG] Best backend selected: backend2
2025-02-08 00:35:31,386 [DEBUG] No backend selected.
2025-02-08 00:35:31,391 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:35:31,394 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:35:32,222 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:46:26,238 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 00:46:26,366 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:46:26,369 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:46:26,372 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:46:26,376 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:46:26,379 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:46:26,385 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:46:26,388 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 00:46:26,393 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:46:26,397 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:46:26,404 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 00:46:26,525 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 00:46:26,526 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 00:46:26,526 [DEBUG] Best backend selected: backend1
2025-02-08 00:46:26,527 [DEBUG] Backend backend2 score: 0.25
2025-02-08 00:46:26,527 [DEBUG] Backend backend2 score: 0.25
2025-02-08 00:46:26,527 [DEBUG] Best backend selected: backend2
2025-02-08 00:46:26,528 [DEBUG] No backend selected.
2025-02-08 00:46:26,535 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:46:26,539 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:46:27,559 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:51:14,682 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 00:51:14,790 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:51:14,792 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:51:14,800 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:51:14,804 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:51:14,808 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:51:14,815 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:51:14,820 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 00:51:14,822 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:51:14,829 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:51:14,834 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 00:51:14,919 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 00:51:14,919 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 00:51:14,920 [DEBUG] Best backend selected: backend1
2025-02-08 00:51:14,920 [DEBUG] Backend backend2 score: 0.25
2025-02-08 00:51:14,920 [DEBUG] Backend backend2 score: 0.25
2025-02-08 00:51:14,921 [DEBUG] Best backend selected: backend2
2025-02-08 00:51:14,922 [DEBUG] No backend selected.
2025-02-08 00:51:14,931 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:51:14,996 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:51:15,841 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:56:36,184 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 00:56:36,290 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:56:36,294 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:56:36,298 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:56:36,301 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:56:36,305 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:56:36,314 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:56:36,318 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 00:56:36,320 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:56:36,323 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:56:36,330 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 00:56:36,419 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 00:56:36,419 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 00:56:36,420 [DEBUG] Best backend selected: backend1
2025-02-08 00:56:36,420 [DEBUG] Backend backend2 score: 0.25
2025-02-08 00:56:36,421 [DEBUG] Backend backend2 score: 0.25
2025-02-08 00:56:36,421 [DEBUG] Best backend selected: backend2
2025-02-08 00:56:36,422 [DEBUG] No backend selected.
2025-02-08 00:56:36,429 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:56:36,433 [DEBUG] Using proactor: IocpProactor
2025-02-08 00:56:37,339 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:01:29,027 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 01:01:29,145 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:01:29,147 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:01:29,153 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:01:29,157 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:01:29,161 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:01:29,164 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:01:29,170 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 01:01:29,172 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:01:29,178 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:01:29,180 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 01:01:29,222 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 01:01:29,222 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 01:01:29,223 [DEBUG] Best backend selected: backend1
2025-02-08 01:01:29,223 [DEBUG] Backend backend2 score: 0.25
2025-02-08 01:01:29,223 [DEBUG] Backend backend2 score: 0.25
2025-02-08 01:01:29,224 [DEBUG] Best backend selected: backend2
2025-02-08 01:01:29,224 [DEBUG] No backend selected.
2025-02-08 01:01:29,230 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:01:29,232 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:01:29,887 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:08:11,900 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 01:08:12,014 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:08:12,016 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:08:12,020 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:08:12,026 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:08:12,029 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:08:12,032 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:08:12,034 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 01:08:12,036 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:08:12,042 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:08:12,044 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 01:08:12,098 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 01:08:12,099 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 01:08:12,099 [DEBUG] Best backend selected: backend1
2025-02-08 01:08:12,099 [DEBUG] Backend backend2 score: 0.25
2025-02-08 01:08:12,100 [DEBUG] Backend backend2 score: 0.25
2025-02-08 01:08:12,100 [DEBUG] Best backend selected: backend2
2025-02-08 01:08:12,100 [DEBUG] No backend selected.
2025-02-08 01:08:12,108 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:08:12,111 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:08:13,201 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:17:52,738 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 01:17:52,863 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:17:52,865 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:17:52,868 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:17:52,872 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:17:52,875 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:17:52,881 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:17:52,884 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 01:17:52,887 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:17:52,890 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:17:52,897 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 01:17:52,938 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 01:17:52,939 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 01:17:52,939 [DEBUG] Best backend selected: backend1
2025-02-08 01:17:52,939 [DEBUG] Backend backend2 score: 0.25
2025-02-08 01:17:52,940 [DEBUG] Backend backend2 score: 0.25
2025-02-08 01:17:52,940 [DEBUG] Best backend selected: backend2
2025-02-08 01:17:52,940 [DEBUG] No backend selected.
2025-02-08 01:17:52,947 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:17:52,951 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:17:53,651 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:23:19,796 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 01:23:19,911 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:23:19,915 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:23:19,921 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:23:19,925 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:23:19,928 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:23:19,931 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:23:19,933 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 01:23:19,937 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:23:19,941 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:23:19,946 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 01:23:19,990 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 01:23:19,990 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 01:23:19,990 [DEBUG] Best backend selected: backend1
2025-02-08 01:23:19,991 [DEBUG] Backend backend2 score: 0.25
2025-02-08 01:23:19,991 [DEBUG] Backend backend2 score: 0.25
2025-02-08 01:23:19,991 [DEBUG] Best backend selected: backend2
2025-02-08 01:23:19,991 [DEBUG] No backend selected.
2025-02-08 01:23:19,997 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:23:19,999 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:23:20,758 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:28:48,829 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 01:28:48,940 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:28:48,946 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:28:48,950 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:28:48,954 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:28:48,957 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:28:48,964 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:28:48,968 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 01:28:48,970 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:28:48,973 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:28:48,978 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 01:28:49,034 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 01:28:49,035 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 01:28:49,035 [DEBUG] Best backend selected: backend1
2025-02-08 01:28:49,036 [DEBUG] Backend backend2 score: 0.25
2025-02-08 01:28:49,036 [DEBUG] Backend backend2 score: 0.25
2025-02-08 01:28:49,036 [DEBUG] Best backend selected: backend2
2025-02-08 01:28:49,037 [DEBUG] No backend selected.
2025-02-08 01:28:49,046 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:28:49,049 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:28:49,801 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:35:18,422 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 01:35:18,537 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:35:18,539 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:35:18,546 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:35:18,550 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:35:18,553 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:35:18,556 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:35:18,558 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 01:35:18,563 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:35:18,566 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:35:18,570 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 01:35:18,619 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 01:35:18,620 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 01:35:18,620 [DEBUG] Best backend selected: backend1
2025-02-08 01:35:18,621 [DEBUG] Backend backend2 score: 0.25
2025-02-08 01:35:18,621 [DEBUG] Backend backend2 score: 0.25
2025-02-08 01:35:18,621 [DEBUG] Best backend selected: backend2
2025-02-08 01:35:18,621 [DEBUG] No backend selected.
2025-02-08 01:35:18,631 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:35:18,634 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:35:19,436 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:46:11,165 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 01:46:11,274 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:46:11,276 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:46:11,283 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:46:11,287 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:46:11,291 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:46:11,293 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:46:11,295 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 01:46:11,299 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:46:11,302 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:46:11,305 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 01:46:11,347 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 01:46:11,348 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 01:46:11,348 [DEBUG] Best backend selected: backend1
2025-02-08 01:46:11,349 [DEBUG] Backend backend2 score: 0.25
2025-02-08 01:46:11,349 [DEBUG] Backend backend2 score: 0.25
2025-02-08 01:46:11,350 [DEBUG] Best backend selected: backend2
2025-02-08 01:46:11,350 [DEBUG] No backend selected.
2025-02-08 01:46:11,356 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:46:11,360 [DEBUG] Using proactor: IocpProactor
2025-02-08 01:46:11,985 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:23:16,847 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 02:23:16,959 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:23:16,961 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:23:16,965 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:23:16,968 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:23:16,978 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:23:16,984 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:23:16,986 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 02:23:16,995 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:23:16,998 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:23:17,001 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 02:23:17,077 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 02:23:17,078 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 02:23:17,078 [DEBUG] Best backend selected: backend1
2025-02-08 02:23:17,079 [DEBUG] Backend backend2 score: 0.25
2025-02-08 02:23:17,083 [DEBUG] Backend backend2 score: 0.25
2025-02-08 02:23:17,084 [DEBUG] Best backend selected: backend2
2025-02-08 02:23:17,084 [DEBUG] No backend selected.
2025-02-08 02:23:17,087 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:23:17,102 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:23:17,928 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:24:08,912 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 02:24:09,045 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:24:09,047 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:24:09,051 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:24:09,062 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:24:09,069 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:24:09,078 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:24:09,080 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 02:24:09,082 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:24:09,088 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:24:09,097 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 02:24:09,280 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 02:24:09,281 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 02:24:09,281 [DEBUG] Best backend selected: backend1
2025-02-08 02:24:09,282 [DEBUG] Backend backend2 score: 0.25
2025-02-08 02:24:09,283 [DEBUG] Backend backend2 score: 0.25
2025-02-08 02:24:09,283 [DEBUG] Best backend selected: backend2
2025-02-08 02:24:09,283 [DEBUG] No backend selected.
2025-02-08 02:24:09,292 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:24:09,296 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:24:10,109 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:24:59,455 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 02:24:59,587 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:24:59,589 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:24:59,595 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:24:59,599 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:24:59,602 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:24:59,605 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:24:59,607 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 02:24:59,611 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:24:59,615 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:24:59,619 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 02:24:59,658 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 02:24:59,662 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 02:24:59,663 [DEBUG] Best backend selected: backend1
2025-02-08 02:24:59,664 [DEBUG] Backend backend2 score: 0.25
2025-02-08 02:24:59,664 [DEBUG] Backend backend2 score: 0.25
2025-02-08 02:24:59,664 [DEBUG] Best backend selected: backend2
2025-02-08 02:24:59,665 [DEBUG] No backend selected.
2025-02-08 02:24:59,670 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:24:59,672 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:25:00,157 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:25:51,430 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 02:25:51,642 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:25:51,651 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:25:51,666 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:25:51,682 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:25:51,689 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:25:51,698 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:25:51,701 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 02:25:51,704 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:25:51,710 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:25:51,716 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 02:25:51,801 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 02:25:51,802 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 02:25:51,802 [DEBUG] Best backend selected: backend1
2025-02-08 02:25:51,803 [DEBUG] Backend backend2 score: 0.25
2025-02-08 02:25:51,803 [DEBUG] Backend backend2 score: 0.25
2025-02-08 02:25:51,803 [DEBUG] Best backend selected: backend2
2025-02-08 02:25:51,804 [DEBUG] No backend selected.
2025-02-08 02:25:51,812 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:25:51,816 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:25:52,729 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:32:04,185 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 02:32:04,308 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:32:04,314 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:32:04,317 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:32:04,321 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:32:04,324 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:32:04,330 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:32:04,332 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 02:32:04,334 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:32:04,338 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:32:04,341 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 02:32:04,390 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 02:32:04,390 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 02:32:04,391 [DEBUG] Best backend selected: backend1
2025-02-08 02:32:04,391 [DEBUG] Backend backend2 score: 0.25
2025-02-08 02:32:04,391 [DEBUG] Backend backend2 score: 0.25
2025-02-08 02:32:04,392 [DEBUG] Best backend selected: backend2
2025-02-08 02:32:04,395 [DEBUG] No backend selected.
2025-02-08 02:32:04,398 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:32:04,401 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:32:05,467 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:41:05,827 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 02:41:05,947 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:41:05,949 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:41:05,956 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:41:05,960 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:41:05,964 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:41:05,972 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:41:05,974 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 02:41:05,976 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:41:05,981 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:41:05,987 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 02:41:06,038 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 02:41:06,038 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 02:41:06,039 [DEBUG] Best backend selected: backend1
2025-02-08 02:41:06,040 [DEBUG] Backend backend2 score: 0.25
2025-02-08 02:41:06,040 [DEBUG] Backend backend2 score: 0.25
2025-02-08 02:41:06,040 [DEBUG] Best backend selected: backend2
2025-02-08 02:41:06,041 [DEBUG] No backend selected.
2025-02-08 02:41:06,046 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:41:06,049 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:41:06,790 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:47:46,062 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 02:47:46,204 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:47:46,207 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:47:46,221 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:47:46,239 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:47:46,250 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:47:46,254 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:47:46,269 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 02:47:46,290 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:47:46,307 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:47:46,321 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 02:47:46,468 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 02:47:46,469 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 02:47:46,470 [DEBUG] Best backend selected: backend1
2025-02-08 02:47:46,471 [DEBUG] Backend backend2 score: 0.25
2025-02-08 02:47:46,472 [DEBUG] Backend backend2 score: 0.25
2025-02-08 02:47:46,472 [DEBUG] Best backend selected: backend2
2025-02-08 02:47:46,476 [DEBUG] No backend selected.
2025-02-08 02:47:46,484 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:47:46,487 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:47:47,428 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:54:24,331 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 02:54:24,451 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:54:24,453 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:54:24,456 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:54:24,459 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:54:24,464 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:54:24,468 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:54:24,471 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 02:54:24,475 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:54:24,478 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:54:24,481 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 02:54:24,522 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 02:54:24,522 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 02:54:24,523 [DEBUG] Best backend selected: backend1
2025-02-08 02:54:24,524 [DEBUG] Backend backend2 score: 0.25
2025-02-08 02:54:24,528 [DEBUG] Backend backend2 score: 0.25
2025-02-08 02:54:24,529 [DEBUG] Best backend selected: backend2
2025-02-08 02:54:24,529 [DEBUG] No backend selected.
2025-02-08 02:54:24,532 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:54:24,535 [DEBUG] Using proactor: IocpProactor
2025-02-08 02:54:24,899 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:01:05,212 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 03:01:05,322 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:01:05,324 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:01:05,327 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:01:05,330 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:01:05,338 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:01:05,343 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:01:05,345 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 03:01:05,347 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:01:05,355 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:01:05,357 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 03:01:05,403 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 03:01:05,403 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 03:01:05,404 [DEBUG] Best backend selected: backend1
2025-02-08 03:01:05,404 [DEBUG] Backend backend2 score: 0.25
2025-02-08 03:01:05,408 [DEBUG] Backend backend2 score: 0.25
2025-02-08 03:01:05,409 [DEBUG] Best backend selected: backend2
2025-02-08 03:01:05,409 [DEBUG] No backend selected.
2025-02-08 03:01:05,410 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:01:05,413 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:01:05,762 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:12:04,688 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 03:12:04,838 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:12:04,845 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:12:04,851 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:12:04,859 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:12:04,867 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:12:04,871 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:12:04,880 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 03:12:04,881 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:12:04,884 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:12:04,886 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 03:12:04,947 [DEBUG] No backend selected.
2025-02-08 03:12:05,448 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:18:51,679 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 03:18:51,823 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:18:51,826 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:18:51,829 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:18:51,841 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:18:51,858 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:18:51,863 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:18:51,871 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 03:18:51,874 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:18:51,877 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:18:51,880 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 03:18:52,057 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 03:18:52,057 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 03:18:52,057 [DEBUG] Best backend selected: backend1
2025-02-08 03:18:52,058 [DEBUG] Backend backend2 score: 0.25
2025-02-08 03:18:52,062 [DEBUG] Backend backend2 score: 0.25
2025-02-08 03:18:52,062 [DEBUG] Best backend selected: backend2
2025-02-08 03:18:52,062 [DEBUG] No backend selected.
2025-02-08 03:18:52,064 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:18:52,070 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:18:53,132 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:24:37,783 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 03:24:37,901 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:24:37,906 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:24:37,910 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:24:37,914 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:24:37,917 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:24:37,924 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:24:37,927 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 03:24:37,930 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:24:37,934 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:24:37,942 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 03:24:38,089 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 03:24:38,091 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 03:24:38,091 [DEBUG] Best backend selected: backend1
2025-02-08 03:24:38,092 [DEBUG] Backend backend2 score: 0.25
2025-02-08 03:24:38,092 [DEBUG] Backend backend2 score: 0.25
2025-02-08 03:24:38,092 [DEBUG] Best backend selected: backend2
2025-02-08 03:24:38,093 [DEBUG] No backend selected.
2025-02-08 03:24:38,099 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:24:38,104 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:24:38,710 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:31:23,027 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 03:31:23,156 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:31:23,158 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:31:23,161 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:31:23,166 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:31:23,170 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:31:23,172 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:31:23,175 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 03:31:23,176 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:31:23,179 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:31:23,182 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 03:31:23,225 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 03:31:23,225 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 03:31:23,226 [DEBUG] Best backend selected: backend1
2025-02-08 03:31:23,226 [DEBUG] Backend backend2 score: 0.25
2025-02-08 03:31:23,226 [DEBUG] Backend backend2 score: 0.25
2025-02-08 03:31:23,227 [DEBUG] Best backend selected: backend2
2025-02-08 03:31:23,227 [DEBUG] No backend selected.
2025-02-08 03:31:23,234 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:31:23,237 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:31:23,578 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:40:22,202 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 03:40:22,353 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:40:22,358 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:40:22,365 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:40:22,374 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:40:22,383 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:40:22,388 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:40:22,393 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 03:40:22,401 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:40:22,407 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:40:22,426 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 03:40:22,489 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 03:40:22,489 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 03:40:22,489 [DEBUG] Best backend selected: backend1
2025-02-08 03:40:22,490 [DEBUG] Backend backend2 score: 0.25
2025-02-08 03:40:22,495 [DEBUG] Backend backend2 score: 0.25
2025-02-08 03:40:22,497 [DEBUG] Best backend selected: backend2
2025-02-08 03:40:22,497 [DEBUG] No backend selected.
2025-02-08 03:40:22,500 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:40:22,503 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:40:23,302 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:47:16,628 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 03:47:16,745 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:47:16,747 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:47:16,750 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:47:16,757 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:47:16,760 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:47:16,763 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:47:16,765 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 03:47:16,766 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:47:16,772 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:47:16,775 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 03:47:16,824 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 03:47:16,824 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 03:47:16,825 [DEBUG] Best backend selected: backend1
2025-02-08 03:47:16,825 [DEBUG] Backend backend2 score: 0.25
2025-02-08 03:47:16,829 [DEBUG] Backend backend2 score: 0.25
2025-02-08 03:47:16,830 [DEBUG] Best backend selected: backend2
2025-02-08 03:47:16,830 [DEBUG] No backend selected.
2025-02-08 03:47:16,831 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:47:16,834 [DEBUG] Using proactor: IocpProactor
2025-02-08 03:47:17,201 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:00:41,476 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 04:00:41,594 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:00:41,596 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:00:41,599 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:00:41,606 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:00:41,609 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:00:41,612 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:00:41,614 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 04:00:41,616 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:00:41,622 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:00:41,625 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 04:00:41,671 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 04:00:41,672 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 04:00:41,672 [DEBUG] Best backend selected: backend1
2025-02-08 04:00:41,673 [DEBUG] Backend backend2 score: 0.25
2025-02-08 04:00:41,673 [DEBUG] Backend backend2 score: 0.25
2025-02-08 04:00:41,674 [DEBUG] Best backend selected: backend2
2025-02-08 04:00:41,674 [DEBUG] No backend selected.
2025-02-08 04:00:41,679 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:00:41,682 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:00:42,437 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:18:28,154 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 04:18:28,262 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:18:28,264 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:18:28,267 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:18:28,274 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:18:28,277 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:18:28,280 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:18:28,282 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 04:18:28,283 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:18:28,290 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:18:28,294 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 04:18:28,341 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 04:18:28,342 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 04:18:28,342 [DEBUG] Best backend selected: backend1
2025-02-08 04:18:28,343 [DEBUG] Backend backend2 score: 0.25
2025-02-08 04:18:28,343 [DEBUG] Backend backend2 score: 0.25
2025-02-08 04:18:28,343 [DEBUG] Best backend selected: backend2
2025-02-08 04:18:28,343 [DEBUG] No backend selected.
2025-02-08 04:18:28,349 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:18:28,354 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:18:28,654 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:27:33,686 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 04:27:33,794 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:27:33,797 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:27:33,802 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:27:33,805 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:27:33,808 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:27:33,813 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:27:33,819 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 04:27:33,821 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:27:33,824 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:27:33,826 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 04:27:33,870 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 04:27:33,871 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 04:27:33,871 [DEBUG] Best backend selected: backend1
2025-02-08 04:27:33,871 [DEBUG] Backend backend2 score: 0.25
2025-02-08 04:27:33,872 [DEBUG] Backend backend2 score: 0.25
2025-02-08 04:27:33,872 [DEBUG] Best backend selected: backend2
2025-02-08 04:27:33,872 [DEBUG] No backend selected.
2025-02-08 04:27:33,880 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:27:33,883 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:27:34,191 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:34:20,540 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 04:34:20,654 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:34:20,656 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:34:20,661 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:34:20,665 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:34:20,668 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:34:20,671 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:34:20,674 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 04:34:20,677 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:34:20,683 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:34:20,685 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 04:34:20,730 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 04:34:20,731 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 04:34:20,731 [DEBUG] Best backend selected: backend1
2025-02-08 04:34:20,731 [DEBUG] Backend backend2 score: 0.25
2025-02-08 04:34:20,735 [DEBUG] Backend backend2 score: 0.25
2025-02-08 04:34:20,735 [DEBUG] Best backend selected: backend2
2025-02-08 04:34:20,736 [DEBUG] No backend selected.
2025-02-08 04:34:20,737 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:34:20,740 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:34:21,056 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:41:07,628 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 04:41:07,745 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:41:07,747 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:41:07,751 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:41:07,756 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:41:07,761 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:41:07,763 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:41:07,766 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 04:41:07,767 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:41:07,772 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:41:07,775 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 04:41:07,822 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 04:41:07,823 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 04:41:07,823 [DEBUG] Best backend selected: backend1
2025-02-08 04:41:07,824 [DEBUG] Backend backend2 score: 0.25
2025-02-08 04:41:07,828 [DEBUG] Backend backend2 score: 0.25
2025-02-08 04:41:07,828 [DEBUG] Best backend selected: backend2
2025-02-08 04:41:07,828 [DEBUG] No backend selected.
2025-02-08 04:41:07,830 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:41:07,832 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:41:08,175 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:47:56,308 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 04:47:56,422 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:47:56,424 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:47:56,428 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:47:56,435 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:47:56,441 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:47:56,444 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:47:56,449 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 04:47:56,453 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:47:56,455 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:47:56,457 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 04:47:56,507 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 04:47:56,507 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 04:47:56,507 [DEBUG] Best backend selected: backend1
2025-02-08 04:47:56,512 [DEBUG] Backend backend2 score: 0.25
2025-02-08 04:47:56,512 [DEBUG] Backend backend2 score: 0.25
2025-02-08 04:47:56,512 [DEBUG] Best backend selected: backend2
2025-02-08 04:47:56,512 [DEBUG] No backend selected.
2025-02-08 04:47:56,517 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:47:56,519 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:47:56,815 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:59:11,409 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 04:59:11,524 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:59:11,528 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:59:11,532 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:59:11,536 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:59:11,539 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:59:11,541 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:59:11,548 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 04:59:11,552 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:59:11,555 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:59:11,557 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 04:59:11,603 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 04:59:11,603 [DEBUG] Backend backend1 score: 0.30000000000000004
2025-02-08 04:59:11,603 [DEBUG] Best backend selected: backend1
2025-02-08 04:59:11,607 [DEBUG] Backend backend2 score: 0.25
2025-02-08 04:59:11,608 [DEBUG] Backend backend2 score: 0.25
2025-02-08 04:59:11,609 [DEBUG] Best backend selected: backend2
2025-02-08 04:59:11,610 [DEBUG] No backend selected.
2025-02-08 04:59:11,613 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:59:11,616 [DEBUG] Using proactor: IocpProactor
2025-02-08 04:59:12,189 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:06:45,778 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 08:06:45,885 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:06:45,887 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:06:45,894 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:06:45,897 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:06:45,900 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:06:45,903 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:06:45,905 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 08:06:45,909 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:06:45,913 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:06:45,918 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 08:06:46,587 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:07:49,704 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 08:07:49,826 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:07:49,828 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:07:49,836 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:07:49,839 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:07:49,842 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:07:49,844 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:07:49,851 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 08:07:49,852 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:07:49,857 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:07:49,859 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 08:07:50,259 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:10:32,717 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 08:10:32,836 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:10:32,838 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:10:32,841 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:10:32,849 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:10:32,853 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:10:32,856 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:10:32,858 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 08:10:32,866 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:10:32,871 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:10:32,873 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 08:10:33,257 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:11:53,064 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 08:11:53,181 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:11:53,183 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:11:53,190 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:11:53,193 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:11:53,196 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:11:53,198 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:11:53,205 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 08:11:53,207 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:11:53,212 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:11:53,215 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 08:11:53,533 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:13:38,161 [WARNING] DuckDuckGo search functionality not available. Install with: pip install duckduckgo-search
2025-02-08 08:13:38,284 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:13:38,290 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:13:38,293 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:13:38,296 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:13:38,299 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:13:38,306 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:13:38,309 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 08:13:38,313 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:13:38,315 [DEBUG] Using proactor: IocpProactor
2025-02-08 08:13:38,317 [INFO] Initialized Crawl4AI backend with config: Crawl4AIConfig(max_depth=5, max_pages=100, follow_links=True, rate_limit=2.0, verify_ssl=True, concurrent_requests=10)
2025-02-08 08:13:38,644 [DEBUG] Using proactor: IocpProactor
