
# Generated by Qodo Gen

import pytest

class TestCodeUnderTest:

    # Process valid HTTP/HTTPS URLs with all components (scheme, netloc, path, query, params)
    def test_process_link_with_valid_url_components(self):
        # Arrange
        config = {
            'allowed_schemes': ['http', 'https'],
            'allowed_domains': ['example.com']
        }
        processor = LinkProcessor(config)
        test_url = "https://example.com/path?key=value;param=1#fragment"

        # Act
        result = processor.process_link(test_url)

        # Assert
        assert result['url'] == test_url
        assert result['scheme'] == 'https'
        assert result['netloc'] == 'example.com'
        assert result['path'] == '/path'
        assert result['query'] == 'key=value;param=1'
        assert result['fragment'] == 'fragment'
        assert result['is_valid'] is True
        assert result['is_relative'] is False

    # Verify the processing of an empty URL and check the returned dictionary's properties.
    def test_process_link_with_empty_url(self):
        # Arrange
        processor = LinkProcessor()
        empty_url = ""

        # Act
        result = processor.process_link(empty_url)

        # Assert
        assert result['url'] == ""
        assert result['is_valid'] is True
        assert result['is_relative'] is True
        assert result['is_internal'] is False

    # Process multiple URLs in a batch with process_links method
    def test_process_links_with_multiple_urls(self):
        # Arrange
        config = {
            'allowed_schemes': ['http', 'https'],
            'allowed_domains': ['example.com', 'test.com']
        }
        processor = LinkProcessor(config)
        urls = [
            "https://example.com/path1",
            "http://test.com/path2?query=1",
            "https://invalid.com/path3"
        ]

        # Act
        results = processor.process_links(urls)

        # Assert
        assert len(results) == 3
        assert results[0]['url'] == "https://example.com/path1"
        assert results[0]['is_valid'] is True
        assert results[1]['url'] == "http://test.com/path2?query=1"
        assert results[1]['is_valid'] is True
        assert results[2]['url'] == "https://invalid.com/path3"
        assert results[2]['is_valid'] is False

    # Extract and normalize URLs from HTML content using BeautifulSoup
    def test_extract_and_normalize_urls(self, mocker):
        # Arrange
        from bs4 import BeautifulSoup
        html_content = '''
        <html>
            <body>
                <a href="http://example.com/page1">Page 1</a>
                <a href="/page2">Page 2</a>
                <a href="http://example.com/page3">Page 3</a>
            </body>
        </html>
        '''
        soup = BeautifulSoup(html_content, 'html.parser')
        config = {
            'base_url': 'http://example.com',
            'allowed_schemes': ['http', 'https'],
            'allowed_domains': ['example.com']
        }
        processor = LinkProcessor(config)

        # Act
        result = processor.extract_links(soup)

        # Assert
        assert len(result) == 3
        assert result[0]['url'] == 'http://example.com/page1'
        assert result[0]['text'] == 'Page 1'
        assert result[1]['url'] == 'http://example.com/page2'
        assert result[1]['text'] == 'Page 2'
        assert result[2]['url'] == 'http://example.com/page3'
        assert result[2]['text'] == 'Page 3'

    # Resolve relative URLs against provided base_url
    def test_process_link_resolves_relative_url(self):
        # Arrange
        config = {
            'base_url': 'https://example.com/base/',
            'allowed_schemes': ['http', 'https'],
            'allowed_domains': ['example.com']
        }
        processor = LinkProcessor(config)
        relative_url = "relative/path"

        # Act
        result = processor.process_link(relative_url)

        # Assert
        expected_url = 'https://example.com/base/relative/path'
        assert result['url'] == expected_url
        assert result['scheme'] == 'https'
        assert result['netloc'] == 'example.com'
        assert result['path'] == '/base/relative/path'
        assert result['is_valid'] is True
        assert result['is_relative'] is False

    # Ensure the normalize_url method resolves paths correctly and removes fragments
    def test_normalize_url_correctly_resolves_paths(self):
        # Arrange
        processor = LinkProcessor({'base_url': 'https://example.com'})
        test_url = "https://example.com/path/to/resource/../another#section"

        # Act
        normalized_url = processor.normalize_url(test_url)

        # Assert
        assert normalized_url == "https://example.com/path/to/another"

    # Handle protocol-relative URLs starting with //
    def test_normalize_url_with_protocol_relative(self):
        # Arrange
        processor = LinkProcessor()
        protocol_relative_url = "//example.com/path"

        # Act
        normalized_url = processor.normalize_url(protocol_relative_url)

        # Assert
        assert normalized_url == "https://example.com/path"

    # Process URLs with special characters in domain names
    def test_process_link_with_special_characters_in_domain(self):
        # Arrange
        config = {
            'allowed_schemes': ['http', 'https'],
            'allowed_domains': ['xn--bcher-kva.com']  # Punycode for b√ºcher.com
        }
        processor = LinkProcessor(config)
        test_url = "https://xn--bcher-kva.com/path?key=value#fragment"

        # Act
        result = processor.process_link(test_url)

        # Assert
        assert result['url'] == test_url
        assert result['scheme'] == 'https'
        assert result['netloc'] == 'xn--bcher-kva.com'
        assert result['path'] == '/path'
        assert result['query'] == 'key=value'
        assert result['fragment'] == 'fragment'
        assert result['is_valid'] is True
        assert result['is_relative'] is False

    # Handle URLs with ports in netloc
    def test_process_link_with_port_in_netloc(self):
        # Arrange
        config = {
            'allowed_schemes': ['http', 'https'],
            'allowed_domains': ['example.com']
        }
        processor = LinkProcessor(config)
        test_url = "https://example.com:8080/path?key=value#fragment"

        # Act
        result = processor.process_link(test_url)

        # Assert
        assert result['url'] == test_url
        assert result['scheme'] == 'https'
        assert result['netloc'] == 'example.com:8080'
        assert result['path'] == '/path'
        assert result['query'] == 'key=value'
        assert result['fragment'] == 'fragment'
        assert result['is_valid'] is True
        assert result['is_relative'] is False

    # Process URLs with complex relative paths (../foo/./bar)
    def test_normalize_url_with_complex_relative_paths(self):
        # Arrange
        config = {
            'base_url': 'https://example.com/base/',
            'allowed_schemes': ['http', 'https'],
            'allowed_domains': ['example.com']
        }
        processor = LinkProcessor(config)
        complex_relative_url = '../foo/./bar'

        # Act
        normalized_url = processor.normalize_url(complex_relative_url)

        # Assert
        assert normalized_url == 'https://example.com/foo/bar'

    # Handle duplicate URLs during link extraction
    def test_extract_links_handles_duplicate_urls(self, mocker):
        # Arrange
        from bs4 import BeautifulSoup
    
        html_content = '''
        <html>
            <body>
                <a href="https://example.com/page1">Link 1</a>
                <a href="https://example.com/page1">Link 1 Duplicate</a>
                <a href="https://example.com/page2">Link 2</a>
            </body>
        </html>
        '''
        soup = BeautifulSoup(html_content, 'html.parser')
        config = {
            'allowed_schemes': ['http', 'https'],
            'allowed_domains': ['example.com']
        }
        processor = LinkProcessor(config)
    
        # Act
        links = processor.extract_links(soup)
    
        # Assert
        assert len(links) == 2  # Only two unique URLs should be processed
        assert links[0]['url'] == 'https://example.com/page1'
        assert links[1]['url'] == 'https://example.com/page2'

    # Process URLs with invalid schemes
    def test_process_link_with_invalid_scheme(self):
        # Arrange
        config = {
            'allowed_schemes': ['http', 'https'],
            'allowed_domains': ['example.com']
        }
        processor = LinkProcessor(config)
        test_url = "ftp://example.com/path"

        # Act
        result = processor.process_link(test_url)

        # Assert
        assert result['url'] == test_url
        assert result['scheme'] == 'ftp'
        assert result['is_valid'] is False

    # Validate URLs against allowed domains configuration
    def test_validate_url_against_allowed_domains(self):
        # Arrange
        config = {
            'allowed_schemes': ['http', 'https'],
            'allowed_domains': ['example.com']
        }
        processor = LinkProcessor(config)
        valid_url = "https://example.com/path"
        invalid_url = "https://notallowed.com/path"

        # Act
        valid_result = processor.process_link(valid_url)
        invalid_result = processor.process_link(invalid_url)

        # Assert
        assert valid_result['is_valid'] is True
        assert invalid_result['is_valid'] is False

    # Skip invalid URLs during HTML link extraction
    def test_extract_links_skips_invalid_urls(self, mocker):
        # Arrange
        from bs4 import BeautifulSoup
        html_content = '''
        <html>
            <body>
                <a href="https://valid.com/path">Valid Link</a>
                <a href="ftp://invalid.com/path">Invalid Link</a>
                <a href="/relative/path">Relative Link</a>
            </body>
        </html>
        '''
        soup = BeautifulSoup(html_content, 'html.parser')
        config = {
            'allowed_schemes': ['http', 'https'],
            'allowed_domains': ['valid.com']
        }
        processor = LinkProcessor(config)

        # Act
        links = processor.extract_links(soup)

        # Assert
        assert len(links) == 2
        assert links[0]['url'] == 'https://valid.com/path'
        assert links[0]['is_valid'] is True
        assert links[1]['url'] == '/relative/path'
        assert links[1]['is_valid'] is True

    # Handle exceptions during URL processing
    def test_process_link_raises_value_error_on_exception(self, mocker):
        # Arrange
        config = {
            'allowed_schemes': ['http', 'https'],
            'allowed_domains': ['example.com']
        }
        processor = LinkProcessor(config)
        invalid_url = "http://[::1]"  # Invalid URL to trigger exception

        # Act & Assert
        with pytest.raises(ValueError, match="Failed to process link:"):
            processor.process_link(invalid_url)

    # Track seen URLs to avoid duplicates during extraction
    def test_extract_links_avoids_duplicate_urls(self, mocker):
        # Arrange
        from bs4 import BeautifulSoup
    
        html_content = '''
        <html>
            <body>
                <a href="https://example.com/page1">Page 1</a>
                <a href="https://example.com/page1">Page 1 Duplicate</a>
                <a href="https://example.com/page2">Page 2</a>
            </body>
        </html>
        '''
        soup = BeautifulSoup(html_content, 'html.parser')
        processor = LinkProcessor()
    
        # Act
        links = processor.extract_links(soup)
    
        # Assert
        assert len(links) == 2
        assert links[0]['url'] == 'https://example.com/page1'
        assert links[1]['url'] == 'https://example.com/page2'

    # Extract URLs from plain text using regex patterns
    def test_extract_urls_from_text(self):
        # Arrange
        processor = LinkProcessor()
        text_content = "Visit our site at https://example.com or follow the link /contact for more info."

        # Act
        extracted_urls = processor.extract_urls(text_content)

        # Assert
        assert "https://example.com" in extracted_urls
        assert "/contact" in extracted_urls
        assert len(extracted_urls) == 2

    # Handle URL normalization by removing fragments and resolving paths
    def test_normalize_url_removes_fragments_and_resolves_paths(self):
        processor = LinkProcessor({'base_url': 'http://example.com'})
        url = 'http://example.com/path/to/resource#section'
        expected_normalized_url = 'http://example.com/path/to/resource'
        assert processor.normalize_url(url) == expected_normalized_url

    # Handle protocol-relative URLs starting with //
    def test_protocol_relative_urls(self):
        processor = LinkProcessor()
        url = '//example.com/path/to/resource'
        expected_normalized_url = 'https://example.com/path/to/resource'
        assert processor.normalize_url(url) == expected_normalized_url

    # Process a single URL and return its components and validation status
    def test_process_link_valid_url(self):
        processor = LinkProcessor(config={'base_url': 'https://example.com', 'allowed_schemes': ['http', 'https']})
        url = 'https://example.com/path?query=1'
        result = processor.process_link(url)
        assert result['url'] == url
        assert result['scheme'] == 'https'
        assert result['netloc'] == 'example.com'
        assert result['path'] == '/path'
        assert result['query'] == 'query=1'
        assert result['is_valid'] is True
        assert result['is_relative'] is False
        assert result['is_internal'] is True

    # Extract URLs from text content
    def test_extract_urls_from_text(self):
        processor = LinkProcessor()
        text = "Visit https://example.com and http://test.com for more info."
        urls = processor.extract_urls(text)
        assert urls == ['https://example.com', 'http://test.com']