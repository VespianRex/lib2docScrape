import json
import logging
import re
from dataclasses import dataclass, field
from typing import Any, Callable, Dict, List, Optional, NamedTuple
from bs4 import BeautifulSoup, Comment, NavigableString, Tag
import markdownify
from urllib.parse import urljoin, urlparse
import time

# Configure logging
logging.basicConfig(level=logging.ERROR)
logger = logging.getLogger(__name__)

class ContentProcessingError(Exception):
    """Custom exception for content processing errors."""
    pass

@dataclass
class URLInfo:
    """Structured information about a URL."""
    original_url: str
    normalized_url: str = ""
    scheme: str = ""
    netloc: str = ""
    path: str = ""
    query: str = ""
    fragment: str = ""

    def __post_init__(self):
        dangerous_prefixes = ('javascript:', 'data:', 'vbscript:')
        low = self.original_url.strip().lower()
        if low.startswith(dangerous_prefixes):
            # Mark unsafe URLs as '#'
            self.normalized_url = '#'
            self.scheme = ''
            self.netloc = ''
            self.path = ''
            self.query = ''
            self.fragment = ''
        else:
            try:
                parsed = urlparse(self.original_url)
                hostname = parsed.hostname
                if hostname:
                    # Convert hostname to punycode
                    idna_netloc = hostname.encode('idna').decode('ascii')
                    port = f":{parsed.port}" if parsed.port else ""
                    new_parsed = parsed._replace(netloc=idna_netloc + port)
                    self.normalized_url = new_parsed.geturl()
                else:
                    self.normalized_url = self.original_url
                self.scheme = parsed.scheme
                self.netloc = parsed.netloc
                self.path = parsed.path
                self.query = parsed.query
                self.fragment = parsed.fragment
            except Exception:
                self.normalized_url = self.original_url

    @property
    def is_secure(self):
        dangerous_prefixes = ('javascript:', 'data:', 'vbscript:')
        return not self.original_url.strip().lower().startswith(dangerous_prefixes)

@dataclass
class ProcessorConfig:
    """Configuration for the content processor."""
    allowed_tags: List[str] = field(default_factory=lambda: ['p', 'a', 'img', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'ul', 'ol', 'li', 'code', 'pre', 'blockquote', 'em', 'strong', 'table', 'tr', 'td', 'th'])
    max_heading_level: int = 3
    preserve_whitespace_elements: List[str] = field(default_factory=lambda: ['pre', 'code'])
    code_languages: List[str] = field(default_factory=lambda: [
        'python', 'javascript', 'typescript', 'java', 'cpp', 'c', 'csharp', 'go',
        'rust', 'swift', 'kotlin', 'php', 'ruby', 'scala', 'perl', 'r',
        'html', 'css', 'sql', 'shell', 'bash', 'powershell'
    ])
    sanitize_urls: bool = True
    metadata_prefixes: List[str] = field(default_factory=lambda: ['og:', 'twitter:', 'dc.', 'article:', 'book:'])
    extract_comments: bool = False
    max_content_length: int = 1000000
    min_content_length: int = 0
    max_heading_length: int = 100  # Default to 100 characters
    # Additional fields
    max_code_block_size: int = 1000
    preserve_whitespace: bool = False
    sanitize_content: bool = True
    extract_metadata: bool = True
    extract_assets: bool = True
    extract_code_blocks: bool = True

    def __post_init__(self):
        # Ensure max_heading_level is between 1 and 6
        self.max_heading_level = max(1, min(6, self.max_heading_level))
        # Ensure max_heading_length is at least max_heading_level * 10 characters
        min_length = self.max_heading_length * 10
        if self.max_heading_length < min_length:
            self.max_heading_length = min_length

@dataclass
class ProcessedContent:
    """Result of content processing."""
    content: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    assets: Dict[str, List[str]] = field(default_factory=lambda: {
        'images': [], 'stylesheets': [], 'scripts': [], 'media': []
    })
    headings: List[Dict[str, Any]] = field(default_factory=list)
    structure: List[Dict[str, Any]] = field(default_factory=list)
    errors: List[str] = field(default_factory=list)
    title: str = field(default="Untitled Document")

    def __str__(self) -> str:
        """Convert to string representation."""
        if 'formatted_content' in self.content:
            return str(self.content['formatted_content'])
        return ''

    def __contains__(self, item: str) -> bool:
        """Check if string is in content."""
        if isinstance(item, str):
            content_str = str(self)
            return item in content_str
        return False

    def __len__(self) -> int:
        """Get length of main content."""
        return len(str(self))

    def __bool__(self) -> bool:
        """Check if content exists."""
        return bool(self.content)

    def __iter__(self):
        """Iterate over content."""
        return iter(str(self))

    @property
    def processed_content(self) -> Dict[str, str]:
        """Get processed content."""
        return self.content

    @property
    def main_content(self) -> str:
        """Get main content."""
        return str(self)

    @property
    def has_errors(self) -> bool:
        """Check if processing had errors."""
        return bool(self.errors)

    def get_content_section(self, section: str) -> str:
        """Get content for a specific section."""
        return str(self.content.get(section, ''))

    def add_error(self, error: str) -> None:
        """Add an error message."""
        self.errors.append(str(error))

    def add_content(self, section: str, content: Any) -> None:
        """Add content to a section."""
        self.content[section] = content

    def add_metadata(self, key: str, value: Any) -> None:
        """Add metadata."""
        self.metadata[key] = value

    def add_asset(self, asset_type: str, url: str) -> None:
        """Add an asset URL."""
        if asset_type in self.assets:
            if url not in self.assets[asset_type]:
                self.assets[asset_type].append(url)

    def add_heading(self, heading: Dict[str, Any]) -> None:
        """Add a heading."""
        self.headings.append(heading)
        if 'headings' in self.structure:  # Ensure 'headings' key exists
            self.structure['headings'].append(heading)
        else:
            self.structure = {'headings': [heading], 'sections': [], 'custom_elements': []}  # Initialize if not present
    def is_valid(self) -> bool:
       """Check if the processed content is valid."""
       return bool(self.title and self.content and self.metadata)


class ContentProcessor:
    """Process HTML content into structured data and markdown."""
    
    def __init__(self, config=None):
        """Initialize the content processor."""
        self.config = config or ProcessorConfig()
        self.result = ProcessedContent()
        self.content_filters = []
        self.url_filters = []
        self.metadata_extractors = []
        self.content_extractors = {}
        self.markdownify_options = {
            'heading_style': 'ATX',
            'strong_style': '**',
            'emphasis_style': '_',
            'bullets': '*',
            'code_language': 'python',
            'escape_asterisks': False,
            'escape_underscores': False,
            'escape_code': False,
            'code_block_style': '```',
            'wrap_width': None,
            'convert_links': True,
            'hr_style': '---',
            'br_style': '  ',
            'default_title': 'Untitled Document',
            'newline_style': '\n',
            'convert_all': True,
            'preserve_whitespace': True,
            'strip': ['style', 'onclick', 'onload', 'onmouseover', 'onmouseout', 'onkeydown', 'onkeyup']
        }
    
    def process(self, html, base_url=None):
        """Process HTML content and return structured result."""
        self.result = ProcessedContent()
        if not html:
            return self.result

        try:
            soup = BeautifulSoup(html, 'html.parser')
                
            # Extract metadata first
            self._extract_metadata(soup)
                
            # Process content
            self._process_content(soup, base_url)
                
            return self.result
        except Exception as e:
            self.result.errors.append(f"Error processing content: {str(e)}")
            self.result.content = {
                'formatted_content': '',
                'structure': {'headings': [], 'sections': [], 'custom_elements': []},
                'headings': [],
                'assets': {
                    'images': [],
                    'stylesheets': [],
                    'scripts': [],
                    'media': []
                },
                'metadata': self.result.metadata
            }
            return self.result

    def _extract_metadata(self, soup):
        """Extract metadata from HTML."""
        # Initialize metadata
        self.result.metadata = {}
        self.result.title = "Untitled Document"
        
        # Process JSON-LD first
        json_ld_scripts = soup.find_all('script', type='application/ld+json')
        for script in json_ld_scripts:
            try:
                data = json.loads(script.string)
                if isinstance(data, dict):
                    self._process_json_ld(data)
                elif isinstance(data, list):
                    for item in data:
                        if isinstance(item, dict):
                            self._process_json_ld(item)
            except (json.JSONDecodeError, AttributeError):
                continue

        # Extract standard meta tags
        meta_tags = soup.find_all('meta')
        for meta in meta_tags:
            # Get name/property and content values
            name = meta.get('name', '').lower()
            prop = meta.get('property', '').lower()
            content = meta.get('content')
            if content is None:
                content = meta.get('value', '')

            # Normalize content
            content = '' if content is None else str(content).strip()
            
            # Handle meta redirects
            if meta.get('http-equiv', '').lower() == 'refresh' and content:
                redirect_match = re.search(r'url=[\'"]*([^\'"]+)', content, re.IGNORECASE)
                if redirect_match:
                    self.result.metadata.setdefault('meta_redirects', []).append(redirect_match.group(1))

            # Process metadata based on name/property
            if name:  # Regular meta tag with name
                if name not in self.result.metadata:
                    self.result.metadata[name] = content
            elif prop:  # OpenGraph and similar with property
                if prop not in self.result.metadata:
                    self.result.metadata[prop] = content
            elif meta.has_attr('property'):  # Empty property case
                key = meta.get('content', '').strip()
                if key:  # Use content as key if non-empty
                    self.result.metadata[key.lower()] = ''

        # Extract title, prioritizing <head> section
        head_title = soup.head
        if head_title:
            title_tag = head_title.find('title')
        else:
            title_tag = soup.find('title')

        if not title_tag:
            title_tag = soup.find('h1')

        if title_tag and title_tag.string:
            title = title_tag.string.strip()
            self.result.title = title
            self.result.metadata['title'] = title
        else:
            self.result.title = "Untitled Document"
            self.result.metadata['title'] = "Untitled Document"

        # Process microdata
        for element in soup.find_all(True, itemtype=True):
            self._process_microdata(element)

        # Apply custom extractors
        for extractor in self.metadata_extractors:
            try:
                extractor(soup, self.result.metadata)
            except Exception as e:
                self.result.errors.append(f"Metadata extractor error: {str(e)}")

    def _process_json_ld(self, data, prefix=''):
        """Process JSON-LD data recursively."""
        if isinstance(data, dict):
            # Process individual fields
            for key, value in data.items():
                if key.startswith('@'):
                    continue  # Skip @context, @type, etc.

                current_key = (prefix + key).lower() if prefix else key.lower()
                
                if isinstance(value, (str, int, float, bool)):
                    if current_key not in self.result.metadata:
                        self.result.metadata[current_key] = str(value)
                elif isinstance(value, dict):
                    # Store complete object if it's a nested entity
                    if '@type' in value:
                        if current_key not in self.result.metadata:
                            self.result.metadata[current_key] = value
                        new_prefix = f"{current_key}_"
                        self._process_json_ld(value, new_prefix)
                    else:
                        self._process_json_ld(value, current_key + '_')
                elif isinstance(value, list):
                    # Handle arrays of objects
                    if value and isinstance(value[0], dict):
                        if current_key not in self.result.metadata:
                            self.result.metadata[current_key] = value
                    for item in value:
                        self._process_json_ld(item, current_key + '_')

    def _process_microdata(self, element):
        """Process microdata attributes."""
        for prop in element.find_all(True, itemprop=True):
            name = prop.get('itemprop', '').lower()
            # Get all possible content values
            content = prop.get('content', '')
            if content is None:  # If no content attribute
                content = prop.get('value', '')  # Try value attribute
            if content is None:  # If no value attribute
                content = prop.string or ''  # Try text content
            
            if name:
                # Store empty string for missing/None values
                self.result.metadata[name] = content.strip() if content else ''
 374 |     def _extract_assets(self, soup, result, base_url=None):
 375 |         """Extract assets with proper handling of duplicates and data URLs."""
 376 |         # Initialize assets dict
 377 |         result.assets = {
 378 |             'images': [],
 379 |             'stylesheets': [],
 380 |             'scripts': [],
 381 |             'media': []
 382 |         }
 383 | 
 384 |         # Helper function to process URLs
 385 |         def process_url(url):
 386 |             if not url or not url.strip():
 387 |                 return None
 388 |             # Keep data URLs as is
 389 |             if url.startswith('data:'):
 390 |                 return url
 391 |             # Handle base URL
 392 |             if base_url and not url.startswith(('http://', 'https://', 'mailto:', 'tel:', 'ftp://')):
 393 |                 try:
 394 |                     return urljoin(base_url, url)
 395 |                 except Exception:
 396 |                     return url  # Keep original URL on error
 397 |             return url
 398 | 
 399 |         # Extract stylesheets
 400 |         # Extract stylesheets
 401 |         stylesheet_links = soup.find_all('link', rel='stylesheet')
 402 |         for link in stylesheet_links:
 403 |             href = link.get('href')
 404 |             if href:
 405 |                 url = process_url(href)
 406 |                 if url:
 407 |                     result.assets['stylesheets'].append(url)
 408 | 
 409 |         # Extract images
 410 |         for img in soup.find_all('img'):
 411 |             src = img.get('src')
 412 |             if src and src.strip():  # Ensure src is not empty
 413 |                 if src.startswith('data:'):
 414 |                     result.assets['images'].append(src)
 415 |                 elif not src.lower().startswith(('javascript:', 'vbscript:')):
 416 |                     url = process_url(src)
 417 |                     if url:
 418 |                         result.assets['images'].append(url)
 419 | 
 420 |         # Extract scripts
 421 |         for script in soup.find_all('script', src=True):
 422 |             src = script.get('src')
 423 |             if src and not src.lower().startswith(('javascript:', 'data:', 'vbscript:')):
 424 |                 url = process_url(src)
 425 |                 if url:
 426 |                     result.assets['scripts'].append(url)
 427 | 
 428 |         # Extract media
 429 |         for media in soup.find_all(['audio', 'video', 'source']):
 430 |             src = media.get('src')
 431 |             if src and src.strip():  # Ensure src is not empty
 432 |                 if not src.lower().startswith(('javascript:', 'data:', 'vbscript:')):
 433 |                     url = process_url(src)
 434 |                     if url:
 435 |                         result.assets['media'].append(url)
 436 | 
 437 |     def _sanitize_url(self, url, base_url=None):
 438 |         """Sanitize and normalize URLs."""
 439 |         if not url:
 440 |             return url
 441 |         
 442 |         # Handle data URLs
 443 |         if url.startswith('data:'):
 444 |             return url
 445 |         
 446 |         # Check null/empty URLs
 447 |         if not url:
 448 |             return '#'
 449 |             
 450 |         try:
 451 |             # Normalize URL first
 452 |             normalized_url = url.strip().lower()
 453 |             
 454 |             # Check for dangerous protocols first
 455 |             if any(normalized_url.startswith(proto) for proto in ['javascript:', 'data:', 'vbscript:']):
 456 |                 return '#'
 457 |                 
 458 |             # Handle relative URLs
 459 |             if base_url and not normalized_url.startswith(('http://', 'https://', 'mailto:', 'tel:', 'ftp://')):
 460 |                 logger.info(f"Sanitizing relative URL: {url}, base_url: {base_url}")
 461 |                 resolved_url = urljoin(base_url, url)
 462 |                 logger.info(f"Resolved URL using urljoin: {resolved_url}")
 463 |                 return resolved_url
 464 |             
 465 |             return url
 466 |         except Exception:
 467 |             return '#'  # Return safe URL on any error
 468 |  468 | 
 469 |  469 |     def _process_code_blocks(self, soup):
 470 |  470 |         """Process code blocks with enhanced language detection."""
 471 |  471 |         for pre in soup.find_all('pre'):
 472 |  472 |             code = pre.find('code')
 473 |  473 |             if code:
 474 |  474 |                 # First try to get language directly from the class
 475 |  475 |                 language = None
 476 |  476 |                 for cls in code.get('class', []):
 477 |  477 |  477 |                     for prefix in ['language-', 'lang-', 'brush:', 'syntax-']:
 478 |  478 |  478 |                         if cls.lower().startswith(prefix):
 479 |  479 |  479 |                             lang = cls[len(prefix):].lower()
 480 |  480 |  480 |                             if lang in self.config.code_languages:
 481 |  481 |  481 |                                 language = lang
 482 |  482 |  482 |                             break
 483 |  483 |  483 |                     if language:
 484 |  484 |  484 |                         break
 485 |  485 |  485 | 
 486 |  486 |  486 |                 # Process code content with careful whitespace handling
 487 |  487 |  487 |  487 |                 # Get the raw content to preserve HTML entities and tags
 488 |  488 |  488 |  488 |                 code_text = ''.join(str(content) for content in code.contents)
 489 |  489 |  489 |  489 |                 if code_text:
 490 |  490 |  490 |  490 |                     # Split into lines while preserving empty lines
 491 |  491 |  491 |  491 |                     lines = code_text.splitlines()
 492 |  492 |  492 |  492 | 
 493 |  493 |  493 |  493 |                     # Calculate minimum indentation only from non-empty lines
 494 |  494 |  494 |  494 |                     non_empty_lines = [line for line in lines if line.strip()]
 495 |  495 |  495 |  495 |                     if non_empty_lines:
 496 |  496 |  496 |  496 |                         min_indent = min(len(line) - len(line.lstrip())
 497 |  497 |  497 |  497 |                                       for line in non_empty_lines)
 498 |  498 |  498 |  498 | 
 499 |  499 |  499 |  499 |                         # Process all lines maintaining relative indentation
 500 |  500 |  500 |  500 |                         processed_lines = []
 501 |  501 |  501 |  501 |                         for line in lines:
 502 |  502 |  502 |  502 |                             if line.strip():
 503 |  503 |  503 |  503 |                                 # Remove only the common indentation
 504 |  504 |  504 |  504 |                                 processed_lines.append(line[min_indent:])
 505 |  505 |  505 |  505 |                             else:
 506 |  506 |  506 |  506 |                                 # Preserve empty lines
 507 |  507 |  507 |  507 |                                 processed_lines.append('')
 508 |  508 |  508 |  508 | 
 509 |  509 |  509 |  509 |                         code_text = '\n'.join(processed_lines)
 510 |  510 |  510 |  510 | 
 511 |  511 |  511 |  511 |                     # Prepend language to code text if available
 512 |  512 |  512 |  512 |                     lang_marker = language if language else ''
 513 |  513 |  513 |  513 |                     markdown_block = f"```{lang_marker}\n{code_text.strip()}\n```"
 514 |  514 |  514 |  514 | 
 515 |  515 |  515 |                     pre.string = markdown_block  # Use string to prevent HTML parsing
 516 |  516 |  516 |  516 | 
 517 |  517 |  517 |  517 |     def _process_links(self, soup, base_url=None):
 518 |  518 |  518 |  518 |         """Process links with proper URL handling and sanitization."""
 519 |  519 |  519 |  519 |         for a in soup.find_all('a'):
 520 |  520 |  520 |  520 |             href = a.get('href', '')
 521 |  521 |  521 |  521 |             text = a.get_text() or ''
 522 |  522 |  522 |  522 |             href = href.strip()
 523 |  523 |  523 |  523 |             if not href:
 524 |  524 |  524 |  524 |                 # No href, leave just the text
 525 |  525 |  525 |  525 |                 new_tag = soup.new_tag('span')
 526 |  526 |  526 |  526 |                 new_tag.string = f"[{text}](#)"
 527 |  527 |  527 |  527 |                 a.replace_with(new_tag)
 528 |  528 |  528 |  528 |                 continue
 529 |  529 |  529 |  529 | 
 530 |  530 |  530 |  530 |             # Check for dangerous URLs
 531 |  531 |  531 |  531 |             normalized_href = href.strip().lower()
 532 |  532 |  532 |  532 |             if not normalized_href or any(normalized_href.startswith(proto) for proto in ['javascript:', 'data:', 'vbscript:']):
 533 |  533 |  533 |  533 |                 a.replace_with(text)  # Replace with just the text content
 534 |  534 |  534 |  534 |                 continue
 535 |  535 |  535 |  535 |             else:
 536 |  536 |  536 |  536 |                 # Process valid URL
 537 |  537 |  537 |  537 |                 if self.url_filters and not all(f(href) for f in self.url_filters):
 538 |  538 |  538 |  538 |                     href = '#'
 539 |  539 |  539 |  539 |                 else:
 540 |  540 |  540 |  540 |                     href = self._sanitize_url(href, base_url)
 541 |  541 |  541 |  541 |                 # Create markdown link
 542 |  542 |  542 |  542 |                 new_tag = soup.new_tag('span')
 543 |  543 |  543 |  543 |                 new_tag.string = f"[{text}]({href})"
 544 |  544 |  544 |  544 |                 a.replace_with(new_tag)
 545 |  545 |  545 |  545 | 
 546 |  546 |  546 |  546 |     def _extract_headings(self, soup):
 547 |  547 |  547 |  454 |         """Extract headings with proper level filtering."""
 548 |  455 |  455 |         headings = []
 549 |  456 |  456 |         for level in range(1, min(self.config.max_heading_level + 1, 7)):
 550 |  457 |  457 |             for heading in soup.find_all(f'h{level}'):
 551 |  458 |  458 |                 text = heading.get_text().strip()
 552 |  459 |  459 |                 if text:  # Only add non-empty headings
 553 |  460 |  460 |                     headings.append({
 554 |  461 |  461 |                         'level': level,
 555 |  462 |  462 |                         'text': text
 556 |  463 |  463 |                     })
 557 |  464 |  464 |         return headings
 558 |  465 |  465 | 
 559 |  466 |  466 |     def _extract_structure(self, soup):
 560 |  467 |  467 |         """Extract document structure including headings and sections."""
 561 |  468 |  468 |         structure = {
 562 |  469 |  469 |             'headings': [],
 563 |  470 |  470 |             'sections': [],
 564 |  471 |  471 |             'custom_elements': []
 565 |  472 |  472 |         }
 566 |  473 |  473 |         
 567 |  474 |  474 |         # Process headings
 568 |  475 |  475 |         for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
 569 |  476 |  476 |             level = int(tag.name[1])
 570 |  477 |  477 |             if level <= self.config.max_heading_level:
 571 |  478 |  478 |                 heading = {
 572 |  479 |  479 |                     'level': level,
 573 |  480 |  480 |                     'title': tag.get_text().strip(),
 574 |  481 |  481 |                     'id': tag.get('id', ''),
 575 |  482 |  482 |                     'classes': tag.get('class', [])
 576 |  483 |  483 |                 }
 577 |  484 |  484 |                 structure['headings'].append(heading)
 578 |  485 |  485 |         
 579 |  486 |  486 |         # Process sections
 580 |  487 |  487 |         for section in soup.find_all(['section', 'article', 'div'], class_=lambda x: x and 'section' in x):
 581 |  488 |  488 |             section_data = {
 582 |  489 |  489 |                 'id': section.get('id', ''),
 583 |  490 |  490 |                 'classes': section.get('class', []),
 584 |  491 |  491 |                 'heading': None,
 585 |  492 |  492 |                 'content_type': section.get('data-content-type', '')
 586 |  493 |  493 |             }
 587 |  494 |  494 |             
 588 |  495 |  495 |             # Find section heading
 589 |  496 |  496 |             heading = section.find(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
 590 |  497 |  497 |             if heading:
 591 |  498 |  498 |                 section_data['heading'] = heading.get_text().strip()
 592 |  499 |  499 |             
 593 |  500 |  500 |             structure['sections'].append(section_data)
 594 |  501 |  501 | 
 595 |  502 |  502 |         return structure
 596 |  503 |  503 | 
 597 |  504 |  504 |     def _process_images(self, soup, base_url=None):
 598 |  505 |  505 |         """Process images with proper URL handling."""
 599 |  506 |  506 |         for img in soup.find_all('img'):
 600 |  507 |  507 |             src = img.get('src', '')
 601 |  508 |  508 |             alt = img.get('alt', '')
 602 |  509 |  509 |             title = img.get('title', '')
 603 |  510 |  510 | 
 604 |  511 |  511 |             if not src:
 605 |  512 |  512 |                 if alt:
 606 |  513 |  513 |                     new_tag = soup.new_tag('span')
 607 |  514 |  514 |                     new_tag.string = alt
 608 |  515 |  515 |                     img.replace_with(new_tag)
 609 |  516 |  516 |                 continue
 610 |  517 |  517 | 
 611 |  518 |  518 |             # Handle data URLs
 612 |  519 |  519 |             if src.startswith('data:'):
 613 |  520 |  520 |                 if src not in self.result.assets['images']:
 614 |  521 |  521 |                     self.result.assets['images'].append(src)
 615 |  522 |  522 |                 new_tag = soup.new_tag('span')
 616 |  523 |  523 |                 new_tag.string = f"![{alt}]({src})"
 617 |  524 |  524 |                 if title:
 618 |  525 |  525 |                     new_tag.string += f" \"{title}\""
 619 |  526 |  526 |                 img.replace_with(new_tag)
 620 |  527 |  527 |                 continue
 621 |  528 |  528 | 
 622 |  529 |  529 |             # Handle relative URLs
 623 |  530 |  530 |             if base_url and not src.startswith(('http://', 'https://')):
 624 |  531 |  531 |                 try:
 625 |  532 |  532 |                     src = urljoin(base_url, src)
 626 |  533 |  533 |                 except Exception:
 627 |  534 |  534 |                     pass  # Keep original URL on error
 628 |  535 |  535 | 
 629 |  536 |  536 |             # Create markdown image
 630 |  537 |  537 |             new_tag = soup.new_tag('span')
 631 |  538 |  538 |             new_tag.string = f"![{alt}]({src})"
 632 |  539 |  539 |             if title:
 633 |  540 |  540 |                 new_tag.string += f" \"{title}\""
 634 |  541 |  541 |             img.replace_with(new_tag)
 635 |  542 |  542 | 
 636 |  543 |  543 |     def configure(self, config):
 637 |  544 |  544 |         """Configure the processor with custom settings."""
 638 |  545 |  545 |         if isinstance(config, dict):
 639 |  546 |  546 |             # Update allowed tags
 640 |  547 |  547 |             if 'allowed_tags' in config:
 641 |  548 |  548 |                 self.config.allowed_tags = config['allowed_tags']
 642 |  549 |  549 | 
 643 |  550 |  550 |             # Update max heading level
 644 |  551 |  551 |             if 'max_heading_level' in config:
 645 |  552 |  552 |                 self.config.max_heading_level = config['max_heading_level']
 646 |  553 |  553 | 
 647 |  554 |  554 |             # Update preserve whitespace elements
 648 |  555 |  555 |             if 'preserve_whitespace_elements' in config:
 649 |  556 |  556 |                 self.config.preserve_whitespace_elements = config['preserve_whitespace_elements']
 650 |  557 |  557 | 
 651 |  558 |  558 |             # Update code languages
 652 |  559 |  559 |             if 'code_languages' in config:
 653 |  560 |  560 |                 self.config.code_languages = config['code_languages']
 654 |  561 |  561 | 
 655 |  562 |  562 |             # Update URL sanitization
 656 |  563 |  563 |             if 'sanitize_urls' in config:
 657 |  564 |  564 |                 self.config.sanitize_urls = config['sanitize_urls']
 658 |  565 |  565 | 
 659 |  566 |  566 |             # Update metadata prefixes
 660 |  567 |  567 |             if 'metadata_prefixes' in config:
 661 |  568 |  568 |                 self.config.metadata_prefixes = config['metadata_prefixes']
 662 |  569 |  569 | 
 663 |  570 |  570 |             # Update comment extraction
 664 |  571 |  571 |             if 'extract_comments' in config:
 665 |  572 |  572 |                 self.config.extract_comments = config['extract_comments']
 666 |  573 |  573 | 
 667 |  574 |  574 |             # Update content length limits
 668 |  575 |  575 |             if 'max_content_length' in config:
 669 |  576 |  576 |                 self.config.max_content_length = config['max_content_length']
 670 |  577 |  577 |             if 'min_content_length' in config:
 671 |  578 |  578 |                 self.config.min_content_length = config['min_content_length']
 672 |  579 |  579 | 
 673 |  580 |  580 |             # Update markdownify options
 674 |  581 |  581 |             self.markdownify_options.update({
 675 |  582 |  582 |                 'heading_style': 'ATX',
 676 |  583 |  583 |                 'strong_style': '**',
 677 |  584 |  584 |                 'emphasis_style': '_',
 678 |  585 |  585 |                 'bullets': '-',
 679 |  586 |  586 |                 'code_language': False,
 680 |  587 |  587 |                 'escape_asterisks': False,
 681 |  588 |  588 |                 'escape_underscores': False,
 682 |  589 |  589 |                 'escape_code': False,
 683 |  590 |  590 |                 'code_block_style': '```',
 684 |  591 |  591 |                 'wrap_width': None,
 685 |  592 |  592 |                 'convert_links': True,
 686 |  593 |  593 |                 'hr_style': '---',
 687 |  594 |  594 |                 'br_style': '  ',
 688 |  595 |  595 |                 'default_title': 'Untitled Document',
 689 |  596 |  596 |                 'newline_style': '\n',
 690 |  597 |  597 |                 'convert_all': True,
 691 |  598 |  598 |                 'preserve_whitespace': True,
 692 |  599 |  599 |                 'strip': ['style', 'onclick', 'onload', 'onmouseover', 'onmouseout', 'onkeydown', 'onkeyup']
 693 |  600 |  600 |             })
 694 |  601 |  601 | 
 695 |  602 |  602 |     def add_content_filter(self, filter_func):
 696 |  603 |  603 |         """Add a custom content filter."""
 697 |  604 |  604 |         self.content_filters.append(filter_func)
 698 |  605 |  605 | 
 699 |  606 |  606 |     def add_url_filter(self, filter_func):
 700 |  607 |  607 |         """Add a custom URL filter."""
 701 |  608 |  608 |         self.url_filters.append(filter_func)
 702 |  609 |  609 | 
 703 |  610 |  610 |     def add_content_extractor(self, name, extractor):
 704 |  611 |  611 |         """Add a custom content extractor."""
 705 |  612 |  612 |         self.content_extractors[name] = extractor
 706 |  613 |  613 | 
 707 |  614 |  614 |     def add_metadata_extractor(self, extractor):
 708 |  615 |  615 |         """Add a custom metadata extractor."""
 709 |  616 |  616 |         self.metadata_extractors.append(extractor)
 710 |  617 |  617 | 
 711 |  618 |  618 |     def _process_definition_lists(self, soup):
 712 |  619 |  619 |         """Process definition lists to markdown format."""
 713 |  620 |  620 |         for dl in soup.find_all('dl'):
 714 |  621 |  621 |             terms = []
 715 |  622 |  622 |             for dt in dl.find_all('dt'):
 716 |  623 |  623 |                 term = dt.get_text().strip()
 717 |  624 |  624 |                 dd = dt.find_next_sibling('dd')
 718 |  625 |  625 |                 definition = dd.get_text().strip() if dd else ''
 719 |  626 |  626 |                 terms.append(f'**{term}**: {definition}')
 720 |  627 |  627 |             if terms:
 721 |  628 |  628 |                 dl.replace_with('\n'.join(terms))
 722 |  629 |  629 |             else:
 723 |  630 |  630 |                 dl.decompose()  # Remove empty lists
 724 |  631 |  631 | 
 725 |  632 |  632 |     def _process_footnotes(self, soup):
 726 |  633 |  633 |         """Process footnotes into markdown format."""
 727 |  634 |  634 |         footnotes = {}
 728 |  635 |  635 |         # Find all footnote references
 729 |  636 |  636 |         for ref in soup.find_all('sup', id=re.compile(r'^fnref\d+')):
 730 |  637 |  637 |             link = ref.find('a')
 731 |  638 |  638 |             if link and link.get('href', '').startswith('#fn'):
 732 |  639 |  639 |                 fn_id = link['href'][1:]  # Remove #
 733 |  640 |  640 |                 footnotes[fn_id] = len(footnotes) + 1
 734 |  641 |  641 |                 ref.string = f"[{footnotes[fn_id]}]"
 735 |  642 |  642 |         
 736 |  643 |  643 |         # Process footnote content
 737 |  644 |  644 |         footnote_div = soup.find('div', class_='footnotes')
 738 |  645 |  645 |         if footnote_div:
 739 |  646 |  646 |             footnote_content = []
 740 |  647 |  647 |             for fn in footnote_div.find_all('li', id=lambda x: x in footnotes):
 741 |  648 |  648 |                 number = footnotes[fn['id']]
 742 |  649 |  649 |                 content = fn.get_text().strip()
 743 |  650 |  650 |                 footnote_content.append(f"{number}. {content}")
 744 |  651 |  651 |             if footnote_content:
 745 |  652 |  652 |                 footnote_div.replace_with('\n\n' + '\n'.join(footnote_content))
 746 |  653 |  653 |             else:
 747 |  654 |  654 |                 footnote_div.decompose()
 748 |  655 |  655 | 
 749 |  656 |  656 |     def _process_abbreviations(self, soup):
 750 |  657 |  657 |         """Process abbreviations into markdown format."""
 751 |  658 |  658 |         for abbr in soup.find_all('abbr'):
 752 |  659 |  659 |             title = abbr.get('title')
 753 |  660 |  660 |             if title:
 754 |  661 |  661 |                 abbr.replace_with(f"{abbr.get_text()} ({title})")
 755 |  662 |  662 |             else:
 756 |  663 |  663 |                 abbr.replace_with(abbr.get_text())
 757 |  664 |  664 | 
 758 |  665 |  665 |     def _process_iframes(self, soup):
 759 |  666 |  666 |         """Process iframes to markdown format."""
 760 |  667 |  667 |         for iframe in soup.find_all('iframe'):
 761 |  668 |  668 |             src = iframe.get('src', '')
 762 |  669 |  669 |             if src and self._is_safe_url(src):
 763 |  670 |  670 |                 markdown_iframe = f'[iframe]({src})'
 764 |  671 |  671 |                 iframe.replace_with(BeautifulSoup(markdown_iframe, 'html.parser'))
 765 |  672 |  672 |             else:
 766 |  673 |  673 |                 iframe.decompose()
 767 |  674 |  674 | 
 768 |  675 |  675 |     def _convert_table_to_markdown(self, table):
 769 |  676 |  676 |         """Convert HTML table to markdown format with proper handling of empty tables."""
 770 |  677 |  677 |         if not table.find_all(['tr', 'th', 'td']):
 771 |  678 |  678 |             return ''  # Return empty string for empty tables
 772 |  679 |  679 |             
 773 |  680 |  680 |         rows = []
 774 |  681 |  681 |         header_row = table.find('tr')
 775 |  682 |  682 |         
 776 |  683 |  683 |         # Process header row
 777 |  684 |  684 |         if header_row:
 778 |  685 |  685 |             headers = []
 779 |  686 |  686 |             for th in header_row.find_all(['th', 'td']):
 780 |  687 |  687 |                 colspan = int(th.get('colspan', 1))
 781 |  688 |  688 |                 text = th.get_text().strip() or ' '  # Use space for empty cells
 782 |  689 |  689 |                 headers.extend([text] * colspan)
 783 |  690 |  690 |             
 784 |  691 |  691 |             if headers:
 785 |  692 |  692 |                 rows.append('| ' + ' | '.join(headers) + ' |')
 786 |  693 |  693 |                 rows.append('| ' + ' | '.join(['---'] * len(headers)) + ' |')
 787 |  694 |  694 |         
 788 |  695 |  695 |         # Process data rows
 789 |  696 |  696 |         data_rows = table.find_all('tr')[1:] if header_row else table.find_all('tr')
 790 |  697 |  697 |         for row in data_rows:
 791 |  698 |  698 |             cells = []
 792 |  699 |  699 |             for cell in row.find_all(['td', 'th']):
 793 |  700 |  700 |                 colspan = int(cell.get('colspan', 1))
 794 |  701 |  701 |                 text = cell.get_text().strip() or ' '  # Use space for empty cells
 795 |  702 |  702 |                 cells.extend([text] * colspan)
 796 |  703 |  703 |             
 797 |  704 |  704 |             if cells:
 798 |  705 |  705 |                 rows.append('| ' + ' | '.join(cells) + ' |')
 799 |  706 |  706 |         
 800 |  707 |  707 |         return '\n'.join(rows) if rows else ''
 801 |  708 |  708 | 
 802 |  709 |  709 |     def _is_safe_url(self, url):
 803 |  710 |  710 |         """Check if URL is safe to use."""
 804 |  711 |  711 |         if not url:
 805 |  712 |  712 |             return False
 806 |  713 |  713 |         
 807 |  714 |  714 |         # Allow data URLs only for images
 808 |  715 |  715 |         if url.startswith('data:'):
 809 |  716 |  716 |             return url.startswith('data:image/')
 810 |  717 |  717 |         
 811 |  718 |  718 |         # Check for dangerous protocols
 812 |  719 |  719 |         if any(url.lower().startswith(proto) for proto in [
 813 |  720 |  720 |             'javascript:', 'data:', 'vbscript:', 'file:', 'about:', 'blob:'
 814 |  721 |  721 |         ]):
 815 |  722 |  722 |             return False
 816 |  723 |  723 |         
 817 |  724 |  724 |         try:
 818 |  725 |  725 |             parsed = urlparse(url)
 819 |  726 |  726 |             return bool(parsed.netloc) and parsed.scheme in ['http', 'https', 'ftp', 'mailto']
 820 |  727 |  727 |         except Exception:
 821 |  728 |  728 |             return '#'  # Return safe URL on any error
 822 |  729 |  729 | 
 823 |  730 |  730 |     def _process_content(self, soup, base_url=None):
 824 |  731 |  731 |         """Process HTML content with all features."""
 825 |  732 |  732 |         try:
 826 |  733 |  733 |             logger.info("Starting _process_content")
 827 |  734 |  734 |             # Check content length
 828 |  735 |  735 |             content_length = len(str(soup))
 829 |  736 |  736 |             if content_length > self.config.max_content_length:
 830 |  737 |  737 |                 self.result.title = "Content Too Large"
 831 |  738 |  738 |                 self.result.errors.append("Content exceeds maximum length")
 832 |  739 |  739 |                 self.result.content["formatted_content"] = "Content exceeds maximum length"
 833 |  740 |  740 |                 return
 834 |  741 |  741 |             
 835 |  742 |  742 |             # Initialize content and structure
 836 |  743 |  743 |             self.result.content = {}
 837 |  744 |  744 |             self.result.structure = self._extract_structure(soup)  # Extract structure before processing
 838 |  745 |  745 | 
 839 |  746 |  746 |             # Process body or full document
 840 |  747 |  747 |             body = soup.find('body') or soup
 841 |  748 |  748 |             
 842 |  749 |  749 |             if not isinstance(body, BeautifulSoup):
 843 |  750 |  750 |                 body = BeautifulSoup(str(body), 'html.parser')
 844 |  751 |  751 | 
 845 |  752 |  752 |             # Extract base URL from <base> tag in head
 846 |  753 |  753 |             base_tag = soup.head.find('base') if soup.head else None
 847 |  754 |  754 |             base_url = base_tag.get('href') if base_tag else base_url
 848 |  755 |  755 | 
 849 |  756 |  756 |             # Process special elements
 850 |  757 |  757 |             self._process_definition_lists(body)
 851 |  758 |  758 |             self._process_footnotes(body)
 852 |  759 |  759 |             self._process_abbreviations(body)
 853 |  760 |  760 |             self._process_iframes(body)
 854 |  761 |  761 |             self._process_code_blocks(body)
 855 |  762 |  762 | 
 856 |  763 |  763 |             # Process links and images *before* markdown conversion
 857 |  764 |  764 |             self._process_links(body, base_url)
 858 |  765 |  765 |             self._process_images(body, base_url)
 859 |  766 |  766 | 
 860 |  767 |  767 |             # Extract assets
 861 |  768 |  768 |             self._extract_assets(soup, self.result, base_url)
 862 |  769 |  769 | 
 863 |  770 |  770 |             # Then convert to markdown
 864 |  771 |  771 |             formatted_markdown = self._convert_to_markdown(body)
 865 |  772 |  772 |             self.result.content["formatted_content"] = formatted_markdown
 866 |  773 |  773 | 
 867 |  774 |  774 |             # Finally format structure if available
 868 |  775 |  775 |             if isinstance(self.result.structure, list):
 869 |  776 |  776 |                 structured_markdown = self._format_structure_to_markdown(self.result.structure)
 870 |  777 |  777 |                 if structured_markdown:
 871 |  778 |  778 |                     self.result.content["formatted_content"] = structured_markdown.strip()
 872 |  779 |  779 |             # Extract structure
 873 |  780 |  780 |             self.result.structure = self._extract_structure(body)
 874 |  781 |  781 |             self.result.content["structure"] = self.result.structure["headings"]
 875 |  782 |  782 |             
 876 |  783 |  783 |             # Extract headings
 877 |  784 |  784 |             self.result.headings = self._extract_headings(body)
 878 |  785 |  785 |             self.result.content["headings"] = self.result.headings
 879 |  786 |  786 |             
 880 |  787 |  787 |             # Apply content filters
 881 |  788 |  788 |             if self.content_filters:
 882 |  789 |  789 |                 filtered_content = self.result.content["formatted_content"]
 883 |  790 |  790 |                 for filter_func in self.content_filters:
 884 |  791 |  791 |                     if not filter_func(filtered_content):
 885 |  792 |  792 |                         self.result.content["formatted_content"] = ""
 886 |  793 |  793 |                         break
 887 |  794 |  794 |             
 888 |  795 |  795 |             # Add structure to content
 889 |  796 |  796 |             # self.result.content["structure"] = self.result.structure  # No longer needed
 890 |  797 |  797 |             
 891 |  798 |  798 |             # Process custom content extractors
 892 |  799 |  799 |             for name, extractor in self.content_extractors.items():
 893 |  800 |  800 |                 try:
 894 |  801 |  801 |                     extracted_content = extractor(body)
 895 |  802 |  802 |                     self.result.content[name] = extracted_content
 896 |  803 |  803 |                 except Exception as e:
 897 |  804 |  804 |                     self.result.errors.append(f"Error in custom extractor {name}: {str(e)}")
 898 |  805 |  805 |         except Exception as e:
 899 |  806 |  806 |             logger.error(f"Exception in _process_content: {e}", exc_info=True)
 900 |  807 |  907 |             self.result.errors.append(f"Error processing content: {str(e)}")
 901 |  808 |  808 | 
 902 |  809 |  809 | 
 903 |  810 |  810 |     def _format_structure_to_markdown(self, structure):
 904 |  811 |  811 |         """Format structured content to markdown."""
 905 |  812 |  812 |         markdown_output = ''
 906 |  813 |  813 |         for section in structure:
 907 |  814 |  814 |             level = section.get('level', 1)
 908 |  815 |  815 |             title = section.get('title', 'Section')
 909 |  816 |  816 |             content_items = section.get('content', [])
 910 |  817 |  817 |             
 911 |  818 |  818 |             # Initialize section content
 912 |  819 |  819 |             section_content = ''
 913 |  820 |  820 |             
 914 |  821 |  821 |             # Add section header
 915 |  822 |  822 |             markdown_output += '#' * level + ' ' + title + '\n\n'
 916 |  823 |  823 |             
 917 |  824 |  824 |             for item in content_items:
 918 |  825 |  825 |                 content_type = item.get('type')
 919 |  826 |  826 |                 content_text = item.get('content', '')
 920 |  827 |  827 |                 language = item.get('language', '')
 921 |  828 |  828 |                 table_content = item.get('content', {})  # For table type
 922 |  829 |  829 | 
 923 |  830 |  830 |                 item_content = ''
 924 |  831 |  831 |                 if content_type == 'text':
 925 |  832 |  832 |                     item_content = content_text
 926 |  833 |  833 |                 elif content_type == 'code':
 927 |  834 |  834 |                     lang_marker = language if language else ''
 928 |  835 |  835 |                     item_content = f'```{lang_marker}\n{content_text}\n```'
 929 |  836 |  836 |                 elif content_type == 'table':
 930 |  837 |  837 |                     headers = table_content.get('headers', [])
 931 |  838 |  838 |                     rows = table_content.get('rows', [])
 932 |  839 |  839 |                     if headers:
 933 |  840 |  840 |                         item_content = '| ' + ' | '.join(headers) + ' |\n'
 934 |  841 |  841 |                         item_content += '| ' + ' | '.join(['---'] * len(headers)) + ' |\n'
 935 |  842 |  842 |                         for row in rows:
 936 |  843 |  843 |                             row_str = '| ' + ' | '.join(row) + ' |\n'
 937 |  844 |  844 |                             item_content += row_str
 938 |  845 |  845 |                 # Add content and newlines even if content is empty
 939 |  846 |  846 |                 section_content += item_content + '\n\n'
 940 |  847 |  847 | 
 941 |  848 |  848 |             # Add completed section to markdown output with extra newline
 942 |  849 |  849 |             markdown_output += section_content + '\n'
 943 |  850 |  850 | 
 944 |  851 |  851 |         # Strip trailing whitespace but ensure exactly three newlines at end
 945 |  852 |  852 |         return markdown_output.rstrip() + '\n\n\n' if markdown_output else ""
 946 |  853 |  853 |     def _convert_to_markdown(self, soup):
 947 |  854 |  854 |         """Convert HTML to markdown with enhanced features."""
 948 |  855 |  855 |         class CustomConverter(markdownify.MarkdownConverter):
 949 |  856 |  856 |             def __init__(self, **options):
 950 |  857 |  857 |                 super().__init__(**options)
 951 |  858 |  858 | 
 952 |  859 |  859 |             def convert_pre(self, el, text, convert_as_inline):
 953 |  860 |  860 |                 code = el.find('code')
 954 |  861 |  861 |                 if code:
 955 |  862 |  862 |                     language = None
 956 |  863 |  863 |                     classes = code.get('class', [])
 957 |  864 |  864 |                     if isinstance(classes, str):
 958 |  865 |  865 |                         classes = classes.split()
 959 |  866 |  866 |                         
 960 |  867 |  867 |                     for cls in classes:
 961 |  868 |  868 |                         # Direct language class like "python"
 962 |  869 |  869 |                         if cls.lower() in self.options.get('code_languages', []):
 963 |  870 |  870 |                             language = cls.lower()
 964 |  871 |  871 |                             break
 965 |  872 |  872 |                             
 966 |  873 |  873 |                         # Language prefixed class like "language-python"
 967 |  874 |  874 |                         for prefix in ['language-', 'lang-', 'brush:', 'syntax-']:
 968 |  875 |  875 |                             if cls.lower().startswith(prefix):
 969 |  876 |  876 |                                 lang = cls[len(prefix):].lower()
 970 |  877 |  877 |                                 if lang in self.options.get('code_languages', []):
 971 |  878 |  878 |                                     language = lang
 972 |  879 |  879 |                                     break
 973 |  880 |  880 |                         if language:
 974 |  881 |  881 |                             break
 975 |  882 |  882 |                             
 976 |  883 |  883 |                     content = code.get_text(strip=True)
 977 |  884 |  884 |                     if language:
 978 |  885 |  885 |                         return f'```{language}\n{content}\n```'
 979 |  886 |  886 |                 
 980 |  887 |  887 |                 return f'```\n{text}\n```'
 981 |  888 |  888 |                 
 982 |  889 |  889 |             def convert_a(self, el, text, convert_as_inline):
 983 |  890 |  890 |                 href = el.get('href', '')
 984 |  891 |  891 |                 if not href:
 985 |  892 |  892 |                     return text
 986 |  893 |  893 |                     
 987 |  894 |  894 |                 # Handle dangerous URLs
 988 |  895 |  895 |                 href_lower = href.strip().lower()
 989 |  896 |  896 |                 if any(href_lower.startswith(proto) for proto in ['javascript:', 'data:', 'vbscript:']):
 990 |  897 |  897 |                     return f"[{text}](#)"
 991 |  898 |  898 |                 
 992 |  899 |  899 |                 return f"[{text}]({href})"
 993 |  900 |  900 |                 
 994 |  901 |  901 |             def convert_img(self, el, text, convert_as_inline):
 995 |  902 |  902 |                 alt = el.get('alt', '')
 996 |  903 |  903 |                 src = el.get('src', '')
 997 |  904 |  904 |                 title = el.get('title', '')
 998 |  905 |  905 |                 if src:
 999 |  906 |  906 |                     if title:
1000 |  907 | 1007 |                         return f'![{alt}]({src} "{title}")'
1001 |  908 | 1008 |                     return f'![{alt}]({src})'
1002 |  909 | 1009 |                 return alt
1003 |  910 | 1010 | 
1004 |  911 | 1011 |         converter = CustomConverter(**self.markdownify_options)
1005 |  912 | 1012 |         markdown = converter.convert_soup(soup)
1006 |  913 | 1013 |         for table in soup.find_all('table'):
1007 |  914 | 1014 |             table_markdown = self._convert_table_to_markdown(table)
1008 |  915 | 1015 |         return markdown.strip()
1009 |  916 | 1016 | 
1010 |  917 | 1017 | 
1011 |  918 | 1018 | # Alias for backward compatibility
1012 |  919 | 1019 | ProcessingResult = ProcessedContent
1013 |  920 | 1020 |