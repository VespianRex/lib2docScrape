--- Original.py
+++ Updated.py
@@ -1,8 +1,7 @@
 import hashlib
 import json
 import logging
-
-import re
+import re
 from datetime import datetime
 from typing import Any, Dict, List, Optional, Set, Tuple
 from dataclasses import asdict
@@ -22,7 +21,7 @@ class DocumentVersion(BaseModel):
     hash: str
     changes: Dict[str, Any]
 
-    @field_validator('version_id')
+    @field_validator("version_id")
     def validate_version_id(cls, v: str) -> str:
         """Validate version ID format."""
         if not v.startswith('v'):
@@ -34,7 +33,7 @@ class DocumentVersion(BaseModel):
     """Model for document metadata."""
     title: str
     url: str
-    category: str = "uncategorized"
+    category: Optional[str] = None
     tags: List[str] = []
     versions: List[DocumentVersion] = []
     references: Dict[str, List[str]] = {}
@@ -54,7 +53,7 @@ class DocumentMetadata(BaseModel):
         except TypeError as e:
             # Fallback if json serialization fails (e.g., non-serializable objects)
             version_hash = hashlib.sha256(str(processed_content_dict).encode()).hexdigest()
-            logging.warning(f"JSON serialization failed for hashing version content for URL {self.url}. Falling back to str(). Error: {e}")
+            logging.warning(f"JSON serialization failed for hashing version content for URL {self.url}. Falling back to str(). Error: {e}")
 
         # Check if content has changed
         if not self.versions or self.versions[-1].hash != version_hash:
@@ -63,7 +62,7 @@ class DocumentMetadata(BaseModel):
                 version_id=f"v{version_num}",
                 timestamp=datetime.now(),
                 hash=version_hash,
-                changes=processed_content_dict # Store the whole dict
+                changes=processed_content_dict  # Store the whole dict
             )
             self.versions.append(version)
             self.last_updated = version.timestamp
@@ -85,7 +84,7 @@ class DocumentCollection(BaseModel):
 class SearchIndex(BaseModel):
     """Model for search index entries."""
     term: str
-    documents: List[Tuple[str, float]]  # (document_id, relevance_score)
+    documents: List[Tuple[str, float]]  # (document_id, relevance_score)
     context: Dict[str, List[str]] = Field(default_factory=dict)  # document_id -> list of context snippets
 
 
@@ -105,17 +104,14 @@ class DocumentOrganizer:
         Initialize the document organizer.
         
         Args:
-            config: Optional organization configuration
+            config: Optional organization configuration
         """
         self.config = config or OrganizationConfig()
         self.documents: Dict[str, DocumentMetadata] = {}
         self.collections: Dict[str, DocumentCollection] = {}
-        self.search_indices: Dict[str, Dict[str, str]] = {}
+        self.search_indices: Dict[str, SearchIndex] = {}
         self._setup_default_categories()
 
-    def _setup_default_categories(self) -> None:
-        """Setup default category rules."""
-        if not self.config.category_rules:
            self.config.category_rules = {
                "api": ["api", "endpoint", "rest", "graphql"],
                "guide": ["guide", "tutorial", "how-to", "howto"],
@@ -124,36 +120,7 @@ class DocumentOrganizer:
                "concept": ["concept", "overview", "introduction"]
             }
 
-    def _generate_document_id(self, url: str, content: str) -> str:
-        """
-        Generate a unique document ID.
-        
-        Args:
-            url: Document URL
-            content: Document content
-            
-        Returns:
-            Unique document ID
-        """
-        combined = f"{url}:{content}"
-        return hashlib.sha256(combined.encode()).hexdigest()[:16]
-
-    def _calculate_content_hash(self, content: Dict[str, Any]) -> str:
-        """
-        Calculate hash of document content.
-        
-        Args:
-            content: Document content
-            
-        Returns:
-            Content hash
-        """
-        content_str = json.dumps(content, sort_keys=True)
-        return hashlib.sha256(content_str.encode()).hexdigest()
-
-    def _categorize_document(self, content: ProcessedContent) -> str:
-        """
-        Categorize document based on content and rules.
-        
+    def _categorize_document(self, content: ProcessedContent) -> Optional[str]:
+        """
+        Categorize document based on content and rules.
+
         Args:
             content: Processed document content
             
@@ -161,41 +128,46 @@ class DocumentOrganizer:
             Document category or None if uncategorized
         """
         # Get text content for matching
-        text = str(content.content.get("text", "")).lower()
+        text = ""
+        if isinstance(content.content, dict) and "text" in content.content:
+            text = str(content.content["text"]).lower()
+        else:
+            text = str(content).lower()
+            
         title = content.title.lower()
         url = content.url.lower()
         
         # Check each category's rules
         for category, patterns in self.config.category_rules.items():
             for pattern in patterns:
-                pattern = pattern.lower()
+                pattern = pattern.lower()
                 if (pattern in url or 
                     pattern in title or 
                     pattern in text):
                     return category
         
         # Return None if no rules match
         return None
 
     def _extract_references(self, content: ProcessedContent) -> Dict[str, List[str]]:
-        """
-        Extract references from content.
-        
-        Args:
-            content: Processed document content
-            
-        Returns:
-            Dictionary of reference types and their values
-        """
         references = {
             "internal": [],
             "external": [],
             "code": []
         }
         
         # Extract links
-        for link in content.content.get("links", []):
-            if isinstance(link, dict) and "url" in link:
-                url = link["url"]
-                if url.startswith(("http://", "https://")):
+        if "links" in content.content:
+            for link in content.content.get("links", []):
+                if isinstance(link, dict) and "url" in link:
+                    url = link["url"]
+                    if url.startswith(("http://", "https://")):
+                        try:
+                            content_domain = urlparse(content.url).netloc
+                            url_domain = urlparse(url).netloc
+                            if content_domain and url_domain and content_domain in url_domain:
+                                references["internal"].append(url)
+                            else:
+                                references["external"].append(url)
+                        except Exception:
+                            # If there's any error parsing the URL, consider it external
+                            references["external"].append(url)
                     if urlparse(url).netloc == urlparse(content.url).netloc:
                         references["internal"].append(url)
                     else:
@@ -209,112 +181,6 @@ class DocumentOrganizer:
                 
         return references
 
-    def _generate_index_terms(self, content: ProcessedContent) -> List[str]:
-        """
-        Generate search index terms from content.
-        
-        Args:
-            content: Processed document content
-            
-        Returns:
-            List of index terms
-        """
-        terms = set()
-        
-        # Add title terms
-        terms.update(self._tokenize(content.title))
-        
-        # Add heading terms
-        for heading in content.content.get("headings", []):
-            terms.update(self._tokenize(heading["text"]))
-        
-        # Add code terms
-        for code_block in content.content.get("code_blocks", []):
-            if code_block.get("language"):
-                terms.add(f"lang:{code_block['language']}")
-        
-        # Add metadata terms
-        for key, value in content.metadata.get("meta_tags", {}).items():
-            if isinstance(value, str):
-                terms.update(self._tokenize(f"{key}:{value}"))
-        
-        return list(terms - self.config.stop_words)
-
-    def _tokenize(self, text: str) -> Set[str]:
-        """
-        Tokenize text into terms.
-        
-        Args:
-            text: Text to tokenize
-            
-        Returns:
-            Set of tokens
-        """
-        # Convert to lowercase and split on non-alphanumeric
-        tokens = re.findall(r'\w+', text.lower())
-        return set(tokens) # Return all tokens, filter later
-
-    def _update_search_indices(self, doc_id: str, content: ProcessedContent):
-        """Update search indices with document content."""
-        # Initialize indices if they don't exist
-        if not hasattr(self, 'search_indices'):
-            self.search_indices = {}
-
-        # Create text index from title and content
-        text_content = f"{content.title} {content.content.get('formatted_content', '')}"
-        self.search_indices['text'] = self.search_indices.get('text', {})
-        self.search_indices['text'][doc_id] = text_content
-
-        # Create headings index
-        headings = content.content.get('headings', [])
-        self.search_indices['headings'] = self.search_indices.get('headings', {})
-        self.search_indices['headings'][doc_id] = [h['text'] for h in headings]
-
-        # Create code index
-        code_blocks = content.content.get('code_blocks', [])
-        self.search_indices['code'] = self.search_indices.get('code', {})
-        self.search_indices['code'][doc_id] = [block['content'] for block in code_blocks]
-
-    def add_document(self, content: ProcessedContent) -> str:
-        """Add a document to the organizer and update indices."""
-        # Check if document with same URL already exists
-        existing_doc_id = None
-        for doc_id, doc in self.documents.items():
-            if doc.url == content.url:
-                existing_doc_id = doc_id
-                break
-                
-        if existing_doc_id:
-            # Update existing document with new version
-            doc = self.documents[existing_doc_id]
-            doc.title = content.title  # Update title in case it changed
-            doc.category = self._categorize_document(content)
-            doc.tags = self._extract_tags(content)
-            doc.references = self._extract_references(content)
-            doc.index_terms = self._extract_index_terms(content)
-            # Add new version with version limit
-            doc.add_version(asdict(content), max_versions=self.config.max_versions_to_keep)
-            # Update search indices
-            self._update_search_indices(existing_doc_id, content)
-            return existing_doc_id
-        else:
-            # Create new document
-            doc_id = str(uuid4())
-            doc = DocumentMetadata(
-                title=content.title,
-                url=content.url,
-                category=self._categorize_document(content) or "uncategorized", # Ensure default if None
-                tags=self._extract_tags(content),
-                versions=[],
-                references=self._extract_references(content),
-                index_terms=self._extract_index_terms(content)
-            )
-            # Add the initial version using the dictionary representation of the ProcessedContent
-            doc.add_version(asdict(content), max_versions=self.config.max_versions_to_keep)
-            self.documents[doc_id] = doc
-
-            # Update search indices
-            self._update_search_indices(doc_id, content)
-
-            return doc_id
-
     def create_collection(self, name: str, description: str, doc_ids: List[str]) -> str:
         """
         Create a new document collection.
@@ -326,7 +192,7 @@ class DocumentOrganizer:
         Returns:
             Collection ID
         """
-        collection_id = hashlib.sha256(name.encode()).hexdigest()[:16]
+        collection_id = str(uuid4())[:16]
         
         self.collections[collection_id] = DocumentCollection(
             name=name,
@@ -344,6 +210,154 @@ class DocumentOrganizer:
         Returns:
             List of (doc_id, score, context_matches) tuples
         """
+        # [... Rest of search method implementation ...]
+        
+    def _tokenize(self, text: str) -> Set[str]:
+        """
+        Tokenize text into terms.
+        
+        Args:
+            text: Text to tokenize
+            
+        Returns:
+            Set of tokens
+        """
+        if not isinstance(text, str):
+            text = str(text)
+        # Convert to lowercase and split on non-alphanumeric
+        tokens = re.findall(r'\w+', text.lower())
+        return set(tokens)  # Return all tokens, filter stop words elsewhere
+
+    def _extract_tags(self, content: ProcessedContent) -> List[str]:
+        """Extract tags from content."""
+        tags = set()
+        
+        # Extract from title
+        title_words = content.title.lower().split()
+        tags.update(w for w in title_words if len(w) > 3 and w not in self.config.stop_words)
+        
+        # Extract from content text
+        if isinstance(content.content, dict) and "text" in content.content:
+            text_words = content.content["text"].lower().split()
+            tags.update(w for w in text_words if len(w) > 3 and w not in self.config.stop_words)
+            
+        return list(tags)
+
+    def _extract_index_terms(self, content: ProcessedContent) -> List[str]:
+        """Extract index terms from content."""
+        terms = set()
+        
+        # Add title terms
+        terms.update(self._tokenize(content.title))
+        
+        # Add heading terms
+        for heading in content.headings:
+            if isinstance(heading, dict) and "text" in heading:
+                terms.update(self._tokenize(heading["text"]))
+        
+        # Add terms from main content
+        if isinstance(content.content, dict):
+            # Check for formatted_content
+            if "formatted_content" in content.content:
+                terms.update(self._tokenize(content.content["formatted_content"]))
+            # Check for text
+            elif "text" in content.content:
+                terms.update(self._tokenize(content.content["text"]))
+        
+        # Add code-related terms
+        for block in content.content.get("code_blocks", []):
+            if isinstance(block, dict):
+                # Add language as a term
+                if "language" in block:
+                    lang = block["language"]
+                    if lang:
+                        terms.add(lang.lower())
+                # Add tokenized code content
+                if "content" in block:
+                    terms.update(self._tokenize(block["content"]))
+        
+        # Remove stop words
+        final_terms = [term for term in terms if term not in self.config.stop_words]
+        
+        return final_terms
+
+    def _update_search_indices(self, doc_id: str, content: ProcessedContent) -> None:
+        """
+        Update search indices with document content.
+        
+        Args:
+            doc_id: Document ID
+            content: Processed content
+        """
+        # Extract terms to index
+        terms = self._extract_index_terms(content)
+        
+        # Extract searchable text content
+        text_content = content.title
+        if isinstance(content.content, dict):
+            if "formatted_content" in content.content:
+                text_content += " " + str(content.content["formatted_content"])
+            elif "text" in content.content:
+                text_content += " " + str(content.content["text"])
+        
+        # Index each term
+        for term in terms:
+            if term not in self.search_indices:
+                self.search_indices[term] = SearchIndex(
+                    term=term,
+                    documents=[(doc_id, 1.0)],
+                    context={doc_id: [self._get_context(text_content, term)]}
+                )
+            else:
+                # Update existing index
+                index = self.search_indices[term]
+                
+                # Check if document already in this term's index
+                doc_ids = [doc_id for doc_id, _ in index.documents]
+                if doc_id not in doc_ids:
+                    index.documents.append((doc_id, 1.0))
+                    
+                # Update context
+                context = self._get_context(text_content, term)
+                if context:
+                    if doc_id not in index.context:
+                        index.context[doc_id] = [context]
+                    else:
+                        index.context[doc_id].append(context)
+
+    def add_document(self, content: ProcessedContent) -> str:
+        """
+        Add a document to the organizer and update indices.
+        
+        Args:
+            content: Processed document content
+            
+        Returns:
+            Document ID
+        """
+        # Check if document with same URL already exists
+        existing_doc_id = None
+        for doc_id, doc in self.documents.items():
+            if doc.url == content.url:
+                existing_doc_id = doc_id
+                break
+                
+        if existing_doc_id:
+            # Update existing document with new version
+            doc = self.documents[existing_doc_id]
+            doc.title = content.title  # Update title in case it changed
+            doc.category = self._categorize_document(content)
+            doc.tags = self._extract_tags(content)
+            doc.references = self._extract_references(content)
+            doc.index_terms = self._extract_index_terms(content)
+            
+            # Add new version with version limit
+            doc.add_version(asdict(content), max_versions=self.config.max_versions_to_keep)
+            
+            # Update search indices
+            self._update_search_indices(existing_doc_id, content)
+            
+            return existing_doc_id
+        else:
+            # Create new document
+            doc_id = str(uuid4())
+            doc = DocumentMetadata(
+                title=content.title,
+                url=content.url,
+                category=self._categorize_document(content),
+                tags=self._extract_tags(content),
+                versions=[],
+                references=self._extract_references(content),
+                index_terms=self._extract_index_terms(content)
+            )
+            
+            # Add the initial version using the dictionary representation of the ProcessedContent
+            doc.add_version(asdict(content), max_versions=self.config.max_versions_to_keep)
+            self.documents[doc_id] = doc
+
+            # Update search indices
+            self._update_search_indices(doc_id, content)
+
+            return doc_id
+
+    def search(self, query: str, category: Optional[str] = None) -> List[Tuple[str, float, List[str]]]:
+        """
+        Search for documents matching the query.
+        
+        Args:
+            query: Search query string
+            category: Optional category to filter results
+            
+        Returns:
+            List of (doc_id, score, context_matches) tuples
+        """
         results = []
         query_lower = query.lower()
         query_terms = self._tokenize(query)
@@ -415,7 +429,7 @@ class DocumentOrganizer:
                     # Check for matches in content text
                     if 'content' in latest_version.changes and isinstance(latest_version.changes['content'], dict):
                         content_dict = latest_version.changes['content']
-                        # Check formatted content
+                        
                         if 'formatted_content' in content_dict:
                             formatted_content = content_dict['formatted_content'].lower()
                             for phrase in query_phrases:
@@ -423,17 +437,16 @@ class DocumentOrganizer:
                                     score += 2
                                     matches.append(f"Content contains: {phrase}")
                         
-                        # Check text content
                         if 'text' in content_dict:
                             text_content = content_dict['text'].lower()
                             for phrase in query_phrases:
                                 if phrase in text_content:
                                     score += 2
                                     matches.append(f"Content contains: {phrase}")
-                                    
-                            # Add explicit match for query terms in text content
-                            for term in query_terms:
-                                if term in text_content:
-                                    score += 1
+                                 
+                            for term in query_terms:
+                                if term in text_content:
+                                    score += 1
                                     matches.append(f"Text contains: {term}")
             
             # If we have any matches or the document has relevant content, include it
@@ -443,7 +456,7 @@ class DocumentOrganizer:
         # Sort by relevance score
         results.sort(key=lambda x: x[1], reverse=True)
         return results
-
+
     # _tokenize_text method is now redundant and can be removed
     # def _tokenize_text(self, text: str) -> List[str]:
     #     """Tokenize text into words."""
@@ -491,96 +504,4 @@ class DocumentOrganizer:
                             logging.debug(f"Similarity: {similarity:.4f}, Threshold: {self.config.min_similarity_score}")
 
                         related.append((other_id, similarity))
-        
-        return sorted(related, key=lambda x: x[1], reverse=True)
-
-    def _compute_hash(self, content: ProcessedContent) -> str:
-        """Compute a hash of the document content."""
-        import hashlib
-        content_str = str(content.content)
-        return hashlib.sha256(content_str.encode()).hexdigest()
-
-    def _determine_category(self, content: ProcessedContent) -> str:
-        """
-        Determine document category based on content and rules.
-        
-        Args:
-            content: Processed document content
-            
-        Returns:
-            Document category
-        """
-        # Check URL first
-        url_lower = content.url.lower()
-        for category, patterns in self.config.category_rules.items():
-            if any(pattern in url_lower for pattern in patterns):
-                return category
-
-        # Check title
-        title_lower = content.title.lower()
-        for category, patterns in self.config.category_rules.items():
-            if any(pattern in title_lower for pattern in patterns):
-                return category
-
-        # Check content text
-        if isinstance(content.content, dict) and "text" in content.content:
-            text_lower = content.content["text"].lower()
-            for category, patterns in self.config.category_rules.items():
-                if any(pattern in text_lower for pattern in patterns):
-                    return category
-
-        # If no patterns match, return uncategorized
-        return "uncategorized"
-
-    def _extract_tags(self, content: ProcessedContent) -> List[str]:
-        """Extract tags from content."""
-        tags = set()
-        
-        # Extract from title
-        title_words = content.title.lower().split()
-        tags.update(w for w in title_words if len(w) > 3 and w not in self.config.stop_words)
-        
-        # Extract from content text
-        if "text" in content.content:
-            text_words = content.content["text"].lower().split()
-            tags.update(w for w in text_words if len(w) > 3 and w not in self.config.stop_words)
-            
-        return list(tags)
-
-    def _extract_references(self, content: ProcessedContent) -> Dict[str, List[str]]:
-        """Extract references from content."""
-        references = {
-            "internal": [],
-            "external": [],
-            "code": []  # Add code references
-        }
-        
-        # Extract links
-        if "links" in content.content:
-            for link in content.content["links"]:
-                if isinstance(link, dict) and "url" in link:
-                    url = link["url"]
-                    if url.startswith(("http://", "https://")):
-                        # Safely extract domain from content.url
-                        try:
-                            content_domain = urlparse(content.url).netloc
-                            url_domain = urlparse(url).netloc
-                            if content_domain and url_domain and content_domain in url_domain:
-                                references["internal"].append(url)
-                            else:
-                                references["external"].append(url)
-                        except Exception:
-                            # If there's any error parsing the URL, consider it external
-                            references["external"].append(url)
-                            
-        # Extract code references
-        if "code_blocks" in content.content:
-            for block in content.content["code_blocks"]:
-                if isinstance(block, dict) and "content" in block:
-                    references["code"].append(block["content"])
-                    
-        return references
-
-    def _compute_hash(self, content: ProcessedContent) -> str:
-        """Compute hash of document content."""
-        content_str = json.dumps(content.content, sort_keys=True)
-        return hashlib.sha256(content_str.encode()).hexdigest()
+        return sorted(related, key=lambda x: x[1], reverse=True)